1
00:00:00,008 --> 00:00:05,424
In this lecture we're going to talk

2
00:00:05,424 --> 00:00:12,465
about how to instantiate
vector space model so

3
00:00:12,465 --> 00:00:19,875
that we can get very
specific ranking function.

4
00:00:22,974 --> 00:00:27,888
So this is to continue the discussion
of the vector space model,

5
00:00:27,888 --> 00:00:32,810
which is one particular approach
to design a ranking function.

6
00:00:34,420 --> 00:00:38,551
And we're going to talk about how
we use the general framework of

7
00:00:38,551 --> 00:00:42,810
the the vector space
model as a guidance to

8
00:00:42,810 --> 00:00:48,270
instantiate the framework to derive
a specific ranking function.

9
00:00:48,270 --> 00:00:53,380
And we're going to cover the symbolist
instantiation of the framework.

10
00:00:55,360 --> 00:00:58,390
So as we discussed in
the previous lecture,

11
00:00:58,390 --> 00:01:00,600
the vector space model
is really a framework.

12
00:01:00,600 --> 00:01:02,619
And this didn't say.

13
00:01:05,266 --> 00:01:11,040
As we discussed in the previous lecture,
vector space model is really a framework.

14
00:01:11,040 --> 00:01:13,160
It does not say many things.

15
00:01:14,710 --> 00:01:15,520
So, for example,

16
00:01:15,520 --> 00:01:19,470
here it shows that it did not say
how we should define the dimension.

17
00:01:20,770 --> 00:01:25,939
It also did not say how we place
a document vector in this space.

18
00:01:27,130 --> 00:01:31,470
It did not say how we place a query
vector in this vector space.

19
00:01:32,500 --> 00:01:37,250
And, finally, it did not say how we
should measure the similarity between

20
00:01:37,250 --> 00:01:39,020
the query vector and the document vector.

21
00:01:40,570 --> 00:01:44,860
So you can imagine,
in order to implement this model,

22
00:01:46,040 --> 00:01:52,940
we have to say specifically
how we compute these vectors.

23
00:01:52,940 --> 00:01:54,830
What is exactly xi?

24
00:01:54,830 --> 00:01:56,700
And what is exactly yi?

25
00:01:58,460 --> 00:02:02,260
This will determine where
we place a document vector,

26
00:02:02,260 --> 00:02:04,560
where we place a query vector.

27
00:02:04,560 --> 00:02:05,230
And, of course,

28
00:02:05,230 --> 00:02:08,869
we also need to say exactly what
should be the similarity function.

29
00:02:11,120 --> 00:02:16,653
So if we can provide a definition
of the concepts that would define

30
00:02:16,653 --> 00:02:22,590
the dimensions and these xi's or
yi's and namely weights of terms for

31
00:02:22,590 --> 00:02:28,725
queries and document, then we will be
able to place document vectors and

32
00:02:28,725 --> 00:02:33,080
query vectors in this well defined space.

33
00:02:33,080 --> 00:02:36,414
And then,
if we also specify similarity function,

34
00:02:36,414 --> 00:02:39,685
then we'll have a well
defined ranking function.

35
00:02:41,427 --> 00:02:47,630
So let's see how we can do that and
think about the instantiation.

36
00:02:47,630 --> 00:02:52,460
Actually, I would suggest you to
pause the lecture at this point,

37
00:02:52,460 --> 00:02:54,980
spend a couple minutes to think about.

38
00:02:54,980 --> 00:02:58,280
Suppose you are asked
to implement this idea.

39
00:02:59,590 --> 00:03:05,810
You have come up with the idea of vector
space model, but you still haven't figured

40
00:03:05,810 --> 00:03:10,310
out how to compute these vectors exactly,
how to define the similarity function.

41
00:03:10,310 --> 00:03:10,810
What would you do?

42
00:03:12,540 --> 00:03:15,857
So, think for a couple of minutes,
and then proceed.

43
00:03:20,581 --> 00:03:26,460
So, let's think about some simplest ways
of instantiating this vector space model.

44
00:03:26,460 --> 00:03:28,810
First, how do we define the dimension?

45
00:03:28,810 --> 00:03:31,430
Well, the obvious choice is to use

46
00:03:31,430 --> 00:03:34,636
each word in our vocabulary
to define the dimension.

47
00:03:34,636 --> 00:03:38,775
And show that there are N
words in our vocabulary.

48
00:03:38,775 --> 00:03:41,160
Therefore, there are N dimensions.

49
00:03:41,160 --> 00:03:42,818
Each word defines one dimension.

50
00:03:42,818 --> 00:03:46,273
And this is basically
the bag of words with

51
00:03:48,965 --> 00:03:52,395
Now let's look at how we
place vectors in this space.

52
00:03:54,395 --> 00:03:57,175
Again here, the simplest strategy is to

53
00:03:58,700 --> 00:04:03,650
use a Bit Vector to represent
both the query and a document.

54
00:04:04,720 --> 00:04:07,937
And that means each element, xi and

55
00:04:07,937 --> 00:04:12,020
yi will be taking a value
of either zero or 1.

56
00:04:13,270 --> 00:04:14,300
When it's 1,

57
00:04:14,300 --> 00:04:20,750
it means the corresponding word is
present in the document or in the query.

58
00:04:20,750 --> 00:04:25,180
When it's 0,
it's going to mean that it's absent.

59
00:04:27,070 --> 00:04:31,210
So you can imagine if the user
types in a few words in the query,

60
00:04:31,210 --> 00:04:35,790
then the query vector will only
have a few 1's, many, many zeros.

61
00:04:37,630 --> 00:04:41,450
The document vector,
generally we have more 1's, of course.

62
00:04:41,450 --> 00:04:46,700
But it will also have many zeros since
the vocabulary is generally very large.

63
00:04:46,700 --> 00:04:50,720
Many words don't really
occur in any document.

64
00:04:52,110 --> 00:04:56,570
Many words will only occasionally
occur in a document.

65
00:04:58,680 --> 00:05:01,770
A lot of words will be absent
in a particular document.

66
00:05:04,390 --> 00:05:09,770
So now we have placed the documents and
the query in the vector space.

67
00:05:11,450 --> 00:05:14,240
Let's look at how we
measure the similarity.

68
00:05:15,770 --> 00:05:19,400
So, a commonly used similarity
measure here is Dot Product.

69
00:05:20,900 --> 00:05:25,590
The Dot Product of two
vectors is simply defined as

70
00:05:25,590 --> 00:05:30,590
the sum of the products of the
corresponding elements of the two vectors.

71
00:05:30,590 --> 00:05:38,596
So, here we see that it's
the product of x1 and y1.

72
00:05:38,596 --> 00:05:40,228
So, here.

73
00:05:40,228 --> 00:05:43,420
And then, x2 multiplied by y2.

74
00:05:43,420 --> 00:05:47,100
And then, finally, xn multiplied by yn.

75
00:05:47,100 --> 00:05:48,810
And then, we take a sum here.

76
00:05:50,630 --> 00:05:52,610
So that's a Dot Product.

77
00:05:52,610 --> 00:05:57,580
Now, we can represent this in a more
general way using a sum here.

78
00:05:58,740 --> 00:06:04,120
So this is only one of the many different
ways of measuring the similarity.

79
00:06:04,120 --> 00:06:10,640
So, now we see that we have
defined the dimensions,

80
00:06:10,640 --> 00:06:16,050
we have defined the vectors, and we have
also defined the similarity function.

81
00:06:16,050 --> 00:06:21,495
So now we finally have the simplest
vector space model, which is based

82
00:06:21,495 --> 00:06:26,882
on the bit vector [INAUDIBLE] dot product
similarity and bag of words [INAUDIBLE].

83
00:06:26,882 --> 00:06:30,195
And the formula looks like this.

84
00:06:30,195 --> 00:06:32,415
So this is our formula.

85
00:06:32,415 --> 00:06:37,670
And that's actually a particular retrieval
function, a ranking function right?

86
00:06:37,670 --> 00:06:42,573
Now we can finally implement this
function using a program language,

87
00:06:42,573 --> 00:06:45,350
and then rank the documents for query.

88
00:06:45,350 --> 00:06:50,110
Now, at this point you should
again pause the lecture

89
00:06:50,110 --> 00:06:53,400
to think about how we can
interpreted this score.

90
00:06:53,400 --> 00:06:56,972
So, we have gone through the process
of modeling the retrieval problem

91
00:06:56,972 --> 00:07:00,620
using a vector space model.

92
00:07:00,620 --> 00:07:05,185
And then,
we make assumptions about how we place

93
00:07:05,185 --> 00:07:09,780
vectors in the vector space, and
how do we define the similarity.

94
00:07:09,780 --> 00:07:14,270
So in the end, we've got a specific
retrieval function shown here.

95
00:07:15,370 --> 00:07:18,370
Now, the next step is to think about
whether this retrieval function

96
00:07:18,370 --> 00:07:21,160
actually makes sense, right?

97
00:07:21,160 --> 00:07:24,140
Can we expect this function
to actually perform well

98
00:07:24,140 --> 00:07:27,400
when we used it to rank documents for
user's queries?

99
00:07:28,790 --> 00:07:35,870
So it's worth thinking about what is
this value that we are calculating.

100
00:07:35,870 --> 00:07:38,220
So, in the end, we'll get a number.

101
00:07:38,220 --> 00:07:40,240
But what does this number mean?

102
00:07:40,240 --> 00:07:40,990
Is it meaningful?

103
00:07:42,200 --> 00:07:44,390
So, spend a couple minutes
to sort of think about that.

104
00:07:45,880 --> 00:07:46,540
And, of course,

105
00:07:46,540 --> 00:07:52,600
the general question here is do you
believe this is a good ranking function?

106
00:07:52,600 --> 00:07:54,680
Would it actually work well?

107
00:07:54,680 --> 00:07:58,329
So, again,
think about how to interpret this value.

108
00:07:58,329 --> 00:08:00,190
Is it actually meaningful?

109
00:08:01,280 --> 00:08:03,190
Does it mean something?

110
00:08:03,190 --> 00:08:06,560
This is related to how well
the document matched the query.

111
00:08:08,260 --> 00:08:11,530
So, in order to assess
whether this simplest

112
00:08:11,530 --> 00:08:15,520
vector space model actually works well,
let's look at the example.

113
00:08:17,170 --> 00:08:22,570
So, here I show some sample documents and
a sample query.

114
00:08:22,570 --> 00:08:26,390
The query is news about
the presidential campaign.

115
00:08:26,390 --> 00:08:28,580
And we have five documents here.

116
00:08:28,580 --> 00:08:32,280
They cover different terms in the query.

117
00:08:34,710 --> 00:08:39,890
And if you look at these documents for
a moment, you may realize that

118
00:08:41,880 --> 00:08:47,070
some documents are probably relevant, and
some others are probably not relevant.

119
00:08:48,300 --> 00:08:54,690
Now, if I asked you to rank these
documents, how would you rank them?

120
00:08:54,690 --> 00:08:57,270
This is basically our ideal ranking.

121
00:08:57,270 --> 00:09:01,180
When humans can examine the documents,
and then try to rank them.

122
00:09:03,430 --> 00:09:06,900
Now, so think for a moment,
and take a look at this slide.

123
00:09:06,900 --> 00:09:10,210
And perhaps by pausing the lecture.

124
00:09:12,510 --> 00:09:18,750
So I think most of you would
agree that d4 and d3 are probably

125
00:09:18,750 --> 00:09:23,353
better than others because they
really cover the query well.

126
00:09:23,353 --> 00:09:26,860
They match news,
presidential and campaign.

127
00:09:27,900 --> 00:09:33,160
So, it looks like these documents
are probably better than the others.

128
00:09:33,160 --> 00:09:37,230
They should be ranked on top.

129
00:09:37,230 --> 00:09:41,810
And the other three d2, d1, and
d5 are really not relevant.

130
00:09:41,810 --> 00:09:45,990
So we can also say d4 and
d3 are relevant documents, and

131
00:09:45,990 --> 00:09:50,150
d1, d2 and d5 are non-relevant.

132
00:09:50,150 --> 00:09:55,290
So now let's see if our simplest
vector space model could do the same,

133
00:09:55,290 --> 00:09:57,400
or could do something closer.

134
00:09:57,400 --> 00:10:01,250
So, let's first think about
how we actually use this model

135
00:10:01,250 --> 00:10:02,272
to score documents.

136
00:10:02,272 --> 00:10:04,000
All right.

137
00:10:04,000 --> 00:10:07,420
Here I show two documents, d1 and d3.

138
00:10:07,420 --> 00:10:10,390
And we have the query also here.

139
00:10:10,390 --> 00:10:15,130
In the vector space model, of course we
want to first compute the vectors for

140
00:10:15,130 --> 00:10:16,830
these documents and the query.

141
00:10:16,830 --> 00:10:18,860
Now, I showed the vocabulary here as well.

142
00:10:18,860 --> 00:10:22,850
So these are the end dimensions
that we'll be thinking about.

143
00:10:22,850 --> 00:10:26,620
So what do you think is the vector for
the query?

144
00:10:27,700 --> 00:10:32,870
Note that we're assuming
that we only use zero and 1

145
00:10:32,870 --> 00:10:39,230
to indicate whether a term is absent or
present in the query or in the document.

146
00:10:39,230 --> 00:10:42,380
So these are zero,1 bit vectors.

147
00:10:43,880 --> 00:10:45,790
So what do you think is the query vector?

148
00:10:47,820 --> 00:10:51,200
Well, the query has four words here.

149
00:10:51,200 --> 00:10:54,380
So for these four words,
there will be a 1.

150
00:10:54,380 --> 00:10:55,980
And for the rest, there will be zeros.

151
00:10:57,680 --> 00:10:59,290
Now, what about the documents?

152
00:10:59,290 --> 00:11:00,610
It's the same.

153
00:11:00,610 --> 00:11:03,430
So d1 has two rows, news and about.

154
00:11:03,430 --> 00:11:07,367
So, there are two 1's here,
and the rest are zeroes.

155
00:11:07,367 --> 00:11:12,220
Similarly, so now that we

156
00:11:12,220 --> 00:11:16,380
have the two vectors,
let's compute the similarity.

157
00:11:17,470 --> 00:11:19,550
And we're going to use Do Product.

158
00:11:19,550 --> 00:11:21,610
So you can see when we use Dot Product,

159
00:11:21,610 --> 00:11:26,030
we just multiply the corresponding
elements, right?

160
00:11:26,030 --> 00:11:30,894
So these two will be formal product,

161
00:11:30,894 --> 00:11:33,920
and these two will
generate another product,

162
00:11:33,920 --> 00:11:38,210
and these two will generate yet
another product and so on, so forth.

163
00:11:40,020 --> 00:11:46,320
Now you can easily see if we do that,
we actually don't have to care about

164
00:11:48,180 --> 00:11:54,170
these zeroes because whenever we have
a zero the product will be zero.

165
00:11:54,170 --> 00:11:57,538
So when we take a sum
over all these pairs,

166
00:11:57,538 --> 00:12:02,940
then the zero entries will be gone.

167
00:12:04,400 --> 00:12:08,010
As long as you have one zero,
then the product would be zero.

168
00:12:08,010 --> 00:12:14,710
So, in the fact, we're just
counting how many pairs of 1 and 1.

169
00:12:14,710 --> 00:12:18,220
In this case, we have seen two,
so the result will be 2.

170
00:12:18,220 --> 00:12:20,240
So what does that mean?

171
00:12:20,240 --> 00:12:25,190
Well, that means this number, or
the value of this scoring function,

172
00:12:25,190 --> 00:12:33,130
is simply the count of how many unique
query terms are matched in the document.

173
00:12:33,130 --> 00:12:39,350
Because if a term is matched in the
document, then there will be two one's.

174
00:12:41,390 --> 00:12:44,740
If it's not, then there will
be zero on the document side.

175
00:12:46,310 --> 00:12:50,410
Similarly, if the document has a term but
the term is not in the query,

176
00:12:50,410 --> 00:12:53,220
there will be a zero in the query vector.

177
00:12:53,220 --> 00:12:55,020
So those don't count.

178
00:12:55,020 --> 00:12:58,760
So, as a result,
this scoring function basically

179
00:12:58,760 --> 00:13:03,820
measures how many unique query
terms are matched in a document.

180
00:13:03,820 --> 00:13:05,770
This is how we interpret this score.

181
00:13:07,150 --> 00:13:10,520
Now, we can also take a look at d3.

182
00:13:10,520 --> 00:13:18,003
In this case, you can see the result
is 3 because d3 matched to the three

183
00:13:18,003 --> 00:13:23,140
distinctive query words news, presidential
campaign, whereas d1 only matched the two.

184
00:13:23,140 --> 00:13:28,200
Now in this case, this seems
reasonable to rank d3 on top of d1.

185
00:13:29,260 --> 00:13:33,440
And this simplest vector
space model indeed does that.

186
00:13:33,440 --> 00:13:35,050
So that looks pretty good.

187
00:13:35,050 --> 00:13:40,030
However, if we examine this model in
detail, we likely will find some problems.

188
00:13:40,030 --> 00:13:44,891
So, here I'm going to show all
the scores for these five documents.

189
00:13:44,891 --> 00:13:49,977
And you can easily verify they're
correct because we're basically

190
00:13:49,977 --> 00:13:55,070
counting the number of unique query
terms matched in each document.

191
00:13:56,470 --> 00:13:59,270
Now note that this measure
actually makes sense, right?

192
00:13:59,270 --> 00:14:03,740
It basically means if a document
matches more unique query terms,

193
00:14:03,740 --> 00:14:07,210
then the document will be
assumed to be more relevant.

194
00:14:07,210 --> 00:14:09,190
And that seems to make sense.

195
00:14:09,190 --> 00:14:16,870
The only problem is here we can note that
there are three documents, d2, d3 and d4.

196
00:14:16,870 --> 00:14:22,320
And they tied with a 3 as a score.

197
00:14:25,050 --> 00:14:31,000
So, that's a problem because if you look
at them carefully, it seems that the d4

198
00:14:31,000 --> 00:14:36,920
should be ranked above d3 because

199
00:14:36,920 --> 00:14:42,100
d3 only mentions the presidential once,
but d4 mentioned it multiple times.

200
00:14:42,100 --> 00:14:47,634
In the case of d3,
presidential could be an dimension.

201
00:14:47,634 --> 00:14:51,360
But d4 is clearly above
the presidential campaign.

202
00:14:51,360 --> 00:14:58,200
Another problem is that d2 and
d3 also have the same score.

203
00:14:58,200 --> 00:15:01,880
But if you look at the three words
that are matched, in the case of d2,

204
00:15:01,880 --> 00:15:07,020
it matched the news, about and campaign.

205
00:15:07,020 --> 00:15:11,500
But in the case of d3, it matched news,
presidential and campaign.

206
00:15:12,530 --> 00:15:17,960
So intuitively this reads better
because matching presidential

207
00:15:17,960 --> 00:15:21,920
is more important than matching about,

208
00:15:21,920 --> 00:15:24,910
even though about and
the presidential are both in the query.

209
00:15:26,170 --> 00:15:30,730
So intuitively,
we would like d3 to be ranked above d2.

210
00:15:30,730 --> 00:15:32,750
But this model doesn't do that.

211
00:15:33,860 --> 00:15:37,150
So that means this model
is still not good enough.

212
00:15:37,150 --> 00:15:39,109
We have to solve these problems.

213
00:15:41,188 --> 00:15:41,991
To summarize,

214
00:15:41,991 --> 00:15:45,770
in this lecture we talked about how
to instantiate a vector space model.

215
00:15:47,610 --> 00:15:49,540
We mainly need to do three things.

216
00:15:49,540 --> 00:15:51,796
One is to define the dimension.

217
00:15:51,796 --> 00:15:59,896
The second is to decide how to place
documents as vectors in the vector space,

218
00:15:59,896 --> 00:16:05,761
and to also place a query in
the vector space as a vector.

219
00:16:07,862 --> 00:16:11,900
And third is to define
the similarity between two vectors,

220
00:16:11,900 --> 00:16:15,790
particularly the query vector and
the document vector.

221
00:16:17,080 --> 00:16:22,430
We also talked about various simple way
to instantiate the vector space model.

222
00:16:22,430 --> 00:16:27,910
Indeed, that's probably the simplest
vector space model that we can derive.

223
00:16:27,910 --> 00:16:31,480
In this case,
we use each word to define the dimension.

224
00:16:31,480 --> 00:16:37,430
We use a zero, 1 bit vector to
represent a document or a query.

225
00:16:37,430 --> 00:16:42,690
In this case, we basically only care
about word presence or absence.

226
00:16:42,690 --> 00:16:43,790
We ignore the frequency.

227
00:16:45,560 --> 00:16:49,220
And we use the Dot Product
as the similarity function.

228
00:16:50,360 --> 00:16:53,304
And with such a instantiation,

229
00:16:53,304 --> 00:16:58,870
we showed that the scoring
function is basically to score

230
00:16:58,870 --> 00:17:03,260
a document based on the number of distinct
query words matched in the document.

231
00:17:04,650 --> 00:17:09,800
We also showed that such a simple vector
space model still doesn't work well, and

232
00:17:09,800 --> 00:17:10,720
we need to improve it.

233
00:17:12,540 --> 00:17:18,797
And this is a topic that we're
going to cover in the next lecture.

234
00:17:18,797 --> 00:17:28,797
[MUSIC]