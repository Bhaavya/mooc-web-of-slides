1
00:00:00,076 --> 00:00:03,466
[MUSIC]

2
00:00:06,698 --> 00:00:14,732
So now let's take a look at the specific
method that's based on regression.

3
00:00:14,732 --> 00:00:17,627
Now, this is one of the many
different methods, and in fact,

4
00:00:17,627 --> 00:00:19,309
it's one of the simplest methods.

5
00:00:19,309 --> 00:00:24,550
And I choose this to explain
the idea because it's simple.

6
00:00:26,360 --> 00:00:34,350
So in this approach, we simply assume
that the relevance of document with

7
00:00:34,350 --> 00:00:39,760
respect to a query is related to a linear
combination of all the features.

8
00:00:39,760 --> 00:00:47,400
Here I used Xi to denote the feature.

9
00:00:47,400 --> 00:00:51,770
So Xi of Q and D is a feature.

10
00:00:51,770 --> 00:00:54,720
And we can have as many
features as we would like.

11
00:00:55,890 --> 00:01:01,670
And we assume that these features
can be combined in a linear manner.

12
00:01:03,580 --> 00:01:06,150
And each feature is controlled
by a parameter here,

13
00:01:06,150 --> 00:01:08,880
and this beta i is a parameter.

14
00:01:08,880 --> 00:01:10,790
That's a weighting parameter.

15
00:01:10,790 --> 00:01:15,940
A larger value would mean the feature
would have a higher weight,

16
00:01:15,940 --> 00:01:18,525
and it would contribute more
to the scoring function.

17
00:01:18,525 --> 00:01:23,069
This specific form of the function
actually also involves

18
00:01:23,069 --> 00:01:27,154
a transformation of
the probability of relevance.

19
00:01:27,154 --> 00:01:30,428
So this is the probability of relevance.

20
00:01:30,428 --> 00:01:35,611
And we know that the probability of
relevance is within the range from 0 to 1.

21
00:01:36,870 --> 00:01:41,863
And we could have just assumed that
the scoring function is related to

22
00:01:41,863 --> 00:01:43,856
this linear combination.

23
00:01:43,856 --> 00:01:47,479
So we can do a linear regression.

24
00:01:47,479 --> 00:01:53,940
But then, the value of this linear
combination could easily go beyond 1.

25
00:01:53,940 --> 00:01:59,220
So this transformation
here would map the 0

26
00:01:59,220 --> 00:02:05,540
to 1 range to the whole
range of real values,

27
00:02:05,540 --> 00:02:08,330
you can verify it by yourself.

28
00:02:10,350 --> 00:02:16,700
So this allows us then to connect
to the probability of variance

29
00:02:16,700 --> 00:02:23,010
which is between 0 and 1 to a linear
combination of arbitrary features.

30
00:02:23,010 --> 00:02:28,133
And if we rewrite this into a probability
function, we would get the next one.

31
00:02:28,133 --> 00:02:34,299
So on this equation, now we'll
have the probability of relevance.

32
00:02:35,690 --> 00:02:39,168
And on the right hand side,
we'll have this form.

33
00:02:39,168 --> 00:02:42,448
Now, this form is clearly nonnegative, and

34
00:02:42,448 --> 00:02:46,344
it still involves a linear
combination of features.

35
00:02:46,344 --> 00:02:50,890
And it's also clear that if this value is,

36
00:02:50,890 --> 00:02:58,991
this is actually negative of the linear
combination in the equation above.

37
00:02:58,991 --> 00:03:04,415
If this value here is large,

38
00:03:04,415 --> 00:03:11,879
then it would mean this value is small.

39
00:03:11,879 --> 00:03:17,278
And therefore,
this whole probability would be large.

40
00:03:17,278 --> 00:03:22,034
And that's we expect, that basically,
it would mean if this combination

41
00:03:22,034 --> 00:03:26,496
gives us a high value, then
the document's more likely irrelevant.

42
00:03:26,496 --> 00:03:29,015
So this is our hypothesis.

43
00:03:29,015 --> 00:03:33,955
Again, this is not necessarily the best
hypothesis, but this is a simple

44
00:03:33,955 --> 00:03:39,109
way to connect these features with
the probability of relevance.

45
00:03:40,470 --> 00:03:44,578
So now we have this combination function.

46
00:03:44,578 --> 00:03:48,617
The next task is to
estimate the parameters so

47
00:03:48,617 --> 00:03:52,346
that the function cache will be applied.

48
00:03:52,346 --> 00:03:57,430
But without knowing the beta values,
it's harder to apply this function.

49
00:03:58,520 --> 00:04:04,068
So let's see how can
estimate our beta values.

50
00:04:04,068 --> 00:04:07,190
All right,
let's take a look at a simple example.

51
00:04:08,780 --> 00:04:11,405
In this example, we have three features.

52
00:04:11,405 --> 00:04:15,060
One is the BM25 score of the document and
the query.

53
00:04:15,060 --> 00:04:19,044
One is the PageRank score of the document,
which might or

54
00:04:19,044 --> 00:04:21,211
might not depend on the query.

55
00:04:21,211 --> 00:04:25,681
We might have a topic-sensitive PageRank,
that would depend on the query.

56
00:04:25,681 --> 00:04:29,946
Otherwise, the general PageRank
doesn't really depend on the query.

57
00:04:29,946 --> 00:04:35,221
And then we have BM25 score on
the anchor test of the document.

58
00:04:35,221 --> 00:04:40,630
Now, these are then the feature values for
a particular document query pair.

59
00:04:41,910 --> 00:04:47,370
And in this case, the document is D1 and
the judgment says that it's relevant.

60
00:04:48,790 --> 00:04:54,547
Here's another training instance and
it's these feature values,

61
00:04:54,547 --> 00:04:57,832
but in this case, it's not relevant.

62
00:04:57,832 --> 00:05:02,806
This is an oversimplified case where
we just have two instances, but

63
00:05:02,806 --> 00:05:06,675
it's sufficient to illustrate the point.

64
00:05:06,675 --> 00:05:09,885
So what we can do is we use
the maximum likelihood estimator to

65
00:05:09,885 --> 00:05:11,797
actually estimate the parameters.

66
00:05:13,170 --> 00:05:17,801
Basically, we're going to
predict the relevance status

67
00:05:17,801 --> 00:05:22,040
of the document based
on the feature values.

68
00:05:22,040 --> 00:05:25,534
That is, given that we observed
these feature values here.

69
00:05:28,264 --> 00:05:32,653
Can we predict the relevance here?

70
00:05:32,653 --> 00:05:39,070
Now, of course, the prediction would be
using this function that you see here.

71
00:05:39,070 --> 00:05:42,680
And we hypothesize that the probability
of relevance is related to

72
00:05:42,680 --> 00:05:43,920
features in this way.

73
00:05:43,920 --> 00:05:51,260
So we are going to see, for what values of
beta we can predict the relevance well.

74
00:05:51,260 --> 00:05:58,480
What do we mean by predicting
the relevance well?

75
00:05:58,480 --> 00:06:02,037
Well, we just mean, in the first case, for

76
00:06:02,037 --> 00:06:06,667
D1 this expression right here
should give high values.

77
00:06:06,667 --> 00:06:10,452
In fact, we'll hope this
to gave a value close to 1.

78
00:06:10,452 --> 00:06:13,470
Why?
Because this is a relevant document.

79
00:06:14,750 --> 00:06:17,954
On the other hand,
in the second case, for D2,

80
00:06:17,954 --> 00:06:22,310
we hope this value will be small, right.

81
00:06:22,310 --> 00:06:23,040
Why?

82
00:06:23,040 --> 00:06:26,310
Because it's a non-relevant document.

83
00:06:26,310 --> 00:06:30,250
So now let's see how this can
be mathematically expressed.

84
00:06:30,250 --> 00:06:34,954
And this is similar to expressing
the probability of document,

85
00:06:34,954 --> 00:06:39,657
only that we are not talking about
the probability of words, but

86
00:06:39,657 --> 00:06:43,771
talking about the probability
of relevance, 1 or 0.

87
00:06:43,771 --> 00:06:48,736
So what's the probability
of this document being

88
00:06:48,736 --> 00:06:52,880
relevant if it has these feature values?

89
00:06:54,250 --> 00:06:57,890
Well, this is just this expression.

90
00:06:57,890 --> 00:07:00,970
We just need to plug in the Xi's.

91
00:07:00,970 --> 00:07:03,296
So that's what we will get.

92
00:07:03,296 --> 00:07:08,116
It's exactly like what we have seen above,

93
00:07:08,116 --> 00:07:14,772
only that we replaced these
Xi's with now specific values.

94
00:07:14,772 --> 00:07:21,247
So for example, this 0.7 goes to here and

95
00:07:21,247 --> 00:07:25,451
this 0.11 goes to here.

96
00:07:25,451 --> 00:07:27,369
And these are different feature values,

97
00:07:27,369 --> 00:07:29,405
and we combine them in
this particular way.

98
00:07:29,405 --> 00:07:31,770
The beta values are still unknown.

99
00:07:31,770 --> 00:07:37,202
But this gives us the probability
that this document is relevant,

100
00:07:37,202 --> 00:07:39,342
if we assume such a model.

101
00:07:39,342 --> 00:07:39,853
Okay?

102
00:07:39,853 --> 00:07:44,553
And we want to maximize this probability,
since this is a relevant document.

103
00:07:44,553 --> 00:07:47,850
What do we do for the second document?

104
00:07:47,850 --> 00:07:53,309
Well, we want to compute the probability
that the prediction is non-relevant.

105
00:07:53,309 --> 00:08:00,257
So this would mean we have to
compute 1 minus this expression,

106
00:08:00,257 --> 00:08:07,880
since this expression is actually
the probability of relevance.

107
00:08:07,880 --> 00:08:12,524
So to compute the non-relevance
from relevance,

108
00:08:12,524 --> 00:08:17,062
we just do 1 minus
the probability of relevance.

109
00:08:17,062 --> 00:08:18,480
Okay?

110
00:08:18,480 --> 00:08:24,450
So this whole expression then
just is our probability of

111
00:08:24,450 --> 00:08:29,220
predicting these two relevance values.

112
00:08:29,220 --> 00:08:32,759
One is 1 here, one is 0.

113
00:08:32,759 --> 00:08:37,782
And this whole equation
is our probability of

114
00:08:37,782 --> 00:08:42,680
observing a 1 here and observing a 0 here.

115
00:08:44,090 --> 00:08:48,370
Of course, this probability
depends on the beta values.

116
00:08:50,130 --> 00:08:55,318
So then our goal is to adjust
the beta values to make this whole

117
00:08:55,318 --> 00:09:00,121
thing reach its maximum,
make it as large as possible.

118
00:09:00,121 --> 00:09:02,540
So that means we're going to compute this.

119
00:09:02,540 --> 00:09:07,280
The beta is just the parameter
values that would

120
00:09:07,280 --> 00:09:11,914
maximize this whole likelihood expression.

121
00:09:11,914 --> 00:09:16,284
And what it means is,
if you look at the function, is,

122
00:09:16,284 --> 00:09:21,224
we're going to choose betas to
make this as large as possible and

123
00:09:21,224 --> 00:09:26,449
make this also as large as possible,
which is equivalent to say,

124
00:09:26,449 --> 00:09:29,400
make this part as small as possible.

125
00:09:30,560 --> 00:09:32,360
And this is precisely what we want.

126
00:09:34,530 --> 00:09:38,834
So once we do the training,
now we will know the beta values.

127
00:09:38,834 --> 00:09:43,330
So then this function
would be well-defined.

128
00:09:43,330 --> 00:09:50,690
Once beta values are known, both this and
this would be completely specified.

129
00:09:50,690 --> 00:09:53,380
So for any new query and new document,

130
00:09:53,380 --> 00:09:56,924
we can simply compute the features for
that pair.

131
00:09:56,924 --> 00:10:00,941
And then we just use this formula
to generate the ranking score.

132
00:10:00,941 --> 00:10:06,700
And this scoring function can be used to
rank documents for a particular query.

133
00:10:06,700 --> 00:10:11,787
So that's the basic idea
of learning to rank.

134
00:10:11,787 --> 00:10:21,787
[MUSIC]