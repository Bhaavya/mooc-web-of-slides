<meta charset="utf-8"/>
<co-content>
 <h1 level="1">
  Week 3 Overview
 </h1>
 <p>
  During this week's lessons, you will learn how to evaluate an information retrieval system (a search engine), including
the basic measures for evaluating a set of retrieved results and the major
measures for evaluating a ranked list, including the average precision (AP) and
the normalized discounted cumulative gain (nDCG), and practical issues in
evaluation, including statistical significance testing and pooling.
 </p>
 <h2 level="2">
  Time
 </h2>
 <p>
  This module should take
  <strong>
   approximately 6 hours
  </strong>
  of dedicated time to complete, with its videos and assignments.
 </p>
 <h2 level="2">
  Activities
 </h2>
 <p>
  The activities for this module are listed below (with assignments in bold):
 </p>
 <table columns="2" rows="4">
  <tr>
   <td>
    <p>
     <strong>
      Activity
     </strong>
    </p>
   </td>
   <td>
    <p>
     <strong>
      Estimated Time Required
     </strong>
    </p>
   </td>
  </tr>
  <tr>
   <td>
    <p>
     Week 3 Video Lectures
    </p>
   </td>
   <td>
    <p>
     2 hours
    </p>
   </td>
  </tr>
  <tr>
   <td>
    <p>
     <strong>
      Week 3
     </strong>
     <strong>
      Graded
     </strong>
     <strong>
      Quiz
     </strong>
    </p>
   </td>
   <td>
    <p>
     1 hour
    </p>
   </td>
  </tr>
  <tr>
   <td>
    <p>
     <strong>
      Programming Assignment 1
     </strong>
    </p>
   </td>
   <td>
    <p>
     3 hours
    </p>
   </td>
  </tr>
 </table>
 <h2 level="2">
  Goals and Objectives
 </h2>
 <p>
  After you actively engage in the learning experiences in this module, you should be able to:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Explain the Cranfield evaluation methodology and how it works for evaluating a text retrieval system.
   </p>
  </li>
  <li>
   <p>
    Explain how to evaluate a set of retrieved documents and how to compute precision, recall, and F1.
   </p>
  </li>
  <li>
   <p>
    Explain how to evaluate a ranked list of documents.
   </p>
  </li>
  <li>
   <p>
    Explain how to compute and plot a precision-recall curve.
   </p>
  </li>
  <li>
   <p>
    Explain how to compute average precision and mean average precision (MAP).
   </p>
  </li>
  <li>
   <p>
    Explain how to evaluate a ranked list with multi-level relevance judgments.
   </p>
  </li>
  <li>
   <p>
    Explain how to compute normalized discounted cumulative gain.
   </p>
  </li>
  <li>
   <p>
    Explain why it is important to perform statistical significance tests.
   </p>
  </li>
 </ul>
 <h2 level="2">
  Guiding Questions
 </h2>
 <p>
  Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Why is evaluation so critical for research and application development in text retrieval?
   </p>
  </li>
  <li>
   <p>
    How does the Cranfield evaluation methodology work?
   </p>
  </li>
  <li>
   <p>
    How do we evaluate a set of retrieved documents?
   </p>
  </li>
  <li>
   <p>
    How do you compute precision, recall, and F1?
   </p>
  </li>
  <li>
   <p>
    How do we evaluate a ranked list of search results?
   </p>
  </li>
  <li>
   <p>
    How do you compute average precision? How do you compute mean average precision (MAP) and geometric mean average precision (gMAP)?
   </p>
  </li>
  <li>
   <p>
    What is mean reciprocal rank?
   </p>
  </li>
  <li>
   <p>
    Why is MAP more appropriate than precision at k documents when comparing two retrieval methods?
   </p>
  </li>
  <li>
   <p>
    Why is precision at k documents more meaningful than average precision from a user’s perspective?
   </p>
  </li>
  <li>
   <p>
    How can we evaluate a ranked list of search results using multi-level relevance judgments?
   </p>
  </li>
  <li>
   <p>
    How do you compute normalized discounted cumulative gain (nDCG)?
   </p>
  </li>
  <li>
   <p>
    Why is normalization necessary in nDCG? Does MAP need a similar normalization? 
Why is it important to perform statistical significance tests when we compare the retrieval accuracies of two search engine systems?
   </p>
  </li>
 </ul>
 <h2 level="2">
  Additional Readings and Resources
 </h2>
 <ul bullettype="bullets">
  <li>
   <p>
    Mark
Sanderson.
    <em>
     Test collection based evaluation of information retrieval systems.
Foundations and Trends in Information Retrieval
    </em>
    4, 4 (2010), 247-375.
   </p>
  </li>
  <li>
   <p>
    C. Zhai and S. Massung.
    <em>
     Text Data Management and
Analysis: A Practical Introduction to Information Retrieval and Text Mining
    </em>
    ,
ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Chapter 9
   </p>
  </li>
 </ul>
 <h2 level="2">
  Key Phrases and Concepts
 </h2>
 <p>
  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Cranfield evaluation methodology
   </p>
  </li>
  <li>
   <p>
    Precision and recall
   </p>
  </li>
  <li>
   <p>
    Average precision, mean average precision (MAP), and geometric mean average precision (gMAP)
   </p>
  </li>
  <li>
   <p>
    Reciprocal rank and mean reciprocal rank
   </p>
  </li>
  <li>
   <p>
    F-measure
   </p>
  </li>
  <li>
   <p>
    Normalized discounted cumulative Gain (nDCG)
   </p>
  </li>
  <li>
   <p>
    Statistical significance test
   </p>
  </li>
 </ul>
 <h2 level="2">
  Tips for Success
 </h2>
 <p>
  To do well this week, I recommend that you do the following:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.
   </p>
  </li>
  <li>
   <p>
    When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.
   </p>
  </li>
  <li>
   <p>
    It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.
   </p>
  </li>
  <li>
   <p>
    Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!
   </p>
  </li>
 </ul>
 <h2 level="2">
  Getting and Giving Help
 </h2>
 <p>
  You can get/give help via the following means:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Use the
    <strong>
     <a href="https://courserahelp.zendesk.com/hc/en-us/">
      Learner Help Center
     </a>
    </strong>
    to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the
    <strong>
     Contact Us!
    </strong>
    link available on each topic's page within the Learner Help Center.
   </p>
  </li>
  <li>
   <p>
    Use the
    <strong>
     <a href="https://www.coursera.org/learn/text-retrieval/forum/VNWXSgylEeaZrBJIefqa4w/discussions?sort=lastActivityAtDesc&amp;page=1">
      Content Issues
     </a>
    </strong>
    forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues
   </p>
  </li>
 </ul>
 <p>
 </p>
</co-content>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
