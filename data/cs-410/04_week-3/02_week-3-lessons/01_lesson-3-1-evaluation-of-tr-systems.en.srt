1
00:00:00,000 --> 00:00:03,655
[MUSIC]

2
00:00:07,739 --> 00:00:14,807
This lecture is about Evaluation of
Text Retrieval Systems In the previous

3
00:00:14,807 --> 00:00:19,490
lectures, we have talked about
the a number of Text Retrieval Methods,

4
00:00:19,490 --> 00:00:22,120
different kinds of ranking functions.

5
00:00:23,550 --> 00:00:27,040
But how do we know which
one works the best?

6
00:00:27,040 --> 00:00:28,390
In order to answer this question,

7
00:00:28,390 --> 00:00:33,000
we have to compare them and that means we
have to evaluate these retrieval methods.

8
00:00:34,790 --> 00:00:37,000
So this is the main topic of this lecture.

9
00:00:40,462 --> 00:00:42,730
First, lets think about why
do we have to do evaluation?

10
00:00:42,730 --> 00:00:44,290
I already give one reason.

11
00:00:44,290 --> 00:00:48,770
That is, we have to use evaluation
to figure out which retrieval method

12
00:00:48,770 --> 00:00:50,330
works better.

13
00:00:50,330 --> 00:00:54,310
Now this is very important for
advancing our knowledge.

14
00:00:54,310 --> 00:01:00,173
Otherwise, we wouldn't know whether a new
idea works better than an old idea.

15
00:01:00,173 --> 00:01:04,710
In the beginning of this course, we talked
about the problem of text retrieval.

16
00:01:04,710 --> 00:01:07,200
We compare it with data base retrieval.

17
00:01:08,440 --> 00:01:14,390
There we mentioned that text retrieval
is an empirically defined problem.

18
00:01:14,390 --> 00:01:18,240
So evaluation must rely on users.

19
00:01:18,240 --> 00:01:22,210
Which system works better,
would have to be judged by our users.

20
00:01:25,020 --> 00:01:28,850
So, this becomes a very
challenging problem

21
00:01:28,850 --> 00:01:32,510
because how can we get users
involved in the evaluation?

22
00:01:32,510 --> 00:01:35,281
How can we do a fair comparison
of different method?

23
00:01:37,208 --> 00:01:39,970
So just go back to the reasons for
evaluation.

24
00:01:41,210 --> 00:01:42,660
I listed two reasons here.

25
00:01:42,660 --> 00:01:47,060
The second reason, is basically what I
just said, but there is also another

26
00:01:47,060 --> 00:01:51,910
reason which is to assess the actual
utility of a Text Regional system.

27
00:01:51,910 --> 00:01:55,200
Imagine you're building your
own such annual applications,

28
00:01:55,200 --> 00:02:01,660
it would be interesting knowing how well
your search engine works for your users.

29
00:02:01,660 --> 00:02:02,350
So in this case,

30
00:02:02,350 --> 00:02:07,850
matches must reflect the utility to
the actual users in real occasion.

31
00:02:07,850 --> 00:02:11,840
And typically, this has to be
done by using user starters and

32
00:02:11,840 --> 00:02:13,960
using the real search engine.

33
00:02:16,340 --> 00:02:18,630
In the second case, or the second reason,

34
00:02:19,970 --> 00:02:26,130
the measures actually all need to collated
with the utility to actually use this.

35
00:02:26,130 --> 00:02:30,120
Thus, they don't have to accurately
reflect the exact utility to users.

36
00:02:31,980 --> 00:02:37,680
So the measure only needs to be good
enough to tell which method works better.

37
00:02:38,860 --> 00:02:41,780
And this is usually done
through a test collection.

38
00:02:41,780 --> 00:02:48,110
And this is the main idea that we'll
be talking about in this course.

39
00:02:48,110 --> 00:02:53,520
This has been very important for
comparing different algorithms and

40
00:02:53,520 --> 00:02:56,910
for improving search
engine system in general.

41
00:02:58,910 --> 00:03:01,880
So let's talk about what to measure.

42
00:03:01,880 --> 00:03:06,750
There are many aspects of searching
that we can measure, we can evaluate.

43
00:03:06,750 --> 00:03:09,000
And here,
I listed the three major aspects.

44
00:03:09,000 --> 00:03:11,190
One, is effectiveness or accuracy.

45
00:03:11,190 --> 00:03:13,710
How accurate are the search results?

46
00:03:13,710 --> 00:03:18,110
In this case, we're measuring a system's
capability of ranking relevant documents

47
00:03:18,110 --> 00:03:20,150
on top of non relevant ones.

48
00:03:20,150 --> 00:03:21,850
The second, is efficiency.

49
00:03:21,850 --> 00:03:24,470
How quickly can you get the results?

50
00:03:24,470 --> 00:03:27,710
How much computing resources
are needed to answer a query?

51
00:03:27,710 --> 00:03:31,150
In this case, we need to measure the space
and time overhead of the system.

52
00:03:32,540 --> 00:03:34,890
The third aspect is usability.

53
00:03:34,890 --> 00:03:38,950
Basically the question is,
how useful is a system for new user tasks.

54
00:03:38,950 --> 00:03:40,840
Here, obviously, interfaces and

55
00:03:40,840 --> 00:03:45,870
many other things also important and
would typically have to do user studies.

56
00:03:47,410 --> 00:03:51,670
Now in this course, we're going to
talk mostly about effectiveness and

57
00:03:51,670 --> 00:03:52,710
accuracy measures.

58
00:03:52,710 --> 00:03:55,340
Because the efficiency and

59
00:03:55,340 --> 00:04:00,230
usability dimensions are not
really unique to search engines.

60
00:04:00,230 --> 00:04:08,640
And so they are needed for
without any other software systems.

61
00:04:08,640 --> 00:04:13,347
And there is also good coverage
of such and other causes.

62
00:04:13,347 --> 00:04:18,780
But how to evaluate search
engine's quality or accuracy is

63
00:04:18,780 --> 00:04:23,110
something unique to text retrieval and
we're going to talk a lot about this.

64
00:04:23,110 --> 00:04:28,428
The main idea that people have proposed
before using a test set to evaluate

65
00:04:28,428 --> 00:04:33,850
the text retrieval algorithm is called
the Cranfield Evaluation Methodology.

66
00:04:33,850 --> 00:04:40,145
This one actually was developed
a long time ago, developed in 1960s.

67
00:04:40,145 --> 00:04:44,785
It's a methodology for
laboratory test of system components.

68
00:04:45,985 --> 00:04:49,305
Its sampling methodology that has
been very useful, not just for

69
00:04:49,305 --> 00:04:50,880
search engine evaluation.

70
00:04:50,880 --> 00:04:55,930
But also for evaluating virtually
all kinds of empirical tasks, and

71
00:04:55,930 --> 00:05:01,180
for example in natural language processing
or in other fields where the problem

72
00:05:01,180 --> 00:05:05,620
is empirical to find, we typically
would need to use such a methodology.

73
00:05:05,620 --> 00:05:09,450
And today with the big data challenging

74
00:05:09,450 --> 00:05:13,590
with the use of machine
learning everywhere.

75
00:05:13,590 --> 00:05:17,430
This methodology has been very popular,
but it was first developed for

76
00:05:17,430 --> 00:05:20,100
a search engine application in the 1960s.

77
00:05:20,100 --> 00:05:25,250
So the basic idea of this approach is
to build a reusable test collection and

78
00:05:25,250 --> 00:05:26,200
define measures.

79
00:05:27,220 --> 00:05:30,350
Once such a test collection is built,
it can be used again and

80
00:05:30,350 --> 00:05:32,990
again to test different algorithms.

81
00:05:32,990 --> 00:05:36,180
And we're going to define measures
that allow you to quantify

82
00:05:36,180 --> 00:05:39,660
performance of a system and algorithm.

83
00:05:41,070 --> 00:05:42,960
So how exactly will this work?

84
00:05:42,960 --> 00:05:47,090
Well we can do have a sample collection of
documents and this is adjusted to simulate

85
00:05:47,090 --> 00:05:49,986
the real document collection
in the search application.

86
00:05:49,986 --> 00:05:53,210
We're going to also have a sample
set of queries, or topics.

87
00:05:53,210 --> 00:05:55,240
This is a little simulator
that uses queries.

88
00:05:56,270 --> 00:05:58,980
Then, we'll have to have
those relevance judgments.

89
00:05:58,980 --> 00:06:03,930
These are judgments of which documents
should be returned for which queries.

90
00:06:03,930 --> 00:06:08,250
Ideally, they have to be made by
users who formulated the queries.

91
00:06:08,250 --> 00:06:12,930
Because those are the people that know
exactly what documents would be used for.

92
00:06:12,930 --> 00:06:14,690
And finally, we have to have matches for

93
00:06:14,690 --> 00:06:19,830
quantify how well our system's result
matches the ideal ranked list.

94
00:06:19,830 --> 00:06:24,560
That would be constructed base
on user's relevance judgements.

95
00:06:24,560 --> 00:06:30,917
So this methodology is very useful for
starting retrieval algorithms,

96
00:06:30,917 --> 00:06:36,130
because the test can be reused many times.

97
00:06:36,130 --> 00:06:41,340
And it will also provide a fair
comparison for all the methods.

98
00:06:41,340 --> 00:06:43,370
We have the same criteria or

99
00:06:43,370 --> 00:06:47,570
same dataset to be used to
compare different algorithms.

100
00:06:47,570 --> 00:06:50,810
This allows us to compare
a new algorithm with

101
00:06:50,810 --> 00:06:55,660
an old algorithm that was divided many
years ago, by using the same standard.

102
00:06:55,660 --> 00:06:59,580
So this is the illustration of this works,
so as I said,

103
00:06:59,580 --> 00:07:03,930
we need our queries that are showing here.

104
00:07:03,930 --> 00:07:05,180
We have Q1, Q2 etc.

105
00:07:05,180 --> 00:07:08,300
We also need the documents and
that's called the document caching and

106
00:07:08,300 --> 00:07:10,580
on the right side you will see
we need relevance judgments.

107
00:07:10,580 --> 00:07:19,150
These are basically the binary judgments
of documents with respect to a query.

108
00:07:19,150 --> 00:07:23,790
So for example,
D1 is judged as being relevant to Q1,

109
00:07:23,790 --> 00:07:27,920
D2 is judged as being relevant as well,
and D3 is judged as not relevant.

110
00:07:27,920 --> 00:07:28,980
And the Q1 etc.

111
00:07:28,980 --> 00:07:32,490
These will be created by users.

112
00:07:34,190 --> 00:07:38,460
Once we have these, and
we basically have a test collection.

113
00:07:38,460 --> 00:07:43,560
And then if you have two systems,
you want to compare them,

114
00:07:43,560 --> 00:07:47,260
then you can just run each
system on these queries and

115
00:07:47,260 --> 00:07:50,580
the documents and
each system would then return results.

116
00:07:50,580 --> 00:07:56,347
Let's say if the queries Q1 and
then we would have the results here.

117
00:07:56,347 --> 00:08:02,350
Here I show R sub A as
the results from system A.

118
00:08:02,350 --> 00:08:05,170
So this is, remember we talked about

119
00:08:05,170 --> 00:08:09,750
task of computing approximation
of the relevant document set.

120
00:08:09,750 --> 00:08:13,910
R sub A is system A's approximation here.

121
00:08:14,980 --> 00:08:20,000
And R sub B is system B's
approximation of relevant documents.

122
00:08:21,100 --> 00:08:22,810
Now, let's take a look at these results.

123
00:08:22,810 --> 00:08:24,190
So which is better?

124
00:08:24,190 --> 00:08:26,810
Now imagine if a user,
which one would you like?

125
00:08:26,810 --> 00:08:31,145
Now let's take a look at the both results.

126
00:08:31,145 --> 00:08:33,270
And there are some differences and

127
00:08:33,270 --> 00:08:40,160
there are some documents that
are returned by both systems.

128
00:08:40,160 --> 00:08:44,200
But if you look at the results,
you will feel that maybe

129
00:08:44,200 --> 00:08:48,640
A is better in the sense that we don't
have many number element documents.

130
00:08:48,640 --> 00:08:52,260
And among the three documents returned,
the two of them are relevant.

131
00:08:52,260 --> 00:08:55,430
So that's good, it's precise.

132
00:08:55,430 --> 00:08:58,770
On the other hand one council
say maybe B is better,

133
00:08:58,770 --> 00:09:01,280
because we've got all of
them in the documents.

134
00:09:01,280 --> 00:09:03,500
We've got three instead of two.

135
00:09:03,500 --> 00:09:06,690
So which one is better and
how do we quantify this?

136
00:09:08,820 --> 00:09:12,670
Well, obviously this question
highly depends on a user's task.

137
00:09:12,670 --> 00:09:14,820
It depends on users as well.

138
00:09:14,820 --> 00:09:19,950
You might even imagine for
some users may be system A is better.

139
00:09:19,950 --> 00:09:23,747
If the user is not interested in
getting all the random documents.

140
00:09:23,747 --> 00:09:28,582
Right, in this case the user doesn't
have to read a million users will

141
00:09:28,582 --> 00:09:31,020
see most of the relevant documents.

142
00:09:31,020 --> 00:09:34,230
On the other hand,
one can also imagine the user might need

143
00:09:34,230 --> 00:09:37,130
to have as many random
documents as possible.

144
00:09:37,130 --> 00:09:41,617
For example, if you're doing a literature
survey you might be in the sigma category,

145
00:09:41,617 --> 00:09:43,844
and you might find that
system B is better.

146
00:09:43,844 --> 00:09:48,985
So in the case, we will have to also
define measures that will quantify them.

147
00:09:48,985 --> 00:09:53,408
And we might need it to define multiple
measures because users have different

148
00:09:53,408 --> 00:09:55,798
perspectives of looking at the results.

149
00:09:58,259 --> 00:10:08,259
[MUSIC]