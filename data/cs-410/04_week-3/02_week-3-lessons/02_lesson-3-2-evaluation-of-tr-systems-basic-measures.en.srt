1
00:00:00,012 --> 00:00:06,908
[SOUND]
This

2
00:00:06,908 --> 00:00:13,498
lecture is about the basic measures for
evaluation of text retrieval systems.

3
00:00:13,498 --> 00:00:18,955
In this lecture,
we're going to discuss how we design basic

4
00:00:18,955 --> 00:00:24,528
measures to quantitatively
compare two retrieval systems.

5
00:00:24,528 --> 00:00:29,163
This is a slide that you have seen
earlier in the lecture where we talked

6
00:00:29,163 --> 00:00:32,318
about the Granville
evaluation methodology.

7
00:00:32,318 --> 00:00:39,122
We can have a test faction that consists
of queries, documents, and [INAUDIBLE].

8
00:00:39,122 --> 00:00:47,930
We can then run two systems on these
data sets to contradict the evaluator.

9
00:00:47,930 --> 00:00:49,528
Their performance.

10
00:00:49,528 --> 00:00:54,828
And we raise the question,
about which set of results is better.

11
00:00:54,828 --> 00:00:57,928
Is system A better or is system B better?

12
00:00:57,928 --> 00:01:02,398
So let's now talk about how to
accurately quantify their performance.

13
00:01:02,398 --> 00:01:07,841
Suppose we have a total of 10 relevant
documents in the collection for

14
00:01:07,841 --> 00:01:08,848
this query.

15
00:01:08,848 --> 00:01:15,162
Now, the relevant judgments show on
the right in [INAUDIBLE] obviously.

16
00:01:15,162 --> 00:01:19,908
And we have only seen 3 [INAUDIBLE] there,
[INAUDIBLE] documents there.

17
00:01:19,908 --> 00:01:26,133
But, we can imagine there are other Random
documents in judging for this query.

18
00:01:26,133 --> 00:01:30,895
So now, intuitively,
we thought that system

19
00:01:30,895 --> 00:01:35,668
A is better because it
did not have much noise.

20
00:01:35,668 --> 00:01:42,019
And in particular we have seen
that among the three results,

21
00:01:42,019 --> 00:01:46,251
two of them are relevant but in system B,

22
00:01:46,251 --> 00:01:52,248
we have five results and
only three of them are relevant.

23
00:01:52,248 --> 00:01:56,418
So intuitively it looks like
system A is more accurate.

24
00:01:56,418 --> 00:02:00,670
And this infusion can be captured
by a matching holder position,

25
00:02:00,670 --> 00:02:05,866
where we simply compute to what extent
all the retrieval results are relevant.

26
00:02:05,866 --> 00:02:07,788
If you have 100% position,

27
00:02:07,788 --> 00:02:11,638
that would mean that all
the retrieval documents are relevant.

28
00:02:11,638 --> 00:02:16,476
So in this case system A has
a position of two out of

29
00:02:16,476 --> 00:02:20,606
three System B has some
sweet hold of 5 and

30
00:02:20,606 --> 00:02:25,208
this shows that system
A is better frequency.

31
00:02:25,208 --> 00:02:30,065
But we also talked about System B
might be prefered by some other units

32
00:02:30,065 --> 00:02:35,220
would like to retrieve as many
random documents as possible.

33
00:02:35,220 --> 00:02:39,839
So in that case we'll have to compare
the number of relevant documents that they

34
00:02:39,839 --> 00:02:42,940
retrieve and
there's another method called recall.

35
00:02:42,940 --> 00:02:48,000
This method uses the completeness
of coverage of random documents

36
00:02:48,000 --> 00:02:51,090
In your retrieval result.

37
00:02:51,090 --> 00:02:57,500
So we just assume that there are ten
relevant documents in the collection.

38
00:02:57,500 --> 00:03:01,510
And here we've got two of them,
in system A.

39
00:03:01,510 --> 00:03:04,130
So the recall is 2 out of 10.

40
00:03:04,130 --> 00:03:07,630
Whereas System B has called a 3,
so it's a 3 out of 10.

41
00:03:07,630 --> 00:03:11,278
Now we can see by recall
system B is better.

42
00:03:11,278 --> 00:03:15,240
And these two measures turn out to
be the very basic of measures for

43
00:03:15,240 --> 00:03:16,978
evaluating search engine.

44
00:03:16,978 --> 00:03:21,824
And they are very important because
they are also widely used in many

45
00:03:21,824 --> 00:03:24,298
other test evaluation problems.

46
00:03:24,298 --> 00:03:28,660
For example, if you look at
the applications of machine learning,

47
00:03:28,660 --> 00:03:34,030
you tend to see precision recall numbers
being reported and for all kinds of tasks.

48
00:03:35,290 --> 00:03:38,520
Okay so, now let's define these
two measures more precisely.

49
00:03:38,520 --> 00:03:44,410
And these measures are to evaluate a set
of retrieved documents, so that means

50
00:03:44,410 --> 00:03:48,950
we are considering that approximation
of the set of relevant documents.

51
00:03:50,100 --> 00:03:53,300
We can distinguish 4 cases depending
on the situation of the documents.

52
00:03:53,300 --> 00:03:59,720
A document can be retrieved or
not retrieved, right?

53
00:03:59,720 --> 00:04:01,570
Because we are talking
about a set of results.

54
00:04:02,710 --> 00:04:05,640
A document can be also relevant or

55
00:04:05,640 --> 00:04:10,310
not relevant depending on whether the user
thinks this is a useful document.

56
00:04:11,950 --> 00:04:16,890
So we can now have counts of documents in.

57
00:04:16,890 --> 00:04:21,610
Each of the four categories again
have a represent the number of

58
00:04:21,610 --> 00:04:24,420
documents that have been retrieved and
relevant.

59
00:04:24,420 --> 00:04:30,530
B for documents that are not retrieved but
rather etc.

60
00:04:31,750 --> 00:04:35,550
No with this table then
we can define precision.

61
00:04:36,690 --> 00:04:42,450
As the ratio of the relevant

62
00:04:42,450 --> 00:04:47,440
retrieved documents A to the total
of relevant retrieved documents.

63
00:04:48,450 --> 00:04:53,390
So, this is just A divided
by The sum of a and c.

64
00:04:53,390 --> 00:04:55,640
The sum of this column.

65
00:04:56,820 --> 00:05:04,360
Singularly recall is defined by
dividing a by the sum of a and b.

66
00:05:04,360 --> 00:05:07,470
So that's again to divide a by.

67
00:05:07,470 --> 00:05:10,360
The sum of the row instead of the column.

68
00:05:10,360 --> 00:05:15,810
All right, so we can see precision and
recall is all focused on looking at the a,

69
00:05:16,930 --> 00:05:20,000
that's the number of
retrieved relevant documents.

70
00:05:20,000 --> 00:05:22,449
But we're going to use
different denominators.

71
00:05:23,590 --> 00:05:27,300
Okay, so what would be an ideal result.

72
00:05:27,300 --> 00:05:31,330
Well, you can easily see being
the ideal case would have precision and

73
00:05:31,330 --> 00:05:34,060
recall oil to be 1.0.

74
00:05:34,060 --> 00:05:39,510
That means We have got 1% of
all the Relevant documents

75
00:05:39,510 --> 00:05:44,770
in our results, and all of the results
that we returned all Relevant.

76
00:05:44,770 --> 00:05:47,540
At least there's no single
Not Relevant document returned.

77
00:05:48,680 --> 00:05:53,920
In reality, however, high recall tends
to be associated with low precision.

78
00:05:53,920 --> 00:05:56,210
And you can imagine why that's the case.

79
00:05:56,210 --> 00:06:00,790
As you go down the to try to get as
many random documents as possible,

80
00:06:00,790 --> 00:06:05,890
you tend to encounter a lot of documents,
so the precision has to go down.

81
00:06:05,890 --> 00:06:11,450
Note that this set can also
be defined by a cut off.

82
00:06:11,450 --> 00:06:15,490
In the rest of this, that's why although
these two measures are defined for

83
00:06:15,490 --> 00:06:20,560
retrieve the documents, they are actually
very useful for evaluating a rank list.

84
00:06:20,560 --> 00:06:24,270
They are the fundamental measures in
task retrieval and many other tasks.

85
00:06:24,270 --> 00:06:30,010
We often are interested in The precision
at ten documents for web search.

86
00:06:30,010 --> 00:06:33,400
This means we look at how many documents

87
00:06:33,400 --> 00:06:35,870
among the top ten results
are actually relevant.

88
00:06:35,870 --> 00:06:38,290
Now, this is a very meaningful measure,

89
00:06:38,290 --> 00:06:43,780
because it tells us how many relevant
documents a user can expect to see

90
00:06:43,780 --> 00:06:47,790
On the first page of where they
typically show ten results.

91
00:06:50,000 --> 00:06:55,780
So precision and recall
are the basic matches and we need to

92
00:06:55,780 --> 00:07:02,040
use them to further evaluate a search
engine, but they are the Building blocks.

93
00:07:03,460 --> 00:07:07,210
We just said that there tends to be
a trailoff between precision and recall,

94
00:07:07,210 --> 00:07:10,730
so naturally it would be
interesting to combine them.

95
00:07:10,730 --> 00:07:15,490
And here's one method that's often used,
called F-measure And

96
00:07:15,490 --> 00:07:21,020
it's a [INAUDIBLE] mean of precision and
recall as defined on this slide.

97
00:07:22,450 --> 00:07:27,741
So, you can see at first, compute the.

98
00:07:29,210 --> 00:07:34,360
Inverse of R and P here,
and then it would interpret

99
00:07:34,360 --> 00:07:41,180
the 2 by using coefficients
depending on parameter beta.

100
00:07:42,850 --> 00:07:47,029
And after some transformation you can
easily see it would be of this form.

101
00:07:49,010 --> 00:07:51,790
And in any case it just becomes
an agent of precision and

102
00:07:51,790 --> 00:07:56,360
recall, and beta is a parameter,
that's often set to 1.

103
00:07:56,360 --> 00:08:01,572
It can control the emphasis
on precision or recall always

104
00:08:01,572 --> 00:08:08,360
set beta to 1 We end up having a special
case of F-Measure, often called F1.

105
00:08:08,360 --> 00:08:13,460
This is a popular measure that's often
used as a combined precision and recall.

106
00:08:13,460 --> 00:08:14,780
And the formula looks very simple.

107
00:08:16,170 --> 00:08:17,948
It's just this, here.

108
00:08:20,718 --> 00:08:24,668
Now it's easy to see that if
you have a Larger precision, or

109
00:08:24,668 --> 00:08:28,570
larger recall than f
measure would be high.

110
00:08:28,570 --> 00:08:32,940
But, what's interesting is that
the trade off between precision and

111
00:08:32,940 --> 00:08:36,260
recall is captured
an interesting way in f1.

112
00:08:36,260 --> 00:08:41,000
So, in order to understand that, we

113
00:08:42,170 --> 00:08:48,270
can first look at the natural
Why not just the combining and

114
00:08:48,270 --> 00:08:53,090
using the symbol arithmetically
as efficient here?

115
00:08:53,090 --> 00:09:00,730
That would be likely the most natural way
of combining them So what do you think?

116
00:09:01,870 --> 00:09:05,940
If you want to think more,
you can pause the video.

117
00:09:07,940 --> 00:09:10,960
So why is this not as good as F1?

118
00:09:13,550 --> 00:09:15,038
Or what's the problem with this?

119
00:09:18,121 --> 00:09:23,270
Now, if you think about
the arithmetic mean,

120
00:09:23,270 --> 00:09:28,300
you can see this is
the sum of multiple terms.

121
00:09:28,300 --> 00:09:31,870
In this case,
it's the sum of precision and recall.

122
00:09:31,870 --> 00:09:36,580
In the case of a sum, the total value
tends to be dominated by the large values.

123
00:09:36,580 --> 00:09:42,850
that means if you have a very high P or
very high R then you really

124
00:09:42,850 --> 00:09:47,820
don't care about whether the other value
is low so the whole sum would be high.

125
00:09:47,820 --> 00:09:53,920
Now this is not desirable because one
can easily have a perfect recall.

126
00:09:53,920 --> 00:09:57,110
We have perfect recall easily.

127
00:09:57,110 --> 00:09:58,140
Can we imagine how?

128
00:09:59,810 --> 00:10:03,830
It's probably very easy to
imagine that we simply retrieve

129
00:10:03,830 --> 00:10:06,399
all the documents in the collection and
then we have a perfect recall.

130
00:10:07,420 --> 00:10:11,130
And this will give us 0.5 as the average.

131
00:10:11,130 --> 00:10:15,583
But such results are clearly not
very useful for the users even

132
00:10:15,583 --> 00:10:20,350
though the average using this
formula would be relevantly high.

133
00:10:21,750 --> 00:10:25,930
In contrast you can see F 1 would
reward a case where precision and

134
00:10:25,930 --> 00:10:27,750
recall are roughly That seminar, so

135
00:10:27,750 --> 00:10:33,360
it would a case where you had
extremely high value for one of them.

136
00:10:35,320 --> 00:10:38,360
So this means f one encodes
a different trade off between that.

137
00:10:38,360 --> 00:10:43,690
Now this example shows
actually a very important.

138
00:10:43,690 --> 00:10:44,230
Methodology here.

139
00:10:44,230 --> 00:10:49,950
But when you try to solve a problem you
might naturally think of one solution,

140
00:10:49,950 --> 00:10:52,120
let's say in this it's
this error mechanism.

141
00:10:53,790 --> 00:10:57,160
But it's important not to
settle on this source.

142
00:10:57,160 --> 00:11:00,790
It's important to think whether you
have other ways to combine that.

143
00:11:02,170 --> 00:11:06,180
And once you think about the multiple
variance It's important to analyze their

144
00:11:06,180 --> 00:11:10,930
difference, and then think about
which one makes more sense.

145
00:11:10,930 --> 00:11:13,280
In this case, if you think more carefully,

146
00:11:13,280 --> 00:11:15,920
you will think that F1
probably makes more sense.

147
00:11:15,920 --> 00:11:18,300
Than the simple.

148
00:11:18,300 --> 00:11:21,670
Although in other cases there
may be different results.

149
00:11:21,670 --> 00:11:25,858
But in this case the seems not reasonable.

150
00:11:25,858 --> 00:11:29,260
But if you don't pay attention
to these subtle differences

151
00:11:29,260 --> 00:11:33,780
you might just take a easy way to
combine them and then go ahead with it.

152
00:11:33,780 --> 00:11:37,360
And here later, you will find that,
the measure doesn't seem to work well.

153
00:11:37,360 --> 00:11:38,620
All right.

154
00:11:38,620 --> 00:11:43,760
So this methodology is actually very
important in general, in solving problems.

155
00:11:43,760 --> 00:11:46,020
Try to think about the best solution.

156
00:11:46,020 --> 00:11:50,890
Try to understand the problem very well,
and then

157
00:11:50,890 --> 00:11:55,890
know why you needed this measure, and why
you need to combine precision and recall.

158
00:11:55,890 --> 00:11:59,530
And then use that to guide you in
finding a good way to solve the problem.

159
00:12:03,320 --> 00:12:08,510
To summarize, we talked about
precision which addresses the question

160
00:12:08,510 --> 00:12:11,530
are there retrievable
results all relevant?

161
00:12:11,530 --> 00:12:13,690
We also talk about the Recall.

162
00:12:13,690 --> 00:12:17,260
Which addresses the question, have all of
the relevant documents been retrieved.

163
00:12:17,260 --> 00:12:21,250
These two, are the two,
basic matches in text and retrieval in.

164
00:12:21,250 --> 00:12:25,270
They are used for
many other tasks, as well.

165
00:12:25,270 --> 00:12:28,670
We talk about F measure as a way to
combine Precision Precision and recall.

166
00:12:29,970 --> 00:12:33,600
We also talked about the tradeoff
between precision and recall.

167
00:12:33,600 --> 00:12:38,140
And this turns out to depend
on the user's search tasks and

168
00:12:38,140 --> 00:12:42,133
we'll discuss this point
more in a later lecture.

169
00:12:42,133 --> 00:12:52,133
[MUSIC]