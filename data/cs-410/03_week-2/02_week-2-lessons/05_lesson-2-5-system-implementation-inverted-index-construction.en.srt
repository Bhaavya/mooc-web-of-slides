1
00:00:00,012 --> 00:00:03,586
[SOUND]

2
00:00:07,325 --> 00:00:10,038
This lecture is about the inverted index

3
00:00:10,038 --> 00:00:11,160
construction.

4
00:00:13,840 --> 00:00:18,520
In this lecture, we will continue
the discussion of system implementation.

5
00:00:18,520 --> 00:00:22,019
In particular, we're going to discuss
how to construct the inverted index.

6
00:00:25,096 --> 00:00:29,259
The construction of the inverted index
is actually very easy if the dataset is

7
00:00:29,259 --> 00:00:30,450
very small.

8
00:00:30,450 --> 00:00:35,430
It's very easy to construct a dictionary
and then store the postings in a file.

9
00:00:36,600 --> 00:00:42,280
The problem is that when our data
is not able to fit to the memory

10
00:00:42,280 --> 00:00:45,180
then we have to use some
special method to deal with it.

11
00:00:46,500 --> 00:00:51,900
And unfortunately, in most retrieval
applications the dataset will be large.

12
00:00:51,900 --> 00:00:55,700
And they generally cannot be
loaded into memory at once.

13
00:00:56,790 --> 00:01:01,843
And there are many approaches to
solve that problem, and sorting-based

14
00:01:01,843 --> 00:01:06,710
method is quite common and
works in four steps as shown here.

15
00:01:06,710 --> 00:01:11,480
First, you collect the local termID,
documentID and frequency tuples.

16
00:01:11,480 --> 00:01:16,946
Basically you will locate the terms
in a small set of documents.

17
00:01:16,946 --> 00:01:24,117
And then once you collect those accounts
you can sort those count based on terms.

18
00:01:24,117 --> 00:01:29,104
So that you will be able to local
a partial inverted index and

19
00:01:29,104 --> 00:01:31,310
these are called rounds.

20
00:01:31,310 --> 00:01:36,930
And then you write them into
a temporary file on the disk and

21
00:01:36,930 --> 00:01:38,940
then you merge in step 3.

22
00:01:38,940 --> 00:01:44,104
Do pairwise merging of these runs, until
you eventually merge all the runs and

23
00:01:44,104 --> 00:01:46,460
generate a single inverted index.

24
00:01:47,700 --> 00:01:50,823
So this is an illustration of this method.

25
00:01:50,823 --> 00:01:54,265
On the left you see some documents and

26
00:01:54,265 --> 00:01:59,942
on the right we have a term lexicon and
a document ID lexicon.

27
00:01:59,942 --> 00:02:08,070
These lexicons are to map string-based
representations of document IDs or

28
00:02:08,070 --> 00:02:12,261
terms into integer representations or

29
00:02:12,261 --> 00:02:18,112
map back from integers to
the stream representation.

30
00:02:18,112 --> 00:02:23,010
The reason why we want our interest
using integers to present these

31
00:02:23,010 --> 00:02:26,930
IDs is because integers
are often easier to handle.

32
00:02:26,930 --> 00:02:29,770
For example,
integers can be used as index for

33
00:02:29,770 --> 00:02:33,240
array, and they are also easy to compress.

34
00:02:34,390 --> 00:02:40,530
So this is one reason why we tend
to map these strings into integers,

35
00:02:42,180 --> 00:02:46,710
so that we don't have to
carry these strings around.

36
00:02:46,710 --> 00:02:48,070
So how does this approach work?

37
00:02:48,070 --> 00:02:49,822
Well, it's very simple.

38
00:02:49,822 --> 00:02:53,260
We're going to scan these
documents sequentially and

39
00:02:53,260 --> 00:02:58,260
then parse the documents and
count the frequencies of terms.

40
00:02:58,260 --> 00:03:03,306
And in this stage we generally sort
the frequencies by document IDs,

41
00:03:03,306 --> 00:03:06,961
because we process each
document sequentially.

42
00:03:06,961 --> 00:03:11,310
So we'll first encounter all
the terms in the first document.

43
00:03:11,310 --> 00:03:18,786
Therefore the document IDs
are all ones in this case.

44
00:03:18,786 --> 00:03:25,503
And this will be followed by document IDs
two and they are natural results in this

45
00:03:25,503 --> 00:03:31,280
only just because we process
the data in a sequential order.

46
00:03:31,280 --> 00:03:34,890
At some point,
we will run out of memory and

47
00:03:34,890 --> 00:03:39,080
that would have to write
them into the disc.

48
00:03:39,080 --> 00:03:45,830
Before we do that we 're going to sort
them, just use whatever memory we have.

49
00:03:45,830 --> 00:03:51,948
We can sort them and then this time
we're going to sort based on term IDs.

50
00:03:51,948 --> 00:03:59,459
Note that here,
we're using the term IDs as a key to sort.

51
00:03:59,459 --> 00:04:03,827
So all the entries that share the same
term would be grouped together.

52
00:04:03,827 --> 00:04:08,557
In this case,
we can see all the IDs of documents

53
00:04:08,557 --> 00:04:14,090
that match term 1 would
be grouped together.

54
00:04:14,090 --> 00:04:18,850
And we're going to write this into
that this is a temporary file.

55
00:04:18,850 --> 00:04:22,800
And would that allows you to
use the memory to process and

56
00:04:22,800 --> 00:04:24,030
makes a batch of documents.

57
00:04:24,030 --> 00:04:26,670
And we're going to do that for
all the documents.

58
00:04:26,670 --> 00:04:32,546
So we're going to write a lot of
temporary files into the disc.

59
00:04:32,546 --> 00:04:35,400
And then the next stage is
we do merge sort basically.

60
00:04:35,400 --> 00:04:38,360
We're going to merge them and
then sort them.

61
00:04:38,360 --> 00:04:41,729
Eventually, we will get
a single inverted index,

62
00:04:41,729 --> 00:04:45,310
where the entries are sorted
based on term IDs.

63
00:04:46,960 --> 00:04:50,870
And on the top, we're going to see
these are the older entries for

64
00:04:50,870 --> 00:04:53,620
the documents that match term ID 1.

65
00:04:53,620 --> 00:05:00,300
So this is basically, how we can do
the construction of inverted index.

66
00:05:00,300 --> 00:05:06,445
Even though the data cannot be
all loaded into the manner.

67
00:05:06,445 --> 00:05:12,562
Now, we mention earlier that
because of hostings are very large,

68
00:05:12,562 --> 00:05:15,848
it's desirable to compress them.

69
00:05:15,848 --> 00:05:20,481
So let's now take a little bit
how we compressed inverted index.

70
00:05:20,481 --> 00:05:24,084
Well the idea of compression in general,
is for

71
00:05:24,084 --> 00:05:28,310
leverage skewed distributions of values.

72
00:05:28,310 --> 00:05:31,090
And we generally have to use
variable-length encoding,

73
00:05:31,090 --> 00:05:36,830
instead of the fixed-length
encoding as we use by default in

74
00:05:36,830 --> 00:05:41,080
a program manager like C++.

75
00:05:41,080 --> 00:05:45,840
And so how can we leverage
the skewed distributions of values

76
00:05:45,840 --> 00:05:48,180
to compress these values?

77
00:05:48,180 --> 00:05:53,827
Well in general, we will use few
bits to encode those frequent

78
00:05:53,827 --> 00:06:00,650
words at the cost of using longer
bit string code those rare values.

79
00:06:00,650 --> 00:06:04,560
So in our case, let's think about how
we can compress the TF, tone frequency.

80
00:06:05,640 --> 00:06:09,640
Now, if you can picture what
the inverted index look like, and

81
00:06:09,640 --> 00:06:13,807
you will see in post things,
there are a lot of tone frequencies.

82
00:06:13,807 --> 00:06:19,089
Those are the frequencies of
terms in all those documents.

83
00:06:19,089 --> 00:06:25,650
Now, if you think about it, what kind
of values are most frequent there?

84
00:06:25,650 --> 00:06:29,980
You probably will be able to guess
that small numbers tend to occur

85
00:06:29,980 --> 00:06:32,540
far more frequently than large numbers.

86
00:06:32,540 --> 00:06:33,990
Why?

87
00:06:33,990 --> 00:06:39,954
Well, think about the distribution of
words and this is to do the sip of slopes,

88
00:06:39,954 --> 00:06:44,810
and many words occur just rarely so
we see a lot of small numbers.

89
00:06:44,810 --> 00:06:48,419
Therefore, we can use fewer bits for
the small, but

90
00:06:48,419 --> 00:06:53,855
highly frequent integers and

91
00:06:53,855 --> 00:06:57,095
that's cost of using more bits for
larger integers.

92
00:06:58,445 --> 00:07:00,005
This is a trade off of course.

93
00:07:00,005 --> 00:07:05,712
If the values are distributed to uniform,
then this won't save us any space,

94
00:07:05,712 --> 00:07:10,824
but because we tend to see many small
values, they are very frequent.

95
00:07:10,824 --> 00:07:15,769
We can save on average even though
sometimes when we see a large number

96
00:07:15,769 --> 00:07:17,740
we have to use a lot of bits.

97
00:07:19,750 --> 00:07:23,700
What about the document IDs
that we also saw in postings?

98
00:07:23,700 --> 00:07:27,240
Well they are not distributed
in the skewed way.

99
00:07:27,240 --> 00:07:31,840
So how can we deal with that?

100
00:07:31,840 --> 00:07:35,488
Well it turns out that we can
use a trick called a d-gap and

101
00:07:35,488 --> 00:07:38,686
that is to store the difference
of these term IDs.

102
00:07:38,686 --> 00:07:43,495
And we can imagine if a term has
matched that many documents then

103
00:07:43,495 --> 00:07:46,640
there will be longest of document IDs.

104
00:07:46,640 --> 00:07:52,030
So when we take the gap, and we take the
difference between adjacent document IDs,

105
00:07:52,030 --> 00:07:54,340
those gaps will be small.

106
00:07:54,340 --> 00:07:57,594
So again, see a lot of small numbers.

107
00:07:57,594 --> 00:08:00,217
Whereas if a term occurred
in only a few documents,

108
00:08:00,217 --> 00:08:04,300
then the gap would be large,
the large numbers would not be frequent.

109
00:08:04,300 --> 00:08:06,610
So this creates some skewed distribution,

110
00:08:06,610 --> 00:08:10,669
that would allow us to
compress these values.

111
00:08:11,850 --> 00:08:15,621
This is also possible because
in order to uncover or

112
00:08:15,621 --> 00:08:21,249
uncompress these document IDs,
we have to sequentially process the data.

113
00:08:21,249 --> 00:08:25,484
Because we stored the difference and
in order to recover the exact

114
00:08:25,484 --> 00:08:29,574
document ID we have to first
recover the previous document ID.

115
00:08:29,574 --> 00:08:34,536
And then we can add the difference to
the previous document ID to restore

116
00:08:34,536 --> 00:08:36,365
the current document ID.

117
00:08:36,365 --> 00:08:40,834
Now this was possible because we only
needed to have sequential access to

118
00:08:40,834 --> 00:08:42,900
those document IDs.

119
00:08:42,900 --> 00:08:46,920
Once we look up the term, we look up all
the document IDs that match the term,

120
00:08:46,920 --> 00:08:48,670
then we sequentially process them.

121
00:08:48,670 --> 00:08:52,070
So it's very natural,
that's why this trick actually works.

122
00:08:53,600 --> 00:08:55,760
And there are many different methods for
encoding.

123
00:08:55,760 --> 00:09:02,116
So binary code is a commonly used
code in just any program language.

124
00:09:02,116 --> 00:09:05,994
We use basically fixed glance in coding.

125
00:09:05,994 --> 00:09:09,276
Unary code, gamma code, and
delta code are all possibilities and

126
00:09:09,276 --> 00:09:11,240
there are many other possibilities.

127
00:09:11,240 --> 00:09:14,130
So let's look at some
of them in more detail.

128
00:09:14,130 --> 00:09:16,900
Binary coding is really
equal length coding, and

129
00:09:16,900 --> 00:09:20,930
that's a property for
randomly distributed values.

130
00:09:20,930 --> 00:09:24,700
The unary coding is a variable
length in coding method.

131
00:09:24,700 --> 00:09:28,891
In this case, integer this 1 will be

132
00:09:28,891 --> 00:09:33,630
encoded as x -1, 1 bit followed by 0.

133
00:09:33,630 --> 00:09:39,329
So for example, 3 will be encoded as 2,
1s followed by 0,

134
00:09:39,329 --> 00:09:45,042
whereas 5 will be encoded as 4,
1s, followed by 0, etc.

135
00:09:45,042 --> 00:09:51,599
So now you can imagine how many bits do we
have to use for a large number like 100?

136
00:09:51,599 --> 00:09:57,450
So how many bits do you have to
use exactly for a number like 100?

137
00:09:57,450 --> 00:10:02,549
Well exactly, we have to use 100 bits.

138
00:10:02,549 --> 00:10:07,150
So it's the same number of bits
as the value of this number.

139
00:10:07,150 --> 00:10:12,360
So this is very inefficient if you
were likely to see some large numbers.

140
00:10:12,360 --> 00:10:17,620
Imagine if you occasionally see a number
like 1,000, you have to use 1,000 bits.

141
00:10:17,620 --> 00:10:22,894
So this only works well if you
are absolutely sure that there will be

142
00:10:22,894 --> 00:10:28,082
no large numbers, mostly very
often you see very small numbers.

143
00:10:28,082 --> 00:10:30,184
Now, how do you decode this code?

144
00:10:30,184 --> 00:10:33,662
Now since these are variable
length encoding methods,

145
00:10:33,662 --> 00:10:37,070
you can't just count how many bits and
then just stop.

146
00:10:38,500 --> 00:10:44,800
You can't say 8-bits or 32-bits,
then you will start another code.

147
00:10:44,800 --> 00:10:50,860
They are variable length, so
you will have to rely on some mechanism.

148
00:10:50,860 --> 00:10:55,192
In this case for unary, you can see
it's very easy to see the boundary.

149
00:10:55,192 --> 00:10:59,161
Now you can easily see 0 would
signal the end of encoding.

150
00:10:59,161 --> 00:11:03,120
So you just count up how many 1s you
have seen and at the end you hit 0.

151
00:11:03,120 --> 00:11:06,560
You have finished one number,
you will start another number.

152
00:11:07,960 --> 00:11:11,266
Now we just saw that unary
coding is too aggressive.

153
00:11:11,266 --> 00:11:13,987
In rewarding small numbers, and

154
00:11:13,987 --> 00:11:20,430
if you occasionally can see a very
big number, it would be a disaster.

155
00:11:20,430 --> 00:11:24,900
So what about some other
less aggressive method?

156
00:11:24,900 --> 00:11:29,200
Well gamma coding's one of them and

157
00:11:29,200 --> 00:11:34,072
in this method we can use unary coding for

158
00:11:34,072 --> 00:11:37,239
a transform form of that.

159
00:11:37,239 --> 00:11:41,210
So it's 1 plus the floor of log of x.

160
00:11:41,210 --> 00:11:47,781
So the magnitude of this value is
much lower than the original x.

161
00:11:47,781 --> 00:11:52,703
So that's why we can afford
using unary code for that.

162
00:11:52,703 --> 00:11:58,728
And so first I have the unary code for
coding this log of x.

163
00:11:58,728 --> 00:12:02,327
And this would be followed by
a uniform code or binary code.

164
00:12:02,327 --> 00:12:08,058
And this basically the same uniform code,
and binary code are the same.

165
00:12:08,058 --> 00:12:15,956
And we're going to use this coder to code
the remaining part of the value of x.

166
00:12:15,956 --> 00:12:22,178
And this is basically precisely
x-1 to the floor of log of x

167
00:12:25,000 --> 00:12:30,376
So the unary code are basically
called the flow of log of x,

168
00:12:30,376 --> 00:12:33,029
well add one there and here.

169
00:12:33,029 --> 00:12:38,428
But the remaining part
we'll be using uniform

170
00:12:38,428 --> 00:12:43,413
code through actually code the difference

171
00:12:43,413 --> 00:12:47,990
between the x and this 2 to the log of x.

172
00:12:49,530 --> 00:12:53,280
And it's easy to show that for this

173
00:12:55,790 --> 00:13:00,297
difference we only need to use up

174
00:13:00,297 --> 00:13:05,390
to this many bits and
the floor of log of x bits.

175
00:13:06,530 --> 00:13:08,410
And this is easy to understand,

176
00:13:08,410 --> 00:13:12,910
if the difference is too large, then we
would have a higher floor of log of x.

177
00:13:14,330 --> 00:13:19,000
So here are some examples for
example, 3 is is encoded as 101.

178
00:13:19,000 --> 00:13:22,575
The first two digits are the unary code.

179
00:13:22,575 --> 00:13:26,706
So this isn't for the value 2,

180
00:13:26,706 --> 00:13:30,990
10 encodes 2 in unary coding.

181
00:13:32,490 --> 00:13:37,300
And so that means the floor of

182
00:13:37,300 --> 00:13:42,398
log of x is 1,
because we won't actually use unary codes.

183
00:13:42,398 --> 00:13:45,620
In code 1 plus the flow of log of x,

184
00:13:45,620 --> 00:13:50,145
since this is two then we know that
the flow of log of x is actually 1.

185
00:13:52,000 --> 00:13:55,720
So that 3 is still larger than 2 to the 1.

186
00:13:55,720 --> 00:14:00,040
So the difference is 1, and
the 1 is encoded here at the end.

187
00:14:01,460 --> 00:14:04,690
So that's why we have 101 for 3.

188
00:14:04,690 --> 00:14:11,554
Now similarly 5 is encoded as 110,
followed by 01.

189
00:14:12,970 --> 00:14:17,981
And in this case the unary code in code 3.

190
00:14:17,981 --> 00:14:25,445
And so this is a unary code 110 and
so the flow of log of x is 2.

191
00:14:25,445 --> 00:14:30,362
And that means we're going to
compute a difference between 5 and

192
00:14:30,362 --> 00:14:32,784
the 2 to the 2 and that's 1.

193
00:14:32,784 --> 00:14:35,803
And so we now have again 1 at the end.

194
00:14:35,803 --> 00:14:39,226
But this time we're going to use 2 bits,

195
00:14:39,226 --> 00:14:43,570
because with this level
of flow of log of x.

196
00:14:43,570 --> 00:14:51,040
We could have more numbers a 5, 6, 7 they
would all share the same prefix here, 110.

197
00:14:51,040 --> 00:14:53,210
So in order to differentiate them,

198
00:14:53,210 --> 00:14:57,690
we have to use 2 bits in
the end to differentiate them.

199
00:14:57,690 --> 00:15:03,670
So you can imagine 6 would be 10 here
in the end instead of 01 after 10.

200
00:15:04,710 --> 00:15:10,615
It's also true that the form of
a gamma code is always the first

201
00:15:10,615 --> 00:15:15,155
odd number of bits, and
in the center there is a 0.

202
00:15:15,155 --> 00:15:17,305
That's the end of the unary code.

203
00:15:18,335 --> 00:15:24,385
And before that or on the left side
of this 0, there will be all 1s.

204
00:15:24,385 --> 00:15:30,265
And on the right side of this 0,
it's binary coding or uniform coding.

205
00:15:32,550 --> 00:15:36,540
So how can you decode such code?

206
00:15:36,540 --> 00:15:39,866
Well you again first do unary coding.

207
00:15:39,866 --> 00:15:45,371
Once you hit 0, you have got the unary
code and this also tell you how

208
00:15:45,371 --> 00:15:50,408
many bits you have to read further
to decode the uniform code.

209
00:15:50,408 --> 00:15:53,693
So this is how you can
decode a gamma code.

210
00:15:53,693 --> 00:15:57,998
There is also a delta code that's
basically the same as a gamma code except

211
00:15:57,998 --> 00:16:01,340
that you replace the unary
prefix with the gamma code.

212
00:16:01,340 --> 00:16:04,980
So that's even less
conservative than gamma code

213
00:16:04,980 --> 00:16:08,910
in terms of wording the small integers.

214
00:16:08,910 --> 00:16:12,150
So that means, it's okay if you
occasionally see a large number.

215
00:16:14,100 --> 00:16:15,190
It's okay with delta code.

216
00:16:16,810 --> 00:16:23,210
It's also fine with the gamma code,
it's really a big loss for unary code.

217
00:16:23,210 --> 00:16:26,710
And they are all operating of course,

218
00:16:26,710 --> 00:16:32,360
at different degrees of favoring short or
favoring small integers.

219
00:16:32,360 --> 00:16:38,560
And that also means they would be
appropriate for a sorting distribution.

220
00:16:38,560 --> 00:16:41,720
But none of them is perfect for
all distributions.

221
00:16:41,720 --> 00:16:45,990
And which method works the best would
have to depend on the actual distribution

222
00:16:45,990 --> 00:16:47,610
in your dataset.

223
00:16:47,610 --> 00:16:49,660
For inverted index compression,

224
00:16:49,660 --> 00:16:52,990
people have found that gamma
coding seems to work well.

225
00:16:55,114 --> 00:16:58,340
So how to uncompress inverted index?

226
00:16:58,340 --> 00:16:59,900
I will just talk about this.

227
00:16:59,900 --> 00:17:02,920
Firstly, you decode
those encoded integers.

228
00:17:02,920 --> 00:17:10,877
And we just I think discussed the how we
decode unary coding and gamma coding.

229
00:17:10,877 --> 00:17:15,720
What about the document IDs that
might be compressed using d-gap?

230
00:17:15,720 --> 00:17:19,320
Well, we're going to do
sequential decoding so

231
00:17:19,320 --> 00:17:23,800
supposed the encoded I list is x1,
x2, x3 etc.

232
00:17:23,800 --> 00:17:28,384
We first decode x1 to obtain
the first document ID, ID1.

233
00:17:28,384 --> 00:17:29,845
Then we can decode x2,

234
00:17:29,845 --> 00:17:34,610
which is actually the difference between
the second ID and the first one.

235
00:17:34,610 --> 00:17:40,240
So we have to add the decoder
value of x2 to ID1 to recover

236
00:17:40,240 --> 00:17:45,630
the value of the ID at
this secondary position.

237
00:17:46,690 --> 00:17:50,420
So this is where you can
see the advantages of

238
00:17:50,420 --> 00:17:52,870
converting document IDs to integers.

239
00:17:52,870 --> 00:17:55,730
And that allows us to do
this kind of compression.

240
00:17:55,730 --> 00:17:59,590
And we just repeat until we
decode all the documents.

241
00:17:59,590 --> 00:18:04,004
Every time we use the document ID in
the previous position to help to recover

242
00:18:04,004 --> 00:18:06,257
the document ID in the next position.

243
00:18:08,871 --> 00:18:18,871
[MUSIC]