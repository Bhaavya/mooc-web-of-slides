<meta charset="utf-8"/>
<co-content>
 <h1 level="1">
  Week 7 Overview
 </h1>
 <p>
  From Week 7 to Week 12, the lectures are based on the Text Mining and
Analytics MOOC. During this week's lessons, you will receive an overview of natural language processing techniques and text
representation, which are the foundation for all kinds of text-mining
applications, and word association mining with a particular focus on mining one
of the two basic forms of word associations (i.e., paradigmatic relations).
 </p>
 <h2 level="2">
  Time
 </h2>
 <p>
  This module should take
  <strong>
   approximately 5 hours
  </strong>
  of dedicated time to complete, with its videos and assignments.
 </p>
 <h2 level="2">
  Activities
 </h2>
 <p>
  The activities for this module are listed below (with required assignments in bold):
 </p>
 <table columns="2" rows="4">
  <tr>
   <td>
    <p>
     <strong>
      Activity
     </strong>
    </p>
   </td>
   <td>
    <p>
     <strong>
      Estimated Time Required
     </strong>
    </p>
   </td>
  </tr>
  <tr>
   <td>
    <p>
     Week 7 Video Lectures
    </p>
   </td>
   <td>
    <p>
     2 hours
    </p>
   </td>
  </tr>
  <tr>
   <td>
    <p>
     <strong>
      Week 7 Graded Quiz
     </strong>
    </p>
   </td>
   <td>
    <p>
     1 hour
    </p>
   </td>
  </tr>
  <tr>
   <td>
    <p>
     <strong>
      Exam 1
     </strong>
    </p>
   </td>
   <td>
    <p>
     2 hours
    </p>
   </td>
  </tr>
 </table>
 <h2 level="2">
  Goals and Objectives
 </h2>
 <p>
  After you actively engage in the learning experiences in this module, you should be able to:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Explain some basic concepts in natural language processing.
   </p>
  </li>
  <li>
   <p>
    Explain different ways to represent text data.
   </p>
  </li>
  <li>
   <p>
    Explain the two basic types of word associations and how to mine paradigmatic relations from text data.
   </p>
  </li>
 </ul>
 <h2 level="2">
  Guiding Questions
 </h2>
 <p>
  Develop your answers to the following guiding questions while watching the video lectures throughout the week.
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    What does a computer have to do in order to understand a natural language sentence?
   </p>
  </li>
  <li>
   <p>
    What is ambiguity?
   </p>
  </li>
  <li>
   <p>
    Why is natural language processing (NLP) difficult for computers?
   </p>
  </li>
  <li>
   <p>
    What is bag-of-words representation?
   </p>
  </li>
  <li>
   <p>
    Why is this word-based representation more robust than representations derived from syntactic and semantic analysis of text?
   </p>
  </li>
  <li>
   <p>
    What is a paradigmatic relation?
   </p>
  </li>
  <li>
   <p>
    What is a syntagmatic relation?
   </p>
  </li>
  <li>
   <p>
    What is the general idea for discovering paradigmatic relations from text?
   </p>
  </li>
  <li>
   <p>
    What is the general idea for discovering syntagmatic relations from text?
   </p>
  </li>
  <li>
   <p>
    Why do we want to do Term Frequency Transformation when computing similarity of context?
   </p>
  </li>
  <li>
   <p>
    How does BM25 Term Frequency transformation work?
   </p>
  </li>
  <li>
   <p>
    Why do we want to do Inverse Document Frequency (IDF) weighting when computing similarity of context?
   </p>
  </li>
 </ul>
 <h2 level="2">
  Additional Readings and Resources
 </h2>
 <p>
  The following readings are optional:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    C. Zhai and S. Massung,
    <em>
     Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining.
    </em>
    ACM and Morgan &amp; Claypool Publishers, 2016. Chapters 1-4, Chapter 13.
   </p>
  </li>
  <li>
   <p>
    Chris Manning and Hinrich Schütze,
    <em>
     Foundations of Statistical Natural Language Processing.
    </em>
    MIT Press. Cambridge, MA: May 1999. Chapter 5 on collocations.
   </p>
  </li>
  <li>
   <p>
    Chengxiang Zhai,
    <em>
     Exploiting context to identify lexical atoms: A statistical view of linguistic context
    </em>
    . Proceedings of the International and Interdisciplinary Conference on Modelling and Using Context (CONTEXT-97), Rio de Janeiro, Brazil, Feb. 4-6, 1997, pp. 119-129.
   </p>
  </li>
  <li>
   <p>
    Shan Jiang and ChengXiang Zhai,
    <em>
     Random walks on adjacency graphs for mining lexical relations from big text data
    </em>
    . Proceedings of IEEE BigData Conference 2014, pp. 549-554.
   </p>
  </li>
 </ul>
 <h2 level="2">
  Key Phrases and Concepts
 </h2>
 <p>
  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Part of speech tagging
   </p>
  </li>
  <li>
   <p>
    Syntactic analysis
   </p>
  </li>
  <li>
   <p>
    Semantic analysis
   </p>
  </li>
  <li>
   <p>
    Ambiguity
   </p>
  </li>
  <li>
   <p>
    Text representation, especially bag-of-words representation
   </p>
  </li>
  <li>
   <p>
    Context of a word; context similarity
   </p>
  </li>
  <li>
   <p>
    Paradigmatic relation
   </p>
  </li>
  <li>
   <p>
    Syntagmatic relation
   </p>
  </li>
 </ul>
 <h2 level="2">
  Tips for Success
 </h2>
 <p>
  To do well this week, I recommend that you do the following:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.
   </p>
  </li>
  <li>
   <p>
    When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.
   </p>
  </li>
  <li>
   <p>
    It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.
   </p>
  </li>
  <li>
   <p>
    Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!
   </p>
  </li>
 </ul>
 <h2 level="2">
  Getting and Giving Help
 </h2>
 <p>
  You can get/give help via the following means:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Use the
    <strong>
     <a href="https://courserahelp.zendesk.com/hc/en-us/">
      Learner Help Center
     </a>
    </strong>
    to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the
    <strong>
     Contact Us!
    </strong>
    link available on each topic's page within the Learner Help Center.
   </p>
  </li>
  <li>
   <p>
    Use the
    <strong>
     <a href="https://www.coursera.org/learn/text-mining/discussions/forums/5mcKtywqEeaaVA48G_0dEQ">
      Content Issues
     </a>
    </strong>
    <strong>
    </strong>
    forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues
   </p>
  </li>
 </ul>
</co-content>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
