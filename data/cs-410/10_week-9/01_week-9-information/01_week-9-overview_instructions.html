<meta charset="utf-8"/>
<co-content>
 <h1 level="1">
  Week 9 Overview
 </h1>
 <p>
  During this week's lessons, you will learn topic analysis in depth, including mixture models and how they work, Expectation-Maximization (EM) algorithm and how it can be used to estimate parameters of a mixture model, the basic topic model, Probabilistic Latent Semantic Analysis (PLSA), and how Latent Dirichlet Allocation (LDA) extends PLSA.
 </p>
 <h2 level="2">
  Time
 </h2>
 <p>
  This module should take
  <strong>
   approximately 10 hours
  </strong>
  of dedicated time to complete, with its videos and assignments.
 </p>
 <h2 level="2">
  Activities
 </h2>
 <p>
  The activities for this module are listed below (with required assignments in bold):
 </p>
 <table columns="2" rows="5">
  <tr>
   <td>
    <p>
     <strong>
      Activity
     </strong>
    </p>
   </td>
   <td>
    <p>
     <strong>
      Estimated Time Required
     </strong>
    </p>
   </td>
  </tr>
  <tr>
   <td>
    <p>
     Week 9 Video Lectures
    </p>
   </td>
   <td>
    <p>
     2 hours
    </p>
   </td>
  </tr>
  <tr>
   <td>
    <p>
     <strong>
      Week 9
     </strong>
     <strong>
      Graded
     </strong>
     <strong>
      Quiz
     </strong>
    </p>
   </td>
   <td>
    <p>
     1 hour
    </p>
   </td>
  </tr>
  <tr>
   <td>
    <p>
     <strong>
      Programming Assignment 3
     </strong>
    </p>
   </td>
   <td>
    <p>
     6 hours
    </p>
   </td>
  </tr>
  <tr>
   <td>
    <p>
     <strong>
      Technology Review
     </strong>
     (due Week 14)
    </p>
   </td>
   <td>
    <p>
     1 hour
    </p>
   </td>
  </tr>
 </table>
 <h2 level="2">
  Goals and Objectives
 </h2>
 <p>
  After you actively engage in the learning experiences in this module, you should be able to:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Explain what a mixture of unigram language model is and why using a background language in a mixture can help “absorb” common words in English.
   </p>
  </li>
  <li>
   <p>
    Explain what PLSA is and how it can be used to mine and analyze topics in text.
   </p>
  </li>
  <li>
   <p>
    Explain the general idea of using a generative model for text mining.
   </p>
  </li>
  <li>
   <p>
    Explain how to compute the probability of observing a word from a mixture model like PLSA.
   </p>
  </li>
  <li>
   <p>
    Explain the basic idea of the EM algorithm and how it works.
   </p>
  </li>
  <li>
   <p>
    Explain the main difference between LDA and PLSA.
   </p>
  </li>
 </ul>
 <h2 level="2">
  Guiding Questions
 </h2>
 <p>
  Develop your answers to the following guiding questions while watching the video lectures throughout the week.
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    What is a mixture model? In general, how do you compute the probability of observing a particular word from a mixture model? What is the general form of the expression for this probability?
   </p>
  </li>
  <li>
   <p>
    What does the maximum likelihood estimate of the component word distributions of a mixture model behave like? In what sense do they “collaborate” and/or “compete”? Why can we use a fixed background word distribution to force a discovered topic word distribution to reduce its probability on the common (often non-content) words?
   </p>
  </li>
  <li>
   <p>
    What is the basic idea of the EM algorithm? What does the E-step typically do? What does the M-step typically do? In which of the two steps do we typically apply the Bayes rule? Does EM converge to a global maximum?
   </p>
  </li>
  <li>
   <p>
    What is PLSA? How many parameters does a PLSA model have? How is this number affected by the size of our data set to be mined? How can we adjust the standard PLSA to incorporate a prior on a topic word distribution?
   </p>
  </li>
  <li>
   <p>
    How is LDA different from PLSA? What is shared by the two models?
   </p>
  </li>
 </ul>
 <h2 level="2">
  Additional Readings and Resources
 </h2>
 <p>
  The following readings are optional:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    C. Zhai and S. Massung,
    <em>
     Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining
    </em>
    . ACM and Morgan &amp; Claypool Publishers, 2016. Chapter 17.
   </p>
  </li>
  <li>
   <p>
    Blei, D. 2012.
    <em>
     Probabilistic Topic Models
    </em>
    . Communications of the ACM 55 (4): 77–84. doi: 10.1145/2133806.2133826.
   </p>
  </li>
  <li>
   <p>
    Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
    <em>
     Automatic Labeling of Multinomial Topic Models
    </em>
    . Proceedings of ACM KDD 2007, pp. 490-499, DOI=10.1145/1281192.1281246.
   </p>
  </li>
  <li>
   <p>
    Yue Lu, Qiaozhu Mei, and Chengxiang Zhai. 2011.
    <em>
     Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA
    </em>
    . Information Retrieval, 14, 2 (April 2011), 178-203. doi: 10.1007/s10791-010-9141-9.
   </p>
  </li>
 </ul>
 <h2 level="2">
  Key Phrases and Concepts
 </h2>
 <p>
  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Mixture model
   </p>
  </li>
  <li>
   <p>
    Component model
   </p>
  </li>
  <li>
   <p>
    Constraints on probabilities
   </p>
  </li>
  <li>
   <p>
    Probabilistic Latent Semantic Analysis (PLSA)
   </p>
  </li>
  <li>
   <p>
    Expectation-Maximization (EM) algorithm
   </p>
  </li>
  <li>
   <p>
    E-step and M-step
   </p>
  </li>
  <li>
   <p>
    Hidden variables
   </p>
  </li>
  <li>
   <p>
    Hill climbing
   </p>
  </li>
  <li>
   <p>
    Local maximum
   </p>
  </li>
  <li>
   <p>
    Latent Dirichlet Allocation (LDA)
   </p>
  </li>
 </ul>
 <h2 level="2">
  Tips for Success
 </h2>
 <p>
  To do well this week, I recommend that you do the following:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.
   </p>
  </li>
  <li>
   <p>
    When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.
   </p>
  </li>
  <li>
   <p>
    It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.
   </p>
  </li>
  <li>
   <p>
    Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!
   </p>
  </li>
 </ul>
 <h2 level="2">
  Getting and Giving Help
 </h2>
 <p>
  You can get/give help via the following means:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Use the
    <strong>
     <a href="https://courserahelp.zendesk.com/hc/en-us/">
      Learner Help Center
     </a>
    </strong>
    to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the
    <strong>
     Contact Us!
    </strong>
    link available on each topic's page within the Learner Help Center.
   </p>
  </li>
  <li>
   <p>
    Use the
    <strong>
     <a href="https://www.coursera.org/learn/text-mining/discussions/forums/5mcKtywqEeaaVA48G_0dEQ">
      Content Issues
     </a>
    </strong>
    <strong>
    </strong>
    forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues.
   </p>
  </li>
 </ul>
 <p>
 </p>
 <p>
 </p>
 <p>
 </p>
</co-content>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
