think bayesian [ music ] hi welcome course first video see basic principle will use throughout course 
man run possible explanation hurry sport always run see dragon 
man run principle 1 use prior knowledge hurry sport always run see dragon let s learn example imagine run park see another man run ask run come four different explanation first hurry second sport third always run fourth see dragon principle 1 use prior knowledge previous experience know dragon exist exclude fourth option next consideration 
man run principle 1 use prior knowledge hurry sport always run see dragon low prior probability 
man run principle 2 choose answer explain observation hurry sport always run see dragon principle 2 choose answer explain observation imagine see wear sport suit case it´s unlikely he´s sport 
man run principle 2 choose answer explain observation hurry sport contradict datum always run see dragon exclude number two 
man run principle 3 avoid make extra assumption hurry sport always run see dragon principle 3 avoid make extra assumption last two option third option always run make lot extra assumption exclude 
man run principle 3 avoid make extra assumption hurry sport always run many assumption see dragon principle also know outcome racer 
man run hurry sport always run see dragon finally left one case hurry 
main principle principle 1 use prior knowledge principle 2 choose answer explain observation principle 3 avoid make extra assumption conclude have see three principle use prior knowledge choose answer explain observation finally avoid make extra assumption 
review probability continue let s review basic principle probability theory 
probability define probability follow way imagine source randomness example dice repeat experiment multiple time number experiment go infinity get probability fraction time event occur example would expect fair dice event throw five would frequency one-sixth event throw odd number 
random variable discrete continuous would somewhere around one-half consider two different type random variable depend value take discrete continuous discrete random variable either finite number value take example dice infinite count number time certain event happened example continuous random variable would tomorrow s temperature 
discrete probability mass function ( pmf ) convenient way find discrete distribution call probability mass function map number point refer probability example case will get point equal 1 produce 03 probability 05 probability 03 point probability also note point sum 1 
continuous probability density function ( pdf ) b convenient way define continuous distribution call probability density function assign non-negative value point compute probability point fall range example b integrate function give range give slide 
independence x independent marginal joint also need notion independence two run variable consider independent joint probability probability x equal product marginal probability x time probability 
draw deck 52 card let s see example imagine deck 52 card take randomly 2 card first random variable would picture draw first card second would picture draw second card kind variable dependent since impossible 
two coin p ( x1 = h x2 = ) = p ( x1 = h ) p ( x2 = ) take one card two time another example throw two coin independently probability first coin land head second would land tail equal product two probability random variable independent 
conditional probability probability x give happened joint conditional marginal last thing will need conditional probability want answer question probability x give something call happened give formula see slide probability x give equal joint probability p x marginal probability p 
conditional probability midterm final let s consider example imagine student want pass course two exam midterm final probability student pass midterm 04 probability student pass midterm final want find probability pass final give already pass midterm apply formula previous slide give value around 60 % 
chain rule p ( x ) = p ( x|y ) p ( ) will need two trick deal formula first call chain rule derive definition conditional probability joint probability x equal product x give probability y induction prove formula three variable 
chain rule p ( x ) = p ( x|y ) p ( ) p ( x z ) = p ( x|y z ) p ( z ) p ( z ) probability x z equal probability x give z probability give z finally probability z similar way obtain formula 
chain rule p ( x ) = p ( x|y ) p ( ) p ( x z ) = p ( x|y z ) p ( z ) p ( z ) p ( x1 xn ) = n i=1 p ( xi x1 xi 1 ) arbitrary number point would probability current point 
sum rule marginalization p ( x ) = z1 1 p ( x ) dy give previous point last rule call sum rule want find marginal distribution p ( x ) know joint probability p ( x ) integrate random variable give formula 
baye theorem — parameter — observation posterior likelihood evidence prior finally important formula course baye theorem want find probability theta give x theta parameter model example neural network parameter x observation example image deal definition conditional probability say ratio joint probability marginal probability p ( x ) also apply chain rule will get follow formula probability x give theta time probability theta probability x formula important component name probability theta call prior show us prior knowledge know parameter example know parameter distribute around term probability x give theta call likelihood show well parameter explain datum thing get probability theta give x call posterior probability parameter observe datum finally term denominator call evidence [ music ] 

bayesian approach statistic 
different approach statistic frequentist bayesian 
uncertainty interpretation bayesian frequentist subjective objective 
datum parameter bayesian frequentist ✓ random x fix ✓ fix x random 
datum parameter bayesian frequentist 
training bayesian frequentist maximum likelihood ✓b = arg max p ( x|✓ ) ✓ 
training frequentist bayesian baye theorem p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) 
classification training prediction 
regularization regularizer p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) 
regularization 5 p ( θ ) 4 3 2 1 0 00 02 04 06 08 θ ( probability ’ head ’ ) 10 
p ( θ ) regularization 30 25 20 15 10 05 00 00 02 04 06 08 θ ( probability ’ head ’ ) 10 
on-line learn new prior likelihood posterior prior 
on-line learn 
on-line learn 
on-line learn 
on-line learn 

define model 
bayesian * node random variable edge direct impact rain grass wet * ’ mix bayesian neural network 
bayesian * node random variable edge direct impact sprinkler rain grass wet * ’ mix bayesian neural network 
probabilistic model bn model joint probability variable parent r g 
probabilistic model bn model joint probability variable r g p ( r g ) = p ( g|s r ) · p ( s|r ) · p ( r ) 
probabilistic model bn model joint probability variable r g p ( r g ) = p ( g|s r ) · p ( s|r ) · p ( r ) 
probabilistic model bn model joint probability variable r g p ( r g ) = p ( g|s r ) · p ( s|r ) · p ( r ) 
probabilistic model bn model joint probability variable r g p ( r g ) = p ( g|s r ) · p ( s|r ) · p ( r ) 
probabilistic model bn model joint probability variable r g p ( r g ) = p ( g|s r ) · p ( s|r ) · p ( r ) 
naïve baye classifier … 
naïve baye classifier plate notation c n 

example thief & alarm 
model 
model 
model 
model 
model 
model thief earthquake e r alarm radio p ( e r ) = p ( ) p ( e ) p ( a|t e ) p ( r|e ) 
model thief earthquake e r alarm radio p ( e r ) = p ( ) p ( e ) p ( a|t e ) p ( r|e ) 
distribution e r 
технический слайд thief earthquake e r 
distribution prior e r 𝑷 ( 𝐓 = 𝟏 ) 𝟏𝟎 ( 𝟑 𝑷 ( 𝐄 = 𝟏 ) 𝟏𝟎 ( 𝟐 
distribution prior e r 𝑷 𝐀 = 𝟏 𝐓 𝐄 ) 𝑷 ( 𝐓 = 𝟏 ) 𝟏𝟎 ( 𝟑 𝑷 ( 𝐄 = 𝟏 ) 𝟏𝟎 ( 𝟐 𝐓=𝟎 𝐄=𝟎 0 𝐄=𝟏 10 𝐓=𝟏 1 1 
distribution prior e r 𝑷 𝐀 = 𝟏 𝐓 𝐄 ) 𝑷 ( 𝐓 = 𝟏 ) 𝟏𝟎 ( 𝟑 𝑷 ( 𝐄 = 𝟏 ) 𝟏𝟎 ( 𝟐 𝐓=𝟎 𝐄=𝟎 0 𝐄=𝟏 10 𝐓=𝟏 1 1 𝑷 𝐑 𝐄 ) 𝐄=𝟎 0 𝐄=𝟏 2 
distribution prior e r 𝑷 𝐀 = 𝟏 𝐓 𝐄 ) 𝑷 ( 𝐓 = 𝟏 ) 𝟏𝟎 ( 𝟑 𝑷 ( 𝐄 = 𝟏 ) 𝟏𝟎 ( 𝟐 𝐓=𝟎 𝐄=𝟎 0 𝐄=𝟏 10 𝐓=𝟏 1 1 𝑷 𝐑 𝐄 ) 𝐄=𝟎 0 𝐄=𝟏 2 
< < part marker board > > ( 8 mins ) 
correct model thief earthquake e r alarm radio 

example linear regression 
univariate normal 
univariate normal 
univariate normal 
univariate normal 
univariate normal mean 
univariate normal variance 
multivariate normal 
multivariate normal 
multivariate normal 
multivariate normal 
multivariate normal = 1 1 1 2 full parameter ( # $ ) & = 3 0 0 1 = 2 0 0 2 diagonal spherical parameter 𝐷 parameter 1 
linear regression 
linear regression 
linear regression 
least square problem l ( w ) = n x i=1 ( w xi 2 yi ) = w x 2 min w 
least square problem l ( w ) = n x ( w xi 2 yi ) = w x i=1 2 min w b = arg min l ( w ) w w 
model weight datum w x target 
model weight datum w x target p ( w y|x ) = p ( y|x w ) p ( w ) 
model weight datum w x target p ( w y|x ) = p ( y|x w ) p ( w ) p ( y|w x ) = n ( y|wt x 2 ) 
model weight datum w x target p ( w y|x ) = p ( y|x w ) p ( w ) p ( y|w x ) = n ( y|wt x p ( w ) = n ( w|0 2 ) 2 ) 
training технический слайд ( на доске ) 6 минут p ( w y|x ) = p ( y|x w ) p ( w ) p ( y|w x ) = n ( y|wt x p ( w ) = n ( w|0 p ( w|y x ) = 2 ) 2 ) 
training технический слайд ( на доске ) p ( w y|x ) = p ( y|x w ) p ( w ) p ( y|w x ) = n ( y|wt x p ( w ) = n ( w|0 2 2 ) ) p ( w y|x ) p ( w|y x ) = p ( w y|x ) p ( y|x ) 
training технический слайд ( на доске ) p ( w y|x ) = p ( y|x w ) p ( w ) p ( y|w x ) = n ( y|wt x p ( w ) = n ( w|0 2 2 ) ) p ( w y|x ) p ( w|y x ) = p ( w y|x ) p ( y|x ) p ( w y|x ) max w 
training технический слайд ( на доске ) p ( w y|x ) = p ( y|x w ) p ( w ) p ( y|w x ) = n ( y|wt x p ( w ) = n ( w|0 2 2 ) ) p ( w y|x ) p ( w|y x ) = p ( w y|x ) p ( y|x ) p ( w y|x ) max log p ( w y|x ) max w w 
training технический слайд ( на доске ) p ( w y|x ) = p ( y|x w ) p ( w ) p ( y|w x ) = n ( y|wt x p ( w ) = n ( w|0 2 ) log p ( w y|x ) max w 2 ) 
training технический слайд ( на доске ) p ( w y|x ) = p ( y|x w ) p ( w ) p ( y|w x ) = n ( y|wt x p ( w ) = n ( w|0 2 2 ) ) log p ( w y|x ) max w 1 2 w x 2 | 1 2 2 2 | max 2 w 
training технический слайд ( на доске ) p ( w y|x ) = p ( y|x w ) p ( w ) p ( y|w x ) = n ( y|wt x p ( w ) = n ( w|0 2 2 ) ) log p ( w y|x ) max w 1 2 w x 2 wt x | 1 2 2 2 | max 2 w y||2 + c||w||2 min w 

n i=1 
let us also consider multidimensional case problem xi d-dimensional vector draw multivariate normal distribution parameter mean µ ∈ rd covariance matrix σ ∈ rd×d   1 1 −1 exp − ( x − µ ) σ ( x − µ ) n ( x|µ σ ) = ( 2π ) 2 ( det σ ) 2 2 similarly log likelihood model take form n x1 nd n log p ( x|µ σ ) = − log ( 2π ) − log det σ − ( xi − µ ) σ−1 ( xi − µ ) = 2 2 2 i=1 n  nd n 1 x −1 − log ( 2π ) − log det σ − xi σ xi − µt σ−1 xi − xti σ−1 µ + µt σ−1 µ = 2 2 2 i=1  use fact σ symmetric thus σ−1 also symmetric lead −1 −1 µ σ xi = µ σ x t = xti  −1 σ = xti σ−1 µ  n  nd n 1 x −1 − log ( 2π ) − log det σ − xi σ xi − 2xti σ−1 µ + µt σ−1 µ 2 2 2 i=1 obtain mle µ need compute derivative expression respect vector µ set zero use follow vector differentiation rule ∂ ( ) = ∈ rd ∈ rd ∂y ∂ ( ay ) = 2ay ∂y ∈ rd symmetric matrix ∈ rd×d apply log likelihood expression get n n  x −1 ∂ 1x log p ( x|µ σ ) = − −2σ−1 xi + 2σ−1 µ = σ ( xi − µ ) = 0 ∂µ 2 i=1 i=1 µm l n 1 x xi = n i=1 

analytical inference 
posterior distribution 
posterior distribution likelihood prior p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) evidence 
posterior distribution likelihood prior p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) evidence 𝑷 𝑿 
posterior distribution likelihood prior p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) evidence 𝑷 𝑿 van gogh starry night 
posterior distribution likelihood prior p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) evidence 𝑷 𝑿 van gogh starry night van gogh starry night rhone 
maximum posteriori 
maximum posteriori 
maximum posteriori ✓mp = arg max p ( ✓|x ) ✓ 
maximum posteriori ✓mp = arg max p ( ✓|x ) ✓ ✓mp p ( x|✓ ) p ( ✓ ) = arg max ✓ p ( x ) 
maximum posteriori ✓mp = arg max p ( ✓|x ) ✓ ✓mp p ( x|✓ ) p ( ✓ ) = arg max ✓ p ( x ) ✓mp = arg max p ( x|✓ ) p ( ✓ ) ✓ 
maximum posteriori ✓mp = arg max p ( ✓|x ) ✓ ✓mp p ( x|✓ ) p ( ✓ ) = arg max ✓ p ( x ) ✓mp = arg max p ( x|✓ ) p ( ✓ ) ✓ optimization problem 
map problem invariant reparametrization 10 pf ( x ) f ( x ) 08 06 04 px 02 00 0 2 4 6 8 10 12 
map problem ’ use prior p ( xk ✓ ) pk 1 ( ✓ ) pk ( ✓ ) = p ( xk ) 
map problem ’ use prior p ( xk ✓ ) pk 1 ( ✓ ) pk ( ✓ ) = p ( xk ) p ( xk ✓ ) ( ✓ ✓mp ) pk ( ✓ ) = = ( ✓ p ( xk ) ✓mp ) 
map problem ⇤ map solution l ( ✓ ) = [ ✓ = ✓ ] min p ( θ|x ) ✓ 7 6 5 4 3 2 1 map 00 02 04 06 θ 08 10 
map problem objective solution l ( ✓ ) = [ ✓ = ✓⇤ ] min mode ✓ l ( ✓ ) = e ( ✓ ✓⇤ ) 2 min mean l ( ✓ ) = e|✓ ✓⇤ | min median ✓ ✓ 
map problem ’ compute credible region ✓mp = 1253 
map problem ’ compute credible region ✓mp = 1253 ✓mp = 1253 ± 1000 ✓mp = 1253 ± 0001 
summary pro • easy compute con • invariant reparametrization • ’ use prior • find untypical point • ’ compute credible interval 

conjugate distribution [ music ] video see another approach avoid compute evidence 
baye formula fix model choice p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) fix datum call conjugate distribution s baye formula note likelihood fix model evidence fix datum vary though prior choice let s select way s prove easier compute procedure 
conjugate prior 𝑃 𝜃 conjugate 𝑃 𝑋|𝜃 p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) prior say conjugate likelihood prior posterior lie family distribution example prior normal parameterized parameter mu sigma will expect posterior also normal 
example ( v ) = p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) mean end variance s example likelihood normal since around theta suffix square sigma square would conjugate prior well choose normal distribution 
two gaussian p ( x1 ) ⇠ n ( µ1 n ( x|µ 2 ) = p 2 1 ) 1 2⇡ 2 e p ( x2 ) ⇠ n ( µ2 ( x µ ) 2 2 2 = const · e 2 2 ) parabola two normal distribution happen multiply renormalize way integrate one 
two gaussian p ( x1 ) ⇠ n ( µ1 n ( x|µ 2 ) = p 2 1 ) 1 2⇡ 2 e p ( x2 ) ⇠ n ( µ2 ( x µ ) 2 2 2 = const · e 2 2 ) parabola actually will get normal distribution happen normal distribution actually constant time exponent power parabola multiply two long distribution two parabola sum get another parabola 
solution p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) solution problem would normal distribution normal theta parameterized mean variance sigma square posterior would also normal parameterized mu mean variance b square let s see work numerical example 
example p ( x|✓ ) p ( ✓ ) n ( x|✓ 1 ) n ( ✓|0 1 ) p ( ✓|x ) = = p ( x ) p ( x ) imagine likelihood normal center around theta variance 1 prior standard normal mean 0 variance 1 
example p ( x|✓ ) p ( ✓ ) n ( x|✓ 1 ) n ( ✓|0 1 ) p ( ✓|x ) = = p ( x ) p ( x ) p ( ✓|x ) e 1 2 ( x ✓ ) 2 e 1 2 2✓ drop constant get follow form product two exponent rearrange term 
example p ( x|✓ ) p ( ✓ ) n ( x|✓ 1 ) n ( ✓|0 1 ) p ( ✓|x ) = = p ( x ) p ( x ) p ( ✓|x ) e p ( ✓|x ) e 1 2 ( x ( ✓ ✓ ) 2 x 2 2 ) e 1 2 2✓ get exponent easily find mean variance 
example p ( x|✓ ) p ( ✓ ) n ( x|✓ 1 ) n ( ✓|0 1 ) p ( ✓|x ) = = p ( x ) p ( x ) p ( ✓|x ) e p ( ✓|x ) e 1 2 ( x ( ✓ ✓ ) 2 e x 2 2 ) x 1 p ( ✓|x ) = n ( | ) 2 2 1 2 2✓ mean x square x half variance would also half [ music ] 

distribution gamma 
gamma distribution 
gamma distribution b > 0 
gamma distribution b > 0 10 γ ( 1 1 ) 08 γ ( 15 1 ) 06 γ ( 2 1 ) γ ( 18 3 ) 04 02 00 0 2 4 6 8 10 
gamma distribution 10 γ ( 1 1 ) 08 γ ( 15 1 ) 06 γ ( 2 1 ) γ ( 18 3 ) 04 02 00 0 2 4 6 8 10 
gamma distribution ( n ) = ( n 10 1 ) γ ( 1 1 ) 08 γ ( 15 1 ) 06 γ ( 2 1 ) γ ( 18 3 ) 04 02 00 0 2 4 6 8 10 
технический слайд ( n ) = ( n г ( 5 ) = 24 10 1 ) γ ( 1 1 ) 08 γ ( 15 1 ) 06 γ ( 2 1 ) γ ( 18 3 ) 04 02 00 0 2 4 6 8 10 
statistic e [ ] = b mode [ ] = 1 b var [ ] = b2 
example run 5km ± 100m day 
технический слайд run 5km ± 100m day random variable can model normal 
example std run 5km ± 100m day expectation 
example run 5km ± 100m day 𝔼 𝑥 = + = 5 𝑉𝑎𝑟 𝑥 = ⁄+1 = 013 
example run 5km ± 100m day 𝔼 𝑥 = + = 5 𝑉𝑎𝑟 𝑥 = ⁄+1 = 013 ⇒ 𝑎 = 2500 b = 500 
example run 5km ± 100m day 𝔼 𝑥 = + = 5 𝑉𝑎𝑟 𝑥 = ⁄+1 = 013 ⇒ 𝑎 = 2500 b = 500 40 35 30 25 20 15 10 05 00 γ ( 2500 500 ) 0 1 2 3 4 km 5 6 7 
example normal precision 
precision precision = 1 variance 2 040 035 030 025 020 015 010 005 000 high precision low variance low precision high variance −100 −75 −50 −25 00 25 50 75 100 
precision n ( x|µ 2 ) = p 1 2⇡ 2 e ( x µ ) 2 2 2 
precision n ( x|µ n ( x|µ 2 ) = p 1 ) = 1 2⇡ 2 p p e 2⇡ e ( x µ ) 2 2 2 ( x µ ) 2 2 
functional form n ( x|µ 1 ) = p p e 2⇡ ( x µ ) 2 2 
functional form n ( x|µ 1 n ( x|µ 1 ) = ) p p e 2⇡ 1 2 e ( x b µ ) 2 2 
functional form n ( x|µ 1 n ( x|µ 1 ) = ) p p e 2⇡ 1 2 e p ( ) ( x µ ) 2 2 b 1 2 e b 
functional form n ( x|µ 1 n ( x|µ 1 ) = ) p p e 2⇡ 1 2 e p ( ) p ( x ) = p ( | ) p ( ) p ( x ) ( x µ ) 2 2 b 1 2 e b e ( + ( x µ ) 2 2 ) 
functional form n ( x|µ 1 n ( x|µ 1 ) = ) p p e 2⇡ 1 2 e p ( ) p ( x ) = p ( | ) p ( ) p ( x ) ( x µ ) 2 2 b 1 2 e b e ( + ( x µ ) 2 2 ) 
functional form n ( x|µ 1 n ( x|µ 1 ) = ) p p e 2⇡ 1 2 e p ( ) p ( x ) = p ( | ) p ( ) p ( x ) ( x µ ) 2 2 b 1 2 e b e ( + ( x µ ) 2 2 ) 
functional form n ( x|µ 1 n ( x|µ 1 ) = ) p p e 2⇡ 1 2 e ( x b µ ) 2 2 
functional form n ( x|µ 1 n ( x|µ 1 p ( ) 1 p p e 2⇡ ) = 1 2 ) e b e ( x b µ ) 2 2 
functional form n ( x|µ 1 n ( x|µ 1 p ( ) 1 p p e 2⇡ ) = 1 2 ) e b p ( ) = ( a b ) e ( x b µ ) 2 2 
gamma prior p ( ) = ( a b ) 1 e b 
gamma prior p ( ) = ( a b ) p ( x ) p ( | ) p ( ) 1 e b 
gamma prior 1 p ( ) = ( a b ) e b p ( x ) p ( | ) p ( ) p ( x ) 1 2 e ( x µ ) 2 2 · 1 e b 
gamma prior 1 p ( ) = ( a b ) e b p ( x ) p ( | ) p ( ) p ( x ) 1 2 p ( x ) 1 2 a ( x e 1 e µ ) 2 2 ( + · ( x 1 µ ) 2 2 ) e b 
gamma prior 1 p ( ) = ( a b ) e b p ( x ) p ( | ) p ( ) p ( x ) 1 2 p ( x ) 1 2 a ( x e p ( x ) = ( + 1 e 1 2 b µ ) 2 2 · ( + + ( x 1 µ ) 2 2 ) ( x µ ) 2 ) 2 e b 

distribution beta 
beta distribution b ( x|a b ) = 1 1 x ( 1 b ( b ) x ) b 1 
beta distribution b ( x|a b ) = 1 1 x ( 1 b ( b ) x 2 [ 0 1 ] b > 0 x ) b 1 
beta distribution b ( x|a b ) = 1 1 x ( 1 b ( b ) x ) b 1 x 2 [ 0 1 ] b > 0 30 b ( 01 01 ) 25 b ( 1 1 ) b ( 2 3 ) 20 b ( 8 4 ) 15 10 05 00 00 02 04 06 08 10 
beta distribution b ( x|a b ) = 30 1 1 x ( 1 b ( b ) x ) b 1 b ( 01 01 ) 25 b ( 1 1 ) b ( 2 3 ) 20 b ( 8 4 ) 15 10 05 00 00 02 04 06 08 10 
beta distribution b ( x|a b ) = 30 1 1 x ( 1 b ( b ) x ) b 1 b ( 01 01 ) 25 b ( 1 1 ) b ( 2 3 ) 20 b ( 8 4 ) 15 10 05 00 00 02 04 06 08 10 
statistic b ( x|a b ) = 1 1 x ( 1 b ( b ) ex = x ) b a+b 1 mode [ x ] = a+b 2 var [ x ] = ( a+b ) 2ab ( a+b 1 ) 1 
example movie rank 08 ± 01 
example технический слайд movie rank 08 ± 01 1 — best movie 0 — batman & robin 
example movie rank 08 ± 01 ex = var [ x ] = a+b = 08 ab ( a+b ) 2 ( a+b 1 ) = 012 
example movie rank 08 ± 01 ex = var [ x ] = a+b = 08 ab ( a+b ) 2 ( a+b 1 ) = 012 
example movie rank 08 ± 01 ex = var [ x ] = 4 a+b = 08 ab ( a+b ) 2 ( a+b 1 ) = 012 b ( 12 3 ) p ( x ) 3 2 1 0 00 02 04 06 x 08 10 
example bernoulli 
beta prior p ( x|✓ ) = ✓n1 ( 1 ✓ ) n0 
beta prior p ( x|✓ ) = ✓n1 ( 1 ✓ ) n0 p ( ✓ ) = b ( ✓|a b ) ✓a 1 ( 1 ✓ ) b 1 
beta prior p ( x|✓ ) = ✓n1 ( 1 ✓ ) n0 p ( ✓ ) = b ( ✓|a b ) ✓a p ( ✓|x ) p ( x|✓ ) p ( ✓ ) 1 ( 1 ✓ ) b 1 
beta prior p ( x|✓ ) = ✓n1 ( 1 ✓ ) n0 p ( ✓ ) = b ( ✓|a b ) ✓a 1 ( 1 ✓ ) b 1 p ( ✓|x ) p ( x|✓ ) p ( ✓ ) p ( ✓|x ) ✓n1 ( 1 ✓ ) n0 · ✓a 1 ( 1 ✓ ) b 1 
beta prior p ( x|✓ ) = ✓n1 ( 1 ✓ ) n0 p ( ✓ ) = b ( ✓|a b ) ✓a 1 ( 1 ✓ ) b 1 p ( ✓|x ) p ( x|✓ ) p ( ✓ ) p ( ✓|x ) ✓n1 ( 1 p ( ✓|x ) ✓n1 a ✓ ) n0 · ✓a 1 ( 1 1 ( 1 ✓ ) n0 b 1 ✓ ) b 1 
beta prior p ( x|✓ ) = ✓n1 ( 1 ✓ ) n0 p ( ✓ ) = b ( ✓|a b ) ✓a 1 ✓ ) b ( 1 1 p ( ✓|x ) p ( x|✓ ) p ( ✓ ) p ( ✓|x ) ✓n1 ( 1 p ( ✓|x ) ✓n1 a ✓ ) n0 · ✓a 1 ( 1 1 ( 1 ✓ ) n0 b p ( ✓|x ) = b ( n1 + n0 + b ) 1 ✓ ) b 1 
summary 
summary p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) 
summary p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) 
summary p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) 
summary p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) 
pro con pro 
pro con pro • exact posterior 
pro con pro • exact posterior • easy on-line learn eg p ( ✓|x ) = b ( n1 + n0 + b ) 
pro con pro • exact posterior • easy on-line learn eg p ( ✓|x ) = b ( n1 + n0 + b ) con 
pro con pro • exact posterior • easy on-line learn eg p ( ✓|x ) = b ( n1 + n0 + b ) con • conjugate prior may inadequate 

expectation maximization welcome week two practical bayesian method be alexander novikov week be go cover latent variable model 
week 2 • latent variable need use • common latent variable model ( cluster dimensionality reduction ) latent variable need apply real problem 
week 2 • latent variable need use • common latent variable model ( cluster dimensionality reduction ) • train expectation maximization algorithm • extension expectation maximization handle miss datum second topic week expectation maximization algorithm key topic course method train latent variable model see numerous extension expectation maximization algorithm follow week let s get start latent variable model 
latent ( hide ) variable variable never observe latent variable random variable unobservable training test phase latent hide latin example think example phenomenon like height length maybe speed measure directly other example incidence altruism ca nt measure altruism quantitative scale variable usually call latent motivate need introduce concept probabilistic modele let s consider follow example 
see company want hire employee 
bunch candidate 
candidate datum 
john high school grade 40 helen 37 jack 32 emma 29 example average high school grade 
john high university school grade grade 40 40 helen 37 36 jack 32 a emma 29 32 
john high university iq school grade score grade 40 40 120 helen 37 36 a jack 32 a 112 emma 29 32 a university grade 
john high university iq school grade score grade 40 40 120 phone interview 4 helen 37 36 a 4 jack 32 a 112 4 emma 29 32 a 4 maybe take iq test stuff like 
john high university iq school grade score grade 40 40 120 phone interview onsite interview 4 helen 37 36 a 4 jack 32 a 112 4 emma 29 32 a 4 also conduct phone screening interview hr manager call ask bunch simple question make sure understand company want bring person onsite make actual technical interview problem many candidate ca nt invite s expensive pay flight hotel stuff like natural idea arise let s predict onsite interview performance bring predict good enough predict good fit company well be business 
john high university iq school grade score grade 40 40 120 phone interview onsite interview 4 helen 37 36 a 4 jack 32 a 112 4 emma 29 32 a 4 sophia … high university iq school grade score grade 35 36 a phone interview 4 onsite interview 100 may historical datum bunch person know feature like grade iq score know onsite perform already conduct interview standard regression problem training datum set circle datum new person want predict onsite performance want bring onsite interview whose predict onsite performance good however two main problem ca nt apply standard regression method machine learn first miss value 
john high university iq school grade score grade 40 40 120 phone interview onsite interview 4 helen 37 36 a 4 jack 32 a 112 4 emma 29 32 a 4 sophia … high university iq school grade score grade 35 36 a phone interview 4 onsite interview 100 example nt know university grade jack nt attend university nt mean good fit company maybe never bother attend one nt want ignore jack want anyway predict meaningful onsite field performance score second reason nt want use standard regression method like linear regression neural network 
john high university iq school grade score grade 40 40 120 phone interview onsite interview 4 helen 37 36 a 4 jack 32 a 112 4 emma 29 32 a 4 may want quantify uncertainty prediction imagine person may predict performance really good certainly want bring onsite maybe even want hire right away other predict performance good someone predict performance example 50 may mean person good fit company may also mean be sure nt know anything ask algorithm predict performance return number nt mean anything case may want quantify uncertainty algorithm prediction algorithm quite sure person perform level 50 100 example may want bring onsite hand guy predict performance also 50 be really uncertain performance may want bring anyway see maybe nt know anything may good reason uncertainty may example lot miss value maybe datum little bit contradictory maybe algorithms nt used see person like two reason miss value want quantify uncertainty bring us need probabilistic model datum 
probabilistic model iq score gpa phone interview high school grade onsite interview discuss week one one usual way build probabilistic model start draw random variable understand connection random variable random variable correlate way particular case 
probabilistic model iq score gpa phone interview high school grade onsite interview look like everything connect everything like person s university grade high directly influence belief high school grade iq score true pair variable station possible edge like everything connect everything mean have fail capture structure probabilistic model end flexible least structure model possibly situation 
probabilistic model high school gpa iq phone interview onsite interview probability 10 10 1 4 100 0001 10 10 1 4 100 00023 … … … … 40 40 180 4 100 0000001 gpa iq p h o assume build probabilistic model datum assign probability possible combination feature exponentially many combination different university grade different iq score stuff like assign probability table probability billion entry s impractical treat parameter treat probability parameter something else always assume parametric model right 
probabilistic model x1 x2 gpa iq x5 phone school x4 onsite x3 exp ( | x ) p ( x1 x2 x3 x4 x5 ) = z say five random variable probability combination simple function example exponent linear function divide normalization constant case reduce model complexity lot like five parameter want train 
probabilistic model x1 x2 gpa iq x5 phone school x4 onsite x3 exp ( | x ) p ( x1 x2 x3 x4 x5 ) = z problem normalization constant normalize thing proper probability sum one consider normalization constant sum possible configuration gigantic sum consider billion possible configuration compute mean training inference impractical else well turn introduce new variable nt actually model world 
probabilistic model iq score gpa intelligence high school grade onsite interview phone interview call intelligence assume person internal hide property call intelligence example measure scale one intelligence directly cause iq score university grade stuff like course connection non-deterministic intelligent person bad day write test poorly direct causation intelligence directly cause observation assume model reduce model complexity lot raise lot feature model much simpler work 
probabilistic model x1 x2 iq gpa intell school x3 p ( x1 x2 x3 x4 x5 ) = x5 phone x4 onsite 100 x i=1 100 x i=1 p ( x1 x2 x3 x4 x5 | ) p ( ) = p ( x1 | ) p ( x5 | ) p ( ) 
probabilistic model x1 x2 iq gpa intell school x3 p ( x1 x2 x3 x4 x5 ) = x5 phone x4 onsite 100 x i=1 100 x i=1 p ( x1 x2 x3 x4 x5 | ) p ( ) = p ( x1 | ) p ( x5 | ) p ( ) be go write probabilistic model used rule sum probability s sum possible configuration give intelligence time prior probability conditional probability factorize product small probability structure model instead one huge table combination five different feature five small table assign probability pair like iq score give intelligence mean be able reduce model complexity model without reduce flexibility 
latent variable model pro • simpler model ( less edge ) 
latent variable model pro • simpler model ( less edge ) • fewer parameter summarize introduce latent variable may simplify model reduce number phase 
latent variable model pro • simpler model ( less edge ) • fewer parameter • latent variable sometimes meaningful consequence reduce number parameter positive feature latent variable sometimes interpretable example intelligence variable new person estimate intelligence scale one 100 example mean well s obvious nt know scale mean be even sure intelligence mean actual intelligence never tell model variable intelligence say variable anyway variable interpretable compare intelligence accord scale different person datum set 
latent variable model pro • simpler model ( less edge ) • fewer parameter • latent variable sometimes meaningful con • harder work downside latent variable model harder work training latent variable model rely lot math math week next video discuss method training latent variable model 

probabilistic cluster [ music ] discuss latent variable model cluster cluster imagine bank bunch customer 
cluster income debt want datum 
hard cluster represent customer two dimensional plane point datum want decompose customer three different cluster well example want find person spend money car make promotion car related loan something useful different retail company bank company like find meaningful subset customer work non supervised problem nt label raw datum raw axis usually cluster do hard way datum point assign color datum point orange belong orange cluster one blue sometimes person soft cluster 
soft cluster instead instead assign datum point particular cluster assign datum point probability distribution cluster example orange point top picture certainly orange probability distribution like almost 100 % belong orange cluster almost 0 belong rest point border orange blue kind settle example 40 % probability belong blue cluster 60 % probability belong orange cluster 0 green nt know cluster point actually belong instead assign datum point particular cluster assume datum point belong every cluster different probability build cluster method property treat everything probabilistically want well several reason first may want handle miss datum naturally another reason may want consider cluster probabilistic way tune hyperparamter usually want tune hyperparameter plot like consider bunch different value example hyperparameter number cluster 
hyperparameter tune gmm previous image free cluster try different amount like 5 particular value number cluster may train cluster model call gmm will discuss later detail be go plot training performance like blue line be plot log likelihood higher better see whenever increase number cluster actually performance training set improve kind usual thing hyperparameter cluster model think better s actually example put one cluster per datum point model loss optimal s meaningful solution problem consider validation performance model increase start increase number cluster stagnate somehow start decrease usual picture tune hyperparameter tune bunch model choose one perform best validation set probability small cluster turn thing hard assignment cluster well least s always 
hyperparameter tune gmm k-mean train one popular hard cluster algorithmic tenet think cluster better training validation loss nt meaningful way understand number cluster want performance validation set probabilistic way deal cluster also ideal example be sure whether want 20 cluster 60 80 give least something boundary reasonable variable hyperparmeter first reason may want consider probabilistic approach cluster 
generate new datum point junbo zhao https 160903126pdf second one may want build generative model datum treat everything probabilistically may sample new datum point model datum case customer mean simple new point traditional grid look like point used training set be point example image celebrity face sampling new image probability distribution mean generate fake celebrity image scratch kind fun application build probabilistic model datum 
summary want cluster datum soft way • allow tune hyper parameter • generative model datum summarize want build probabilistic model cluster may help us two way first may allow us tune hyper parameter may give us generative model datum next video will build latent variable model cluster [ music ] 

probabilistic model datum [ music ] previous video decide solve cluster problem make probabilistic model datum model datum probabilistically well nt know many distribution far 
probabilistic model datum know gaussian right week one learn feed parameter gaussian distribution datum set s reasonable thing turn gaussian best model kind datum recall decide datum consist several cluster group may far away even simple example see gaussian model datum point one big circle maybe ellipse case assign hyper mobility center circle s way gaussian work see center gaussian kind fall region cluster many datum point problem modele kind datum one gaussian model everything one big circle region cluster datum point gaussian assign hypergrowth region else better probabilistic model use well one gaussian nt work s use several like three example 
gaussian mixture model ( gmm ) case kind assume datum point come one three gaussian gaussian explain one cluster datum point put formally density datum point 
gaussian mixture model ( gmm ) p ( x | ✓ ) = ⇡1 n ( x|µ1 ⌃1 ) + ⇡2 n ( x|µ2 ⌃2 ) + ⇡3 n ( x|µ3 ⌃3 ) equal weight sum three gaussian density weight negative number sum 1 make actual probability distribution parameter theta weight three group parameter three gaussian location mu vector shape covariance matrix sigma notice succeed fitting kind model datum solve cluster problem datum point may find gaussian datum point come exactly alternative find cluster index may say point come one gaussian point one particular cluster model sometimes call gaussian mixture model gmm short great find model would like use positive negative feature model compare plain one gaussian 
gaussian mixture model ( gmm ) p ( x | ✓ ) = ⇡1 n ( x|µ1 ⌃1 ) + ⇡2 n ( x|µ2 ⌃2 ) + ⇡3 n ( x|µ3 ⌃3 ) ✓ = { ⇡1 ⇡2 ⇡3 µ1 µ2 µ3 ⌃1 ⌃2 ⌃3 } 
gaussian mixture model ( gmm ) gaussian flexibility gmm well obviously gaussian much less flexible gaussian mixture model allow us fit complicate dataset actually turn may fit almost probability distribution gaussian mixture model arbitrarily high accuracy well course always happen kind theorem worst case may use exponentially many gaussian s practical theorem anyway gaussian mixture model powerful flexible model downside obviously number parameter 
gmm vs guassian gaussian gmm flexibility # parameter parameter µ ⌃ { ⇡1 ⇡2 ⇡3 } { µ1 µ2 µ3 } { ⌃1 ⌃2 ⌃3 } increase number gaussian use datum increase number parameter texture right okay great decide use kind gaussian mixture model fit find parameter s bi mu sigma vector matrix well simplest way fit probability distribution use maximum likelihood estimation right 
training gmm max ✓ p ( x | ✓ ) = n i=1 p ( xi | ✓ ) want find value parameter maximise likelihood density datum set give parameter want maximise thing respect parameter s usually much learn assume datum set consist n datum point independent give parameter basically mean factorize likelihood likelihood equal product likelihood individual object well one thing 
training gmm max ✓ p ( x | ✓ ) = n i=1 p ( xi | ✓ ) simplify expression understand maximize substitute definition marginal likelihood xi give parameter used definition slide datum point density mixture gaussian density right okay kind optimization problem one thing forget constraint right 
training gmm max ✓ n n n p ( xp ( x | ✓ ) ✓ ) = p ( xi ( ⇡ | 1✓ ) n ( xi | µ1 ⌃1 ) + ) i=1 i=1 i=1 say weight pi non-negative sum otherwise actual probability distribution seem like be good go may use favorite stochastic optimization algorithm datum flow like item whatever would like use optimize thing find optimal parameter right well turn kind forget one important set constraint covariance [ inaudible ] sigma arbitrary imagine optimization logarithm propose use covariance matrix zero nt work nt define proper gaussian distribution gaussian distribution definition invert matrix compute determinant divide matrix 0s lot problem like division 0 stuff like s good idea assume covariance matrix anything actually set valid covariance matrix something call positive nt worry nt know s important right important part really hard constraint follow s hard adapt favorite stochastic rate descent always follow kind restraint maintain property matrix always positive semi-definite nt know efficiently problem nt know optimize thing least stochastic rate descent well turn even get constraint consider simpler model example say covariance matrix diagonal mean ellipsoid correspond gaussian rotate align axis case s much easier maintain constraint 
training gmm n n n n n ✓ ) | p ( x ( x1i n | µ ( x1 ⌃ ) 1 + ) ) + ) max p ( xp ( x | ✓ ) = | 1✓ ) n ( ⇡ 1 ( ⇡ max p ( x ✓ ) = | µ ⌃ ✓ ✓ i=1 i=1 i=1 i=1 i=1 subject ⇡1 + ⇡2 + ⇡3 = 1 ⇡k ⌃k 0 0 k = 1 2 3 actually use stochastic optimization example example used adam tune learn grade optimize thing see reasonable job blue curve performance item x-axis see epochs y-axis see log likelihood try maximize adam good job right s like ten e-book optimize thing something reasonable think real line ground come minor know probability distribution generate datum know optimal value log likelihood turn even case nt complicate constraint make much better exploit structure problem something be go discuss rest week something call expectation maximization apply work much better like iteration find value better ground truth probably overfitting anyway work good test set also summarize may two reason use stochastic gradient descent first may hard follow constraint may care like positive semidefinite covariance matrix second expectation maximization algorithm exploit structure program 
training gmm n n n n n ✓ ) | p ( x ( x1i n | µ ( x1 ⌃ ) 1 + ) ) + ) max p ( xp ( x | ✓ ) = | 1✓ ) n ( ⇡ 1 ( ⇡ max p ( x ✓ ) = | µ ⌃ ✓ ✓ i=1 i=1 i=1 i=1 i=1 subject ⇡1 + ⇡2 + ⇡3 = 1 ⇡k ⌃k 0 0 k = 1 2 3 sometimes much faster efficient general summary discuss gaussian mixture model flexible probability distribution solve cluster problem fit datum gaussian mixture model 
training gmm n n n n n ✓ ) | p ( x ( x1i n | µ ( x1 ⌃ ) 1 + ) ) + ) max p ( xp ( x | ✓ ) = | 1✓ ) n ( ⇡ 1 ( ⇡ max p ( x ✓ ) = | µ ⌃ ✓ ✓ i=1 i=1 i=1 i=1 i=1 subject ⇡1 + ⇡2 + ⇡3 = 1 ⇡k ⌃k 0 0 k = 1 2 3 sometimes s hard optimize stochastic gradient descent alternative will talk next video 
training gmm n n n n n ✓ ) | p ( x ( x1i n | µ ( x1 ⌃ ) 1 + ) ) + ) max p ( xp ( x | ✓ ) = | 1✓ ) n ( ⇡ 1 ( ⇡ max p ( x ✓ ) = | µ ⌃ ✓ ✓ i=1 i=1 i=1 i=1 i=1 subject ⇡1 + ⇡2 + ⇡3 = 1 ⇡k ⌃k 0 0 k = 1 2 3 
summary • gaussian mixture model flexible probability distribution • hard fit ( train ) sgd [ music ] 

gaussian mixture model ( gmm ) previous video decide use gaussian mixture model fit dataset solve cluster problem better general castigate descent video discuss intuition lead expectation maximization algorithm particular case recall density datum point gaussian mixture model weight sum three general many want gaussian division 
introduce latent variable p ( x | ✓ ) = ⇡1 n ( x|µ1 ⌃1 ) + ⇡2 n ( x|µ2 ⌃2 ) + ⇡3 n ( x|µ3 ⌃3 ) start need introduce latent variable make reasoning model much easier correlate variable well something like 
introduce latent variable p ( x | ✓ ) = ⇡1 n ( x|µ1 ⌃1 ) + ⇡2 n ( x|µ2 ⌃2 ) + ⇡3 n ( x|µ3 ⌃3 ) x say datum point generate used information latent variable exist s like one latent variable datum point x cause x explain something x reasonable thing assume take 3 radius 1 2 3 show us gaussian particular datum point come actually nt know datum point nt know gaussian come s latent variable right nt observe never training testing helpful know latent variable helpful understand something model later fit gaussian mixture model datum may find distribution latent variable even datum may ask question think value latent variable particular datum point answer question basically cluster right give us belief gaussian datum point come decide use kind latent variable model reasonable assume latent variable prior distribution pi s exactly weight gaussian 
introduce latent variable p ( x | ✓ ) = ⇡1 n ( x|µ1 ⌃1 ) + ⇡2 n ( x|µ2 ⌃2 ) + ⇡3 n ( x|µ3 ⌃3 ) x p ( = c | ✓ ) = ⇡c latent variable equal cluster number example one probability pi one likelihood density datum point x give know gaussian come 
introduce latent variable p ( x | ✓ ) = ⇡1 n ( x|µ1 ⌃1 ) + ⇡2 n ( x|µ2 ⌃2 ) + ⇡3 n ( x|µ3 ⌃3 ) x p ( = c | ✓ ) = ⇡c p ( x | = c ✓ ) = n ( x | µc ⌃c ) gaussian distribution come gaussian datum point know come gaussian number c s density gaussian thing parameter gaussian number c introduce kind latent variable model may look p x give theta represent model change model check still give general result original model write p x give theta 
introduce latent variable p ( x | ✓ ) = ⇡1 n ( x|µ1 ⌃1 ) + ⇡2 n ( x|µ2 ⌃2 ) + ⇡3 n ( x|µ3 ⌃3 ) x p ( = c | ✓ ) = ⇡c p ( x | = c ✓ ) = n ( x | µc ⌃c ) p ( x | ✓ ) = 3 x c=1 p ( x | = c ✓ ) p ( = c | ✓ ) give parameter latent variable model get s rule sum probability say p x give theta equal sum respect t marginalize respect possible value 1-3 assign join distribution p x equal likelihood time prior substitute definition prior likelihood summation understand last formula slide exactly equal first formula introduce latent variable way marginalize variable get exactly mean use latent variable model train observed datum x give us exactly result would get use original model let s try build intuition train latent variable model 
expectation maximization dataset { x1 xn } say dataset say s one dimensional datum point 1 n one number sublist illustration can goal find maximum likelihood estimation find parameter right find parameter well turn know source 
expectation maximization estimate parameter ✓ know value latent variable datum point find parameter sigma easy be go say blue datum point datum point know come first gaussian look separately older orange datum point fit one gaussian already know s fitting datum set blue point gaussian distribution see formula bottom slide s something already do week one mean know source know value latent variable s easy estimate parameter datum actually nt hard assignment 
expectation maximization estimate parameter ✓ source know easy p xi µ1 = # blue point blue rather soft assignment posterior distribution 
expectation maximization estimate parameter ✓ source know easy p p ( ti = 1 | xi ✓ ) xi µ1 = p p ( ti = 1 | xi ✓ ) 2 1 = p 2 p ( = 1 | x ✓ ) ( x µ ) 1 p p ( ti = 1 | xi ✓ ) mean datum point nt assume belong one cluster rather assume belong cluster simultaneously gaussian simultaneously different probability posterior p give x parameter probability also easy estimate parameter instead signing respect blue point average position get location blue gaussian will sum point weight posterior p = 1 1 datum point mean particular datum point completely blue certainly belong blue gaussian used blue datum point average weight datum point p = 1 0 mean datum point certainly orange nt use compute blue gaussian mean datum point example p = 1 08 mean datum point kind certain think belong blue gaussian s sure highly affect position blue gaussian little bit affect position orange gaussian will direct kind formula later strict consideration believe know posterior probably can easily estimate parameter way bottom line know source matter soft segment hard segment easily estimate parameter gaussian mixture model practice nt right nt know source 
expectation maximization ’ know source estimate source 
expectation maximization ’ know source give p ( x | = 1 ✓ ) = n ( 2 1 ) find p ( = 1 | x ✓ ) well turn know parameter gaussian location variance easily estimate source use baye rule 
expectation maximization ’ know source know parameter easy baye rule soft assignment posterior probability equal example blue gaussian particle datum point proportional join probability likelihood time prior know parameter theta easily compute likelihood prior give point easily compute posterior probability basically source basically assignment datum point cluster soft assignment think normalization constant problematic turn two value two probability distribution p give datum theta think two possible value explicitly normalize think signing respect two unnormalized probability s big deal summarize kind chicken egg problem 
expectation maximization chicken egg problem • need gaussian parameter estimate source • need source estimate gaussian parameter know gaussian parameter easily estimate source know source easily estimate gaussian parameter case well expectation maximisation algorithm particular case suggest us natural thing let s first 
expectation maximization chicken egg problem • need gaussian parameter estimate source • need source estimate gaussian parameter em algorithm start 2 randomly place gaussian parameter ✓ convergence repeat ) point compute p ( = c | xi ✓ ) xi look like come cluster c b ) update gaussian parameter ✓ fit point assign internalize parameter somehow randomly iteration loop let s repeat two step convergence first let s fix parameter assume true one estimate source next sub-step let s use source re-estimate parameter update belief parameter way repeating two step long enough hopefully obtain reasonable solution probability distribution fitting problem able fit gaussian mixture model datum 


gmm em example 
gmm em example 
gmm em example p ( ti = 1 | xi ✓ ) 
gmm em example p ( ti = 1 | xi ✓ ) 
gmm em example 001 p ( ti = 1 | xi ✓ ) ⇡ ⇡1 001 + 000002 
gmm em example p ( ti = 1 | xi ✓ ) 
gmm em example 
gmm em example 
gmm em example 
gmm em example 
gmm em example 
gmm em local maximum example 
gmm em local maximum example 
gmm em local maximum example 
gmm em local maximum example 
gmm em local maximum example 
gmm em local maximum example 
gmm em local maximum example 
gmm em local maximum example 
summary • gaussian mixture model flexible probabilistic approach cluster problem • expectation maximization algorithm train gmm faster stochastic gradient descent also handle complicate constraint • expectation maximization suffer local maxima ( exact solution np-hard ) 

general form expectation maximization [ sound ] previous model cover expectation maximization algorithm gaussian mixture model module will talk channel form expectation maximization allow train almost latent variable model think s go intense go worth end start need mathematical tool work inequality be go derive 
concave function f ( x ) x first tool connect concavity [ inaudible ] concave function well concave function f function two point b 
concave function f ( x ) x point property value 
concave function f ( x ) x def f ( x ) concave b ↵ f ( ↵a + ( 1 ↵ ) b ) 0↵1 ↵f ( ) + ( 1 ↵ ) f ( b ) function point blue point greater equal value segment connect f ( ) f ( b ) orange point put formally two point b weight alpha 0 1 mix two datum point weight alpha get point system f mixture greater equal mixture f s blue point greater equal orange point yet word n two point b whole segment connect f ( ) f ( b ) lay function f 
jensen ’ inequality f ( ↵a + ( 1 ↵ ) b ) ↵f ( ) + ( 1 ↵ ) f ( b ) concave function like logarithm example prove kind property point example free point free weight alpha 
jensen ’ inequality f ( ↵a + ( 1 ↵ ) b ) ↵f ( ) + ( 1 ↵1 + ↵2 + ↵3 = 1 ↵k f ( ↵1 a1 + ↵2 a2 + ↵3 a3 ) ↵ ) f ( b ) 0 ↵1 f ( a1 ) + ↵2 f ( a2 ) + ↵3 f ( a3 ) non-negative sum prove concave function f mixture free point weight alpha greater equal mixture f s generalize thing first call alpha s know non-negative phase sum 1 
jensen ’ inequality f ( ↵a + ( 1 ↵ ) b ) ↵f ( ) + ( 1 ↵1 + ↵2 + ↵3 = 1 ↵k f ( ↵1 a1 + ↵2 a2 + ↵3 a3 ) ↵ ) f ( b ) 0 ↵1 f ( a1 ) + ↵2 f ( a2 ) + ↵3 f ( a3 ) p ( = a1 ) = ↵1 p ( = a2 ) = ↵2 p ( = a3 ) = ↵3 let s say random variable g take three value a1 a2 a3 probability alpha rewrite inequality 
jensen ’ inequality f ( ↵a + ( 1 ↵ ) b ) ↵f ( ) + ( 1 ↵1 + ↵2 + ↵3 = 1 ↵k ff ( ↵ ( ↵11aa11++↵↵22aa22++↵↵33aa3 ) | { z } ep ( ) ↵ ) f ( b ) 0 ↵1 f ( a1 ) + ↵2 f ( a2 ) + ↵3 f ( a3 ) | { z } p ( = a1 ) = ↵1 p ( = a2 ) = ↵2 p ( = a3 ) = ↵3 ep ( ) f ( ) think f expect value greater equal expect value f t thing definition expect value average weight probability something call jensen s inequality 
jensen ’ inequality f ( ↵a + ( 1 ↵ ) b ) ↵f ( ) + ( 1 jensen ’ inequality f ep ( ) ep ( ) f ( ) ↵ ) f ( b ) summarize everything [ cough ] state concave function f probabilistic distribution p f expect value greater equal expect value f t hold true even free point take arbitrary many point like infinitely many case origin turn integral inequality still hold f expect value greater equal expect value f concave function f 
kullback–leibler divergence final mathematical concept need something call kullback-leibler divergence way measure difference two probabilistic distribution 
kullback–leibler divergence 04 02 00 5 0 5 x say two gaussian 
kullback–leibler divergence parameter difference 1 04 02 00 5 0 5 x one locate 0 one locate variance 1 okay measure different two gaussian well natural thinker measure distance parameter s one variance location different well may reasonable measure distance 
kullback–leibler divergence parameter difference 1 parameter difference 1 04 0040 02 0035 00 5 0 5 x 5 0 5 x let s consider two gaussian location 0 1 variance variance case difference parameter s intuitively seem like two gaussian green red one much closer first two build better way measure difference gaussian probability distribution well kullback-leibler divergence something try solve problem 
kullback–leibler divergence parameter difference 1 parameter difference 1 kl ( q1 k p1 ) = 05 kl ( q2 k p2 ) = 0005 04 0040 02 0035 00 5 0 5 x 5 0 5 x example particular distribution kl divergence first two one blue orange gaussian kl divergence within green red one kind reflect intuition second set gaussian much closer let s look definition kullback-leibler divergence 
kullback–leibler divergence parameter difference 1 parameter difference 1 kl ( q1 k p1 ) = 05 kl ( q2 k p2 ) = 0005 04 0040 02 0035 00 5 0 5 x kl ( q k p ) = 5 z 0 q ( x ) q ( x ) log dx p ( x ) 5 x may look scary little bit idea really simple s integral q ( x ) time logarithm something well integral q ( x ) expect value function expect value logarithm ratio right logarithm ratio try measure different two distribution current point x log scale basically different two distribution datum point point x-axis log scale be take expect value quantity be kind average across whole space across whole line line real number be get something like main difference two distribution 
kullback–leibler divergence kl ( q k p ) = z q ( x ) q ( x ) log dx p ( x ) give definition kullback-leibler divergence property will use follow video 
kullback–leibler divergence kl ( q k p ) = z q ( x ) q ( x ) log dx p ( x ) kl ( q k p ) = kl ( p k q ) first s non-symmetric right change p q will different expression one reason s proper distance distribution strict mathematical sense anyway s useful measure kind distance difference distribution 
kullback–leibler divergence kl ( q k p ) = z q ( x ) q ( x ) log dx p ( x ) kl ( q k p ) = kl ( p k q ) 2 kl ( q k q ) = 0 property make [ cough ] two distribution coincide distance really easy see substitute p q expression expect value logarithm q divide q q divide q 1 point logarithm expect value 0 also kullback-leibler divergence distribution 0 
kullback–leibler divergence kl ( q k p ) = z q ( x ) q ( x ) log dx p ( x ) kl ( q k p ) = kl ( p k q ) 2 kl ( q k q ) = 0 kl ( q k p ) 0 finally kl divergence non-negative distribution s kind easy prove use minus kl divergence 
kullback–leibler divergence kl ( q k p ) = z q ( x ) q ( x ) log dx p ( x ) kl ( q k p ) = kl ( p k q ) 2 kl ( q k q ) = 0 kl ( q k p ) proof 0 ✓ ◆ ✓ ◆ q p kl ( q k p ) = eq log = eq log p q z p p ( x )  log ( eq ) = log q ( x ) dx = 0 q q ( x ) look minus kl divergence equal expect value logarithm ratio put minus inside expect value expect value [ inaudible ] 
kullback–leibler divergence kl ( q k p ) = z q ( x ) q ( x ) log dx p ( x ) kl ( q k p ) = kl ( p k q ) 2 kl ( q k q ) = 0 kl ( q k p ) proof 0 ✓ ◆ ✓ ◆ q p kl ( q k p ) = eq log = eq log p q z p p ( x )  log ( eq ) = log q ( x ) dx = 0 q q ( x ) finally use progress logarithm minus logarithm something inverse logarithm inverse expect value logarithm may know logarithm concave function s kind easy prove look wikipedia believe anyway mean apply jensen s inequality jensen s inequality say expect value 
kullback–leibler divergence kl ( q k p ) = z q ( x ) q ( x ) log dx p ( x ) kl ( q k p ) = kl ( p k q ) 2 kl ( q k q ) = 0 kl ( q k p ) proof 0 ✓ ◆ ✓ ◆ q p kl ( q k p ) = eq log = eq log p q z p p ( x )  log ( eq ) = log q ( x ) dx = 0 q q ( x ) logarithm less equal logarithm expect value finally expect value p divide q equal 
kullback–leibler divergence kl ( q k p ) = z q ( x ) q ( x ) log dx p ( x ) kl ( q k p ) = kl ( p k q ) 2 kl ( q k q ) = 0 kl ( q k p ) proof 0 ✓ ◆ ✓ ◆ q p kl ( q k p ) = eq log = eq log p q z p p ( x )  log ( eq ) = log q ( x ) dx = 0 q q ( x ) integral q time p divide q q vanish integral b always 1 n distribution b s property distribution finally logarithm one 0 prove minus kl divergence always non-positive mean kl divergence non-negative 
kullback–leibler divergence kl ( q k p ) = z q ( x ) q ( x ) log dx p ( x ) summary way compare distribution proper distance kl ( q k p ) = kl ( p k q ) 2 kl ( q k q ) = 0 kl ( q k p ) 0 summarize kl divergence way compare distribution s non-symmetric equal 0 compare distribution s non-negative [ sound ] [ music ] 

general form expectation maximization 
general form expectation maximization ti xi 
general form expectation maximization ti p ( xi | ✓ ) = 3 x c=1 xi p ( xi | ti = c ✓ ) p ( ti = c | ✓ ) 
general form expectation maximization max ✓ p ( x | ✓ ) = log n i=1 p ( xi | ✓ ) 
general form expectation maximization max log p ( x | ✓ ) = log ✓ n i=1 p ( xi | ✓ ) 
general form expectation maximization max log p ( x | ✓ ) = log ✓ n i=1 p ( xi | ✓ ) 
general form expectation maximization max log p ( x | ✓ ) = log ✓ = n i=1 n x i=1 p ( xi | ✓ ) log p ( xi | ✓ ) 
general form expectation maximization log p ( x | ✓ ) = = n x log i=1 l ( ✓ ) n x i=1 3 x k=1 log p ( xi | ✓ ) p ( xi ti = k | ✓ ) 
general form expectation maximization log p ( x | ✓ ) = = = n x x i=1 i=1 n x i=1 log p ( xi | ✓ ) 33 x x log k c✓ ) log p ( x p ( x | ✓ ) = = l ( ✓ ) k=1 c=1 
general form expectation maximization log p ( x | ✓ ) = = = n x x i=1 i=1 n x i=1 log p ( xi | ✓ ) 33 x x log k c✓ ) log p ( x p ( x | ✓ ) = = l ( ✓ ) k=1 c=1 jensen ’ inequality l ( ✓ ) 
general form expectation maximization log p ( x | ✓ ) = = = n x x i=1 i=1 n x i=1 log p ( xi | ✓ ) 33 x x log k c✓ ) log p ( x p ( x | ✓ ) = = l ( ✓ ) k=1 c=1 l ( ✓ ) 
general form expectation maximization log p ( x | ✓ ) = = = n x x i=1 i=1 n x i=1 log p ( xi | ✓ ) 33 x x log k c✓ ) log p ( x p ( x | ✓ ) = = l ( ✓ ) k=1 c=1 l ( ✓ ) 
general form expectation maximization log p ( x | ✓ ) = = = n x x i=1 i=1 n x i=1 log p ( xi | ✓ ) 33 x x log k c✓ ) p ( x | ✓ ) log p ( x = = l ( ✓ ) k=1 c=1 
general form expectation maximization log p ( x | ✓ ) = = = n x x i=1 i=1 n x i=1 log p ( xi | ✓ ) 33 x x q ( ti = c ) log | ✓ ) ti = c | ✓ ) log p ( xi ti = k p ( x l ( ✓ ) k=1 c=1 q ( ti = c ) 
general form expectation maximization log p ( x | ✓ ) = = = n x x i=1 i=1 n x i=1 log p ( xi | ✓ ) 33 x x q ( ti = c ) log | ✓ ) ti = c | ✓ ) log p ( xi ti = k p ( x k=1 c=1 q ( ti = c ) l ( ✓ ) jensen ’ inequality log x c ↵c vc x c ↵c log ( vc ) 
general form expectation maximization log p ( x | ✓ ) = = = n x x n x i=1 log p ( xi | ✓ ) 33 x x q ( ti = c ) log | ✓ ) ti = c | ✓ ) log p ( xi ti = k p ( x i=1 k=1 c=1 i=1 n x 3 x l ( ✓ ) q ( ti = c ) p ( xi ti = c | ✓ ) q ( ti = c ) log q ( = c ) i=1 c=1 jensen ’ inequality log x c ↵c vc x c ↵c log ( vc ) 
general form expectation maximization log p ( x | ✓ ) = = = n x x n x i=1 log p ( xi | ✓ ) 33 x x q ( ti = c ) log | ✓ ) ti = c | ✓ ) log p ( xi ti = k p ( x i=1 k=1 c=1 i=1 n x 3 x l ( ✓ ) q ( ti = c ) p ( xi ti = c | ✓ ) q ( ti = c ) log q ( = c ) i=1 c=1 = l ( ✓ q ) 
general form expectation maximization log p ( x | ✓ ) l ( ✓ q ) q 
general form expectation maximization log p ( x | ✓ ) l ( ✓ q ) q 
general form expectation maximization log p ( x | ✓ ) l ( ✓ q ) q 
general form expectation maximization log p ( x | ✓ ) l ( ✓ q ) q q k+1 = arg max l ( ✓k q ) q 
general form expectation maximization log p ( x | ✓ ) l ( ✓ q ) q q k+1 = arg max l ( ✓k q ) q 
general form expectation maximization log p ( x | ✓ ) l ( ✓ q ) q q k+1 = arg max l ( ✓k q ) q 
general form expectation maximization log p ( x | ✓ ) l ( ✓ q ) q q k+1 = arg max l ( ✓k q ) q 
summary expectation maximization log p ( x | ✓ ) l ( ✓ q ) q variational lower bound e-step q k+1 = arg max l ( ✓k q ) q m-step ✓k+1 = arg max l ( ✓ q k+1 ) ✓ 

e-step detail 
e-step detail log p ( x | ✓ ) l ( ✓ q ) 
e-step detail log p ( x | ✓ ) l ( ✓ q ) k max l ( ✓ q ) e-step q 
e-step detail log p ( x | ✓ ) l ( ✓ q ) k max l ( ✓ q ) e-step q 
e-step summary log p ( x | ✓ ) e-step l ( ✓ q ) = x kl ( q ( ti ) k p ( ti | xi ✓ ) ) arg max l ( ✓k q ) = p ( ti | xi ✓ ) q ( ti ) 
e-step summary log p ( x | ✓ ) e-step l ( ✓ q ) = x kl ( q ( ti ) k p ( ti | xi ✓ ) ) arg max l ( ✓k q ) = p ( ti | xi ✓ ) q ( ti ) 

m-step detail 
m-step detail 
m-step detail const wrt ✓ 
m-step detail const wrt ✓ 
m-step detail 
m-step detail ( usually ) concave function wrt ✓ easy optimize 
expectation maximization algorithm k = 1 … e-step q k+1 ⇥ k = arg min kl q ( ) k p ( | x ✓ ) q q k+1 ( ti ) = p ( ti | xi ✓k ) m-step ✓k+1 = arg max eqk+1 log p ( x | ✓ ) ✓ ⇤ 
convergence guaranty 
convergence guaranty 
convergence guaranty log p ( x | ✓k+1 ) l ( ✓k+1 q k+1 ) l ( ✓k q k+1 ) = log p ( x | ✓k ) 
convergence guaranty log p ( x | ✓k+1 ) l ( ✓k+1 q k+1 ) l ( ✓k q k+1 ) = log p ( x | ✓k ) 
convergence guaranty log p ( x | ✓k+1 ) l ( ✓k+1 q k+1 ) l ( ✓k q k+1 ) = log p ( x | ✓k ) 
convergence guaranty log p ( x | ✓k+1 ) log p ( x | ✓k ) 
convergence guaranty log p ( x | ✓k+1 ) log p ( x | ✓k ) • iteration em ’ decrease objective ( good debug ) • guaranty converge local maximum ( saddle point ) 

summary expectation maximization congratulation make end form expectation maximization algorithm let s summarize property think algorithm first algorithm method training latent variable model 
summary expectation maximization • method training latent variable model ti xi model latent variable observed observed variable x apply algorithm train sometimes efficiently general purpose optimizational rhythm like stochastic gradient decent well one nice property allow 
summary expectation maximization • method training latent variable model • handle miss datum john high university iq school grade score grade 40 40 120 phone interview 4 helen 37 36 a 4 jack 32 a 112 4 emma 29 32 a 4 handle miss datum sometimes allow favorite machine learn algorithms probabilistic case sometimes possible treat machine learn algorithm probabilistic term say example observe variable x 3 example nt know value feature say s latent variable x s observed one latent apply train model latent variable get reasonable solution case well course always use heuristic treat miss value example throw away column row get rid miss value maybe substitute miss value zero average heuristic sense really none work although like guarantee always work usually practice apply problem miss value better substitute miss value zero throw away bunch datum 
summary expectation maximization • method training latent variable model • handle miss datum • sequence simple task instead one hard main idea expectation maximization algorithm want maximize margin log likelihood s hard s hard optimization problem instead build variation lower bound depend original parameter theta variational parameter q distribution introduce be try maximize lower bond respect theta q kind low core professional duration fix one fix q example maximize respect theta fix theta maximize respect q way be instead original complicate optimization problem be solve sequence different optimization problem practical case sometimes much simpler example gaussian mixture model solve substep s optimization program analytically almost timing condition complexity expectation maximization algorithm encourage converge critical point maybe optimal least local maximum settle point sometimes help handle complicate constraint 
summary expectation maximization • method training latent variable model • handle miss datum • sequence simple task instead one hard • guaranty converge original problem may constraint like matrix sigma positive semi-definite represent valid covariance matrix right s hard apply s hard force constraint favorite stochastic gradient descent method algorithm be substitute original problem sequence simpler one nth step course also enforce constraint since optimization task much simpler s usually much simpler enforce constraint nth step original problem gaussian mixture model case example nth step concave optimization really much easier original one add nontrivial positive semi-definitive constraint nt make problem much harder still solve nth step analytically 
summary expectation maximization • method training latent variable model • handle miss datum • sequence simple task instead one hard • guaranty converge • help complicate parameter constraint ⌃c 0 expectation maximization algorithm numerous extension talk later course distribution q pasteur distribution latent variable give datum parameter hard work may approximation first restrict set possible qs restrict set qs consider find best q kind strategic family will see small example next module mostly week three week five also nt want approximate want something least expect least occur average case use sampling instead work pasteur distribution directly try sample point approximate expect value m-step solve approximately week four sample complicate distribution negative feature give local maximum global one s kind expect s problem ca nt expect anyone give optimal solution reasonable time 
summary expectation maximization • method training latent variable model • handle miss datum • sequence simple task instead one hard • guaranty converge • help complicate parameter constraint • numerous extension - variational e-step restrict set possible q ( week 3 5 ) - sampling m-step ( week 4 ) also require little bit math get used kind make familiar kind duration become natural application new problem [ music ] 
summary expectation maximization • method training latent variable model • handle miss datum • sequence simple task instead one hard • guaranty converge • help complicate parameter constraint • numerous extension con • local maximum ( saddle point ) • require math ) 

application em 
gaussian mixture model revisit p ( x | ✓ ) = ⇡1 n ( x|µ1 ⌃1 ) + ⇡2 n ( x|µ2 ⌃2 ) + ⇡3 n ( x|µ3 ⌃3 ) ✓ = { ⇡1 ⇡2 ⇡3 µ1 µ2 µ3 ⌃1 ⌃2 ⌃3 } 
gaussain mixture model connection e-step em point compute q ( ti ) = p ( ti | xi ✓ ) 
gaussain mixture model connection e-step em point compute q ( ti ) = p ( ti | xi ✓ ) gmm point compute p ( ti | xi ✓ ) 
gaussain mixture model connection e-step em point compute q ( ti ) = p ( ti | xi ✓ ) gmm point compute p ( ti | xi ✓ ) m-step em update parameter maximize max eq log p ( x | ✓ ) ✓ gmm update gaussian parameter fit point assign p p ( ti = 1 | xi ✓ ) xi µ1 = p p ( ti = 1 | xi ✓ ) 
gaussain mixture model connection e-step em point compute q ( ti ) = p ( ti | xi ✓ ) gmm point compute p ( ti | xi ✓ ) m-step em update parameter maximize max eq log p ( x | ✓ ) ✓ gmm update gaussian parameter fit point assign p p ( ti = 1 | xi ✓ ) xi µ1 = p p ( ti = 1 | xi ✓ ) 

k-mean connection let s look popular algorithm hard cluster call k-mean see connect general form expectation maximization would really cool drive special case em feed general framework hard cluster datum set point want assign datum point cluster ca nt say suddenly point belong several cluster simultaneously datum point assign one one cluster okay 
k-mean randomly initialize parameter ✓ = { µ1 µc } k-mean algorithm suggest solve hard cluster problem follow way 
k-mean randomly initialize parameter ✓ = { µ1 µc } convergence repeat ) point compute closest centroid ci = arg min kxi c µc k2 first let s initialize parameter randomly parameter mean location cluster nt weight nt shape cluster sigma location 
k-mean randomly initialize parameter ✓ = { µ1 µc } convergence repeat ) point compute closest centroid ci = arg min kxi c µc k2 b ) update centroid p xi µc = # { ci = c } ci c iteration be repeating two sub step convergence first sub step find datum point find closest cluster assign datum point cluster datum point assign belong closest cluster closest mean accord euclidean distance next sub step k-mean suggest update parameter find example update first gaussian first cluster centroid mu_1 will find datum point assign first cluster first sub step find average center cluster update center repeat two thing convergence look carefully algorithm look really similar em right also random initialization iteration repeat two step also look really close stuff em first compute property datum point update parameter used property let s see somehow convert gaussian mixture model em apply obtain exactly k-mean algorithm prove k-mean special case em algorithm apply gaussian mixture model maybe model 
k-mean gmm perspective gmm k-mean • fix covariance identical ⌃c = first say nt additional parameter right let s say covariance matrix shape identical matrix mean shape gaussian 
k-mean gmm perspective gmm k-mean • fix covariance identical ⌃c = 1 • fix weight uniform ⇡c = # guassian uniform circle fix radius prior weight pi uniform also equal one divide number cluster way mu parameter gaussian mixture model kind restrict gaussian mixture model 
k-mean gmm perspective gmm k-mean • fix covariance identical ⌃c = 1 • fix weight uniform ⇡c = # guassian 1 p ( xi | ti = c ✓ ) = exp z 05kxi µc k2 density datum point give know cluster look like s normalization time exponent point five time euclidean distance x1 mu_c mu_c center gaussian number c prove exactly formula multidimensional gaussian multivarian gaussian identical covariance matrix sigma 
k-mean em perspective e-step q k+1 ⇥ k = arg min kl q ( ) k p ( | x ✓ ) aq ⇤ equal identical matrix 
k-mean em perspective e-step q k+1 ⇥ k = arg min kl q ( ) k p ( | x ✓ ) aq2q q set delta-function ⇤ let s look e step restrict gaussian mixture model e step expectation-maximization algorithm suggest 
k-mean em perspective e-step ⇥ k = arg min kl q ( ) k p ( | x ✓ ) aq2q q set delta-function p ti x ✓ ) 1 p ( q k+1 1 2 ⇤ find minimum kl divergence variational distribution q posterior distribution p ( ) give datum parameter go find hard assignment soft assignment right obtain gaussian mixture model datum point q say probability belong one cluster another one gaussian another want want derive k-mean model want step hard cluster well let s find best optimal q q among family simple qs let s look q among delta function delta function mean probability distribution take one value probability one value probability zero s really certain outcome let s imagine posterior distribution look like datum point belong cluster number two probability like 06 cluster number one probability think delta function approximation closest q accord kl distance among family delta function 
k-mean em perspective e-step ⇥ k = arg min kl q ( ) k p ( | x ✓ ) aq2q q set delta-function p ti x ✓ ) 1 q ( ti ) p ( q k+1 1 2 ⇤ well distribution like put probability mass class highest probability accord posterior restrict set possible qs e-step be actually approximate actual posterior distribution delta function way optimal q e-step look like right 
k-mean em perspective e-step q k+1 ( ti ) = ⇢ 1 ti = ci 0 otherwise s delta function s probability one t_i equal predefined value zero otherwise c_i predefined value well s probable configuration accord posterior distribution 
k-mean em perspective e-step q k+1 ( ti ) = ⇢ 1 ti = ci 0 otherwise ci = arg max p ( ti = c | xi ✓ ) = arg min kxi c c µ c k2 find c_i maximize posterior probability latent variable t_i give datum parameter theta 
k-mean em perspective e-step q k+1 ( ti ) = ⇢ 1 ti = ci 0 otherwise ci = arg max p ( ti = c | xi ✓ ) = arg min kxi c c 1 p ( ti | xi ✓ ) = p ( xi | ti ✓ ) p ( ti | ✓ ) z 1 = exp 05kxi µc k2 ⇡c z µ c k2 recall posterior distribution proportional full joint distribution x u p parameter also normalization constant nt care want maximize thing respect t_i respect c like respect value t_i 
k-mean em perspective e-step q k+1 ( ti ) = ⇢ 1 ti = ci 0 otherwise ci = arg max p ( ti = c | xi ✓ ) = arg min kxi c c 1 p ( ti | xi ✓ ) = p ( xi | ti ✓ ) p ( ti | ✓ ) z 1 = exp 05kxi µc k2 ⇡c z µ c k2 normalization nt change anything 
k-mean em perspective e-step q k+1 ( ti ) = ⇢ 1 ti = ci 0 otherwise ci = arg max p ( ti = c | xi ✓ ) = arg min kxi c c 1 p ( ti | xi ✓ ) = p ( xi | ti ✓ ) p ( ti | ✓ ) z 1 = exp 05kxi µc k2 ⇡c z µ c k2 recall thing equal normalization time x give exponent minus 05 time euclidean distance time prior agree prior uniform nt depend c also nt care maximize thing finally want maximize expression respect value t_i normalization one divide z pi c actually nt depend c throw away change anything maximize exponent minus something minimize something minimize 05 time euclidean distance finally expression like 
k-mean em perspective e-step q k+1 ( ti ) = ⇢ 1 ti = ci 0 otherwise min kx ci = arg max p ( ti µcc | 2xi ✓ ) = arg min kxi cc c µ c k2 optimal q e-step delta function take value arg minimum euclidean distance closest cluster center probability one probability zero anything else 
k-mean em perspective e-step q k+1 ( ti ) = ⇢ 1 ti = ci 0 otherwise min kx ci = arg max p ( ti µcc | 2xi ✓ ) = arg min kxi cc c exactly like k-mean µ c k2 okay exactly like k-mean mean restrict gaussian mixture model apply em restrict possible value variational distribution q - step delta function get exactly first sub step iteration let s derive formula m-step blackboard see s connect formula second sub step k-mean x m-step expectation maximization apply restrict gaussian mixture model particular choice q delta function 

ci c m-step expectation maximization algorithm restrict gaussian mixture model maximize follow expression find maximum sum respect individual object datum set training object expect value respect variational distribution q t_i joined log likelihood x_i t_i give parameter parameter suggest mu give mu maximize expression respect mu mu one mu number cluster maximize thing explicitly finally get zero set zero recall previous video maximize expression gaussian mixture model reuse result video one recall optimum parameter μ number c example follow one average respect datum datum point training datum set time variational distribution q weight ti equal c weight accord card gaussian c datum point xi divide normalization sum weight q ti equal c recall particular case e-step used delta function instead full variational distribution q q look like q ti equal one ti equal ci ci-star already pre-compute e-step zero otherwise continue expression simplify used definition q ti particular case thing sum respect subject object nt ti equal ci-star c contribute sum right object nt come gaussian number c weight q zero numerator denominator contribute sum thus write like say thing equal sum respect object gaussian number c ci-star equal c arrange datum point divide normalization number datum point s number ci-star equal c exactly formula k-mean algorithm second subset summarize proven m-step expectation maximization algorithm apply restrict gaussian mixture model without parameter leave location mu gaussian parameter use q restrict q delta function restrict e-step m-step obtain kind formula find update mu center datum point assign cluster number c use exactly k-mean 
k-mean em perspective m-step ✓k+1 = arg max eqk+1 log p ( x | ✓ ) ✓ µk+1 c p xi = # { ci = c } ci c exactly like k-mean summarize k-mean actually special case apply expectation maximization algorithm gaussian mixture model first gaussian mixture model kind restrict one say covariance matrix identical prior gaussian c uniform also use simplified e-step instead consider full posterior distribution e-step be approximate delta function kind make e-step m-step faster also restrict flexibility model efficiency em algorithm anyway kind restriction still give valid em algorithm still guarantee converge nice property s really cool take usual machine learn algorithm cluster probabilistic case sense show s special case em algorithm apply restrict gaussian mixture model used knowledge example change something k-mean know adapt training algorithm change example introduce miss datum way build miss datum k-mean without heuristic like set miss datum zero extend restrict em algorithm handle miss datum naturally actually kind trick simplify e-step consider 
k-mean em perspective summary k-mean actually em gaussian mixture model • fix covariance matrix ⌃c = • simplified e-step ( approximate p ( ti | xi ✓ ) delta function ) thus k-mean faster less flexible gmm restrict set distribution strongly connect something call variational difference will discuss week four 

ice cream conspiracy 
principal component analysis ice cream priсe $ 6 4 2 22 24 26 temperature c 
principal component analysis ice cream priсe $ 6 4 2 22 24 26 temperature c 
principal component analysis ice cream priсe $ 6 4 2 22 24 26 temperature c 
principal component analysis ice cream priсe $ 6 4 2 22 24 26 temperature c 
ice cream priсe $ principal component analysis 6 4 2 22 24 26 temperature c 2d 1d 
principal component analysis projection tobamovirus datum used pca full datum set ppca 136 miss value [ source tip m e & bishop c m ( 1999 ) probabilistic principal component analysis ] 
ice cream price $ principal component analysis 6 4 2 22 24 26 temperature c 2d p ( ti ) = n ( 0 ) 1d 
ice cream price $ principal component analysis 6 4 2 22 24 26 temperature c 2d 1d p ( ti ) = n ( 0 ) xi = ( 1 1 ) ti + ( 25 4 ) 
ice cream price $ principal component analysis 6 4 2 22 24 26 temperature c 2d 1d p ( ti ) = n ( 0 ) xi = w ti + b + ` ` ⇠ n ( 0 ⌃ ) 
ice cream price $ principal component analysis 6 4 2 22 24 26 temperature c 2d 1d p ( ti ) = n ( 0 ) xi = w ti + b + ` ` ⇠ n ( 0 ⌃ ) 
principal component analysis ti xi p ( ti ) = n ( 0 ) p ( xi | ti ✓ ) = n ( w ti + b ⌃ ) 
principal component analysis xi ti p ( ti ) = n ( 0 ) p ( xi | ti ✓ ) = n ( w ti + b ⌃ ) max p ( x | ✓ ) = ✓ n i=1 p ( xi | ✓ ) 
principal component analysis xi ti p ( ti ) = n ( 0 ) p ( xi | ti ✓ ) = n ( w ti + b ⌃ ) max p ( x | ✓ ) = ✓ n i=1 p ( xi | ✓ ) 
principal component analysis xi ti p ( ti ) = n ( 0 ) p ( xi | ti ✓ ) = n ( w ti + b ⌃ ) max p ( x | ✓ ) = ✓ = n i=1 p ( xi | ✓ ) yz p ( xi | ti ✓ ) p ( ti ) dti 
principal component analysis xi ti p ( ti ) = n ( 0 ) p ( xi | ti ✓ ) = n ( w ti + b ⌃ ) max p ( x | ✓ ) = ✓ = n i=1 p ( xi | ✓ ) yz p ( xi | ti ✓ ) p ( ti ) dti | { z } conjugacy n ( µi ⌃i ) 

principal component analysis 
principal component analysis e-step p ( xi | ti ✓ ) p ( ti ) q ( ti ) = p ( ti | xi ✓ ) = z 
principal component analysis e-step p ( xi | ti ✓ ) p ( ti ) q ( ti ) = p ( ti | xi ✓ ) = z 
principal component analysis e-step p ( xi | ti ✓ ) p ( ti ) q ( ti ) = p ( ti | xi ✓ ) = z e ) = n ( e µi ⌃ 
principal component analysis m-step max eq ( ) ✓ x log p ( xi | ti ✓ ) p ( ti ) 
principal component analysis m-step max eq ( ) ✓ = x x eq ( ti ) log log p ( xi | ti ✓ ) p ( ti ) ✓ 1 exp ( ) exp ( ) z ◆ 
principal component analysis m-step max eq ( ) ✓ = x + x x log p ( xi | ti ✓ ) p ( ti ) 1 log z eq ( ti ) log ( exp ( ) exp ( ) ) 
principal component analysis m-step max eq ( ) ✓ = x + x x log p ( xi | ti ✓ ) p ( ti ) 1 log z ✓ eq ( ti ) log exp ( ) exp ✓ t2i 2 ◆◆ 
principal component analysis m-step max eq ( ) ✓ = x + x x log p ( xi | ti ✓ ) p ( ti ) 1 log z ✓ eq ( ti ) log exp ✓ ( x w ti 2 2 b ) 2 ◆ exp ✓ t2i 2 ◆◆ 
principal component analysis m-step max eq ( ) ✓ = x + x x log p ( xi | ti ✓ ) p ( ti ) 1 log z ✓ ✓ ( x eq ( ti ) log exp | 2 w ti b ) 2 2 { z at2i cti d ◆ exp ✓ t2i 2 ◆◆ } 
summary probabilistic formulation pca • allow miss value • straightforward iterative scheme large dimensionality • mixture ppca • hyperparameter tune ( number component choose diagonal full covariance ) 

approximate inference [ sound ] hi welcome week three time see algorithm call variational inference algorithm compute posterior probability approximately first let s see even care compute approximate posterior 
analytical inference p ( x|z ) p ( z ) p ( z ) = p ( z|x ) = p ( x ) ⇤ see base formula help us compute posterior latent variable give datum denote posterior probability f * ( z ) 
analytical inference p ( x|z ) p ( z ) p ( z ) = p ( z|x ) = p ( x ) ⇤ • easy conjugate prior prior conjugate likelihood really easy compute posterior however case really hard 
analytical inference p ( x|z ) p ( z ) p ( z ) = p ( z|x ) = p ( x ) ⇤ • easy conjugate prior • hard otherwise one important case call variational autoencoder see week five variational autoencoder model likelihood neural network would normal distribution datum give mean neural network mu z variance neural network sigma square z case s conjugacy 
analytical inference p ( x|z ) p ( z ) p ( z ) = p ( z|x ) = p ( x ) ⇤ • easy conjugate prior • hard otherwise 2 example p ( x|z ) = n ( x|µ ( z ) ( z ) ) neural network ca nt compute posterior used baye formula 
need exact posterior 025 020 015 010 005 000 −8 −6 −4 −2 0 2 4 6 8 actually need exact posterior example distribution nt seem belong know family distribution 
need exact posterior 025 n 020 015 010 005 000 −8 −6 −4 −2 0 2 4 6 8 however can approximate used gaussian practical consideration really good approximation example would match mean variance [ inaudible ] shape week will see method help us find best approximation full posterior work follow first select family distribution q will call variational family example can family normal distribution arbitrary mean coherence matrix diagonal one next try approximate full posterior star z variational distribution q z find best match distribution used kl divergence try minimize kl divergence q * family distribution q depend q left obtain different result q s small true posterior lie will distribution match full posterior distance full posterior distribution approximate would exactly kl divergence select larger q posterior can match approximate distribution however larger qs harder compute variational inference example select q family possible distribution possible way compute posterior would example base formula have already see hard s one problem approach will see later will compute z star point however ca nt compute even one point will compute evidence p x sometimes really hard however nice property kl divergence will see s optimization objective s kl divergence variational distribution normalize posterior will denote p hat z equal evidence kl divergence definition integral k z time logarithm ratio first distribution second note take z integral get follow formula get two integral first kl divergence variational distribution normalize distribution integral actually see take logarithm z integral thing left integral q z [ inaudible ] finally will kl divergence plus constant since optimized subjective remove constant since depend variational distribution [ inaudible ] objective next video will see method call mean-field approximation [ sound ] 

variational inference 
variational inference select family distribution q 
variational inference select family distribution q 0 b example n ( µ @ 2 1 2 2 0 1 0c 2 ) 
variational inference ( технический слайд ) select family distribution q 0 b example n ( µ @ 2 1 2 2 0 1 0c 2 variational family ) 
variational inference select family distribution q 0 b example n ( µ @ 2 1 2 2 0 1 0c 2 ) find best approximation q ( z ) p⇤ ( z ) 
variational inference select family distribution q 0 b example n ( µ @ 2 1 2 2 0 1 0c 2 ) find best approximation q ( z ) p⇤ ( z ) 
choice variational family q ⇥ qk l k ⇤ ⇤ p p⇤ q q = p⇤ q p⇤ 2 q 
choice variational family ( технический слайд ) в стиле если q не накроет * то будет расстояние какое-то а если накроет то q и * совпадут + немного темнее фигуру q ⇥ qk l k ⇤ ⇤ p p⇤ q q q = p⇤ p⇤ 2 q larger q = > harder 
unnormalized distribution p ( x|z ) p ( z ) pb ( z ) p ( z ) = p ( z|x ) = = p ( x ) z ⇤ 
unnormalized distribution p ( x|z ) p ( z ) pb ( z ) p ( z ) = p ( z|x ) = = p ( x ) z ⇤ optimization 
unnormalized distribution p ( x|z ) p ( z ) pb ( z ) p ( z ) = p ( z|x ) = = p ( x ) z ⇤ optimization 
unnormalized distribution p ( x|z ) p ( z ) pb ( z ) p ( z ) = p ( z|x ) = = p ( x ) z ⇤ optimization 
unnormalized distribution p ( x|z ) p ( z ) pb ( z ) p ( z ) = p ( z|x ) = = p ( x ) z ⇤ optimization 
unnormalized distribution p ( x|z ) p ( z ) pb ( z ) p ( z ) = p ( z|x ) = = p ( x ) z ⇤ optimization 
unnormalized distribution p ( x|z ) p ( z ) pb ( z ) p ( z ) = p ( z|x ) = = p ( x ) z ⇤ optimization ⇥ ⇤ kl q ( z ) k pb ( z ) min z 

mean field 
mean field select family distribution q 
mean field select family distribution q q = { q | q ( z ) = q i=1 qi ( zi ) } 
mean field select family distribution q q = { q | q ( z ) = q qi ( zi ) } i=1 ⇤ find best approximation q ( z ) p ( z ) 
example p⇤ ( z1 z2 ) ⇡ q1 ( z1 ) q2 ( z2 ) 
example p⇤ ( z1 z2 ) ⇡ q1 ( z1 ) q2 ( z2 ) p⇤ ( z1 z2 ) = n ( 0 ⌃ ) 
example p⇤ ( z1 z2 ) ⇡ q1 ( z1 ) q2 ( z2 ) p⇤ ( z1 z2 ) = n ( 0 ⌃ ) ⇣ 2 ⌘ 1 0 q1 ( z1 ) q2 ( z2 ) = n ( 0 0 2 ) 2 
example p⇤ ( z1 z2 ) ⇡ q1 ( z1 ) q2 ( z2 ) p⇤ ( z1 z2 ) = n ( 0 ⌃ ) ⇣ 2 ⌘ 1 0 q1 ( z1 ) q2 ( z2 ) = n ( 0 0 2 ) 2 p⇤ ( z ) 
example p⇤ ( z1 z2 ) ⇡ q1 ( z1 ) q2 ( z2 ) p⇤ ( z1 z2 ) = n ( 0 ⌃ ) ⇣ 2 ⌘ 1 0 q1 ( z1 ) q2 ( z2 ) = n ( 0 0 2 ) 2 p⇤ ( z ) q ( z ) 
optimization kl ( q k p⇤ ) = kl ( i=1 q k p⇤ ) coordinate descend kl ( q k p⇤ ) min q1 kl ( q k p⇤ ) min q2 3 … min q1 q2 qd 
технический слайд ( < = 125 min ) на доске вывод основной формулы + conditional conj 

example is model 
is model yi 2 { 1 1 } n nbr ( ) 1 x p ( ) exp ( j 2 x yi j + j2nbr ( ) ( ) x bi ) 
is model ferromagnetic j > 0 1 x p ( ) exp ( j 2 antiferromagnetic x j2nbr ( ) j < 0 x yi j + bi ) ( ) 
normalization constant 1 p ( ) = ( ) z x n2 = ( ) 2 term n n 
mean field p ( ) ⇡ q ( ) = qi ( yi ) positive field nearby feel negative field 
технический слайд ( 5 минут на доску ) 
технический слайд 
технический слайд 
example 
example 
example 
optimization solution 
optimization solution capture statistic 
optimization solution capture statistic mode high probability 
optimization solution capture statistic mode high probability 
optimization solution ⇤ kl ( q k p ) = z q ( z ) q ( z ) log ⇤ dz p ( z ) 
optimization solution ⇤ kl ( q k p ) = z q ( z ) q ( z ) log ⇤ dz p ( z ) 
optimization solution ⇤ kl ( q k p ) = z q ( z ) q ( z ) log ⇤ dz = 1 p ( z ) 

topic modele module see really useful model call latent dirichlet allocation used topic modele video see topic modele 
recommender system example want build recommender system want recommend book example read sherlock holme book system would recommend example equatorial book 
topic modele 60 % detective 30 % adventure 10 % horror document distribution topic let s see would like extract feature book example would like feature correspond topic would topic detective topic adventure horror would try decompose book topic example sherlock holme book would 60 % detective book 30 % adventure maybe 10 % horror read mystery dark book define document distributional topic document assign probability meet topic example will meet detective topic probability 06 adventure topic probability 03 
topic modele sport 20 % football 10 % hockey 5 % goal 1 % score … 
topic modele sport 20 % football 10 % hockey 5 % goal 1 % score … economy 24 % money 9 % dollar 7 % euro 3 % bank … let s see topic example topic related sport would expect word like football hockey golf score 
topic modele sport 20 % football 10 % hockey 5 % goal 1 % score … economy 24 % money 9 % dollar 7 % euro 3 % bank … politic 10 % president 4 % usa 3 % union 1 % law … appear topic topic economy expect money dollar euro bank popular word finally politic will president usa union 
topic modele sport 20 % football 10 % hockey 5 % goal 1 % score … economy 24 % money 9 % dollar 7 % euro 3 % bank … politic 10 % president 4 % usa 3 % union 1 % law … football player usa salary dollar law name let s see use topic generate text example want generate sentence football player usa salary dollar 
topic modele sport 20 % football 10 % hockey 5 % goal 1 % score … economy 24 % money 9 % dollar 7 % euro 3 % bank … politic 10 % president 4 % usa 3 % union 1 % law … football player usa salary dollar word football clearly topic sport word usa topic politic word dollar topic economy 
topic modele sport economy 20 % football 10 % hockey 5 % goal 1 % score … 24 % money 9 % dollar 7 % euro 3 % bank … politic 10 % president 4 % usa 3 % union 1 % law … football player usa salary dollar topic distribution word define topic distribution word word vocabulary assign probability meet exact word sentence example word football topic sport happen 20 percent case word lower probability definition actually useful interpret topic matter see generate label topic generate distribution word see frequent word able assign label topic example nt show first word topic sport clearly say since word football hockey frequent topic probably sport 
similarity 60 % detective 30 % adventure = ⇣ 06 ⌘ = ⇣ 062 ⌘ 10 % horror 62 % detective 33 % adventure 5 % horror 03 01 033 005 a b would find topic document try convey similarity say whether book similar one read assign vector book example sherlock holme book vector 06 03 01 corresponding probability topic document call vector a similar way compute vector equatorial book 
distance = ⇣ 06 ⌘ = 03 01 ⇣ 062 ⌘ 033 005 euclidean distance ka bk2 = sx ( ai bi ) 2 ⇡ 0004 cosine similarity b co ( b ) = ⇡ 0997 kak · kbk two vector compute distance similarity use example euclidean distance cosine similarity next rank book accord similarity example recommend similar book case will recommendation person read sherlock holme book recommend equatorial book example 
goal construct topic sport 20 % football 10 % hockey … economy politic 24 % money 9 % dollar … 10 % president 4 % usa … assign topic text 60 % detective 30 % adventure 10 % horror two goal first construct topic collection document want find topic present want automatically fully unsupervised way collection text want find topic probability first goal second goal assign topic text would like decompose arbitrary book distribution topic example compose sherlock holme three topic probability exactly throughout module 

dirichlet distribution 
dirichlet distribution 
dirichlet distribution p k ✓k = 1 ( 0 0 1 ) ( 03 01 05 ) ( 1 0 0 ) ( 0 1 0 ) 
dirichlet distribution ↵k > 0 ( 0 0 1 ) ( 03 01 05 ) ( 1 0 0 ) ( 0 1 0 ) 
dirichlet distribution ↵ = ( 01 01 01 ) ↵ = ( 10 10 10 ) 
dirichlet distribution ↵ = ( 5 2 2 ) ↵ = ( 5 5 2 ) 
statistic dir ( ✓|↵ ) = e✓i = 1 b ( ↵ ) ↵i ↵0 cov ( ✓i ✓j ) = ↵0 = k p k=1 ↵k k q k=1 ✓k↵k 1 ↵i ↵0 [ i=j ] ↵i ↵j ↵20 ( ↵0 1 ) 
example massively multiplayer online role-playing game ( mmorpg ) player 1 strength stamina speed 
example massively multiplayer online role-playing game ( mmorpg ) player 2 strength stamina speed 
example massively multiplayer online role-playing game ( mmorpg ) player 3 strength stamina speed 
example massively multiplayer online role-playing game ( mmorpg ) average player strength ↵ = ( 3 1 1 ) ) stamina speed 
conjugate prior 𝑃 𝜃 conjugate 𝑃 𝑋|𝜃 p ( x|✓ ) p ( ✓ ) p ( ✓|x ) = p ( x ) 
multinomial likelihood p ( x|✓ ) = x1 n ✓ x1 xk 1 xk ✓k 
multinomial likelihood p ( x|✓ ) = x1 n ✓ x1 xk 1 p ( ✓ ) = dir ( ✓|↵ ) = xk ✓k 1 b ( ↵ ) k q k=1 ✓k↵k 1 
multinomial likelihood p ( x|✓ ) = x1 n ✓ x1 xk 1 p ( ✓ ) = dir ( ✓|↵ ) = p ( ✓|x ) k q k=1 xk ✓k 1 b ( ↵ ) ✓k↵k xk 1 k q k=1 ✓k↵k 1 
multinomial likelihood p ( x|✓ ) = x1 n ✓ x1 xk 1 p ( ✓ ) = dir ( ✓|↵ ) = p ( ✓|x ) k q k=1 xk ✓k 1 b ( ↵ ) ✓k↵k xk 1 k q k=1 ✓k↵k 1 

latent dirichlet allocation 
topic document distribution topic 
topic document distribution topic 80 % document 20 % ` cat ` dog 
topic document distribution topic 80 % document 20 % ` cat topic distribution word ` dog 
topic document distribution topic 80 % document 20 % ` cat ` dog topic distribution word topic “ сat ” 40 % 30 % 5 % 5 % topic “ dog ” 40 % 30 % 5 % 5 % 
text generation 80 % 20 % ` cat topic “ cat ” 40 % 30 % 5 % 5 % ` dog topic “ dog ” 40 % 30 % 5 % 5 % cat meow dog 
text generation 80 % 20 % ` cat topic “ cat ” 40 % 30 % 5 % 5 % ` dog topic “ dog ” 40 % 30 % 5 % 5 % cat meow dog 
text generation 80 % 20 % ` cat topic “ cat ” 40 % 30 % 5 % 5 % ` dog topic “ dog ” 40 % 30 % 5 % 5 % cat meow dog 
model distribution topic 
model topic word zd1 distribution topic zd2 … zdn zdn 2 { 1 } 
model topic word zd1 word wd1 distribution topic zd2 wd2 … zdn zdn 2 { 1 } wdn 2 { 1v } … wdnd 
lda model n 
lda model n document 
lda model n document generate topic probability 
lda model n word document generate topic probability 
lda model n word document select topic generate topic probability 
lda model n word document select topic select word topic generate topic probability 
lda model n word document select topic select word topic generate topic probability 
lda model p ( w z ⇥ ) = q d=1 p ( ✓d ) n qd n=1 p ( zdn ✓d ) p ( wdn zdn ) 
lda model p ( w z ⇥ ) = q d=1 p ( ✓d ) ⇠ dir ( ↵ ) p ( ✓d ) n qd n=1 p ( zdn ✓d ) p ( wdn zdn ) 
lda model p ( w z ⇥ ) = q d=1 p ( ✓d ) ⇠ dir ( ↵ ) p ( ✓d ) n qd n=1 p ( zdn ✓d ) p ( wdn zdn ) 
lda model p ( w z ⇥ ) = q p ( ✓d ) d=1 p ( ✓d ) ⇠ dir ( ↵ ) p ( wdn zdn ) = zdn wdn n qd n=1 p ( zdn ✓d ) p ( wdn zdn ) 
lda model p ( w z ⇥ ) = q p ( ✓d ) d=1 p ( ✓d ) ⇠ dir ( ↵ ) p ( wdn zdn ) = n qd n=1 p ( zdn ✓d ) p ( wdn zdn ) constraint zdn wdn p w tw 1 
lda model know unknown w datum parameter distribution word topic unknown z latent variable topic word unknown ⇥ latent variable distribution topic document 
технический слайд ( 15 мин на доску ) • вывод формул var em на доске 

extension & summary right final video let s see extension latent dirichlet allocation also summarize have see week 
sparsity document p ( w z ⇥ ) = q d=1 p ( ✓d ) ⇠ dir ( ↵ ) p ( ✓d ) n qd n=1 p ( zdn ✓d ) p ( wdn zdn ) ↵ ` ) topic document ↵ # ) less topic document ↵ select p ( w ↵ ) max ↵ joint distribution w z theta let s try find interpretation alpha alpha high will topic document however alpha small will topic document vary alpha obtain either sparse dense topic dense document alpha also select used maximum likelihood principle try maximize probability datum give alpha 
sparsity topic sparse prior p ( w z ⇥ ) = q t=1 p ( t• ) ⇠ dir ( ) p ( t• ) p ( w z | ) sparsity document also think sparsity topic make matrix phi random variable wo nt parameter anymore try model add extra term product topic distribute corresponding distribution model used dirichlet distribution parameter beta would show sparse topic example topic really sparse word topic example will topic football will hockey beta would bit larger will topic sport will football hockey board say like equal probability 
topic correlation logistic normal distribution p ( ✓d ) ⇠ p ( n ( µ ⌃ ) ) • • • • star astronomer universe galaxy • • • • laser optical light particle • • • • physics particle experiment physicist second extension call correlated topic model dirichlet distribution ca nt model beautiful correlation topic however case really useful example see topic laser document would probably also physics bit correlation topic appear frequently another one other do used so-called logistic normal distribution define fully show picture vary covariance matrix sigma correlation different object show picture 
dynamic topic model ⌧ 1 ⌧ ⌧ p ( bt• bt• ) ⇠ n ( bt• ⌧ 1 t• 2 ) ⌧ = softmax [ bt• ] nerve ca2 neuron 1880 2000 [ blei lafferty “ dynamic topic model “ https info6 dynamic_topic_modelsp df ] finally dynamic topic model example model collectional document old time word used frequently past used frequently however synonym used example word nerve frequently used 1880 s however used really rarely however word neuron used often model use follow trick will extra random variable b left actually obtain used normal distribution state position b add gaussian noise time random work space b concept obtain phi softmax b apply softmax get probability distribution used will dynamic value phi able track dynamic word topic throughout time 
summary • many topic interpretable • work well rare word • fast even huge text collection • multicore & distribute implementation • many feature add extension right have see latent dirichlet allocation module lot pro s con s main pro topic generate really interpretable really useful interpret model example trim collection document find collection example see lot topic adventure maybe collection adventure book also model work well rare word also fast even huge text collection really good implementation allow multicore distribute training also many feature useful add extension have see 

• mcmc — silver bullet probabilistic modele 
markov chain monte carlo ( mcmc ) • mcmc — silver bullet probabilistic modele • learn exploit specific problem speed mcmc hi welcome week four practical bayesian method week be go cover markov chain monte carlo kind silver bullet probabilistic programming allow train imprint almost model without much trouble 
markov chain monte carlo ( mcmc ) • mcmc — silver bullet probabilistic modele • learn exploit specific problem speed mcmc • understand limitation will discuss extension technique make faster particular problem expose structure problem will also discuss limitation like speed method 
monte carlo let s start discussion markov chain monte carlo monte carlo part monte carlo originated manhattan project give us atomic bomb quick dirty answer question mathematician arrive idea monte carlo simulation complicate model problem want predict something model instead think carefully know writing equation solve may simulate model lot lot time see example average simulation kind smooth prediction happen probably reason thing invented manhattan project s one first really huge scientific project already lot mean approximate integral kind simulation second already computer reasonably useful state actually assimilate stuff mentally automatically monte carlo algorithm turn pretty efficient make list top 10 algorithms 20th century 
monte carlo name monte carlo give name city monte carlo famous casino probably know everything manhattan project code name let s see small example be talk 
monte carlo estimate expect value sampling monte carlo apply really simple problem 
monte carlo estimate expect value sampling ⇡ = e [ x2 + 2  1 ] 4 say want approximate value pi know pi area unit circle circle radius one kind use information approximate idea consider rectangle zero one axe quarter circle appear rectangle exactly area pi divide four s one-fourth circle area equal integral basically say point whether point circle integrate thing along rectangle will get area compute integral s hard throw lot lot point 
monte carlo estimate expect value sampling ⇡ = e [ x2 + 2  1 ] 4 x 1 ⇡ [ x2s + ys2  1 ] s=1 xs y ⇠ u ( 0 1 ) unit rectangle see fraction point put circle quarter circle use approximation integral really see maybe best method approximate value pi example spend like 3000 sample assemble 3000 point pi approximate 31 best accuracy get much computation anyway method really simple s like two line code almost programming language general theme monte carlo simulation monte carlo usually give something really easy program really universally applicable lot lot problem scalable parallelization like 100 computer really easily parallelize thing 100 time efficient case many algorithms much harder parallelize sometimes monte carlo method slow compare alternative usually sit carefully weekend like write lot equation think may come better way solve problem monte carlo simulation sometimes sometimes s quick dirty approach give first approximation anyway s useful little bit general family technique 
monte carlo estimate expect value sampling x 1 ep ( x ) f ( x ) ⇡ f ( xs ) s=1 let s say want approximate expect value function f respect distribution p x use 
monte carlo estimate expect value sampling x 1 ep ( x ) f ( x ) ⇡ f ( xs ) s=1 xs ⇠ p ( x ) sample point end point distribution p x use average f x size approximation kind used sample average instead expect value thing lot nice various co-property like s unbiased meaning sample enough point get closer closer actual true answer least high probability anyway be [ inaudible ] fast converge 
monte carlo need estimate expect value monte carlo may ask question like care need approximate expect value well permalink programming probably reason let s consider two first want full bayesian inference 
monte carlo need estimate expect value • full bayesian inference ( see week 1 ) p ( | x ytrain xtrain ) z = p ( | x w ) p ( w | ytrain xtrain ) dw = ep ( w|ytrain xtrain ) p ( | x w ) like cover little bit week one instead pragmatic model find best failure parameter may want tweak parameter latent variable new object x want predict s label example training datum set x train train 
monte carlo need estimate expect value • full bayesian inference ( see week 1 ) p ( | x ytrain xtrain ) z = p ( | x w ) p ( w | ytrain xtrain ) dw = ep ( w|ytrain xtrain ) p ( | x w ) may want integrate latent variable like imagine example x image want predict like whether s image cat dog p give x w maybe example neural network weight w take input image x output distribution label y instead used one neural network kind optimal best set weight w be consider possible neural network architecture possible value weight s w consider neural network pass image neural network write answer p average prediction weight give pasteur s division weight p w give training datum set s like infinitely large ensemble neural network weight ensemble give pasteur s distribution p w give training datum set p w give training datum set give idea well weight behave weight neural network value higher reasonable weight lower weight nt make sense way be full base inference be benefit probabilistic modele like example predict uncertainty prediction well basically instead fix value weight distribution weight thing course intractable complicate function like neural network compute integral analytically approximation one way say thing expect value neural network 
monte carlo need estimate expect value • full bayesian inference ( see week 1 ) p ( | x ytrain xtrain ) z = p ( | x w ) p ( w | ytrain xtrain ) dw = ep ( w|ytrain xtrain ) p ( | x w ) neural network output compute expect failure neural network output respect distributional weight approximate thing monte carlo sample p w give training datum set one example monte carlo useful approximate expect value probabilistic modele may ask find posterior distribution weight p w training datum set 
monte carlo need estimate expect value • full bayesian inference ( see week 1 ) p ( | x ytrain xtrain ) z = p ( | x w ) p ( w | ytrain xtrain ) dw = ep ( w|ytrain xtrain ) p ( | x w ) p ( ytrain | xtrain w ) p ( w ) p ( w | ytrain xtrain ) = z well posterior distribution proportional joint distribution likelihood p train give x train w time prior w divide normalization constant numerator easy defined model can example defined p w prior normal likelihood output neural network compute thing value x w denominator normalization constant z much harder contain something go site ca nt ever compute s kind general situation know distribution want sample normalization constant 
monte carlo need estimate expect value • full bayesian inference ( see week 1 ) • m-step em-algorithm ( see week 2 ) right example kind expect value approximation useful probabilistic modele m-step expectation maximization algorithm cover week two e step expectation maximization find posterior distribution latent variable give datum set parameter 
monte carlo need estimate expect value • full bayesian inference ( see week 1 ) • m-step em-algorithm ( see week 2 ) max eq log p ( x | ✓ ) ✓ m-step want maximize expect value algorithm joint probability respect parameter theta expect value respect posterior distribution q posterior distribution q hard work ca nt integrate thing analytically thing first approximate q lay simple class like factorize distribution s previous week week three variational inference be approximate posterior compute integral analytically simplified posterior use exact posterior sample datum set sample set latent variable value approximate expect value average respect sample maximize approximation instead true expect value two example need expect [ inaudible ] probabilistic modele monte carlo simulation help follow video go use be go use formulation problem like want learn sample complicate probabilistic distribution know normalization constant always write like p x meaning x parameter want sample first example full bayesin interference p w give datum second example p latent variable give datum case full ingredient write know p x meaning [ inaudible ] distribution want sample 

sampling 1d distribution 
1d sampling ( discrete ) p 06 03 01 a1 a2 a3 
1d sampling ( discrete ) p 06 03 01 a1 a2 a3 always sample uniform u [ 0 1 ] 
1d sampling ( discrete ) p 06 03 01 a1 0 a2 a3 06 07 1 
1d sampling ( discrete ) p 06 03 01 a1 0 a2 a3 06 07 1 
1d sampling ( discrete ) p 06 03 01 a1 0 a2 a3 06 07 1 
1d sampling ( discrete ) p 06 03 01 a1 a2 a3 r ⇠ u [ 0 1 ] 0 06 07 1 
1d sampling ( discrete ) p 06 03 01 a1 a2 a3 r r ⇠ u [ 0 1 ] 0 06 07 1 
summary 1d discrete distribution finite number value easy 
summary 1d discrete distribution finite number value easy least number value < 100 000 
continuous sampling 
1d sampling ( continuous ) sampling gaussian distribution 
1d sampling ( continuous ) sampling gaussian distribution = 12 x i=1 xi 6 xi ⇠ u [ 0 1 ] 
1d sampling ( continuous ) sampling gaussian distribution = 12 x i=1 xi 6 xi ⇠ u [ 0 1 ] p ( z ) ⇡ n ( 0 1 ) 
1d sampling ( continuous ) sampling gaussian distribution call library function j z = numpyrandomrandn ( ) 
1d sampling ( continuous ) 
1d sampling ( continuous ) q ( x ) = n ( 1 32 ) p ( x )  2q ( x ) 
1d sampling ( continuous ) x e ⇠ q ( x ) 
1d sampling ( continuous ) x e ⇠ q ( x ) 
1d sampling ( continuous ) x e ⇠ q ( x ) ⇠ u [ 0 2q ( e x ) ] p ( x ) accept x e probability 2q ( x ) 
1d sampling ( continuous ) x e ⇠ q ( x ) ⇠ u [ 0 2q ( e x ) ] p ( x ) accept x e probability 2q ( x )  p ( x ) 
1d sampling ( continuous ) x e ⇠ q ( x ) ⇠ u [ 0 2q ( e x ) ] p ( x ) accept x e probability 2q ( x )  p ( x ) 
1d sampling ( continuous ) x e ⇠ q ( x ) ⇠ u [ 0 2q ( e x ) ] p ( x ) accept x e probability 2q ( x )  p ( x ) 
1d sampling ( continuous ) p ( x )  q ( x ) 1 accept point average 
1d sampling ( continuous ) pb ( x )  q ( x ) z 
1d sampling ( continuous ) pb ( x )  zm | { z } q ( x ) f 
summary pro • work distribution ( even unnormalized ) 
summary pro • work distribution ( even unnormalized ) con • q p different ( large ) reject point 
summary pro • work distribution ( even unnormalized ) con • q p different ( large ) reject point • large d-dimensional distribution 

markov chain monte carlo ( mcmc ) 
markov chain 
markov chain 05 03 05 07 
markov chain 05 03 05 07 ( l l ) = 03 ( r l ) = 05 ( l r ) = 07 ( r r ) = 05 
markov chain 05 03 05 07 timestamp 1 log l 
markov chain 05 03 05 07 timestamp 2 log l r 
markov chain 05 03 05 07 timestamp 3 log l r r 
markov chain 05 03 05 07 timestamp 4 log l r r l 
markov chain 05 03 05 07 timestamp 5 log l r r l r 
markov chain 05 03 05 07 x1 p ( left ) p ( right ) 1 0 
markov chain 05 03 05 07 x1 x2 p ( left ) p ( right ) 1 03 0 07 
markov chain 05 03 05 07 x1 x2 x3 p ( left ) p ( right ) 1 03 0 07 
markov chain 05 03 05 07 x1 x2 x3 p ( left ) p ( right ) 1 03 0 07 p ( x3 ) = p ( x3 | x2 = l ) p ( x2 = l ) + p ( x3 | x2 = r ) p ( x2 = r ) 
markov chain 05 03 05 07 p ( left ) x1 x2 x3 p ( right ) 0 1 03 07 032 + 07 · 05 p ( x3 ) = p ( x3 | x2 = l ) p ( x2 = l ) + p ( x3 | x2 = r ) p ( x2 = r ) 
markov chain 05 03 05 07 p ( left ) x1 x2 x3 p ( right ) 0 1 03 07 03 · 07 + 07 · 05 032 + 07 · 05 p ( x3 ) = p ( x3 | x2 = l ) p ( x2 = l ) + p ( x3 | x2 = r ) p ( x2 = r ) 
markov chain 05 03 05 07 x1 x2 x3 p ( left ) p ( right ) 1 03 044 0 07 056 
markov chain 05 03 05 07 x1 x2 x3 p ( left ) p ( right ) 1 03 044 0 07 056 ⇡ 042 ⇡ 058 
markov chain 05 03 05 07 l r r l r l l 
markov chain 05 03 05 07 l r r l r l l 
markov chain 05 03 05 07 l r r l r l l l r r l r l r 
markov chain 05 03 05 07 l r r l r l l l r r l r l r l r l r r r r l r r l l l r l l r l r r l p ( l ) ⇡ 042 p ( r ) ⇡ 058 
markov chain 10 lily billion 
markov chain 10 lily billion maybe frog position continuous 
markov chain 10 lily billion maybe frog position continuous still sample 
used markov chain • want sample p ( x ) 
used markov chain • want sample p ( x ) • build markov chain converge p ( x ) 
used markov chain • want sample p ( x ) • build markov chain converge p ( x ) • start x0 
used markov chain • want sample p ( x ) • build markov chain converge p ( x ) • start x0 • k = 0 1 … xk+1 ⇠ ( xk xk+1 ) 
used markov chain • want sample p ( x ) • build markov chain converge p ( x ) • start x0 • k = 0 1 … xk+1 ⇠ ( xk xk+1 ) • eventually xk look like sample p ( x ) 
markov chain always converge 
markov chain always converge 1 0 0 1 
markov chain always converge 1 0 0 1 x1 x2 x3 p ( left ) p ( right ) 1 0 1 0 1 0 converge 
markov chain definition distribution ⇡ call stationary ⇡ ( x0 ) = x x ( x x0 ) ⇡ ( x ) 
markov chain theorem ( x x0 ) > 0 x x0 exist unique ⇡ ⇡ ( x0 ) = x x ( x x0 ) ⇡ ( x ) 
markov chain theorem ( x x0 ) > 0 x x0 exist unique ⇡ ⇡ ( x0 ) = x x ( x x0 ) ⇡ ( x ) markov chain converge ⇡ start point 

gibbs sampling 
gibbs sampling pb ( x1 x2 x3 ) p ( x1 x2 x3 ) = z 
gibbs sampling pb ( x1 x2 x3 ) p ( x1 x2 x3 ) = z 0 0 0 start ( x1 x2 x3 ) e g ( 0 0 0 ) 
gibbs sampling pb ( x1 x2 x3 ) p ( x1 x2 x3 ) = z 0 0 0 start ( x1 x2 x3 ) e g ( 0 0 0 ) x11 ⇠ p ( x1 | x2 = x02 x3 = x03 ) 
gibbs sampling pb ( x1 x2 x3 ) p ( x1 x2 x3 ) = z 0 0 0 start ( x1 x2 x3 ) e g ( 0 0 0 ) x11 ⇠ p ( x1 | x2 = x02 x3 = x03 ) pb ( x1 x02 x03 ) = z1 
gibbs sampling pb ( x1 x2 x3 ) p ( x1 x2 x3 ) = z 0 0 0 start ( x1 x2 x3 ) e g ( 0 0 0 ) x11 ⇠ p ( x1 | x2 = x02 x3 = x03 ) x12 ⇠ p ( x2 | x1 = x11 x3 = x03 ) 
gibbs sampling pb ( x1 x2 x3 ) p ( x1 x2 x3 ) = z 0 0 0 start ( x1 x2 x3 ) e g ( 0 0 0 ) x11 ⇠ p ( x1 | x2 = x02 x3 = x03 ) x12 ⇠ p ( x2 | x1 = x11 x3 = x03 ) x13 ⇠ p ( x3 | x1 = x11 x2 = x12 ) 
gibbs sampling pb ( x1 x2 x3 ) p ( x1 x2 x3 ) = z 0 0 0 start ( x1 x2 x3 ) e g ( 0 0 0 ) k = 0 1 … k k xk+1 ⇠ p ( x | x = x x = x 1 2 3 2 3 ) 1 k+1 k xk+1 ⇠ p ( x | x = x x = x 2 1 3 3 ) 2 1 k+1 k+1 xk+1 ⇠ p ( x | x = x x = x ) 3 1 2 3 1 2 

gibbs sampling demo 
gibbs sampling demo prove gibbs sampling scheme indeed give correct way sampling desire distribution underlie markov chain indeed converge distribution b let us look small demo work practice let s look simple two-dimensional distribution look like gaussian note actually nt need gibbs sampling first distribution two dimensional rejection sampling scheme also apply conditionality large also gaussian efficiently sample without much trouble fancy technique illustrational purpose gibbs sampling work even million dimensional space complicate distribution let s start point 00 initialization point first first substep first iteration gibbs sampling suggest find conditional distribution x2 give x1 0 
gibbs sampling demo look like know x2 0 x1 somewhere around 5 6 shape gaussian distribution point region region particular area x2 
gibbs sampling demo let s sample point look like example okay first substep 
gibbs sampling demo next substep compute conditional distribution x2 give x1 one generate x1 6 somewhere around conditional distribution x2 like 
gibbs sampling demo position point position gaussian distribution lie x1 accord 6 
gibbs sampling demo let s sample get point like 
gibbs sampling demo result iteration one next point markov chain process note converge yet even find region space density distribution less somewhere low density region use sample yet true distribution let us proceed next iteration sample conditional distribution x1 give 
gibbs sampling demo x2 around 1 somewhere 
gibbs sampling demo okay let s sample let s move point 
gibbs sampling demo end substep one second iteration substep two generate point like conditional distribution 
gibbs sampling demo one substep end 
gibbs sampling demo 
gibbs sampling demo first iteration finally repeat process like 10 iteration 
gibbs sampling demo get point like already look like something gaussian demo gibbs sampling work summarize property 
summary pro • reduce multidimensional sampling sequence 1d sampling 
summary pro • reduce multidimensional sampling sequence 1d sampling • line code first gibbs sampling idea instead one complicate problem generate sample multidimensional distribution may know normalization constant reduce sequence one-dimensional sampling problem nice make everything efficient note first video week discuss scheme generate point one sample spending time generate chain sample generate first sample unless example converge wait first five hundred iteration throw away first sample get one sample care 
summary pro • reduce multidimensional sampling sequence 1d sampling • line code con • highly correlated sample another positive feature gibbs sampling really easy implement line coding pattern really universally applicable almost always sample one-dimensional conditional distribution way sample multidimensional one 
summary pro • reduce multidimensional sampling sequence 1d sampling • line code con • highly correlated sample • slow convergence ( mix ) negative feature give really correlated sample recall demo sample kind close previous one large step gibbs scheme always kind local step go one dimension time mean sample similar turn mean efficient estimate expect value would wish let us say wait first 1000 iteration gibbs sampling throw point away say useless us markov chain converge yet 1001 iteration start collect sample write use expect value estimation example may say collect 1000 sample well may think 1000 sample 
summary pro • reduce multidimensional sampling sequence 1d sampling • line code con • highly correlated sample • slow convergence ( mix ) • parallel close effect like one hundred example one hundred independent sample efficiency estimate expect phase correlated sample much lower efficiency estimate thing uncorrelated sample problem efficiency problem one negative feature small step hard converge convergence markov chain sometimes call mix lot step find high density region probability distribution throw point away converge yet use point small local step wait convergence longer would step large finally already discuss method parallel want sample like one million dimensional space anyway sample dimension one time really slow next video discuss scheme build markov chain used mitigate problem 

metropolis-hasting previous video discuss one way build markov chain converge zag distribution namely gibbs sampling discuss downsides produce highly correlated sample 
metropolis-hasting sometimes gibbs sample correlated convert somewhat slowly distribution downsides come one property gibbs kind local small step sample space video be go discuss scheme build markov chain instead produce one predefined markov chain give whole family markov chain choose one desire property like converge faster maybe produce less correlated sample family technique call metropolis-hasting idea 
metropolis-hasting sometimes gibbs sample correlated apply rejection sampling markov chain apply rejection sampling idea two markov chain 
metropolis-hasting k = 1 2 … 0 • sample x wrong q ( xk x0 ) let s start sum markov chain maybe nt anything desire distribution b bright side markov chain q kind freely sample space s large step thus converge faster produce almost uncorrelated sample s nice markov chain nt produce want 
metropolis-hasting k = 1 2 … 0 • sample x wrong q ( xk x0 ) • accept proposal x0 probability ( xk x0 ) dissemble distribution b let s introduce critic critic something say wait minute go right please stay wait generate reasonable direction go critic make sure markov chain nt go region probability desire distribution b low well least nt go often accept proposal 
metropolis-hasting k = 1 2 … 0 • sample x wrong q ( xk x0 ) • accept proposal x0 probability ( xk x0 ) • otherwise stay xk xk+1 = xk generate sample transition probability markov chain q probability give critic otherwise object stay previous equation general idea follow part discuss build critic give markov chain q desire distribution b first let s discuss overall scheme look like term markov chain markov chain q change procedure generate point 
metropolis-hasting k = 1 2 … 0 • sample x wrong q ( xk x0 ) • accept proposal x0 probability ( xk x0 ) • otherwise stay xk xk+1 = xk ( x x0 ) = q ( x x0 ) ( x x0 ) x = x0 overall scheme generate point like transition point x x prime probability well first need generate x prime proposal distribution q first markov chain q accept point happen probability give critic x x prime true point equal x equal x prime 
metropolis-hasting k = 1 2 … 0 • sample x wrong q ( xk x0 ) • accept proposal x0 probability ( xk x0 ) • otherwise stay xk xk+1 = xk ( x x0 ) = q ( x x0 ) ( x x0 ) x = x0 ( x x ) = q ( x x ) ( x x ) x + q ( x x0 ) ( 1 ( x x0 ) ) x0 6=x be equal something little bit complicate can propose move x accept proposal propose move anywhere else reject proposal happen probability one minus probability give critic one accept probability transitional probability overall markov chain defined process propose point accept probability want markov chain generate point desire distribution 
metropolis-hasting k = 1 2 … 0 • sample x wrong q ( xk x0 ) • accept proposal x0 probability ( xk x0 ) • otherwise stay xk xk+1 = xk ( x x0 ) = q ( x x0 ) ( x x0 ) x = x0 ( x x ) = q ( x x ) ( x x ) x + q ( x x0 ) ( 1 ( x x0 ) ) x0 6=x 0 choose ⇡ ( x ) = x x ⇡ ( x ) ( x x0 ) want distribution point stationary markov chain t idea let alone choice underlie command markov chain q give freedom user method try choose quota hold input markov chain q desire distribution pie want choose critic distribution pie indeed stationary final markov chain g well s easy equation kind complicate summation inside complicate definition transition probability t solve problem will introduce one concept 
detailed balance ⇡ ( x0 ) = x x ⇡ ( x ) ( x x0 ) call detailed balance equation recall definition stationary distribution s like detailed balance equation follow equation 
detailed balance ⇡ ( x ) ( x x0 ) = ⇡ ( x0 ) ( x0 x ) ⇡ ( x0 ) = x x ⇡ ( x ) ( x x0 ) give sufficient condition stationary distribution detailed balance hold distribution pie nt necessarily stationary one transition probability d detailed balance equation say us well say start distribution pie make one step one timestep markov chain g probability start x move x prime reverse start x prime move x s true probability distribution pie stationary 
detailed balance proof ⇡ ( x ) ( x x0 ) = ⇡ ( x0 ) ( x0 x ) ⇡ ( x0 ) = x x x x ⇡ ( x ) ( x x0 ) 0 ⇡ ( x ) ( x x ) = x x ⇡ ( x0 ) ( x0 x ) g s kind easy prove let s write right hand side definition stationary distribution substitute inside definition substitute effect time transition probability first equation detailed balance equation thus will get summation x expression point expressed nt depend x 
detailed balance proof ⇡ ( x ) ( x x0 ) = ⇡ ( x0 ) ( x0 x ) ⇡ ( x0 ) = x x x x ⇡ ( x ) ( x x0 ) 0 ⇡ ( x ) ( x x ) = x x ⇡ ( x0 ) ( x0 x ) = ⇡ ( x0 ) x x ( x0 x ) move pie x prime outside summation get something finally summation x brand x 
detailed balance proof ⇡ ( x ) ( x x0 ) = ⇡ ( x0 ) ( x0 x ) ⇡ ( x0 ) = x x x x ⇡ ( x ) ( x x0 ) 0 ⇡ ( x ) ( x x ) = x x ⇡ ( x0 ) ( x0 x ) = ⇡ ( x0 ) x | x ( x0 x ) { z 1 } respect x one kind probability distribution sum possible outcome one 
detailed balance proof ⇡ ( x ) ( x x0 ) = ⇡ ( x0 ) ( x0 x ) ⇡ ( x0 ) = x x x x ⇡ ( x ) ( x x0 ) 0 ⇡ ( x ) ( x x ) = x x ⇡ ( x0 ) ( x0 x ) = ⇡ ( x0 ) x x = ⇡ ( x0 ) ( x0 x ) finally proven detailed balance hold solution nt necessarily stationary one return problem 
metropolis-hasting k = 1 2 … 0 • sample x wrong q ( xk x0 ) • accept proposal x0 probability ( xk x0 ) • otherwise stay xk xk+1 = xk ( x x0 ) = q ( x x0 ) ( x x0 ) x = x0 ( x0 x0 ) = q ( x0 x0 ) choose ⇡ ( x0 ) = x x ⇡ ( x ) ( x x0 ) markov chain q nt produce us sample want distribution pie want simplify want define critic markov chain create critic markov chain indeed produce sample distribution pie distribution pie stationary since property hard check 
metropolis-hasting k = 1 2 … 0 • sample x wrong q ( xk x0 ) • accept proposal x0 probability ( xk x0 ) • otherwise stay xk xk+1 = xk ( x x0 ) = q ( x x0 ) ( x x0 ) x = x0 ( x0 x0 ) = q ( x0 x0 ) choose ⇡ ( x ) ( x x0 ) = ⇡ ( x0 ) ( x0 x ) instead be go check distribution pie detailed balance equation true next video be go use blackboard find critic detailed balance equation hold give markov chain q distribution pie 

◆ [ music ] s critic s probability probability accept move propose markov s way detail equation hold desire distribution pi want sample markov chain q want fix critic a want fix [ inaudible ] equation hold pi indeed stationary distribution mark chain s mark chain converge point start point 
metropolis hasting k = 1 2 … 0 • sample x wrong q ( xk x0 ) • accept proposal x0 probability ( xk x0 ) • otherwise stay xk xk+1 = xk ✓ ⇡ b ( x0 ) 0 ⇡ b ( x ) q ( x x ) ( x x ) = min 1 ( x ) ⇡ b ( x ) q ( x x0 ) 0 ◆ let s see choose property hold let s compose equality move part depend one part ( x x ) divide ( x x ) equal follow equation one side x prime q x prime x denominator thing swap x x prime [ sound ] something computable right know q compute thing two value let s call ratio raw let s assume moment row less s always greater 0 s ratio probability let s assume s also less s assumption may hold much let s assume case ratio eight would make equal less one let s use x prime one ox x prime x way desire hold true right x x prime divide x prime x [ inaudible ] may notice can choose number like 4 divide 2 example 1 divide also work overall mark chain deconverge distribution [ inaudible ] think probability accept move way increase probability necessarily inject move risk capacity [ inaudible ] reject choose maximum acceptance probability make keep ratio low ratio greater one well choose x prime x x prime divide one p one n summarize case always less one be always greater one 
choice q q ( x x0 ) > 0 oppose force • q spread improve mix reduce correlation • acceptance probability often low form expression x prime x dot prime equal minimal within two value 1 ratio pie x prime time q x prime dot x thing change parameter x x prime vice versa expression work case ratio less 1 greater check consider two case one case 1 s minimum 1 something greater case wrong choose critic indeed prove overall mark defined kind procedure indeed follow equation distribution probably stationary markov chain convert distribution start point last choose critic may sample distribution y summarise metropolis hasting approach build chain converge desire distribution pi start sum mac chain q nt anything distribution pie mark chain step propose symbol will correct sometimes reject move ask mark chain stay various straight prove defined coreject like acceptance bill expression minimum one ratio matter markov chain q start necessarily converge desire distribution pi note acceptance probability s place use distribution pi want sample note nt know distribution pi exactly may know normalization constant will ratio distribution two different point nt matter normalize distribution ratio care normalization divide part ratio normalization constant z nothing change method well gibbs sampling used nt know normalization constant distribution discuss choose q anything distribution number one well obviously q nonzero anywhere ratio well-defined course efficiency property algorithm depend choice q kind two oppose force first q make large step spread kind roll freely sample space make sample less correlated converge faster hand large step critic reject often waste capacity stay place long be example already converge be high-density region be q propose make too-large move move outside high density region happy say nt want go probability distribution curve ball nt work nt high region next video will see small demo approach work one dimensional case [ sound ] 

demo 
demo q ( x x0 ) = n ( x 1 ) 
demo q ( x x0 ) = n ( x 1 ) 
demo q ( x x0 ) = n ( x 1 ) ✓ 0 0 ⇡ ( x ) q ( x x ) ( x x ) = min 1 ⇡ ( x ) q ( x x0 ) 0 ◆ ✓ 0 ⇡ ( x ) = min 1 ⇡ ( x ) ◆ 
demo q ( x x0 ) = n ( x 1 ) ✓ 0 0 ⇡ ( x ) q ( x x ) ( x x ) = min 1 ⇡ ( x ) q ( x x0 ) 0 ◆ ✓ 0 ⇡ ( x ) = min 1 ⇡ ( x ) ◆ 
demo q ( x x0 ) = n ( x 1 ) ✓ 027 ( x x ) = min 1 007 0 ◆ = min ( 1 387 ) 
demo q ( x x0 ) = n ( x 1 ) ✓ 027 ( x x ) = min 1 007 0 ◆ = min ( 1 387 ) 
demo q ( x x0 ) = n ( x 1 ) 
demo q ( x x0 ) = n ( x 1 ) ✓ 028 ( x x ) = min 1 027 0 ◆ = min ( 1 101 ) 
demo q ( x x0 ) = n ( x 1 ) ✓ 028 ( x x ) = min 1 027 0 ◆ = min ( 1 101 ) 
demo q ( x x0 ) = n ( x 1 ) ✓ 004 ( x x ) = min 1 028 0 ◆ = min ( 1 013 ) 
demo q ( x x0 ) = n ( x 1 ) ✓ 004 ( x x ) = min 1 028 0 ◆ = min ( 1 013 ) 
demo q ( x x0 ) = n ( x 1 ) ✓ 020 ( x x ) = min 1 028 0 ◆ = min ( 1 073 ) 
demo q ( x x0 ) = n ( x 1 ) ✓ 020 ( x x ) = min 1 028 0 ◆ = min ( 1 073 ) 
demo q ( x x0 ) = n ( x 1 ) 
demo q ( x x0 ) = n ( x 1 ) 
demo q ( x x0 ) = n ( x 012 ) 
demo q ( x x0 ) = n ( x 102 ) 
metropolis hasting correction scheme 
metropolis hasting correction scheme recall gibbs sampling 
metropolis hasting correction scheme recall gibbs sampling k k xk+1 ⇠ p ( x | x = x x = x 1 2 3 2 3 ) 1 k+1 k xk+1 ⇠ p ( x | x = x x = x 2 1 3 3 ) 2 1 k+1 k+1 xk+1 ⇠ p ( x | x = x x = x ) 3 1 2 3 1 2 
metropolis hasting correction scheme recall gibbs sampling let make parallel k k xk+1 ⇠ p ( x | x = x x = x 1 2 3 2 3 ) 1 k+1 k xk+1 ⇠ p ( x | x = x x = x 2 1 3 3 ) 2 1 k+1 k+1 xk+1 ⇠ p ( x | x = x x = x ) 3 1 2 3 1 2 
metropolis hasting correction scheme recall gibbs sampling let make parallel k k xk+1 ⇠ p ( x | x = x x = x 1 2 3 2 3 ) 1 k k xk+1 ⇠ p ( x | x = x x = x 2 1 3 1 3 ) 2 k k xk+1 ⇠ p ( x | x = x x = x 3 1 2 1 2 ) 3 
metropolis hasting correction scheme recall gibbs sampling let make parallel ’ wrong correct metropolis hasting k k xk+1 ⇠ p ( x | x = x x = x 1 2 3 2 3 ) 1 k k xk+1 ⇠ p ( x | x = x x = x 2 1 3 1 3 ) 2 k k xk+1 ⇠ p ( x | x = x x = x 3 1 2 1 2 ) 3 
summary rejection sampling apply markov chain pro • choose among family markov chain 
summary rejection sampling apply markov chain pro • choose among family markov chain • work unnormalized density 
summary rejection sampling apply markov chain pro • choose among family markov chain • work unnormalized density • easy implement 
summary rejection sampling apply markov chain pro • choose among family markov chain • work unnormalized density • easy implement con • sample still correlated 
summary rejection sampling apply markov chain pro • choose among family markov chain • work unnormalized density • easy implement con • sample still correlated • choose among family markov chain j 

monte carlo method approximate expect value let s summarize learn far 
week summary monte carlo method approximate expect value sample distribution know normalization constant discuss markov chain monte carlo method monte carlo part approximate expect value sampling main question sample generate sample complicate distribution may know normalization constant usually happen probabilistic modele discuss two main method 
week summary monte carlo method approximate expect value sample distribution know normalization constant two mcmc approach • gibbs sampling – reduce multidimensional sampling sequence 1d • metropolis hasting – rejection sampling markov chain ( give freedom ) come markov chain monte carlo family idea markov chain monte carlo build dynamic system simulate long enough s state start look like sample desire distribution dynamic system call markov chain discuss two way build markov chain converge distribution want sample first method gibbs sampling reduce problem sampling multidimensional distribution sequence one dimensional sampling one dimensional sampling usually much easier have first efficient method one dimensional sampling cyclic chain purpose method like rejection sampling second sometimes even compute normalization constant integrate thing one mention much easier finally sometimes even find analytical solution one dimensional sampling problem original multidimensional sampling structure enough discuss downsides gibbs sampling generate highly correlated update mean converge slowly use lot lot sample approximate expect value accurately enough second method discuss metropolis hasting give whole family markov chain converge desire distribution want kind give freedom choose right freedom come great responsibility choose wisely choose markov chain well suit purpose like choose markov chain converge slowly highly correlated sample waste lot resource method available efficient 
monte carlo vs variational inference monte carlo x 1 ep ( x ) f ( x ) ⇡ f ( xs ) s=1 xs ⇠ p ( x ) unbiased estimate ( larger = > better accuracy ) let s put markov chain monte carlo method context let s compare variational inference cover previous week monte carlo approximate expect value substitute average respect sample obtain distribution main property approximation unbiased wait accuracy get get accuracy low want be willing wait long enough want express property little bit mathematically 
monte carlo vs variational inference monte carlo x 1 ep ( x ) f ( x ) ⇡ f ( xs ) s=1 xs ⇠ p ( x ) unbiased estimate ( larger = > better accuracy ) x 1 ep ( x ) f ( xs ) = ep ( x ) f ( x ) s=1 write like average approximation random variable right repeat process obtain sample average get approximation will get another approximation run variable [ inaudible ] different approximation different sample estimate like call unbiased expect value equal thing want approximate average approximation correct wait long enough approximation converge right answer 
monte carlo vs variational inference monte carlo x 1 ep ( x ) f ( x ) ⇡ f ( xs ) s=1 xs ⇠ p ( x ) unbiased estimate ( larger = > better accuracy ) variational inference ( week 3 ) p ( x ) ⇡ q ( x ) ep ( x ) f ( x ) ⇡ eq ( x ) f ( x ) something true variational inference variational inference be try approximate distribution want sample want work distribution simple family like example family factorised distribution course iteration variational inference become better better think better better approximation distribution class factorised distribution ca nt get error zero never approximate complicate distribution arbitrarily well factorized one 
monte carlo vs variational inference error schematic illustration variational inference time typical picture describe distribution like problem try solve markov chain monte carlo kind work give solution better better better time approach zero error use variational inference usually give nice solution much faster stagnate ca nt get past critical value fairer get zero 
method best • full inference worst p ( ✓|x ) put markov chain monte carlo even context 
method best • full inference • mean field worst p ( ✓|x ) q ( ) q ( ✓ ) ⇡ p ( ✓|x ) recall table end week three variational inference table approximate method store solve probabilistic modele problem cover far first full bayesian inference model parameter latent variable latent variable nt train anything usual sense be try find best fitter parameter instead be try marginalize will think nt know one theoritically get best result practical model full bayesian inference nt feasible 
method best • full inference worst p ( ✓|x ) • mean field q ( ) q ( ✓ ) ⇡ p ( ✓|x ) • mcmc ts ⇥s ⇠ p ( ⇥ | x ) approximation one midfield variational inference assume posterior distribution factorized be try approximate posterior distribution factorized family well add one method 
method best • full inference • mean field q ( ) q ( ✓ ) ⇡ p ( ✓|x ) • mcmc ts ⇥s ⇠ p ( ⇥ | x ) • em algorithm worst p ( ✓|x ) q ( ) ✓ = ✓mp approximate full bayesian inference sampling posterior distribution used sample obtain estimate value care use gibbs sampling metropolis hasting assemble point posterior distribution far latent variable use approximate nt want process slow use expectation maximization algorithm basically try find maximum likelihood estimation parameter theta nt treat theta latent variable instead treat theta parameter try find best theta parameter used iterative scheme expectation maximization algorithm also sometimes intractable sometimes s hard e step case midfield 
method best • full inference • mean field q ( ) q ( ✓ ) ⇡ p ( ✓|x ) • mcmc ts ⇥s ⇠ p ( ⇥ | x ) • em algorithm • variational em worst p ( ✓|x ) q ( ) ✓ = ✓mp q1 ( t1 ) qd ( td ) ✓ = ✓mp variational inference apply expectation maximization basically e step approximate posterior variational distribution cue factorized one markov chain monte carlo em 
method best • full inference • mean field q ( ) q ( ✓ ) ⇡ p ( ✓|x ) • mcmc ts ⇥s ⇠ p ( ⇥ | x ) • em algorithm worst p ( ✓|x ) q ( ) ✓ = ✓mp • variational em q1 ( t1 ) qd ( td ) ✓ = ✓mp • mcmc em ts ⇠ p ( | ⇥ x ) ⇥ = ⇥mp em step instead work distribution cue directly try find expect value respect distribution acquire sample posterior latent variable use sample approximate expect value em step maximize approximation instead original expect value 
summary markov chain monte carlo pro • easy implement • easy parallelize • unbiased estimate ( wait = > accuracy ) con • usually slower alternative final summary markov chain monte carlo method allow training inference probabilistic model s really easy implement s really easy parallelize least term like 100 computer run 100 independent cue center example computer combine sample obtain server finally give unbiased estimate principle wait long enough get low accuracy may want negative side sometimes markov chain monte carlo method slower other be widely applicable sometimes choice next video cover practical example apply markov chain monte carlo problem 

mcmc lda 
latent dirichlet allocation document distribution topic 80 % document 20 % ` cat ` dog topic distribution word topic “ dog ” topic “ cat ” 40 % 40 % 30 % 30 % 5 % 5 % 5 % 5 % 
text generation 80 % 20 % ` cat topic “ cat ” 40 % 30 % 5 % 5 % ` dog topic “ dog ” 40 % 30 % 5 % 5 % cat meow dog 
model topic word zd1 word wd1 distribution topic zd2 wd2 … zdn zdn 2 { 1 } wdn 2 { 1v } … wdnd 
mean field lda ( week 3 ) max p ( w | ) e-step p ( z ⇥ | w ) ⇡ q ( z ⇥ ) m-step max eq log p ( z ⇥ w | ) 
mean field lda ( week 3 ) log q ( z ) = eq ( ⇥ ) log p ( ⇥ z w ) + const = eq ( ⇥ ) x x ( ↵ 1 ) log ✓dt + const d=1 t=1 + eq ( ⇥ ) nd x x x [ zdn = 1 ] ( log ✓dt + log twdn ) d=1 n=1 t=1 = nd x x x [ zdn = 1 ] eq ( ⇥ ) log ✓dt + log d=1 n=1 t=1 + const q ( z ) = q ( zd ) = d=1 twdn 
mcmc lda know unknown w datum parameter distribution word topic unknown z latent variable topic word unknown ⇥ latent variable distribution topic document 
mcmc lda let go full bayesian know unknown w datum latent variable distribution word topic unknown z latent variable topic word unknown ⇥ latent variable distribution topic document 
mcmc lda let go full bayesian know w datum unknown latent variable distribution word topic unknown z latent variable topic word unknown ⇥ latent variable distribution topic document p ( ⇥ z|w ) ⇠ { gibbs sampling } 
mcmc lda p ( ⇥ z|w ) ⇠ { gibbs sampling } init 0 ⇥0 z 0 
mcmc lda p ( ⇥ z|w ) ⇠ { gibbs sampling } init 0 1 1 | ⇠ p ( ⇥0 z 0 0 2 0 0 0 ⇥ z w ) 3 
mcmc lda p ( ⇥ z|w ) ⇠ { gibbs sampling } init 0 ⇥0 z 0 1 1 ⇠ p ( | 0 2 0 0 0 ⇥ z w ) 3 1 2 ⇠ p ( | 1 1 0 0 0 ⇥ z w ) 3 
mcmc lda p ( ⇥ z|w ) ⇠ { gibbs sampling } init 1 0 ⇥0 z 0 ⇠ p ( | 1 1 1 1 0 0 0 ⇥ z w ) i+1 
mcmc lda p ( ⇥ z|w ) ⇠ { gibbs sampling } init 1 0 ⇥0 z 0 ⇠ p ( | ✓i1 ⇠ p ( ✓i | 1 1 1 1 1 0 0 0 ⇥ z w ) i+1 0 ✓11 ✓i1 1 ✓i+1 z 0 w ) 
mcmc lda p ( ⇥ z|w ) ⇠ { gibbs sampling } init 1 0 ⇥0 z 0 ⇠ p ( | ✓i1 ⇠ p ( ✓i | zi1 ⇠ p ( zi | 1 1 1 1 0 0 0 ⇥ z w ) i+1 1 0 ✓11 ✓i1 1 ✓i+1 z 0 w ) 1 0 ⇥1 z11 zi1 1 zi+1 w ) 
mcmc lda p ( ⇥ z|w ) ⇠ { gibbs sampling } init 0 ⇥0 z 0 k = 1 2 k+1 ⇠ p ( | ✓ik+1 ⇠ p ( ✓i | zik+1 ⇠ p ( zi | k+1 k+1 k k k ⇥ z w ) i+1 1 1 k+1 k+1 k k ✓1 ✓ik+1 ✓ z w ) i+1 1 k+1 k ⇥k+1 z1k+1 zik+1 z i+1 w ) 1 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = zdn wdn 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = conjugate zdn wdn 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = compute analytically p ( ⇥ | z ) zdn wdn 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = compute analytically p ( ⇥ | z ) z p ( z ) = p ( z | ⇥ ) p ( ⇥ ) d⇥ zdn wdn 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = compute analytically p ( ⇥ | z ) z p ( z ) = p ( z | ⇥ ) p ( ⇥ ) d⇥ p ( z | ⇥ ) p ( ⇥ ) = p ( ⇥ | z ) zdn wdn 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = compute analytically p ( ⇥ | z ) z p ( z ) = p ( z | ⇥ ) p ( ⇥ ) d⇥ zdn wdn 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = compute analytically p ( ⇥ | z ) z p ( z ) = p ( z | ⇥ ) p ( ⇥ ) d⇥ conjugate zdn wdn 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = compute analytically p ( ⇥ | z ) p ( | z w ) z p ( z ) = p ( z | ⇥ ) p ( ⇥ ) d⇥ zdn wdn 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = compute analytically p ( ⇥ | z ) p ( | z w ) z p ( w | z ) p ( ) p ( z ) = p ( z p ( | ⇥ ) p ( ⇥ ) d⇥ | z ) = p ( | z w ) zdn wdn 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = compute analytically p ( ⇥ | z ) p ( | z w ) z p ( w | z ) p ( ) p ( z ) = p ( z p ( | ⇥ ) p ( ⇥ ) d⇥ | z ) = p ( | z w ) zdn wdn 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = zdn wdn compute analytically p ( w | z ) p ( z ) p ( ⇥ | z ) p ( | z w ) p ( z | w ) = c z p ( w | z ) p ( ) p ( z ) = p ( z p ( | ⇥ ) p ( ⇥ ) d⇥ | z ) = p ( | z w ) 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = zdn wdn compute analytically p ( w | z ) p ( z ) p ( ⇥ | z ) p ( | z w ) p ( z | w ) = c z p ( w | z ) p ( ) p ( z ) = p ( z p ( | ⇥ ) p ( ⇥ ) d⇥ | z ) = p ( | z w ) p ( z | w ) ⇠ { gibbs sampling } 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = p ( z | w ) ⇠ { gibbs sampling } zdn wdn 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = p ( z | w ) ⇠ { gibbs sampling } z p ( w ) = p ( w z ) p ( z|w ) dz = ep ( z|w ) p ( w z ) zdn wdn 
collapse gibbs lda model p ( ✓d ) = dir ( ) p ( ) = dir ( ↵ ) p ( zdn ✓d ) = ⇥dzdn p ( wdn zdn ) = p ( z | w ) ⇠ { gibbs sampling } z p ( w ) = p ( w z ) p ( z|w ) dz = ep ( z|w ) p ( w z ) b ⇡ p ( w z ) zdn wdn 
collapse gibbs lda c 25 perplexity 24 23 22 21 10 # 10 $ 10 % 10 & 10 ( float point operation [ source griffith thomas l mark steyver ` find scientific topic ` proceedings national academy science 101suppl 1 ( 2004 ) 5228-5235 ] 10 

bayesian neural network 
bayesian neural network x1 h1 x2 x3 h2 
bayesian neural network x1 w1 1 = 3 1 h1 x2 x3 h2 
bayesian neural network x1 w1 1 = 3 1 h1 x2 x3 h2 
bayesian neural network p ( | x ytrain xtrain ) 
bayesian neural network p ( | x ytrain xtrain ) z = p ( | x w ) p ( w | ytrain xtrain ) dw 
bayesian neural network p ( | x ytrain xtrain ) z = p ( p ( | x x w ) w ) p ( w | ytrain xtrain ) dw | { z } nn output 
bayesian neural network p ( | x ytrain xtrain ) z = p ( | x w ) p ( w | ytrain xtrain ) dw = ep ( w|ytrain xtrain ) p ( | x w ) 
bayesian neural network p ( | x ytrain xtrain ) z = p ( | x w ) p ( w | ytrain xtrain ) dw = ep ( w|ytrain xtrain ) p ( | x w ) p ( w | ytrain xtrain ) ⇠ { gibbs } 
bayesian neural network p ( | x ytrain xtrain ) z = p ( | x w ) p ( w | ytrain xtrain ) dw = ep ( w|ytrain xtrain ) p ( | x w ) p ( w | ytrain xtrain ) ⇠ { gibbs } p ( ytrain | xtrain w ) p ( w ) p ( w | ytrain xtrain ) = z 
bayesian neural network p ( | x ytrain xtrain ) z = p ( | x w ) p ( w | ytrain xtrain ) dw = ep ( w|ytrain xtrain ) p ( | x w ) p ( w | ytrain xtrain ) ⇠ { gibbs } p ( ytrain | xtrain w ) p ( w ) p ( w | ytrain xtrain ) = z depend whole dataset 
langevin monte carlo gibbs metropolis hasting ’ mini-batch 
langevin monte carlo say want sample p ( w | ) 
langevin monte carlo say want sample p ( w | ) start w 0 
langevin monte carlo say want sample p ( w | ) start w 0 k = 1 … wk+1 = wk + ` r log p ( wk | ) + ⌘ k ⌘ k ⇠ n ( 0 2 ) 
langevin monte carlo say want sample p ( w | ) start w 0 k = 1 … wk+1 = wk + ` r log p ( wk | ) + ⌘ k gradient ascent ⌘ k ⇠ n ( 0 2 ) 
langevin monte carlo say want sample p ( w | ) start w 0 k = 1 … wk+1 = wk + ` r log p ( wk | ) + ⌘ k = wk + ` r log p ( wk ) + n x i=1 log p ( yi | xi wk ) ⌘ k ⇠ n ( 0 2 ) + ⌘k 
langevin monte carlo say want sample p ( w | ) start w 0 k = 1 … wk+1 = wk + ` r log p ( wk | ) + ⌘ k = wk + ` r log p ( wk ) + n x i=1 weight decay ckw k k2 log p ( yi | xi wk ) usual cross entropy ⌘ k ⇠ n ( 0 2 ) + ⌘k 
langevin monte carlo • initialize weight w 0 
langevin monte carlo • initialize weight w 0 • say 100 iteration usual sgd add gaussian noise ⌘ k ⇠ n ( 0 2 ) update 
langevin monte carlo • initialize weight w 0 • say 100 iteration usual sgd add gaussian noise ⌘ k ⇠ n ( 0 2 ) update • 100 hundred epochs decide markov chain converge start collect weight value 
langevin monte carlo • initialize weight w 0 • say 100 iteration usual sgd add gaussian noise ⌘ k ⇠ n ( 0 2 ) update • 100 hundred epochs decide markov chain converge start collect weight value • new object predict compute average prediction cnns weight w 100 w 101 w 200 
langevin monte carlo • initialize weight w 0 • say 100 iteration usual sgd add gaussian noise ⌘ k ⇠ n ( 0 2 ) update • 100 hundred epochs decide markov chain converge start collect weight value • new object predict compute average prediction cnns weight w 100 w 101 w 200 • train another cnn mimic ensemble [ balan anoop korattikara et al ` bayesian dark knowledge advance neural information process system 2015 ] 

scalable variational inference welcome week five course week be go talk scale bayesian method large datum set 
scalable variational inference person used thing baye small dataset 
scalable variational inference person used thing baye small dataset • slow big datum even like 10 year ago 
scalable variational inference person used thing baye small dataset • slow big datum • beneficial anyway person used think bayesian method mostly suit small datum set first be expensive computation expensive want full bayesian inference like one million training example go face lot trouble second may beneficial anyway case large datum person used think main idea main benefit bayesian method utilize model able extract much information possible small datum set free large datum set nt need use method want work fine 
scalable variational inference person used thing baye small dataset • slow big datum • beneficial anyway thing change baye meet deep learn thing change bayesian method meet deep learn person start make mixture model neural network instead probabilistic model week combine neural network bayesian method 
scalable variational inference • understand combine deep learn bayesian method 
scalable variational inference • understand combine deep learn bayesian method • learn synthesize image vae will discuss will discuss combine two idea will see particular example variational old encoder allow generate nice sample nice image used neural network probabilistic interpretation 
scalable variational inference • understand combine deep learn bayesian method • learn synthesize image vae • learn state-of-the-art bayesian neural network application second module professor dmitry vetrov tell scalable method bayesian neural network cut edge research area allow compress neural network lot fight severe fitting complicate datum set start let s discuss little bit concept estimation unbiased 
unbiased estimate already touch previous week week four markov chain monte carlo let s make self little bit clear will need build unbiased estimate gradient neural network 
unbiased estimate x 1 ep ( x ) f ( x ) ⇡ f ( xs ) s=1 let s say want estimate expect value be used monte carlo estimation 
unbiased estimate x 1 ep ( x ) f ( x ) ⇡ f ( xs ) s=1 xs ⇠ p ( x ) substitute average respect sample take distribution pure fact 
unbiased estimate x 1 ep ( x ) f ( x ) ⇡ f ( xs ) s=1 xs ⇠ p ( x ) idea may look like blue line distribution pure fact generate sample like 
unbiased estimate x 1 ep ( x ) f ( x ) ⇡ f ( xs ) = r s=1 xs ⇠ p ( x ) 
unbiased estimate x 1 ep ( x ) f ( x ) ⇡ f ( xs ) = r s=1 xs ⇠ p ( x ) take average f_x datum set look like example like red cross actually random variable repeat process 
unbiased estimate x 1 ep ( x ) f ( x ) ⇡ f ( xs ) = r s=1 xs ⇠ p ( x ) generate set sample repeat look right average get approximation expect value 
unbiased estimate x 1 ep ( x ) f ( x ) ⇡ f ( xs ) = r s=1 xs ⇠ p ( x ) repeating process time get sample random variable r random variable distribution 
unbiased estimate x 1 ep ( x ) f ( x ) ⇡ f ( xs ) = r s=1 xs ⇠ p ( x ) average expect value exactly equal expect value f_x want estimate see sample random variable r close expect value want estimate around 
unbiased estimate x 1 ep ( x ) f ( x ) ⇡ f ( xs ) = r s=1 xs ⇠ p ( x ) basically mean use sample like hundred sample estimation make accurate prediction sample 
unbiased estimate x 1 ep ( x ) f ( x ) ⇡ f ( xs ) = r s=1 xs ⇠ p ( x ) average r like close expect value want approximate sample use like accurate prediction become peaked around true value 
unbiased estimate x 1 ep ( x ) f ( x ) ⇡ f ( xs ) = r s=1 xs ⇠ p ( x ) ep ( x ) r = ep ( x ) f ( x ) put formally definition unbiased estimate estimate r call unbiased expect value equal thing want approximate true sample r lie around expect value want approximate true 
unbiased estimate log ep ( x ) f ( x ) ⇡ log xs ⇠ p ( x ) 1 x s=1 f ( xs ) g well look example log expect value try approximate monte carlo s kind natural approximate like log sample average turn s unbiased estimate 
unbiased estimate log ep ( x ) f ( x ) ⇡ log xs ⇠ p ( x ) 1 x s=1 ep ( x ) = log ep ( x ) f ( x ) f ( xs ) g look sample sample variant variable g lay left actual expect value be underestimate true value want approximate even average red cross like around true value around smaller value thus be right job be bias estimation function logarithm expect value 
unbiased estimate • estimator call unbiased expect value equal thing estimate summarize estimate call unbiased expect value equal thing want approximate 
unbiased estimate • estimator call unbiased expect value equal thing estimate • unbiased estimator x 1 ep ( x ) f ( x ) ⇡ f ( xs ) = r s=1 other may look unbiased check s entirely non trivial understand estimator unbiased simplest estimator expect value function unbiasedly estimate average respect sample anything complicate think carefully check be go bias territory nt want check ca nt be better reduce particular problem form expect value function estimate sample average way go sure estimation unbiased 

p ( x ) dataset 
scalable variational inference let start fitting p ( x ) dataset need 
model p ( x ) • generate new datum [ zhao et al energy-based generative adversarial network ] 
model p ( x ) • generate new datum [ dl cade https ] 
model p ( x ) • generate new datum 
model p ( x ) • generate new datum • detect anomaly outlier ( eg fraud detection ) 
model p ( x ) • generate new datum • detect anomaly outlier ( eg fraud detection ) • work miss datum 
model p ( x ) • generate new datum • detect anomaly outlier ( eg fraud detection ) • work miss datum • represent datum nice way ( eg model p ( molecule ) search drug ) 
model p ( x ) b ( x ) = cnn ( x ) • log p 
model p ( x ) b ( x ) = cnn ( x ) • log p exp ( cnn ( x ) ) p ( x ) = z 
model p ( x ) b ( x ) = cnn ( x ) • log p exp ( cnn ( x ) ) p ( x ) = z infeasible 
model p ( x ) b ( x ) = cnn ( x ) • log p • use chain rule infeasible 
model p ( x ) b ( x ) = cnn ( x ) • log p • use chain rule infeasible 
model p ( x ) b ( x ) = cnn ( x ) • log p • use chain rule x1 x2 x3 x4 x5 x6 x7 x8 x9 infeasible 
model p ( x ) b ( x ) = cnn ( x ) • log p infeasible • use chain rule x1 x2 x3 x4 x5 x6 x7 x8 x9 p ( x1 xd ) = p ( x1 ) p ( x2 | x1 ) p ( xd | x1 xd 1 ) 
model p ( x ) b ( x ) = cnn ( x ) • log p infeasible • use chain rule x1 x2 x3 x4 x5 x6 x7 x8 x9 [ oord aaron van den nal kalchbrenner koray kavukcuoglu ` pixel recurrent neural network ( 2016 ) ] p ( x1 xd ) = p ( x1 ) p ( x2 | x1 ) p ( xd | x1 xd p ( xk | x1 xk 1 ) = rnn ( x1 xk 1 ) 1 ) 
model p ( x ) b ( x ) = cnn ( x ) • log p • use chain rule x1 x2 x3 x4 x5 x6 x7 x8 x9 infeasible cool slow generate [ oord aaron van den nal kalchbrenner koray kavukcuoglu ` pixel recurrent neural network ( 2016 ) ] p ( x1 xd ) = p ( x1 ) p ( x2 | x1 ) p ( xd | x1 xd p ( xk | x1 xk 1 ) = rnn ( x1 xk 1 ) 1 ) 
model p ( x ) b ( x ) = cnn ( x ) • log p • use chain rule infeasible cool slow generate • p ( x1 xd ) = p ( x1 ) p ( xd ) 
model p ( x ) b ( x ) = cnn ( x ) • log p • use chain rule infeasible cool slow generate • p ( x1 xd ) = p ( x1 ) p ( xd ) datum 
model p ( x ) b ( x ) = cnn ( x ) • log p • use chain rule infeasible cool slow generate • p ( x1 xd ) = p ( x1 ) p ( xd ) datum sample restrictive 
model p ( x ) b ( x ) = cnn ( x ) • log p • use chain rule infeasible cool slow generate • p ( x1 xd ) = p ( x1 ) p ( xd ) • mixture several gaussian ( gmm ) restrictive 
model p ( x ) b ( x ) = cnn ( x ) • log p • use chain rule infeasible cool slow generate • p ( x1 xd ) = p ( x1 ) p ( xd ) restrictive • mixture several gaussian ( gmm ) still restrictive 
model p ( x ) b ( x ) = cnn ( x ) • log p • use chain rule infeasible cool slow generate • p ( x1 xd ) = p ( x1 ) p ( xd ) restrictive • mixture several gaussian ( gmm ) still restrictive • mixture infinitely many gaussian 
model p ( x ) b ( x ) = cnn ( x ) • log p • use chain rule infeasible cool slow generate • p ( x1 xd ) = p ( x1 ) p ( xd ) restrictive • mixture several gaussian ( gmm ) still restrictive • mixture infinitely many gaussian p ( x ) = z p ( x | ) p ( ) dt 

p ( x | ) p ( ) dt okay decide model distribution 
continuous mixture gaussian p ( x ) = z p ( x | ) p ( ) dt p ( ) = n ( 0 ) fact used continuous mixture gaussian let s develop idea define model fully define prior likelihood let s define prior standard norm force latent variable around zero unique variant 
continuous mixture gaussian p ( x ) = z p ( x | ) p ( ) dt p ( ) = n ( 0 ) p ( x | ) = n ( µ ( ) ⌃ ( ) ) likelihood decide use gaussian right parameter depend somehow define parameter pro-metric way convert parameter gaussian 
continuous mixture gaussian p ( x ) = z p ( x | ) p ( ) dt p ( ) = n ( 0 ) p ( x | ) = n ( µ ( ) ⌃ ( ) ) µ ( ) = w + b ⌃ ( ) = ⌃0 get ppca ( see week 2 ) well use linear function mu parameter w b constant sigma t sigma zero parameter maybe like identity matrix nt matter much will get usual ppca model probabilistic ppca model really nice s powerful enough kind natural image datum let s think change make model powerful linear function powerful enough purpose 
continuous mixture gaussian p ( x ) = z p ( x | ) p ( ) dt p ( ) = n ( 0 ) p ( x | ) = n ( µ ( ) ⌃ ( ) ) µ ( ) = w + b ⌃ ( ) = ⌃0 get ppca ( see week 2 ) x image µ ( ) = cnn1 ( ) ⌃ ( ) = cnn2 ( ) let s use convolutional neural network work nice image datum right let s say mu convolutional neural network apply latent call t get input latent output image mean vector image sigma also commercial neural network take live quarters input output covariance matrix sigma define model kind parametric form 
continuous mixture gaussian p ( x ) = z p ( x | ) p ( ) dt p ( ) = n ( 0 ) p ( x | ) = n ( µ ( ) ⌃ ( ) ) cn n w µ ( ) ⌃ ( ) like 
continuous mixture gaussian p ( x | w ) = z p ( x | w ) p ( ) dt p ( ) = n ( 0 ) p ( x | w ) = n ( µ ( w ) ⌃ ( w ) ) cn n w µ ( ) ⌃ ( ) let s emphasize weight will input w let s put part far model definition forget go train model like pre-meal fact give weight neuron w mixture gaussian parameter gaussian depend lead variable convolutional neural network 
continuous mixture gaussian p ( x | w ) = z p ( x | w ) p ( ) dt p ( ) = n ( 0 ) p ( x | w ) = n ( µ ( w ) ⌃ ( w ) ) cn n w µ ( ) ⌃ ( ) 10000 ⇥ 10000 one problem example image 100 100 10000 pixel image s pretty low resolution s high end anyway even case covariance matrix 10000 s lot want avoid s reasonable ask neural network output 10000 10000 image matrix get rid problem let s say covariance matrix diagonal 
continuous mixture gaussian p ( x | w ) = z p ( x | w ) p ( ) dt p ( ) = n ( 0 ) 2 ⌃ ( w ) ) p ( x | w ) = n ( µ ( w ) diag ( ( w ) ) ) cn n w µ ( ) 2 ( ) instead output whole large matrix sigma will ask neural network produce weight diagonal covariance matrix 10000 sigmas example put number diagonal covariance matrix define actual normal distribution condition latent variable t conditional distribution vectorized s gaussian zero diagonal element covariance matrix s okay mixture vector gaussian factor distribution nt much problem model fully defined train somehow train 
scaling expectation maximization max p ( x | w ) = w z p ( x | w ) p ( ) dt 
scaling expectation maximization max p ( x | w ) = w z p ( x | w ) p ( ) dt latent variable model — use em natural way use maximum likelihood estimation maximize density datum set give parameter parameter conventional unit neural network redefine sum integral marginalize latent variable t since latent variable let s use expectation maximization algorithm specifically invented kind model 
scaling expectation maximization max p ( x | w ) = w z p ( x | w ) p ( ) dt latent variable model — use em log p ( x | w ) maximize w q l ( w q ) l ( w q ) 
scaling expectation maximization max p ( x | w ) = w z p ( x | w ) p ( ) dt latent variable model — use em e-step intractable need compute p ( | x w ) log p ( x | w ) maximize w q l ( w q ) l ( w q ) expectation maximization algorithm recall week two be build lower bond logarithm marginal likelihood p x give w lower modele value something depend w new variational parameter q will maximize lower balance respect w q get lower bound high possible accurate close actual lower margin look like possible problem step play expectation maximisation algorithm use find best year original latent variable intractable case compute integral integral contain convolutional neural network hard analytically e-m actually way go else 
scaling expectation maximization max p ( x | w ) = w z p ( x | w ) p ( ) dt latent variable model — use em e-step intractable mcmc well previous week discuss markov chain monte carlo use use mcmc approximate m-step expectation maximisation 
scaling expectation maximization max p ( x | w ) = w z p ( x | w ) p ( ) dt latent variable model — use em e-step intractable mcmc x 1 eq log p ( x | w ) ⇡ log p ( x ts | w ) s=1 ts ⇠ q ( ) right well way amstaff instead used expect value respect q posterior distribution latent variable previous iteration approximate expect value sample average will maximize iteration instead expect value 
scaling expectation maximization max p ( x | w ) = w z p ( x | w ) p ( ) dt latent variable model — use em e-step intractable mcmc option better x 1 eq log p ( x | w ) ⇡ log p ( x ts | w ) s=1 ts ⇠ q ( ) s option well s go kind slow way iteration expectation optimization run like hundred situation markov chain wait converge start collect sample way kind mess loop reiteration expectation maximisation iteration markov chain monte carlo probably fast let s see else 
scaling expectation maximization max p ( x | w ) = w z p ( x | w ) p ( ) dt latent variable model — use em e-step intractable mcmc option better variational em log p ( x | w ) maximize w q l ( w q ) l ( w q ) subject qi ( ti ) = qe ( ti1 ) qe ( tim ) well try variational inference idea variational inference maximize lower bound 
scaling expectation maximization max p ( x | w ) = w z p ( x | w ) p ( ) dt latent variable model — use em e-step intractable mcmc option better variational em log p ( x | w ) maximize w q l ( w q ) l ( w q ) subject qi ( ti ) = qe ( ti1 ) qe ( tim ) restrict distribution vectorized example later charge datum object 50 dimensional q product 50 one dimensional distribution s nice way go s nice approach approximate expectation maximisation usually work pretty fast 
scaling expectation maximization max p ( x | w ) = w z p ( x | w ) p ( ) dt latent variable model — use em e-step intractable mcmc option better variational em intractable log p ( x | w ) maximize w q l ( w q ) l ( w q ) subject qi ( ti ) = qe ( ti1 ) qe ( tim ) turn case even intractable approximation enough get efficient method training latent variable model approximate even drive even less accurate approximation able build efficient method treat kind model 

w q let s see improve idea variational inference applicable latent variable model 
scaling variational em maximize l ( w q1 qn ) subject qi ( ti ) = qei ( ti1 ) qei ( tim ) w q1 qn idea variational inference maximize lower bound thing want maximize actually respect constraint say variational distribution q object factorized product one-dimensional distribution let s emphasize fact object individual variational distribution q distribution connect way one idea use follow say variational distribution q object factorized enough let s approximate even let s say s gaussian factorized factorized gaussian 
scaling variational em maximize w l ( w q1 qn ) subject qi ( ti ) = n ( mi diag ( s2i ) ) 1 s1 sn n way everything easier right every object latent variable t_i latent variable t_i variational distribution q_i gaussian parameter m_i s_i parameter model want train maximize lower bound respect parameter s nice idea problem add lot parameter training object example latent variable q_i 50-dimensional s vector 50 number 
scaling variational em maximize w l ( w q1 qn ) subject qi ( ti ) = n ( mi diag ( s2i ) ) 1 s1 sn n way 100 parameter training object add 50 number vector m_i object 50 number vector s_i object 100 number 100 parameter training object million training object s good idea add like 100 million parameter model approximation right probably overfeed probably really hard train really high number parameter 
scaling variational em maximize w l ( w q1 qn ) subject qi ( ti ) = n ( mi diag ( s2i ) ) 1 s1 sn n way 100 parameter training object clear test object also s obvious find parameter new object inference prediction generation new object solve optimization problem find parameter slow okay say approximate variational distribution factorized one enough approximation factor variational distribution gaussian nice many parameter object gaussian connect separate parameter let s try connect variational distribution q_i individual object one way say q_i s equal restrictive will able train anything meaningful approach say q_i s distribution 
scaling variational em maximize l ( w q1 qn ) subject qi ( ti ) = n ( ( xi ) diag ( s2 ( xi ) ) ) w depend x_i s weight let s say q_i normal distribution parameter somehow depend x_i turn actually q_i different share parameterization share form even new object easily find variational approximation q pass new object function function find parameter gaussian way need maximize lower bound respect original parameter w parameter phi define parametric way convert x_i s parameter distribution define function x_i parameter phi well already discuss convolutional neural network really powerful tool work image right 
scaling variational em maximize l ( w q1 qn ) subject qi ( ti ) = n ( ( xi ) diag ( s2 ( xi ) ) ) w cn n x let s use convolutional neural network parameter phi look original input image example cat transform parameter variational distribution way defined approximate variational distribution q form right okay let s look closer object try maximize recall lower bound 
scaling variational em maximize w subject cn n x qi ( ti ) = n ( ( xi ) diag ( s2 ( xi ) ) ) x p ( xi | ti w ) p ( ti ) eqi log qi ( ti ) definition equal sum respect object datum set expect value sum logarithm respect variation distribution q_i right recall plane expectation maximization algorithm really hard approximate expect value sampling q expect value used true posterior distribution latent variable t_i true posterior complicate know normalization constant use markov chain monte carlo sample slow approximate q gaussian know parameter know obtain object pass convolutional neural network parameter phi obtain parameter easily sample gaussian q approximate expect value 
scaling variational em maximize w x subject cn n p ( xi | ti w ) p ( ti ) eqi log qi ( ti ) qi ( ti ) = n ( ( xi ) diag ( s2 ( xi ) ) ) x b ti ⇠ n ( ( xi ) diag ( s2 ( xi ) ) low half intractable expect value easily approximate sampling sampling cheap s sampling gaussian recall model defined p x_i s actually defined another convolutional neural network 
scaling variational em maximize w x subject cn n qi ( ti ) = n ( ( xi ) diag ( s2 ( xi ) ) ) x p ( xi | ti w ) p ( ti ) eqi log qi ( ti ) b ti cn n w b ti ⇠ n ( ( xi ) diag ( s2 ( xi ) ) µ ( b ti ) ( b ti ) overall workflow follow start training image x pass first neural network parameter phi get parameter variational distribution q_i sample distribution one datum point something random different depend random seat something pass sample vector latent variable t_i second part neural network convolutional neural network parameter w cnn second part output us distribution image actually try make whole structure return us image close input image possible thing look really close something call auto encoder neural network neural network try output something close possible input model call variational auto encoder contrast usual auto encoder assembly inside variational approximation first part network call encoder encode image latent code distribution latent code second part call decoder decodes latent code image let s look happen forget variance variational distribution q 
scaling variational em maximize w subject cn n x qi ( ti ) = n ( ( xi ) diag ( s2 ( xi ) ) ) x p ( xi | ti w ) p ( ti ) eqi log qi ( ti ) b ti cn n w µ ( b ti ) ( b ti ) ( x ) = 0 b ti = ( xi ) usual autoencoder let s say set always zero okay ( x ) x variational distribution qi actually deterministic one always output main value xi case actually directly pass 
scaling variational em maximize w subject cn n x x p ( xi | ti w ) p ( ti ) eqi log qi ( ti ) qi ( ti ) = n ( ( xi ) diag ( s2 ( xi ) ) ) cn n w µ ( b ti ) ( b ti ) ( x ) = 0 b ti = ( xi ) usual autoencoder x second part network decoder way update usual autoencoder stochastic element inside variance variational distribution q actually something make model different usual autoencoder 
scaling variational em maximize w x subject cn n qi ( ti ) = n ( ( xi ) diag ( s2 ( xi ) ) ) x p ( xi | ti w ) p ( ti ) eqi log qi ( ti ) b ti cn n w b ti ⇠ n ( ( xi ) diag ( s2 ( xi ) ) µ ( b ti ) ( b ti ) okay let s look little bit closer objective be try maximize 
objective interpretation max x p ( xi | ti w ) p ( ti ) eqi log qi ( ti ) lower band variational lower band decomposed sum two term logarithm product sum logarithm right 
objective interpretation x p ( xi | ti w ) p ( ti ) max eqi log q ( ) x p ( ti ) = eqi log p ( xi | ti w ) + eqi log q ( ) second term equation equal minus kullback-leibler divergence variational distribution q prime distribution p ti 
objective interpretation x p ( xi | ti w ) p ( ti ) max eqi log q ( ) x p ( ti ) = eqi log p ( xi | ti w ) + eqi log q ( ) | { z } kl ( qi ( ti ) k p ( ti ) ) definition kl divergence something discuss week two also week three s something measure kind difference distribution maximize minus kl actually try minimize kl try push variational distribution qi close prior possible prior standard normal decide okay 
objective interpretation x p ( xi | ti w ) p ( ti ) max eqi log q ( ) x = eqi log p ( xi | ti w ) kl ( qi ( ti ) k p ( ti ) ) | { z } kxi µ ( ti ) k2 const ( xi ) = 1 simplicity second term first term interpreted follow simplicity set output variance 1 log likelihood xi give ti minus euclidean distance xi predict mu ti 
objective interpretation x p ( xi | ti w ) p ( ti ) max eqi log q ( ) x = eqi log p ( xi | ti w ) kl ( qi ( ti ) k p ( ti ) ) | { z } kxi µ ( ti ) k2 const reconstruction loss approximate posterior q ( ti ) ⇡ p ( ti | xi w ) regularization thing actually reconstruction loss try push xi close reconstruction possible mu ti mean output neural network consider whole variational autoencoder take input image x xi s posterior mu ti plus noise noise constant be training model be try make xi close mu ti possible basically objective usual autoencoder note also compute expect failure reconstruction loss respect qi qi try approximate posterior distrobution latent variable be try say latent variable ti likely cause x accord approximation qi want reconstruction loss low want particular sensible ti s particular xi want reconstruction accurate kind s really close usual autoencoder second part make difference kullback-leibler divergence s something push qi non-deterministic stochastic recall idea set qi variance zero get usual autoencoder right training model choose reduce number noise inside easier train choose inject noise well regularization kl divergence allow qi deterministic qi variance zero kl term infinity choose kind point parameter regularization force overall structure noise inside also notice kl divergence 
detect outlier cn n x kl ( q ( | x ) k p ( ) ) ⇡ 054 force qi close standard gaussian may detect outlier usual image training datum set something close training datum set pass image encoder output distribution qi close standard gaussian train way training try force distribution lie close standard gaussian new image network never see 
detect outlier cn n method detect outlier see supplementary x kl ( q ( | x ) k p ( ) ) ⇡ 625 suspicious behavior something else conditional neural network encoder never see kind image right output distribution ti far away gaussian want nt trained make close gaussian look distance variational distribution qi standard gaussian understand anomalistic point detect outlier also note s kind easy generate new point nearly hallucinate new datum kind model 
generate new sample p ( x | w ) = b ti ⇠ n ( 0 ) z b ti p ( x | w ) p ( ) dt cn n w µ ( b ti ) ( b ti ) model defined way integral respect p make new point new image two step first sample ti prior standard normal pass sample standard gaussian decoder network decode latent code image get new sample fake silly picture fake ad something 

kl ( qi ( ti ) k p ( ti ) ) 
gradient max w x eqi log p ( xi | ti w ) kl ( qi ( ti ) k p ( ti ) ) easy analytical kl ( qi ( ti ) k p ( ti ) ) = x j log j ( ti ) + 2 j ( ti ) + µ2j ( ti ) 2 1 
gradient f ( w ) = x eqi log p ( xi | ti w ) 
gradient f ( w ) = x eqi log p ( xi | ti w ) qi ( ti ) = q ( ti | xi ) = n ( mi diag ( s2i ) ) 
gradient f ( w ) = x eq ( ti xi ) log p ( xi | ti w ) qi ( ti ) = q ( ti | xi ) = n ( mi diag ( s2i ) ) 
gradient rw f ( w ) = rw n x i=1 eq ( ti xi ) log p ( xi | ti w ) 
gradient rw f ( w ) = rw n z x i=1 q ( ti | xi ) log p ( xi | ti w ) dti 
gradient rw f ( w ) = x rw z q ( ti | xi ) log p ( xi | ti w ) dti 
gradient rw f ( w ) = xz rw q ( ti | xi ) log p ( xi | ti w ) dti 
gradient rw f ( w ) = xz q ( ti q ( | xii | x ) ir w ) log p ( xi | ti w ) dti 
gradient rw f ( w ) = = xz x q ( ti q ( | xii | x ) ir w ) log p ( xi | ti w ) dti eq ( ti xi ) rw log p ( xi | ti w ) 
gradient rw f ( w ) = = xz x ⇡ x q ( ti q ( | xii | x ) ir w ) log p ( xi | ti w ) dti eq ( ti xi ) rw log p ( xi | ti w ) rw log p ( xi | b ti w ) b ti ⇠ q ( ti | xi ) 
gradient rw f ( w ) ⇡ n x i=1 rw log p ( xi | b ti w ) gradient standard nn b ti ⇠ q ( ti | xi ) 
gradient rw f ( w ) ⇡ n x i=1 rw log p ( xi | b ti w ) n x n ⇡ rw log p ( xis | b tis w ) n s=1 stochastic gradient standard nn b ti ⇠ q ( ti | xi ) ⇠ u { 1 n } 
gradient rw f ( w ) ⇡ n x i=1 rw log p ( xi | b ti w ) n x n ⇡ rw log p ( xis | b tis w ) n s=1 cn n x b ti cn n w b ti ⇠ n ( ( xi ) diag ( s2 ( xi ) ) µ ( b ti ) ( b ti ) 

log p ( xi | ti w ) 
gradient r f ( w ) = r xz q ( ti | xi ) log p ( xi | ti w ) dti 
gradient r f ( w ) = xz r q ( ti | xi ) log p ( xi | ti w ) dti 
gradient r f ( w ) = = xz xz r q ( ti | xi ) log p ( xi | ti w ) dti q ( ti | xi ) r log p ( xi | ti w ) dti | { z } 0 
gradient r f ( w ) = xz r q ( ti | xi ) log p ( xi | ti w ) dti x z q ( ti | xi ) r q ( ti | xi ) log p ( xi | ti w ) dti = q ( ti | xi ) 
gradient r f ( w ) = xz r q ( ti | xi ) log p ( xi | ti w ) dti x z q ( ti | xi ) r q ( ti | xi ) log p ( xi | ti w ) dti = q ( ti | xi ) rg ( ) r log g ( ) = g ( ) 
gradient r f ( w ) = xz r q ( ti | xi ) log p ( xi | ti w ) dti x z q ( ti | xi ) r q ( ti | xi ) log p ( xi | ti w ) dti = q ( ti | xi ) xz = q ( ti | xi ) r logq ( ti | xi ) log p ( xi | ti w ) dti rg ( ) r log g ( ) = g ( ) 
gradient r f ( w ) = xz r q ( ti | xi ) log p ( xi | ti w ) dti x z q ( ti | xi ) r q ( ti | xi ) log p ( xi | ti w ) dti = q ( ti | xi ) xz = q ( ti | xi ) r logq ( ti | xi ) log p ( xi | ti w ) dti = x eq ( ti xi ) r log q ( ti | xi ) log p ( xi | ti w ) dti log-derivative trick 
gradient r f ( w ) = xz r q ( ti | xi ) log p ( xi | ti w ) dti x z q ( ti | xi ) r q ( ti | xi ) log p ( xi | ti w ) dti = q ( ti | xi ) xz = q ( ti | xi ) r logq ( ti | xi ) log p ( xi | ti w ) dti = x eq ( ti xi ) r log q ( ti | xi ) log p ( xi | ti w ) dti log-derivative trick like 1000000 

log p ( xi | ti w ) 
gradient r f ( w ) = x r eq ( ti xi ) log p ( xi | ti w ) ti ⇠ q ( ti | xi ) = n ( mi diag ( s2i ) ) 
gradient r f ( w ) = x r eq ( ti xi ) log p ( xi | ti w ) ti ⇠ q ( ti | xi ) = n ( mi diag ( s2i ) ) ti = ` + mi ` ⇠ p ( ` ) = n ( 0 ) 
gradient r f ( w ) = x r eq ( ti xi ) log p ( xi | ti w ) ti ⇠ q ( ti | xi ) = n ( mi diag ( s2i ) ) ti = ` si + mi = g ( ` xi ) ` ⇠ p ( ` ) = n ( 0 ) 
gradient r f ( w ) = = x x r eq ( ti xi ) log p ( xi | ti w ) r ep ( ` ) log p ( xi | g ( ` xi ) w ) ti ⇠ q ( ti | xi ) = n ( mi diag ( s2i ) ) ti = ` si + mi = g ( ` xi ) ` ⇠ p ( ` ) = n ( 0 ) 
gradient r f ( w ) = = xz x r eq ( ti xi ) log p ( xi | ti w ) p ( ` ) r log p ( xi | g ( ` xi ) w ) ti ⇠ q ( ti | xi ) = n ( mi diag ( s2i ) ) ti = ` si + mi = g ( ` xi ) ` ⇠ p ( ` ) = n ( 0 ) 
gradient r f ( w ) = = xz = x x r eq ( ti xi ) log p ( xi | ti w ) p ( ` ) r log p ( xi | g ( ` xi ) w ) ep ( ` ) r log p ( xi | g ( ` xi ) w ) ti ⇠ q ( ti | xi ) = n ( mi diag ( s2i ) ) ti = ` si + mi = g ( ` xi ) ` ⇠ p ( ` ) = n ( 0 ) 
gradient r f ( w ) = = xz = x x r eq ( ti xi ) log p ( xi | ti w ) p ( ` ) r log p ( xi | g ( ` xi ) w ) ep ( ` ) r log p ( xi | g ( ` xi ) w ) 
gradient r f ( w ) = x ep ( ` ) r log p ( xi | g ( ` xi ) w ) ti = ` cn n p ( ` ) = n ( 0 ) si + mi = g ( ` xi ) x b ti cn n w ` ⇠ n ( 0 ) µ ( b ti ) ( b ti ) 
variational autoencoder summary • infinite mixture gaussian • learn em + approximate q gaussian + stochastic variational inference • like plain autoencoder noise kl regularization • generate nice image 

nonparametric method welcome week six week will talk non-parametric method especially interesting would gaussian process let s start see non-parametric method 
parametric method define parametric model 2 find best parameter used map estimation know parametric method define model depend parametric theta find optimal value theta maximize maximum posteriori maybe take maximi destination 
parametric method parameter fit linear model datum will parameter b case 
parametric method parameter datum become complex add parameter 
parametric method parameter example fit parabola case will three parameter 
parametric method parameter datum become complex need add parameter example case add eight parameter fit paranorminal eighth degree another case non-parametric method 
non-parametric method parametric method • fix number parameter non-parametric • number parameter depend dataset size non-parametric method number parameter depend datum set size number datum point increase decision boundary become complex parametric method though number parameter fix nt matter much datum 
non-parametric method knn k-nearest neighbor one non-parametric method know k-nearest neighbor case prediction find k case five nearest neighbor depend x prediction average target value neighboring point would example 5 sum target five nearest neighbor point prediction would look red curve nt smooth make smoother 
non-parametric method nadaraya-watson k-nearest neighbor nadaraya-watson can use circle nadaraya-watson regression case weigh point distance would like say point really neighbor point highest weight point far away lower weight written follow x prediction point x equal sum point datum set wi x weight point time yi target highest point weight compute kernel function x point want predict xi xi position highest point also ensure weight sum 1 divide kernel sum point see different row kernel 
kernel gaussian uniform popular one so-called gaussian kernel gaussian kernel measure similarity kernel x1 x2 exponent minus one two sigma square sigma parameter kernel time square distance point plot would look like take higher value sigma value would drop slower would weight point bit higher sigma low kernel would quickly drop zero can also use uniform kernel uniform kernel equally weight point distance age will see have see 
parametric vs non-parametric parametric non-parametric • limit complexity • arbitrary complex • faster inference • need process datum • slow learn prediction • learn remember datum parametric method non-parametric method parametric method fix number parameter complexity limit however non-parametric method decision boundary become complex number datum point increase say roughly speaking model go obviously complex number datum point go infinity parametric method inference quite fast example linear regression feed weight prediction would scalar multiplication new point weight vector non-parametric method though will process datum point make prediction example nadaraya-watson algorithm compute weight datum point parametric method load sometimes slow example training neural network may take hour even day non traditional method training usually remember datum actually bit true since k-nearest neighbor example pre-computer information make prediction bit faster however case case simply remember datum 
gaussian process estimate uncertainty prediction have see non-parametric method one method be particularly interested gaussian process aggression model will able estimate uncertainty prediction desirable property complication like medicine will see next video gaussian process 

gaussian process 
random process trajectory assign random variable 1d distribution 
random process ( технический слайд ) trajectory assign random variable 1d distribution d=1 = > time 
gaussian process definition random process gaussian finite number point joint distribution normal 
gaussian process ( технический слайд ) definition random process gaussian finite number point joint distribution normal want joint point normal + efficient sampling ( plot sampling ) + finite-dimensional distribution 
gaussian process parameter finally 
stationary process definition random process stationary finite-dimensional distribution depend relative position point 
stationary process stationary 
stationary process stationary stationary seasonality 
stationary process stationary 
stationary process stationary stationary trend 
stationary process stationary 
stationary process stationary stationary 
stationary process definition gaussian process stationary variance 
kernel radial basis function ( rbf ) length-scale rational quadratic white noise 
kernel 
kernel 
kernel 

gp machine learn 
task 
task 
task 95 % confidence interval mean 
prediction gaussian process stationary prior 
prediction gaussian process stationary prior 
prediction gaussian process stationary prior 
prediction gaussian process stationary prior 
prediction gaussian process stationary prior 
prediction gaussian process stationary prior 
prediction gaussian process stationary prior 
prediction gaussian process stationary prior 
технически слайд var = 0 @ point grow move away 
preprocess • far datum • remove trend seasonality • subtract mean normalize 
технический слайд posterior stationary remember invert • far datum • remove trend seasonality • subtract mean normalize 

nuance gp 
noisy observation 
noisy observation independent gaussian noise 
noisy observation independent gaussian noise 
noisy observation independent gaussian noise 
kernel parameter 001 
kernel parameter 10 
kernel parameter 2 
kernel parameter optimize gradient ascent 
kernel parameter 
kernel parameter fitting noise 
kernel parameter fitting signal without noise 
kernel parameter fitting noisy signal 
classification latent process class probability training • approximate latent process datum • compute prediction 
classification 
classification 
induce input idea • replace dataset small number point ( like svm ) • predict used point speed • precompute • mean • variance 
induce input ( технический слайд ) idea • replace dataset small number point ( like svm ) • predict used point optimize position & value mle 
induce input full dataset ( 100 point ) 
induce input induce point ( 10 point ) 

bayesian optimization 
black-box optimization gradient know • gradient descent restart gradient unknown • numerically estimate gradient • grid search random search 
black-box optimization • geographic coordinate — amount oil 1 sample = $ 1000000 • hyperparameter nn — objective function 1 sample = 10 hour • drug — effectiveness disease 1 sample = 2 month $ 10000 life rat 
black-box optimization goal optimize minimum number trial 
black-box optimization goal optimize minimum number trial surrogate model • approximate true function • cheap evaluate 
black-box optimization goal optimize minimum number trial surrogate model • approximate true function • cheap evaluate acquisition function • estimate profit optimization • used surrogate model 
surrogate model model arbitrary complex function nonparametric method profitable estimate uncertainty gaussian process 
acquisition function exploration search region high uncertainty exploitation search region high estimate value 
maximum probability improvement ( mpi ) current best value work well value maximum know 
upper confidence bound ( ucb ) 
expect improvement ( ei ) widely used 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
example start point converge train gp find maximum used eg gradient ascent evaluate function maximum 
технический слайд start point converge stop train gp find maximum used eg gradient ascent evaluate function maximum 
random search vs gaussian process rs gp + parallelizable - hard parallelize experiment - need many point high dimension + require less point average function evaluation expensive 

bayesian optimization [ music ] video will see couple example bayesian optimization apply real world problem 
hyperparameter tune network parameter • number layer • layer size • dropout off • batch normalization off • nonlinearity training parameter • learn rate • momentum usually find better optima tune hand honest comparison method research first one hyperparameter tune usually train neural network retrain many time find optimal number layer layer size whether use dropout use batch normalization nonlinearity use relu selu also training parameter like learn rate momentum maybe change different optimizer example sgd can can use bayesian optimization find best value parameter automatically usually find better optima tune hand also allow honest comparison method research example come brilliant method spend lot time tune parameter paper want compare model model really tempting spend much time tune parameter model however can can run automatic hyperparameter tune find best variable parameter model compare case comparison would honest 
discrete continuous variable • treat discrete variable continuous fitting process • maximize possible value discrete variable • multi-arm bandit variable discrete problem actually mixture discrete continuous variable example learn rate continuous parameter whether use drop actually binary decision mix continuous discrete variable gaussian process well simple trick like treat discrete variable continuous fitting process example use drop would value one nt use would value zero try maximize acquisition function optimize [ inaudible ] force whole possible value discrete variable example find maximum acquisition function without drop will find drop select one case better one special case variable discrete call multi-arm bandit widely used information retrieval task example be build search engine result page select lot hyperparameter discrete case bayesian optimization really useful 
drug discovery string representation molecule smile coc1=c ( c=cc ( c1 ) c=o ) smile = simplified molecular input line entry specification another application drug discovery molecule probably drug severe disease case molecule represent used string string call smile construct molecule simply 
encoding-decode train vae smile point reconstruct valid molecule rafael gómez-bombarelli et al https 161002415 build autoencoder try take smile input reproduce output use variational autoencoder talk week five make latent space dense move along space point able reconstruct valid molecule s trick know molecule useful cure disease 
bayesian optimization optimize property like effectiveness cancer property f ( z ) true find maximum acquisition function 2 mol 1 3 4 5 6 rafael gómez-bombarelli et al https 161002415 perform trial new molecule plot latent space latent space want find position maximum molecule best cure disease find maximum latent space simply plug decoder reconstruct molecule trial example vitro viva get value point new value add model reconstruct gaussian process find new maximum acquisition function [ inaudible ] quickly find new drug different disease [ music ] 

mining course overview [ sound ] hi welcome course cluster analysis datum mining start course be go give general course overview first cluster analysis actually fly city easily identify field forest commercial area residential area base feature without anybody s explicit training labele power cluster analysis course be go systematically introduce concept method cluster analysis help answer follow question different proximity measure effective cluster cluster massive number datum point efficiently 
cluster analysis  fly city one easily identify field forest commercial area residential area base feature without anyone ’ explicit “ training ” - power cluster analysis course systematically study cluster analysis method help answer follow 2  different proximity measure effective cluster  cluster massive number datum point efficiently  find cluster arbitrary shape multiple level granularity  judge quality cluster discover system find cluster arbitrary shape multilevel granularity judge quality cluster discover system cluster analysis bring lot value datum mining value cluster analysis cluster analysis may help partition massive datum group base feature cluster analysis also help subsequent datum mining process pattern discovery classification outlier analysis role cluster analysis play datum mining specialization learn various scalable method find cluster massive datum 
value cluster analysis  value cluster analysis  cluster analysis help partition massive datum group base feature  cluster analysis often help subsequent datum mining process pattern discovery classification outlier analysis  3 role cluster analysis play datum mining specialization  learn various scalable method find cluster massive datum  learn mine different kind cluster effectively  also learn evaluate quality cluster find  cluster analysis help classification outlier analysis datum mining task learn mine different kind cluster effectively will also learn evaluate quality cluster will find see cluster analysis help classification outlier analysis datum mining task 
broad application cluster analysis  datum summarization compression reduction   collaborative filter recommendation system customer segmentation   4 cluster stream datum detect trend pattern multimedium datum analysis biological datum analysis social network analysis   find like-minded user similar product dynamic trend detection   example image process vector quantization example cluster image audio clip protein sequence etc key intermediate step datum mining task  generate compact summary datum classification pattern discovery hypothesis generation testing  outlier detection outlier - “ far away ” cluster cluster analysis broad application example datum summarization compression reduction like image process vector quantization need cluster analysis collaborative filter recommendation system customer segmentation find like-minded user similar product cluster analysis dynamic trend detection find cluster stream datum detect trend pattern effective multimedium analysis biological datum analysis social network analysis find may effect measure group video clip find cluster gene protein sequence cluster analysis key intermediate step many datum mining task example want generate compact summary datum classification pattern discovery hypothesis generation testing neat cluster mass wana find outlier detection actually outlier far away cluster course major reference reading 
major reference reading course  textbook han j kamber m & pei j ( 2011 ) datum mining concept technique ( 3rd ed ) morgan kaufmann   chapter related course  chapter 2 get know datum ( section 24 measure datum similarity dissimilarity )  chapter 10 cluster analysis basic concept method  reference list end lecture video 5 textbook publish 2011 datum mining concept technique 3rd edition publish morgan kaufmann book used two chapter first chapter 2 [ inaudible ] cover section 24 measure datum similarity dissimilarity chapter 4 major source related textbook textbook chapter 4 major source related seminar lecture call cluster analysis basic concept method reference list end lecture video course structure six lesson lesson 1 cluster analysis introduction lesson 2 similarity measure cluster analysis two lesson first module module lesson 3 partitioning-based cluster method lesson 4 ( part ) hierarchical cluster method ( ) content module part ii lesson 2 hierarchical cluster method ( ii ) lesson 5 density-based grid-based cluster method form content module 3 
course structure  lesson 1 cluster analysis introduction  lesson 2 similarity measure cluster analysis  lesson 3 partitioning-based cluster method  lesson 4 ( part ) hierarchical cluster method ( )  lesson 4 ( part ii ) hierarchical cluster method ( ii )  lesson 5 density-based grid-based cluster method  lesson 6 cluster validation 6 module 1 module 2 module 3 module 4 finally lesson 6 cluster validation form material module 4 
course general information instructor jiawei han abel bliss professor department computer science university illinois urbana-champaign  teach assistant  course prerequisite familiarity basic datum structure algorithms  course assessment  in-video question  lesson quiz  programming assignment  7 general information course be jiawei han be professor department computer science university illinois urbana-champaign be instructor teach assistant web help course prerequisite long familiar basic datum structure algorithms like work hard be quite okay course assessment in-video question help understand course material need pass minimum requirement lesson quiz two programming assignment thank hope enjoy course thank [ music ] 

10 9 8 7 6 5 4 3 2 1 
author jiawei han bliss professor engineering department computer science university illinois urbana-champaign receive numerous award contribution research knowledge discovery datum mining include acm sigkdd innovation award ( 2004 ) ieee computer society technical achievement award ( 2005 ) ieee w wallace mcdowell award ( 2009 ) fellow acm ieee serve founding editor-in-chief acm transaction knowledge discovery datum ( 2006–2011 ) editorial board member several journal include ieee transaction knowledge datum engineering datum mining knowledge discovery micheline kamber master ’ degree computer science ( specialize artificial intelligence ) concordium university montreal quebec nserc scholar work researcher mcgill university simon fraser university switzerland background datum mining passion writing easyto-understand term help make text favorite professional instructor student jian pei currently associate professor school compute science simon fraser university british columbia receive degree compute science simon fraser university 2002 dr jiawei han ’ supervision publish prolifically premier academic forum datum mining databasis web search information retrieval actively serve academic community publication receive thousand citation several prestigious award associate editor several datum mining datum analytic journal xxxv 
2 get know datum ’ tempting jump straight mining first need get datum ready involve closer look attribute datum value real-world datum typically noisy enormous volume ( often several gigabyte ) may originate hodgepodge heterogenous source chapter get familiar datum knowledge datum useful datum preprocess ( see chapter 3 ) first major task datum mining process want know follow type attribute field make datum kind value attribute attribute discrete continuous-valu datum look like value distribute way visualize datum get better sense spot outlier measure similarity datum object respect other gain insight datum help subsequent analysis “ learn datum ’ helpful datum preprocess ” begin section 21 study various attribute type include nominal attribute binary attribute ordinal attribute numeric attribute basic statistical description used learn attribute ’ value describe section 22 give temperature attribute example determine mean ( average value ) median ( middle value ) mode ( common value ) measure central tendency give us idea “ middle ” center distribution know basic statistic regard attribute make easier fill miss value smooth noisy value spot outlier datum preprocess knowledge attribute attribute value also help fix inconsistency incur datum integration plot measure central tendency show us datum symmetric skewer quantile plot histogram scatter plot graphic display basic statistical description useful datum preprocess provide insight area mining field datum visualization provide many additional technique view datum graphical mean help identify relation trend biase “ hide ” unstructured datum set technique may simple scatter-plot matrix ( datum mining concept technique doi b978-0-12-381479-100002-2 c 2012 elsevier right re-serve 39 
40 chapter 2 get know datum two attribute map onto 2-d grid ) sophisticated method treemaps ( hierarchical partition screen display base attribute value ) datum visualization technique describe section 23 finally may want examine similar ( dissimilar ) datum object example suppose database datum object patient describe symptom may want find similarity dissimilarity individual patient information allow us find cluster like patient within datum set dissimilarity object may also used detect outlier datum perform nearest-neighbor classification ( cluster topic chapter 10 11 nearest-neighbor classification discuss chapter 9 ) many measure assess similarity dissimilarity general measure refer proximity measure think proximity two object function distance attribute value although proximity also calculate base probability rather actual distance measure datum proximity describe section 24 summary end chapter know different attribute type basic statistical measure describe central tendency dispersion ( spread ) attribute datum also know technique visualize attribute distribution compute similarity dissimilarity object 21 datum object attribute type datum set make datum object datum object represent entity—in sale database object may customer store item sale medical database object may patient university database object may student professor course datum object typically describe attribute datum object also refer sample example instance datum point object datum object store database datum tuple row database correspond datum object column correspond attribute section define attribute look various attribute type 211 attribute attribute datum field represent characteristic feature datum object noun attribute dimension feature variable often used interchangeably literature term dimension commonly used datum warehousing machine learn literature tend use term feature statistician prefer term variable datum mining database professional commonly use term attribute well attribute describe customer object include example customer id name address observed value give attribute know observation set attribute used describe give object call attribute vector ( feature vector ) distribution datum involve one attribute ( variable ) call univariate bivariate distribution involve two attribute 
21 datum object attribute type 41 type attribute determine set possible values—nominal binary ordinal numeric—the attribute follow subsection introduce type 212 nominal attribute nominal mean “ relate ” value nominal attribute symbol name thing value represent kind category code state nominal attribute also refer categorical value meaningful order computer science value also know enumeration example 21 nominal attribute suppose hair color marital status two attribute describe person object application possible value hair color black brown blond red auburn gray white attribute marital status take value single married divorce widow hair color marital status nominal attribute another example nominal attribute occupation value teacher dentist programmer farmer although say value nominal attribute symbol “ name thing ” possible represent symbol “ name ” number hair color instance assign code 0 black 1 brown another example customor id possible value numeric however case number intend used quantitatively mathematical operation value nominal attribute meaningful make sense subtract one customer id number another unlike say subtract age value another ( age numeric attribute ) even though nominal attribute may integer value consider numeric attribute integer meant used quantitatively say numeric attribute section 215 nominal attribute value meaningful order quantitative make sense find mean ( average ) value median ( middle ) value attribute give set object one thing interest however attribute ’ commonly occur value value know mode one measure central tendency learn measure central tendency section 22 213 binary attribute binary attribute nominal attribute two category state 0 1 0 typically mean attribute absent 1 mean present binary attribute refer boolean two state correspond true false example 22 binary attribute give attribute smoker describe patient object 1 indicate patient smoke 0 indicate patient similarly suppose 
42 chapter 2 get know datum patient undergo medical test two possible outcome attribute medical test binary value 1 mean result test patient positive 0 mean result negative binary attribute symmetric state equally valuable carry weight preference outcome code 0 one example can attribute gender state male female binary attribute asymmetric outcome state equally important positive negative outcome medical test hiv convention code important outcome usually rarest one 1 ( eg hiv positive ) 0 ( eg hiv negative ) 214 ordinal attribute ordinal attribute attribute possible value meaningful order ranking among magnitude successive value know example 23 ordinal attribute suppose drink size correspond size drink available fast-food restaurant nominal attribute three possible value small medium large value meaningful sequence ( correspond increase drink size ) however tell value much bigger say medium large example ordinal attribute include grade ( eg + a− + ) professional rank professional rank enumerate sequential order example assistant associate full professor private private first class specialist corporal sergeant army rank ordinal attribute useful register subjective assessment quality measure objectively thus ordinal attribute often used survey rating one survey participant ask rate satisfied customer customer satisfaction follow ordinal category 0 dissatisfied 1 somewhat dissatisfied 2 neutral 3 satisfied 4 satisfied ordinal attribute may also obtain discretization numeric quantity splitting value range finite number order category describe chapter 3 datum reduction central tendency ordinal attribute represent mode median ( middle value order sequence ) mean defined note nominal binary ordinal attribute qualitative describe feature object without give actual size quantity value qualitative attribute typically word represent category integer used represent computer code category opposed measurable quantity ( eg 0 small drink size 1 medium 2 large ) follow subsection look numeric attribute provide quantitative measurement object 
21 datum object attribute type 215 43 numeric attribute numeric attribute quantitative measurable quantity represent integer real value numeric attribute interval-scaled ratio-scale interval-scaled attribute interval-scaled attribute measure scale equal-size unit value interval-scaled attribute order positive 0 negative thus addition provide ranking value attribute allow us compare quantify difference value example 24 interval-scaled attribute temperature attribute interval-scaled suppose outdoor temperature value number different day day object order value obtain ranking object respect temperature addition quantify difference value example temperature 20◦ c five degree higher temperature 15◦ c calendar date another example instance year 2002 2010 eight year apart temperature celsius fahrenheit true zero-point neither 0◦ c 0◦ f indicate “ ” ( celsius scale example unit measurement 100 difference melt temperature boil temperature water atmospheric pressure ) although compute difference temperature value talk one temperature value multiple another without true zero say instance 10◦ c twice warm 5◦ c speak value term ratio similarly true zero-point calendar date ( year 0 correspond begin time ) bring us ratio-scale attribute true zero-point exit interval-scaled attribute numeric compute mean value addition median mode measure central tendency ratio-scale attribute ratio-scale attribute numeric attribute inherent zero-point measurement ratio-scale speak value multiple ( ratio ) another value addition value order also compute difference value well mean median mode example 25 ratio-scale attribute unlike temperature celsius fahrenheit kelvin ( k ) temperature scale consider true zero-point ( 0◦ k = −27315◦ c ) point particle comprise matter zero kinetic energy example ratio-scale attribute include count attribute year experience ( eg object employee ) number word ( eg object document ) additional example include attribute measure weight height latitude longitude 
44 chapter 2 get know datum coordinate ( eg cluster house ) monetary quantity ( eg 100 time richer $ 100 $ 1 ) 216 discrete versus continuous attribute presentation organized attribute nominal binary ordinal numeric type many way organize attribute type type mutually exclusive classification algorithms develop field machine learn often talk attribute either discrete continuous type may processed differently discrete attribute finite countably infinite set value may may represent integer attribute hair color smoker medical test drink size finite number value discrete note discrete attribute may numeric value 0 1 binary attribute value 0 110 attribute age attribute countably infinite set possible value infinite value put one-to-one correspondence natural number example attribute customer id countably infinite number customer grow infinity reality actual set value countable ( value put one-to-one correspondence set integer ) zip code another example attribute discrete continuous term numeric attribute continuous attribute often used interchangeably literature ( confuse classic sense continuous value real number whereas numeric value either integer real number ) practice real value represent used finite number digit continuous attribute typically represent floating-point variable 22 basic statistical description datum datum preprocess successful essential overall picture datum basic statistical description used identify property datum highlight datum value treat noise outlier section discuss three area basic statistical description start measure central tendency ( section 221 ) measure location middle center datum distribution intuitively speaking give attribute value fall particular discuss mean median mode midrange addition assess central tendency datum set also would like idea dispersion datum datum spread common datum dispersion measure range quartile interquartile range five-number summary boxplot variance standard deviation datum measure useful identify outlier describe section 222 finally use many graphic display basic statistical description visually inspect datum ( section 223 ) statistical graphical datum presentation software 
22 basic statistical description datum 45 package include bar chart pie chart line graph popular display datum summary distribution include quantile plot quantile–quantile plot histogram scatter plot 221 measure central tendency mean median mode section look various way measure central tendency datum suppose attribute x like salary record set object let x1 x2 xn set n observed value observation x value may also refer datum set ( x ) plot observation salary would value fall give us idea central tendency datum measure central tendency include mean median mode midrange common effective numeric measure “ center ” set datum ( arithmetic ) mean let x1 x2 xn set n value observation numeric attribute x like salary mean set value n x x̄ = xi i=1 n = x1 + x2 + · · · + xn n ( 21 ) correspond built-in aggregate function average ( avg ( ) sql ) provide relational database system example 26 mean suppose follow value salary ( thousand dollar ) show increase order 30 36 47 50 52 52 56 60 63 70 70 used eq ( 21 ) 30 + 36 + 47 + 50 + 52 + 52 + 56 + 60 + 63 + 70 + 70 + 110 12 696 = = 58 12 x̄ = thus mean salary $ 58000 sometimes value xi set may associate weight wi = 1 n weight reflect significance importance occurrence frequency attach respective value case compute n x x̄ = wi xi i=1 n x = w1 x1 + w2 x2 + · · · + wn xn w1 + w2 + · · · + wn wi i=1 call weight arithmetic mean weight average ( 22 ) 
46 chapter 2 get know datum although mean singlemost useful quantity describe datum set always best way measure center datum major problem mean sensitivity extreme ( eg outlier ) value even small number extreme value corrupt mean example mean salary company may substantially push highly paid manager similarly mean score class exam can pull quite bit low score offset effect cause small number extreme value instead use trim mean mean obtain chop value high low extreme example sort value observed salary remove top bottom 2 % compute mean avoid trimming large portion ( 20 % ) end result loss valuable information skewer ( asymmetric ) datum better measure center datum median middle value set order datum value value separate higher half datum set lower half probability statistic median generally apply numeric datum however may extend concept ordinal datum suppose give datum set n value attribute x sort increase order n odd median middle value order set n even median unique two middlemost value value x numeric attribute case convention median take average two middlemost value example 27 median let ’ find median datum example datum already sort increase order even number observation ( ie 12 ) therefore median unique value within two middlemost value 52 56 ( within sixth seventh value list ) convention assign = 108 average two middlemost value median 52+56 2 2 = thus median $ 54000 suppose first 11 value list give odd number value median middlemost value sixth value list value $ 52000 median expensive compute large number observation numeric attribute however easily approximate value assume datum group interval accord xi datum value frequency ( ie number datum value ) interval know example employee may group accord annual salary interval $ 10–20000 $ 20–30000 let interval contain median frequency median interval approximate median entire datum set ( eg median salary ) interpolation used formula  p n 2 − freq l median = l1 + width ( 23 ) freqmedian l1 lower median interval n number value  pboundary entire datum set freq l sum frequency interval 
22 basic statistical description datum 47 lower median interval freqmedian frequency median interval width width median interval mode another measure central tendency mode set datum value occur frequently set therefore determine qualitative quantitative attribute possible greatest frequency correspond several different value result one mode datum set one two three mode respectively call unimodal bimodal trimodal general datum set two mode multimodal extreme datum value occur mode example 28 mode datum example 26 bimodal two mode $ 52000 $ 70000 unimodal numeric datum moderately skewer ( asymmetrical ) follow empirical relation mean − mode ≈ 3 × ( mean − median ) ( 24 ) imply mode unimodal frequency curf moderately skewer easily approximate mean median value know midrange also used assess central tendency numeric datum set average largest smallest value set measure easy compute used sql aggregate function max ( ) min ( ) example 29 midrange midrange datum example 26 30000+110000 2 = $ 70000 unimodal frequency curve perfect symmetric datum distribution mean median mode center value show figure 21 ( ) datum real application symmetric may instead either positively skewer mode occur value smaller median ( figure 21b ) negatively skewer mode occur value greater median ( figure 21c ) mean median mode mode mean median ( ) symmetric datum ( b ) positively skewer datum mean mode median ( c ) negatively skewer datum figure 21 mean median mode symmetric versus positively negatively skewer datum 
48 chapter 2 get know datum 222 measure dispersion datum range quartile variance standard deviation interquartile range look measure assess dispersion spread numeric datum measure include range quantile quartile percentile interquartile range five-number summary display boxplot useful identify outlier variance standard deviation also indicate spread datum distribution range quartile interquartile range start let ’ study range quantile quartile percentile interquartile range measure datum dispersion let x1 x2 xn set observation numeric attribute x range set difference largest ( max ( ) ) smallest ( min ( ) ) value suppose datum attribute x sort increase numeric order imagine pick certain datum point split datum distribution equal-size consecutive set figure datum point call quantile quantile point take regular interval datum distribution divide essentially equalsize consecutive set ( say “ essentially ” may datum value x divide datum exactly equal-sized subset readability refer equal ) kth q-quantile give datum distribution value x q datum value less x ( q − k ) q datum value x k integer 0 < k < q q − 1 q-quantile 2-quantile datum point divide lower upper half datum distribution correspond median 4-quantiles three datum point split datum distribution four equal part part represent one-fourth datum distribution commonly refer quartile 100-quantile commonly refer percentile divide datum distribution 100 equal-sized consecutive set median quartile percentile widely used form quantile 25 % q1 q2 q3 median 75th 25th percentile percentile figure 22 plot datum distribution attribute x quantile plot quartile three quartile divide distribution four equal-size consecutive subset second quartile correspond median 
22 basic statistical description datum 49 quartile give indication distribution ’ center spread shape first quartile denote q1 25th percentile cut lowest 25 % datum third quartile denote q3 75th percentile—it cut lowest 75 % ( highest 25 % ) datum second quartile 50th percentile median give center datum distribution distance first third quartile simple measure spread give range cover middle half datum distance call interquartile range ( iqr ) defined iqr = q3 − q1 ( 25 ) example 210 interquartile range quartile three value split sort datum set four equal part datum example 26 contain 12 observation already sort increase order thus quartile datum third sixth ninth value respectively sort list therefore q1 = $ 47000 q3 $ 63000 thus interquartile range iqr = 63 − 47 = $ 16000 ( note sixth value median $ 52000 although datum set two median since number datum value even ) five-number summary boxplot outlier single numeric measure spread ( eg iqr ) useful describe skewer distribution look symmetric skewer datum distribution figure 21 symmetric distribution median ( measure central tendency ) split datum equal-size half occur skewer distribution therefore informative also provide two quartile q1 q3 along median common rule thumb identify suspect outlier single value fall least 15 × iqr third quartile first quartile q1 median q3 together contain information endpoint ( eg tail ) datum fuller summary shape distribution obtain provide lowest highest datum value well know five-number summary five-number summary distribution consist median ( q2 ) quartile q1 q3 smallest largest individual observation written order minimum q1 median q3 maximum boxplot popular way visualize distribution boxplot incorporate five-number summary follow typically end box quartile box length interquartile range median marked line within box two line ( call whisker ) outside box extend smallest ( minimum ) largest ( maximum ) observation 
50 chapter 2 get know datum 220 200 180 160 unit price ( $ ) 140 120 100 80 60 40 20 branch 1 branch 2 branch 3 branch 4 figure 23 boxplot unit price datum item sell four branch allelectronic give time period deal moderate number observation worthwhile plot potential outlier individually boxplot whisker extend extreme low high observation value less 15 × iqr beyond quartile otherwise whisker terminate extreme observation occur within 15 × iqr quartile remain case plot individually boxplot used comparison several set compatible datum example 211 boxplot figure 23 show boxplot unit price datum item sell four branch allelectronic give time period branch 1 see median price item sell $ 80 q1 $ 60 q3 $ 100 notice two outlying observation branch plot individually value 175 202 15 time iqr 40 boxplot compute ( n log n ) time approximate boxplot compute linear sublinear time depend quality guarantee require variance standard deviation variance standard deviation measure datum dispersion indicate spread datum distribution low standard deviation mean datum observation tend close mean high standard deviation indicate datum spread large range value 
22 basic statistical description datum variance n observation x1 x2 xn numeric attribute x n n x x 1 1 σ2 = ( xi − x̄ ) 2 = xi2 − x̄ 2 n n i=1 51 ( 26 ) i=1 x̄ mean value observation defined eq ( 21 ) standard deviation σ observation square root variance σ 2 example 212 variance standard deviation example 26 find x̄ = $ 58000 used eq ( 21 ) mean determine variance standard deviation datum example set n = 12 use eq ( 26 ) obtain 1 ( 302 + 362 + 472 + 1102 ) − 582 12 ≈ 37917 √ σ ≈ 37917 ≈ 1947 σ2 = basic property standard deviation σ measure spread follow σ measure spread mean consider mean choose measure center σ = 0 spread observation value otherwise σ > 0 importantly observation unlikely several standard deviation away mathematically used chebyshev ’ inequality show  mean  least 1 − k12 × 100 % observation k standard deviation mean therefore standard deviation good indicator spread datum set computation variance standard deviation scalable large databasis 223 graphic display basic statistical description datum section study graphic display basic statistical description include quantile plot quantile–quantile plot histogram scatter plot graph helpful visual inspection datum useful datum preprocess first three show univariate distribution ( ie datum one attribute ) scatter plot show bivariate distribution ( ie involve two attribute ) quantile plot follow subsection cover common graphic display datum distribution quantile plot simple effective way first look univariate datum distribution first display datum give attribute ( allow user 
52 chapter 2 get know datum assess overall behavior unusual occurrence ) second plot quantile information ( see section 222 ) let xi = 1 n datum sort increase order x1 smallest observation xn largest ordinal numeric attribute x observation xi pair percentage fi indicate approximately fi × 100 % datum value xi say “ approximately ” may value exactly fraction fi datum xi note 025 percentile correspond quartile q1 050 percentile median 075 percentile q3 let fi = − 05 n ( 27 ) 1 number increase equal step n range 2n ( slightly 1 0 ) 1 − 2n ( slightly 1 ) quantile plot xi graph fi allow us compare different distribution base quantile example give quantile plot sale datum two different time period compare q1 median q3 fi value glance example 213 quantile plot figure 24 show quantile plot unit price datum table 21 quantile–quantile plot unit price ( $ ) quantile–quantile plot q-q plot graph quantile one univariate distribution corresponding quantile another powerful visualization tool allow user view whether shift go one distribution another suppose two set observation attribute variable unit price take two different branch location let x1 xn datum first branch y1 ym datum second datum set sort increase order = n ( ie number point set ) simply plot yi xi yi xi ( − 05 ) n quantile respective datum set < n ( ie second branch fewer observation first ) point q-q plot yi ( − 05 ) m quantile 140 120 100 80 60 40 20 0 000 q3 median q1 025 050 f-value 075 figure 24 quantile plot unit price datum table 21 100 
22 basic statistical description datum 53 table 21 set unit price datum item sell branch allelectronic unit price ( $ ) count item sell 40 43 47 − 74 75 78 − 115 117 120 275 300 250 − 360 515 540 − 320 270 350 branch 2 ( unit price $ ) 120 110 q3 100 median 90 80 70 q1 60 50 40 40 50 60 70 80 90 branch 1 ( unit price $ ) 100 110 120 figure 25 q-q plot unit price datum two allelectronic branch datum plot ( − 05 ) m quantile x datum computation typically involve interpolation example 214 quantile–quantile plot figure 25 show quantile–quantile plot unit price datum item sell two branch allelectronic give time period point correspond quantile datum set show unit price item sell branch 1 versus branch 2 quantile ( aid comparison straight line represent case give quantile unit price branch darker point correspond datum q1 median q3 respectively ) see example q1 unit price item sell branch 1 slightly less branch word 25 % item sell branch 1 less 
54 chapter 2 get know datum equal $ 60 25 % item sell branch 2 less equal $ 64 50th percentile ( marked median also q2 ) see 50 % item sell branch 1 less $ 78 50 % item branch 2 less $ 85 general note shift distribution branch 1 respect branch 2 unit price item sell branch 1 tend lower branch 2 histogram histogram ( frequency histogram ) least century old widely used “ histos ” mean pole mast “ gram ” mean chart histogram chart pole plot histogram graphical method summarize distribution give attribute x x nominal automobile model item type pole vertical bar draw know value x height bar indicate frequency ( ie count ) x value result graph commonly know bar chart x numeric term histogram prefer range value x partition disjoint consecutive subrange subrange refer bucket bin disjoint subset datum distribution x range bucket know width typically bucket equal width example price attribute value range $ 1 $ 200 ( round nearest dollar ) partition subrange 1 20 21 40 41 60 subrange bar draw height represent total count item observed within subrange histogram partition rule discuss chapter 3 datum reduction example 215 histogram figure 26 show histogram datum set table 21 bucket ( bin ) defined equal-width range represent $ 20 increment frequency count item sell although histogram widely used may effective quantile plot q-q plot boxplot method compare group univariate observation scatter plot datum correlation scatter plot one effective graphical method determine appear relationship pattern trend two numeric attribute construct scatter plot pair value treat pair coordinate algebraic sense plot point plane figure 27 show scatter plot set datum table 21 scatter plot useful method provide first look bivariate datum see cluster point outlier explore possibility correlation relationship two attribute x correlated one attribute imply correlation positive negative null ( uncorrelated ) figure 28 show example positive negative correlation two attribute plot point pattern slope 
22 basic statistical description datum 55 6000 count item sell 5000 4000 3000 2000 1000 0 40–59 60–79 80–99 unit price ( $ ) 100–119 120–139 figure 26 histogram table 21 datum set 700 item sell 600 500 400 300 200 100 0 0 20 40 60 80 unit price ( $ ) 100 120 140 figure 27 scatter plot table 21 datum set ( ) ( b ) figure 28 scatter plot used find ( ) positive ( b ) negative correlation attribute 
56 chapter 2 get know datum figure 29 three case observed correlation two plot attribute datum set lower left upper right mean value x increase value increase suggest positive correlation ( figure 28a ) pattern plot point slope upper left lower right value x increase value decrease suggest negative correlation ( figure 28b ) line best fit draw study correlation variable statistical test correlation give chapter 3 datum integration ( eq ( 33 ) ) figure 29 show three case correlation relationship two attribute give datum set section 232 show scatter plot extend n attribute result scatter-plot matrix conclusion basic datum description ( eg measure central tendency measure dispersion ) graphic statistical display ( eg quantile plot histogram scatter plot ) provide valuable insight overall behavior datum help identify noise outlier especially useful datum clean 23 datum visualization convey datum user effectively datum visualization aim communicate datum clearly effectively graphical representation datum visualization used extensively many applications—for example work report manage business operation tracking progress task popularly take advantage visualization technique discover datum relationship otherwise easily observable look raw datum nowadays person also use datum visualization create fun interesting graphic section briefly introduce basic concept datum visualization start multidimensional datum store relational databasis discuss several representative approach include pixel-oriented technique geometric projection technique icon-based technique hierarchical graph-based technique discuss visualization complex datum relation 
23 datum visualization 231 57 pixel-oriented visualization technique simple way visualize value dimension use pixel color pixel reflect dimension ’ value datum set dimension pixel-oriented technique create window screen one dimension dimension value record map pixel corresponding position window color pixel reflect corresponding value inside window datum value arrange global order share window global order may obtain sort datum record way ’ meaningful task hand example 216 pixel-oriented visualization allelectronic maintain customer information table consist four dimension income credit limit transaction volume age analyze correlation income attribute visualization sort customer income-ascending order use order lay customer datum four visualization window show figure pixel color choose smaller value lighter shading used pixelbased visualization easily observe follow credit limit increase income increase customer whose income middle range likely purchase allelectronic clear correlation income age pixel-oriented technique datum record also order query-dependent way example give point query sort record descend order similarity point query fill window layer datum record linear way may work well wide window first pixel row far away last pixel previous row though next global order moreover pixel next one window even though two next global order solve problem lay datum record space-filling curve ( ) income ( b ) credit_limit ( c ) transaction_volume ( ) age figure 210 pixel-oriented visualization four attribute sort customer income ascend order 
58 chapter 2 get know datum ( ) hilbert curve ( b ) gray code ( c ) z-curve figure 211 frequently used 2-d space-filling curf one datum record dim 6 dim 6 dim 5 dim 1 dim 4 dim 2 dim 3 ( ) dim 5 dim 1 dim 4 dim 2 dim 3 ( b ) figure 212 circle segment technique ( ) represent datum record circle segment ( b ) layer pixel circle segment fill window space-filling curve curve range cover entire n-dimensional unit hypercube since visualization window 2-d use 2-d space-filling curve figure 211 show frequently used 2-d space-filling curf note window rectangular example circle segment technique used window shape segment circle illustrated figure 212 technique ease comparison dimension dimension window locate side side form circle 232 geometric projection visualization technique drawback pixel-oriented visualization technique help us much understand distribution datum multidimensional space example show whether dense area multidimensional subspace geometric 
23 datum visualization 59 80 70 60 50 40 30 20 10 0 0 10 20 30 40 x 50 60 70 80 figure 213 visualization 2-d datum set used scatter plot source rareevent-geoinformatica06pdf projection technique help user find interesting projection multidimensional datum set central challenge geometric projection technique try address visualize high-dimensional space 2-d display scatter plot display 2-d datum point used cartesian coordinate third dimension add used different color shape represent different datum point figure 213 show example x two spatial attribute third dimension represent different shape visualization see point type “ + ” “ × ” tend colocate 3-d scatter plot used three axe cartesian coordinate system also used color display 4-d datum point ( figure 214 ) datum set four dimension scatter plot usually ineffective scatter-plot matrix technique useful extension scatter plot ndimensional datum set scatter-plot matrix n × n grid 2-d scatter plot provide visualization dimension every dimension figure 215 show example visualize iris datum set datum set consist 450 sample three species iris flower five dimension datum set length width sepal petal species scatter-plot matrix become less effective dimensionality increase another popular technique call parallel coordinate handle higher dimensionality visualize n-dimensional datum point parallel coordinate technique draw n equally space axe one dimension parallel one display axe 
60 chapter 2 get know datum figure 214 visualization 3-d datum set used scatter plot source http scatter plotjpg datum record represent polygonal line intersect axis point corresponding associate dimension value ( figure 216 ) major limitation parallel coordinate technique effectively show datum set many record even datum set several thousand record visual clutter overlap often reduce readability visualization make pattern hard find 233 icon-based visualization technique icon-based visualization technique use small icon represent multidimensional datum value look two popular icon-based technique chernoff face stick figure chernoff face introduce 1973 statistician herman chernoff display multidimensional datum 18 variable ( dimension ) cartoon human face ( figure 217 ) chernoff face help reveal trend datum component face eye ears mouth nose represent value dimension shape size placement orientation example dimension map follow facial characteristic eye size eye spacing nose length nose width mouth curvature mouth width mouth openness pupil size eyebrow slant eye eccentricity head eccentricity chernoff face make use ability human mind recognize small difference facial characteristic assimilate many facial characteristic 
23 datum visualization 10 30 50 70 0 10 61 20 80 70 sepal length ( mm ) 60 50 40 70 50 petal length ( mm ) 30 10 45 40 35 30 25 20 sepal width ( mm ) 25 20 15 10 5 0 petal width ( mm ) 40 50 60 70 80 iris species 20 setosa 30 versicolor 40 virginica figure 215 visualization iris datum set used scatter-plot matrix source http gsgscmatgif view large table datum tedious condense datum chernoff face make datum easier user digest way facilitate visualization regularity irregularity present datum although power relate multiple relationship limit another limitation specific datum value show furthermore facial feature vary perceive importance mean similarity two face ( represent two multidimensional datum point ) vary depend order dimension assign facial characteristic therefore mapping carefully choose eye size eyebrow slant find important asymmetrical chernoff face propose extension original technique since face vertical symmetry ( along y-axis ) left right side face identical waste space asymmetrical chernoff face double number facial characteristic thus allow 36 dimension display stick figure visualization technique map multidimensional datum five-piece stick figure figure four limb body two dimension map display ( x ) axe remain dimension map angle 
62 chapter 2 get know datum 10 5 x 0 –5 –10 ⫻1 ⫻2 ⫻3 ⫻4 ⫻5 ⫻6 ⫻7 ⫻8 ⫻9 ⫻10 figure 216 visualization used parallel coordinate source parallel coordithml figure 217 chernoff face face represent n-dimensional datum point ( n ≤ 18 ) or length limb figure 218 show census datum age income map display axe remain dimension ( gender education ) map stick figure datum item relatively dense respect two display dimension result visualization show texture pattern reflect datum trend 
23 datum visualization 63 figure 218 census datum represent used stick figure source professor g grinstein department computer science university massachusett lowell 234 hierarchical visualization technique visualization technique discuss far focus visualize multiple dimension simultaneously however large datum set high dimensionality would difficult visualize dimension time hierarchical visualization technique partition dimension subset ( ie subspace ) subspace visualize hierarchical manner “ worlds-within-world ” also know n-vision representative hierarchical visualization method suppose want visualize 6-d datum set dimension f x1 x5 want observe dimension f change respect dimension first fix value dimension x3 x4 x5 select value say c3 c4 c5 visualize f x1 x2 used 3-d plot call world show figure position origin inner world locate point ( c3 c4 c5 ) outer world another 3-d plot used dimension x3 x4 x5 user interactively change outer world location origin inner world user view result change inner world moreover user vary dimension used inner world outer world give dimension level world used method call “ worlds-withinworld ” another example hierarchical visualization method tree-maps display hierarchical datum set nest rectangle example figure 220 show tree-map visualize google news story news story organized seven category show large rectangle unique color within category ( ie rectangle top level ) news story partition smaller subcategory 
64 chapter 2 get know datum figure 219 “ worlds-within-world ” ( also know n-vision ) source http 1dipstick5gif 235 visualize complex datum relation early day visualization technique mainly numeric datum recently non-numeric datum text social network become available visualize analyze datum attract lot interest many new visualization technique dedicate kind datum example many person web tag various object picture blog entry product reviews tag cloud visualization statistic user-generated tag often tag cloud tag list alphabetically user-preferred order importance tag indicated font size color figure 221 show tag cloud visualize popular tag used web site tag cloud often used two way first tag cloud single item use size tag represent number time tag apply item different user second visualize tag statistic multiple item use size tag represent number item tag apply popularity tag addition complex datum complex relation among datum entry also raise challenge visualization example figure 222 used disease influence graph visualize correlation disease node graph disease size node proportional prevalence corresponding disease two node link edge corresponding disease strong correlation width edge proportional strength correlation pattern two corresponding disease 
24 measure datum similarity dissimilarity 65 figure 220 newsmap use tree-maps visualize google news headline story source wwwcsumd newsmappng summary visualization provide effective tool explore datum introduce several popular method essential idea behind many exist tool method moreover visualization used datum mining various aspect addition visualize datum visualization used represent datum mining process pattern obtain mining method user interaction datum visual datum mining important research development direction 24 measure datum similarity dissimilarity datum mining application cluster outlier analysis nearest-neighbor classification need way assess alike unalike object comparison one another example store may want search cluster customer object result group customer similar characteristic ( eg similar income area residence age ) information used marketing cluster 
66 chapter 2 get know datum figure 221 used tag cloud visualize popular web site tag source snapshot january 23 2010 high blood pressure ( hb ) allergy ( al ) st li overweight ( ov ) en high cholesterol level ( hc ) ki arthritis ( ar ) trouble see ( tr ) li risk diabetes ( ri ) asthma ( ) ca th diabetes ( di ) hayfever ( ha ) hc thyroid problem ( th ) di heart disease ( ) em tr ar hb cancer ( cn ) os sleep disorder ( sl ) ov eczema ( ec ) chronic bronchitis ( ch ) cn osteoporosis ( os ) prostate ( pr ) cardiovascular ( ca ) ps glaucoma ( gl ) ec pr stroke ( st ) liver condition ( li ) ch psa test abnormal ( ps ) kidney ( ki ) endometriosis ( en ) emphysema ( em ) ha al ri sl gl figure 222 disease influence graph person least 20 year old nhane datum set collection datum object object within cluster similar one another dissimilar object cluster outlier analysis also employ clustering-based technique identify potential outlier object highly dissimilar other knowledge object similarity also used nearest-neighbor classification scheme give object ( eg patient ) assign class label ( relate say diagnosis ) base similarity toward object model 
24 measure datum similarity dissimilarity 67 section present similarity dissimilarity measure refer measure proximity similarity dissimilarity related similarity measure two object j typically return value 0 object unalike higher similarity value greater similarity object ( typically value 1 indicate complete similarity object identical ) dissimilarity measure work opposite way return value 0 object ( therefore far dissimilar ) higher dissimilarity value dissimilar two object section 241 present two datum structure commonly used type application datum matrix ( used store datum object ) dissimilarity matrix ( used store dissimilarity value pair object ) also switch different notation datum object previously used chapter since deal object describe one attribute discuss object dissimilarity compute object describe nominal attribute ( section 242 ) binary attribute ( section 243 ) numeric attribute ( section 244 ) ordinal attribute ( section 245 ) combination attribute type ( section 246 ) section 247 provide similarity measure long sparse datum vector term-frequency vector represent document information retrieval know compute dissimilarity useful study attribute also reference later topic cluster ( chapter 10 11 ) outlier analysis ( chapter 12 ) nearest-neighbor classification ( chapter 9 ) 241 datum matrix versus dissimilarity matrix section 22 look way study central tendency dispersion spread observed value attribute x object one-dimensional describe single attribute section talk object describe multiple attribute therefore need change notation suppose n object ( eg person item course ) describe p attribute ( also call measurement feature age height weight gender ) object x1 = ( x11 x12 x1p ) x2 = ( x21 x22 x2p ) xij value object xi jth attribute brevity hereafter refer object xi object i object may tuple relational database also refer datum sample feature vector main memory-based cluster nearest-neighbor algorithms typically operate either follow two datum structure datum matrix ( object-by-attribute structure ) structure store n datum object form relational table n-by-p matrix ( n object ×p attribute )   x11 · · · x1f · · · x1p ··· ··· ··· ··· ···     ( 28 )  xi1 · · · xif · · · xip    ··· ··· ··· ··· ··· xn1 · · · xnf · · · xnp 
68 chapter 2 get know datum row correspond object part notation may use f index p attribute dissimilarity matrix ( object-by-object structure ) structure store collection proximity available pair n object often represent n-by-n table   0  d ( 2 1 ) 0      d ( 3 1 ) ( 3 2 ) 0 ( 29 )       ( n 1 ) ( n 2 ) · · · · · · 0 ( j ) measure dissimilarity “ difference ” object j general ( j ) non-negative number close 0 object j highly similar “ near ” become larger differ note ( ) = 0 difference object furthermore ( j ) = ( j ) ( readability show ( j ) entry matrix symmetric ) measure dissimilarity discuss throughout remainder chapter measure similarity often expressed function measure dissimilarity example nominal datum sim ( j ) = 1 − ( j ) ( 210 ) sim ( j ) similarity object j throughout rest chapter also comment measure similarity datum matrix make two entity “ thing ” namely row ( object ) column ( attribute ) therefore datum matrix often call two-mode matrix dissimilarity matrix contain one kind entity ( dissimilarity ) call one-mode matrix many cluster nearest-neighbor algorithms operate dissimilarity matrix datum form datum matrix transform dissimilarity matrix apply algorithms 242 proximity measure nominal attribute nominal attribute take two state ( section 212 ) example map color nominal attribute may say five state red yellow green pink blue let number state nominal attribute m state denote letter symbol set integer 1 2 m notice integer used datum handle represent specific order 
24 measure datum similarity dissimilarity 69 “ dissimilarity compute object describe nominal attribute ” dissimilarity two object j compute base ratio mismatch ( j ) = p−m p ( 211 ) number match ( ie number attribute j state ) p total number attribute describe object weight assign increase effect assign greater weight match attribute larger number state example 217 dissimilarity nominal attribute suppose sample datum table 22 except object-identifier attribute test-1 available test-1 nominal ( use test-2 test-3 later example ) let ’ compute dissimilarity matrix ( eq 29 )   0  d ( 2 1 ) 0      d ( 3 1 ) ( 3 2 ) 0 ( 4 1 ) ( 4 2 ) ( 4 3 ) 0 since one nominal attribute test-1 set p = 1 eq ( 211 ) ( j ) evaluate 0 object j match 1 object differ thus get  0 1   1 0  0 1 1 0 1     0 see object dissimilar except object 1 4 ( ie ( 4 1 ) = 0 ) table 22 sample datum table contain attribute mixed type object identifier test-1 ( nominal ) test-2 ( ordinal ) test-3 ( numeric ) 1 2 3 4 code code b code c code excellent fair good excellent 45 22 64 28 
70 chapter 2 get know datum alternatively similarity compute sim ( j ) = 1 − ( j ) = p ( 212 ) proximity object describe nominal attribute compute used alternative encode scheme nominal attribute encode used asymmetric binary attribute create new binary attribute state object give state value binary attribute represent state set 1 remain binary attribute set example encode nominal attribute map color binary attribute create five color previously list object color yellow yellow attribute set 1 remain four attribute set proximity measure form encode calculate used method discuss next subsection 243 proximity measure binary attribute let ’ look dissimilarity similarity measure object describe either symmetric asymmetric binary attribute recall binary attribute one two state 0 1 0 mean attribute absent 1 mean present ( section 213 ) give attribute smoker describe patient instance 1 indicate patient smoke 0 indicate patient treat binary attribute numeric mislead therefore method specific binary datum necessary compute dissimilarity “ compute dissimilarity two binary attribute ” one approach involve compute dissimilarity matrix give binary datum binary attribute thought weight 2 × 2 contingency table table 23 q number attribute equal 1 object j r number attribute equal 1 object equal 0 object j number attribute equal 0 object equal 1 object j number attribute equal 0 object j total number attribute p p = q + r + + recall symmetric binary attribute state equally valuable dissimilarity base symmetric binary attribute call symmetric binary dissimilarity object j describe symmetric binary attribute table 23 contingency table binary attribute object j object 1 0 sum 1 q q+s 0 r r t sum q+r s+t p 
24 measure datum similarity dissimilarity 71 dissimilarity j ( j ) = r s q+r s+t ( 213 ) asymmetric binary attribute two state equally important positive ( 1 ) negative ( 0 ) outcome disease test give two asymmetric binary attribute agreement two 1s ( positive match ) consider significant two 0s ( negative match ) therefore binary attribute often consider “ monary ” ( one state ) dissimilarity base attribute call asymmetric binary dissimilarity number negative match consider unimportant thus ignore follow computation ( j ) = r s q+r s ( 214 ) complementarily measure difference two binary attribute base notion similarity instead dissimilarity example asymmetric binary similarity object j compute sim ( j ) = q = 1 − ( j ) q+r s ( 215 ) coefficient sim ( j ) eq ( 215 ) call jaccard coefficient popularly reference literature symmetric asymmetric binary attribute occur datum set mixed attribute approach describe section 246 apply example 218 dissimilarity binary attribute suppose patient record table ( table 24 ) contain attribute name gender fever cough test-1 test-2 test-3 test-4 name object identifier gender symmetric attribute remain attribute asymmetric binary asymmetric attribute value let value ( yes ) p ( positive ) set 1 value n ( negative ) set suppose distance object table 24 relational table patient describe binary attribute name gender fever cough test-1 test-2 test-3 test-4 jack jim mary f n n p n p n n n n n p n n n 
72 chapter 2 get know datum ( patient ) compute base asymmetric attribute accord eq ( 214 ) distance pair three patients—jack mary jim—is ( jack jim ) = 1+1 = 067 1+1+1 ( jack mary ) = 0+1 = 033 2+0+1 ( jim mary ) = 1+2 = 075 1+1+2 measurement suggest jim mary unlikely similar disease highest dissimilarity value among three pair three patient jack mary likely similar disease 244 dissimilarity numeric datum minkowski distance section describe distance measure commonly used compute dissimilarity object describe numeric attribute measure include euclidean manhattan minkowski distance case datum normalize apply distance calculation involve transform datum fall within smaller common range [ −1 1 ] [ 00 10 ] consider height attribute example can measure either meter inch general express attribute smaller unit lead larger range attribute thus tend give attribute greater effect “ weight ” normalize datum attempt give attribute equal weight may may useful particular application method normalize datum discuss detail chapter 3 datum preprocess popular distance measure euclidean distance ( ie straight line “ crow fly ” ) let = ( xi1 xi2 xip ) j = ( xj1 xj2 xjp ) two object describe p numeric attribute euclidean distance object j defined q ( j ) = ( xi1 − xj1 ) 2 + ( xi2 − xj2 ) 2 + · · · + ( xip − xjp ) 2 ( 216 ) another well-known measure manhattan ( city block ) distance name distance block two point city ( 2 block 3 block total 5 block ) defined ( j ) = xi1 − xj1 | + xi2 − xj2 | + · · · + xip − xjp | ( 217 ) euclidean manhattan distance satisfy follow mathematical property non-negativity ( j ) ≥ 0 distance non-negative number identity indiscernible ( ) = 0 distance object 0 
24 measure datum similarity dissimilarity 73 symmetry ( j ) = ( j ) distance symmetric function triangle inequality ( j ) ≤ ( k ) + ( k j ) go directly object object j space make detour object k measure satisfy condition know metric please note non-negativity property imply three property example 219 euclidean distance manhattan distance let x1 = ( 1 2 ) x2 = ( 3 5 ) represent √ two object show figure euclidean distance two 22 + 32 = manhattan distance two 2 + 3 = 5 minkowski distance generalization euclidean manhattan distance defined q ( j ) = h xi1 − xj1 h + xi2 − xj2 h + · · · + xip − xjp h ( 218 ) h real number h ≥ 1 ( distance also call lp norm literature symbol p refer notation h keep p number attribute consistent rest chapter ) represent manhattan distance h = 1 ( ie l1 norm ) euclidean distance h = 2 ( ie l2 norm ) supremum distance ( also refer lmax l∞ norm chebyshev distance ) generalization minkowski distance h → ∞ compute find attribute f give maximum difference value two object difference supremum distance defined formally  1 h p x p h  ( j ) = lim xif − xjf | = max xif − xjf | ( 219 ) h→∞ f 1 f l∞ norm also know uniform norm x2 = ( 3 5 ) 5 4 euclidean distance = ( 22 + 32 ) 2 = 361 3 3 2 x1 = ( 1 2 ) manhattan distance 2+3=5 supremum distance 5–2=3 2 1 1 2 3 figure 223 euclidean manhattan supremum distance two object 
74 chapter 2 get know datum example 220 supremum distance let ’ use two object x1 = ( 1 2 ) x2 = ( 3 5 ) figure second attribute give greatest difference value object 5 − 2 = supremum distance object attribute assign weight accord perceive importance weight euclidean distance compute q ( j ) = w1 xi1 − xj1 2 + w2 xi2 − xj2 2 + · · · + wm xip − xjp 2 ( 220 ) weighting also apply distance measure well 245 proximity measure ordinal attribute value ordinal attribute meaningful order ranking yet magnitude successive value unknown ( section 214 ) example include sequence small medium large size attribute ordinal attribute may also obtain discretization numeric attribute splitting value range finite number category category organized rank range numeric attribute map ordinal attribute f mf state example range interval-scaled attribute temperature ( celsius ) organized follow state −30 −10 −10 10 10 30 represent category cold temperature moderate temperature warm temperature respectively let represent number possible state ordinal attribute order state define ranking 1 mf “ ordinal attribute handled ” treatment ordinal attribute quite similar numeric attribute compute dissimilarity object suppose f attribute set ordinal attribute describe n object dissimilarity computation respect f involve follow step value f ith object xif f mf order state represent ranking 1 mf replace xif corresponding rank rif ∈ { 1 mf } since ordinal attribute different number state often necessary map range attribute onto [ 00 10 ] attribute equal weight perform datum normalization replace rank rif ith object f th attribute zif = rif − 1 mf − 1 ( 221 ) dissimilarity compute used distance measure describe section 244 numeric attribute used zif represent f value ith object 
24 measure datum similarity dissimilarity 75 example 221 dissimilarity ordinal attribute suppose sample datum show earlier table 22 except time object-identifier continuous ordinal attribute test-2 available three state test-2 fair good excellent mf = step 1 replace value test-2 rank four object assign rank 3 1 2 3 respectively step 2 normalizes ranking mapping rank 1 00 rank 2 05 rank 3 step 3 use say euclidean distance ( eq 216 ) result follow dissimilarity matrix  0 10 0   05 05 0 0 10 05      0 therefore object 1 2 dissimilar object 2 4 ( ie ( 2 1 ) = 10 ( 4 2 ) = 10 ) make intuitive sense since object 1 4 excellent object 2 fair opposite end range value test-2 similarity value ordinal attribute interpreted dissimilarity sim ( j ) = 1 − ( j ) 246 dissimilarity attribute mixed type section 242 245 discuss compute dissimilarity object describe attribute type type may either nominal symmetric binary asymmetric binary numeric ordinal however many real databasis object describe mixture attribute type general database contain attribute type “ compute dissimilarity object mixed attribute type ” one approach group type attribute together perform separate datum mining ( eg cluster ) analysis type feasible analysis derive compatible result however real application unlikely separate analysis per attribute type generate compatible result preferable approach process attribute type together perform single analysis one technique combine different attribute single dissimilarity matrix bring meaningful attribute onto common scale interval [ 00 10 ] suppose datum set contain p attribute mixed type dissimilarity ( j ) object j defined ( f ) ( f ) f 1 δij dij pp ( f ) f 1 δij pp ( j ) = ( 222 ) 
76 chapter 2 get know datum ( f ) indicator δij = 0 either ( 1 ) xif xjf miss ( ie measurement attribute f object object j ) ( 2 ) xif = xjf = 0 attribute ( f ) f asymmetric binary otherwise δij = contribution attribute f ( f ) dissimilarity j ( ie dij ) compute dependent type ( f ) f numeric dij = attribute f xif −xjf | maxh xhf −minh xhf h run nonmissing object ( f ) ( f ) f nominal binary dij = 0 xif = xjf otherwise dij = 1 f ordinal compute rank rif zif = rif −1 mf −1 treat zif numeric step identical already see individual attribute type difference numeric attribute normalize value map interval [ 00 10 ] thus dissimilarity object compute even attribute describe object different type example 222 dissimilarity attribute mixed type let ’ compute dissimilarity matrix object table consider attribute different type example 217 221 work dissimilarity matrix individual attribute procedure follow test-1 ( nominal ) test-2 ( ordinal ) outlined earlier process attribute mixed type therefore use dissimilarity matrix obtain test-1 test-2 later compute eq ( 222 ) first however need compute dissimilarity matrix third attribute test-3 ( numeric ) ( 3 ) must compute dij follow case numeric attribute let maxh xh = 64 minh xh = difference two used eq ( 222 ) normalize value dissimilarity matrix result dissimilarity matrix test-3  0 055   045 040  0 100 014 0 086     0 use dissimilarity matrix three attribute computation ( f ) eq ( 222 ) indicator δij = 1 three attribute f get example ( 3 1 ) = 1 ( 1 ) 1 ( 050 ) 1 ( 045 ) 3 = result dissimilarity matrix obtain 
24 measure datum similarity dissimilarity 77 datum describe three attribute mixed type  0 085   065 013  0 083 071 0 079     0 table 22 intuitively guess object 1 4 similar base value test-1 test-2 confirm dissimilarity matrix ( 4 1 ) lowest value pair different object similarly matrix indicate object 1 2 least similar 247 cosine similarity document represent thousand attribute record frequency particular word ( keyword ) phrase document thus document object represent call term-frequency vector example table 25 see document1 contain five instance word team hockey occur three time word coach absent entire document indicated count value datum highly asymmetric term-frequency vector typically long sparse ( ie many 0 value ) application used structure include information retrieval text document cluster biological taxonomy gene feature mapping traditional distance measure study chapter work well sparse numeric datum example two term-frequency vector may many 0 value common meaning corresponding document share many word make similar need measure focus word two document common occurrence frequency word word need measure numeric datum ignore zero-match cosine similarity measure similarity used compare document say give ranking document respect give vector query word let x two vector comparison used cosine measure table 25 document vector term-frequency vector document team coach hockey baseball soccer penalty score win loss season document1 document2 document3 document4 5 3 0 0 0 0 7 1 3 2 0 0 0 0 2 0 2 1 1 1 0 1 0 2 0 0 0 2 2 1 3 0 0 0 0 3 0 1 0 0 
78 chapter 2 get know datum similarity function sim ( x ) = x·y | ( 223 ) | euclidean norm vector x = ( x1 x2 xp ) defined q x12 + x22 + · · · + xp2 conceptually length vector similarly | euclidean norm vector y measure compute cosine angle vector x y cosine value 0 mean two vector 90 degree ( orthogonal ) match closer cosine value 1 smaller angle greater match vector note cosine similarity measure obey property section 244 define metric measure refer nonmetric measure example 223 cosine similarity two term-frequency vector suppose x first two term-frequency vector table x = ( 5 0 3 0 2 0 0 2 0 0 ) = ( 3 0 2 0 1 1 0 1 0 1 ) similar x used eq ( 223 ) compute cosine similarity two vector get xt · = 5 × 3 + 0 × 0 + 3 × 2 + 0 × 0 + 2 × 1 + 0 × 1 + 0 × 0 + 2 × 1 + 0 × 0 + 0 × 1 = 25 p | = 52 + 02 + 32 + 02 + 22 + 02 + 02 + 22 + 02 + 02 = 648 p | = 32 + 02 + 22 + 02 + 12 + 12 + 02 + 12 + 02 + 12 = 412 sim ( x ) = 094 therefore used cosine similarity measure compare document would consider quite similar attribute binary-valu cosine similarity function interpreted term share feature attribute suppose object x possess ith attribute xi = xt · number attribute possessed ( ie share ) x | geometric mean number attribute possessed x number possessed y thus sim ( x ) measure relative possession common attribute simple variation cosine similarity precede scenario sim ( x ) = x·y x·x+y·y−x·y ( 224 ) ratio number attribute share x number attribute possessed x y function know tanimoto coefficient tanimoto distance frequently used information retrieval biology taxonomy 
26 exercise 25 79 summary datum set make datum object datum object represent entity datum object describe attribute attribute nominal binary ordinal numeric value nominal ( categorical ) attribute symbol name thing value represent kind category code state binary attribute nominal attribute two possible state ( 1 0 true false ) two state equally important attribute symmetric otherwise asymmetric ordinal attribute attribute possible value meaningful order ranking among magnitude successive value know numeric attribute quantitative ( ie measurable quantity ) represent integer real value numeric attribute type interval-scaled ratioscale value interval-scaled attribute measure fix equal unit ratio-scale attribute numeric attribute inherent zero-point measurement ratio-scale speak value order magnitude larger unit measurement basic statistical description provide analytical foundation datum preprocess basic statistical measure datum summarization include mean weight mean median mode measure central tendency datum range quantile quartile interquartile range variance standard deviation measure dispersion datum graphical representation ( eg boxplot quantile plot quantile– quantile plot histogram scatter plot ) facilitate visual inspection datum thus useful datum preprocess mining datum visualization technique may pixel-oriented geometric-based icon-based hierarchical method apply multidimensional relational datum additional technique propose visualization complex datum text social network measure object similarity dissimilarity used datum mining application cluster outlier analysis nearest-neighbor classification measure proximity compute attribute type study chapter combination attribute example include jaccard coefficient asymmetric binary attribute euclidean manhattan minkowski supremum distance numeric attribute application involve sparse numeric datum vector term-frequency vector cosine measure tanimoto coefficient often used assessment similarity 26 exercise 21 give three additional commonly used statistical measure already illustrated chapter characterization datum dispersion discuss compute efficiently large databasis 
80 chapter 2 get know datum 22 suppose datum analysis include attribute age age value datum tuple ( increase order ) 13 15 16 16 19 20 20 21 22 22 25 25 25 25 30 33 33 35 35 35 35 36 40 45 46 52 70 ( ) mean datum median ( b ) mode datum comment datum ’ modality ( ie bimodal trimodal etc ) ( c ) midrange datum ( ) find ( roughly ) first quartile ( q1 ) third quartile ( q3 ) datum ( e ) give five-number summary datum ( f ) show boxplot datum ( g ) quantile–quantile plot different quantile plot 23 suppose value give set datum group interval interval corresponding frequency follow age 1–5 6–15 16–20 21–50 51–80 81–110 frequency 200 450 300 1500 700 44 compute approximate median value datum 24 suppose hospital test age body fat datum 18 randomly select adult follow result age % fat 23 95 23 265 27 78 27 178 39 314 41 259 47 274 49 272 50 312 age % fat 52 346 54 425 54 288 56 334 57 302 58 341 58 329 60 412 61 357 ( ) calculate mean median standard deviation age % fat ( b ) draw boxplot age % fat ( c ) draw scatter plot q-q plot base two variable 25 briefly outline compute dissimilarity object describe follow ( ) nominal attribute ( b ) asymmetric binary attribute 
27 bibliographic note 81 ( c ) numeric attribute ( ) term-frequency vector 26 give two object represent tuple ( 22 1 42 10 ) ( 20 0 36 8 ) ( ) ( b ) ( c ) ( ) compute euclidean distance two object compute manhattan distance two object compute minkowski distance two object used q = 3 compute supremum distance two object 27 median one important holistic measure datum analysis propose several method median approximation analyze respective complexity different parameter setting decide extent real value approximate moreover suggest heuristic strategy balance accuracy complexity apply method give 28 important define select similarity measure datum analysis however commonly accept subjective similarity measure result vary depend similarity measure used nonetheless seemingly different similarity measure may equivalent transformation suppose follow 2-d datum set x1 x2 x3 x4 x5 a1 15 2 16 12 15 a2 17 19 18 15 10 ( ) consider datum 2-d datum point give new datum point x = ( 14 16 ) query rank database point base similarity query used euclidean distance manhattan distance supremum distance cosine similarity ( b ) normalize datum set make norm datum point equal use euclidean distance transform datum rank datum point 27 bibliographic note method descriptive datum summarization study statistic literature long onset computer good summary statistical descriptive datum mining method include freedman pisani purf [ fpp07 ] devore [ dev95 ] 
82 chapter 2 get know datum statistics-based visualization datum used boxplot quantile plot quantile–quantile plot scatter plot loess curf see cleveland [ cle93 ] pioneer work datum visualization technique describe visual display quantitative information [ tuf83 ] envision information [ tuf90 ] visual explanation image quantity evidence narrative [ tuf97 ] tufte addition graphic graphic information process bertin [ ber81 ] visualize datum cleveland [ cle93 ] information visualization datum mining knowledge discovery edit fayyad grinstein wierse [ fgw01 ] major conference symposium visualization include acm human factor compute system ( chi ) visualization international symposium information visualization research visualization also publish transaction visualization computer graphic journal computational graphical statistic ieee computer graphic application many graphical user interface visualization tool develop find various datum mining product several book datum mining ( eg datum mining solution westphal blaxton [ wb98 ] ) present many good example visual snapshot survey visualization technique see “ visual technique explore databasis ” keim [ kei97 ] similarity distance measure among various variable introduce many textbook study cluster analysis include hartigan [ har75 ] jain dube [ jd88 ] kaufman rousseeuw [ kr90 ] arabie hubert de soete [ ahs96 ] method combine attribute different type single dissimilarity matrix introduce kaufman rousseeuw [ kr90 ] 
10 cluster analysis basic concept method imagine director customer relationship allelectronic five manager work would like organize company ’ customer five group group assign different manager strategically would like customer group similar possible moreover two give customer different business pattern place group intention behind business strategy develop customer relationship campaign specifically target group base common feature share customer per group kind datum mining technique help accomplish task unlike classification class label ( group id ) customer unknown need discover grouping give large number customer many attribute describe customer profile costly even infeasible human study datum manually come way partition customer strategic group need cluster tool help cluster process grouping set datum object multiple group cluster object within cluster high similarity dissimilar object cluster dissimilarity similarity assessed base attribute value describe object often involve distance measures1 cluster datum mining tool root many application area biology security business intelligence web search chapter present basic concept method cluster analysis section 101 introduce topic study requirement cluster method massive amount datum various application learn several basic cluster technique organized follow category partition method ( section 102 ) hierarchical method ( section 103 ) density-based method ( section 104 ) grid-based method ( section 105 ) section 106 briefly discuss evaluate 1 datum similarity dissimilarity discuss detail section may want refer section quick review datum mining concept technique doi b978-0-12-381479-100010-1 c 2012 elsevier right re-serve 443 
444 chapter 10 cluster analysis basic concept method cluster method discussion advanced method cluster re-serve chapter 11 101 cluster analysis section set groundwork study cluster analysis section 1011 define cluster analysis present example useful section 1012 learn aspect compare cluster method well requirement cluster overview basic cluster technique present section 1013 1011 cluster analysis cluster analysis simply cluster process partition set datum object ( observation ) subset subset cluster object cluster similar one another yet dissimilar object cluster set cluster result cluster analysis refer cluster context different cluster method may generate different clustering datum set partition perform human cluster algorithm hence cluster useful lead discovery previously unknown group within datum cluster analysis widely used many application business intelligence image pattern recognition web search biology security business intelligence cluster used organize large number customer group customer within group share strong similar characteristic facilitate development business strategy enhance customer relationship management moreover consider consultant company large number project improve project management cluster apply partition project category base similarity project audit diagnosis ( improve project delivery outcome ) conduct effectively image recognition cluster used discover cluster “ subclass ” handwritten character recognition system suppose datum set handwritten digit digit labele either 1 2 3 note large variance way person write digit take number 2 example person may write small circle left bottom part other may use cluster determine subclass “ 2 ” represent variation way 2 written used multiple model base subclass improve overall recognition accuracy cluster also find many application web search example keyword search may often return large number hit ( ie page relevant search ) due extremely large number web page cluster used organize search result group present result concise easily accessible way moreover cluster technique develop cluster document topic commonly used information retrieval practice 
101 cluster analysis 445 datum mining function cluster analysis used standalone tool gain insight distribution datum observe characteristic cluster focus particular set cluster analysis alternatively may serve preprocess step algorithms characterization attribute subset selection classification would operate detected cluster select attribute feature cluster collection datum object similar one another within cluster dissimilar object cluster cluster datum object treat implicit class sense cluster sometimes call automatic classification critical difference cluster automatically find grouping distinct advantage cluster analysis cluster also call datum segmentation application cluster partition large datum set group accord similarity cluster also used outlier detection outlier ( value “ far away ” cluster ) may interesting common case application outlier detection include detection credit card fraud monitoring criminal activity electronic commerce example exceptional case credit card transaction expensive infrequent purchase may interest possible fraudulent activity outlier detection subject chapter 12 datum cluster vigorous development contribute area research include datum mining statistic machine learn spatial database technology information retrieval web search biology marketing many application area owing huge amount datum collect databasis cluster analysis recently become highly active topic datum mining research branch statistic cluster analysis extensively study main focus distance-based cluster analysis cluster analysis tool base k-mean k-medoid several method also build many statistical analysis software package system s-plus spss sas machine learn recall classification know supervised learn class label information give learn algorithm supervised tell class membership training tuple cluster know unsupervised learn class label information present reason cluster form learn observation rather learn example datum mining effort focuse find method efficient effective cluster analysis large databasis active theme research focus scalability cluster method effectiveness method cluster complex shape ( eg nonconvex ) type datum ( eg text graph image ) high-dimensional cluster technique ( eg cluster object thousand feature ) method cluster mixed numerical nominal datum large databasis 1012 requirement cluster analysis cluster challenge research field section learn requirement cluster datum mining tool well aspect used compare cluster method 
446 chapter 10 cluster analysis basic concept method follow typical requirement cluster datum mining scalability many cluster algorithms work well small datum set contain fewer several hundred datum object however large database may contain million even billion object particularly web search scenario cluster sample give large datum set may lead bias result therefore highly scalable cluster algorithms need ability deal different type attribute many algorithms design cluster numeric ( interval-based ) datum however application may require cluster datum type binary nominal ( categorical ) ordinal datum mixture datum type recently application need cluster technique complex datum type graph sequence image document discovery cluster arbitrary shape many cluster algorithms determine cluster base euclidean manhattan distance measure ( chapter 2 ) algorithms base distance measure tend find spherical cluster similar size density however cluster can shape consider sensor example often deploy environment surveillance cluster analysis sensor reading detect interesting phenomena may want use cluster find frontier run forest fire often spherical important develop algorithms detect cluster arbitrary shape requirement domain knowledge determine input parameter many cluster algorithms require user provide domain knowledge form input parameter desire number cluster consequently cluster result may sensitive parameter parameter often hard determine especially high-dimensionality datum set user yet grasp deep understand datum require specification domain knowledge burden user also make quality cluster difficult control ability deal noisy datum real-world datum set contain outlier or miss unknown erroneous datum sensor reading example often noisy—some reading may inaccurate due sense mechanism reading may erroneous due interference surround transient object cluster algorithms sensitive noise may produce poor-quality cluster therefore need cluster method robust noise incremental cluster insensitivity input order many application incremental update ( represent newer datum ) may arrive time cluster algorithms incorporate incremental update exist cluster structure instead recompute new cluster scratch cluster algorithms may also sensitive input datum order give set datum object cluster algorithms may return dramatically different clustering depend order object present incremental cluster algorithms algorithms insensitive input order need 
101 cluster analysis 447 capability cluster high-dimensionality datum datum set contain numerous dimension attribute cluster document example keyword regard dimension often thousand keyword cluster algorithms good handle low-dimensional datum datum set involve two three dimension find cluster datum object highdimensional space challenge especially consider datum sparse highly skewer constraint-based cluster real-world application may need perform cluster various kind constraint suppose job choose location give number new automatic teller machine ( atms ) city decide upon may cluster household consider constraint city ’ river highway network type number customer per cluster challenge task find datum group good cluster behavior satisfy specify constraint interpretability usability user want cluster result interpretable comprehensible usable cluster may need tie specific semantic interpretation application important study application goal may influence selection cluster feature cluster method follow orthogonal aspect cluster method compare partition criterium method object partition hierarchy exist among cluster cluster level conceptually method useful example partition customer group group manager alternatively method partition datum object hierarchically cluster form different semantic level example text mining may want organize corpus document multiple general topic “ politic ” “ sport ” may subtopic instance “ football ” “ basketball ” “ baseball ” “ hockey ” exist subtopic “ ” latter four subtopic lower level hierarchy “ sport ” separation cluster method partition datum object mutually exclusive cluster cluster customer group group take care one manager customer may belong one group situation cluster may exclusive datum object may belong one cluster example cluster document topic document may related multiple topic thus topic cluster may exclusive similarity measure method determine similarity two object distance distance defined euclidean space 
448 chapter 10 cluster analysis basic concept method road network vector space space method similarity may defined connectivity base density contiguity may rely absolute distance two object similarity measure play fundamental role design cluster method distance-based method often take advantage optimization technique - continuity-based method often find cluster arbitrary shape cluster space many cluster method search cluster within entire give datum space method useful low-dimensionality datum set highdimensional datum however many irrelevant attribute make similarity measurement unreliable consequently cluster find full space often meaningless ’ often better instead search cluster within different subspace datum set subspace cluster discover cluster subspace ( often low dimensionality ) manifest object similarity conclude cluster algorithms several requirement factor include scalability ability deal different type attribute noisy datum incremental update cluster arbitrary shape constraint interpretability usability also important addition cluster method differ respect partition level whether cluster mutually exclusive similarity measure used whether subspace cluster perform 1013 overview basic cluster method many cluster algorithms literature difficult provide crisp categorization cluster method category may overlap method may feature several category nevertheless useful present relatively organized picture cluster method general major fundamental cluster method classify follow category discuss rest chapter partition method give set n object partition method construct k partition datum partition represent cluster k ≤ n divide datum k group group must contain least one object word partition method conduct one-level partition datum set basic partition method typically adopt exclusive cluster separation object must belong exactly one group requirement may relax example fuzzy partition technique reference technique give bibliographic note ( section 109 ) partition method distance-based give k number partition construct partition method create initial partition used iterative relocation technique attempt improve partition move object one group another general criterion good partition object cluster “ close ” related whereas object different cluster “ far apart ” different various kind 
101 cluster analysis 449 criterium judge quality partition traditional partition method extend subspace cluster rather search full datum space useful many attribute datum sparse achieve global optimality partitioning-based cluster often computationally prohibitive potentially require exhaustive enumeration possible partition instead application adopt popular heuristic method greedy approach like k-mean k-medoid algorithms progressively improve cluster quality approach local optimum heuristic cluster method work well find spherical-shap cluster - medium-size databasis find cluster complex shape large datum set partitioning-based method need extend partitioning-based cluster method study depth section 102 hierarchical method hierarchical method create hierarchical decomposition give set datum object hierarchical method classify either agglomerative divisive base hierarchical decomposition form agglomerative approach also call bottom-up approach start object form separate group successively merge object group close one another group merged one ( topmost level hierarchy ) termination condition hold divisive approach also call top-down approach start object cluster successive iteration cluster split smaller cluster eventually object one cluster termination condition hold hierarchical cluster method distance-based - continuitybased various extension hierarchical method consider cluster subspace well hierarchical method suffer fact step ( merge split ) do never undo rigidity useful lead smaller computation cost worry combinatorial number different choice technique correct erroneous decision however method improve quality hierarchical cluster propose hierarchical cluster method study section 103 density-based method partition method cluster object base distance object method find spherical-shap cluster encounter difficulty discover cluster arbitrary shape cluster method develop base notion density general idea continue grow give cluster long density ( number object datum point ) “ neighborhood ” exceed threshold example datum point within give cluster neighborhood give radius contain least minimum number point method used filter noise outlier discover cluster arbitrary shape density-based method divide set object multiple exclusive cluster hierarchy cluster typically density-based method consider exclusive cluster consider fuzzy cluster moreover density-based method extend full space subspace cluster density-based cluster method study section 104 
450 chapter 10 cluster analysis basic concept method grid-based method grid-based method quantize object space finite number cell form grid structure cluster operation perform grid structure ( ie quantized space ) main advantage approach fast process time typically independent number datum object dependent number cell dimension quantized space used grid often efficient approach many spatial datum mining problem include cluster therefore grid-based method integrate cluster method density-based method hierarchical method gridbase cluster study section 105 method briefly summarize figure cluster algorithms integrate idea several cluster method sometimes difficult classify give algorithm uniquely belong one cluster method category furthermore application may cluster criterium require integration several cluster technique follow section examine cluster method detail advanced cluster method related issue discuss chapter general notation used follow let datum set n object cluster object describe variable variable also call attribute dimension method partition method general characteristic – find mutually exclusive cluster spherical shape – distance-based – may use mean medoid ( etc ) represent cluster center – effective - medium-size datum set hierarchical method – cluster hierarchical decomposition ( ie multiple level ) – correct erroneous merge split – may incorporate technique like microcluster consider object “ linkage ” density-based method – find arbitrarily shape cluster – cluster dense region object space separated low-density region – cluster density point must minimum number point within “ neighborhood ” – may filter outlier grid-based method – use multiresolution grid datum structure – fast process time ( typically independent number datum object yet dependent grid size ) figure 101 overview cluster method discuss chapter note algorithms may combine various method 
102 partition method 451 therefore may also refer point d-dimensional object space object represent bold italic font ( eg p ) 102 partition method simplest fundamental version cluster analysis partition organize object set several exclusive group cluster keep problem specification concise assume number cluster give background knowledge parameter start point partition method formally give datum set n object k number cluster form partition algorithm organize object k partition ( k ≤ n ) partition represent cluster cluster form optimize objective partition criterion dissimilarity function base distance object within cluster “ similar ” one another “ dissimilar ” object cluster term datum set attribute section learn well-known commonly used partition methods—k-mean ( section 1021 ) k-medoid ( section 1022 ) also learn several variation classic partition method scale handle large datum set 1021 k-mean centroid-based technique suppose datum set contain n object euclidean space partition method distribute object k cluster c1 ck ci ⊂ ci ∩ cj = ∅ ( 1 ≤ j ≤ k ) objective function used assess partition quality object within cluster similar one another dissimilar object cluster objective function aim high intracluster similarity low intercluster similarity centroid-based partition technique used centroid cluster ci represent cluster conceptually centroid cluster center point centroid defined various way mean medoid object ( point ) assign cluster difference object p ∈ ci ci representative cluster measure dist ( p ci ) dist ( x ) euclidean distance two point x y quality cluster ci measure withincluster variation sum square error object ci centroid ci defined = k x x dist ( p ci ) 2 ( 101 ) i=1 p∈ci e sum square error object datum set p point space represent give object ci centroid cluster ci ( p ci multidimensional ) word object cluster distance 
452 chapter 10 cluster analysis basic concept method object cluster center square distance sum objective function try make result k cluster compact separate possible optimize within-cluster variation computationally challenge worst case would enumerate number possible partitioning exponential number cluster check within-cluster variation value show problem np-hard general euclidean space even two cluster ( ie k = 2 ) moreover problem np-hard general number cluster k even 2-d euclidean space number cluster k dimensionality space fix problem solve time ( ndk+1 log n ) n number object overcome prohibitive computational cost exact solution greedy approach often used practice prime example k-mean algorithm simple commonly used “ k-mean algorithm work ” k-mean algorithm define centroid cluster mean value point within cluster proceed follow first randomly select k object initially represent cluster mean center remain object object assign cluster similar base euclidean distance object cluster mean k-mean algorithm iteratively improve within-cluster variation cluster compute new mean used object assign cluster previous iteration object reassign used update mean new cluster center iteration continue assignment stable cluster form current round form previous round k-mean procedure summarize figure 102 algorithm k-mean k-mean algorithm partition cluster ’ center represent mean value object cluster input k number cluster datum set contain n object output set k cluster method ( 1 ) arbitrarily choose k object initial cluster center ( 2 ) repeat ( 3 ) ( ) assign object cluster object similar base mean value object cluster ( 4 ) update cluster mean calculate mean value object cluster ( 5 ) change figure 102 k-mean partition algorithm 
102 partition method 453 + + + + + ( ) initial cluster + ( b ) iterate + + + ( c ) final cluster figure 103 cluster set object used k-mean method ( b ) update cluster center reassign object accordingly ( mean cluster marked + ) example 101 cluster k-mean partition consider set object locate 2-d space depict figure 103 ( ) let k = 3 user would like object partition three cluster accord algorithm figure 102 arbitrarily choose three object three initial cluster center cluster center marked + object assign cluster base cluster center nearest distribution form silhouette encircle dot curf show figure 103 ( ) next cluster center update mean value cluster recalculate base current object cluster used new cluster center object redistribute cluster base cluster center nearest redistribution form new silhouette encircle dash curf show figure 103 ( b ) process iterate lead figure 103 ( c ) process iteratively reassigning object cluster improve partition refer iterative relocation eventually reassignment object cluster occur process terminate result cluster return cluster process k-mean method guarantee converge global optimum often terminate local optimum result may depend initial random selection cluster center ( ask give example show exercise ) obtain good result practice common run k-mean algorithm multiple time different initial cluster center time complexity k-mean algorithm ( nkt ) n total number object k number cluster number iteration normally k n n therefore method relatively scalable efficient process large datum set several variant k-mean method differ selection initial k-mean calculation dissimilarity strategy calculate cluster mean 
454 chapter 10 cluster analysis basic concept method k-mean method apply mean set object defined may case application datum nominal attribute involved k-mode method variant k-mean extend k-mean paradigm cluster nominal datum replace mean cluster mode used new dissimilarity measure deal nominal object frequency-based method update mode cluster k-mean k-mode method integrate cluster datum mixed numeric nominal value necessity user specify k number cluster advance see disadvantage study overcome difficulty however provide approximate range k value used analytical technique determine best k compare cluster result obtain different k value k-mean method suitable discover cluster nonconvex shape cluster different size moreover sensitive noise outlier datum point small number datum substantially influence mean value “ make k-mean algorithm scalable ” one approach make k-mean method efficient large datum set use good-sized set sample cluster another employ filter approach used spatial hierarchical datum index save cost compute mean third approach explore microcluster idea first group nearby object “ microcluster ” perform k-mean cluster microcluster microcluster discuss section 103 1022 k-medoid representative object-based technique k-mean algorithm sensitive outlier object far away majority datum thus assign cluster dramatically distort mean value cluster inadvertently affect assignment object cluster effect particularly exacerbate due use squared-error function eq ( 101 ) observed example 102 example 102 drawback k-mean consider six point 1-d space value 1 2 3 8 9 10 25 respectively intuitively visual inspection may imagine point partition cluster { 1 2 3 } { 8 9 10 } point 25 exclude appear outlier would k-mean partition value apply k-mean used k = 2 eq ( 101 ) partition { { 1 2 3 } { 8 9 10 25 } } within-cluster variation ( 1 − 2 ) 2 + ( 2 − 2 ) 2 + ( 3 − 2 ) 2 + ( 8 − 13 ) 2 + ( 9 − 13 ) 2 + ( 10 − 13 ) 2 + ( 25 − 13 ) 2 = 196 give mean cluster { 1 2 3 } 2 mean { 8 9 10 25 } compare partition { { 1 2 3 8 } { 9 10 25 } } k-mean compute withincluster variation ( 1 − 35 ) 2 + ( 2 − 35 ) 2 + ( 3 − 35 ) 2 + ( 8 − 35 ) 2 + ( 9 − 1467 ) 2 + ( 10 − 1467 ) 2 + ( 25 − 1467 ) 2 = 18967 
102 partition method 455 give 35 mean cluster { 1 2 3 8 } 1467 mean cluster { 9 10 25 } latter partition lowest within-cluster variation therefore k-mean method assign value 8 cluster different contain 9 10 due outlier point moreover center second cluster 1467 substantially far member cluster “ modify k-mean algorithm diminish sensitivity outlier ” instead take mean value object cluster reference point pick actual object represent cluster used one representative object per cluster remain object assign cluster representative object similar partition method perform base principle minimize sum dissimilarity object p corresponding representative object absolute-error criterion used defined = k x x dist ( p oi ) ( 102 ) i=1 p∈ci e sum absolute error object p datum set oi representative object ci basis k-medoid method group n object k cluster minimize absolute error ( eq 102 ) k = 1 find exact median ( n2 ) time however k general positive number k-medoid problem np-hard partition around medoid ( pam ) algorithm ( see figure 105 later ) popular realization k-medoid cluster tackle problem iterative greedy way like k-mean algorithm initial representative object ( call seed ) choose arbitrarily consider whether replace representative object nonrepresentative object would improve cluster quality possible replacement try iterative process replace representative object object continue quality result cluster improve replacement quality measure cost function average dissimilarity object representative object cluster specifically let o1 ok current set representative object ( ie medoid ) determine whether nonrepresentative object denote orandom good replacement current medoid oj ( 1 ≤ j ≤ k ) calculate distance every object p closest object set { o1 oj−1 orandom oj+1 ok } use distance update cost function reassignment object { o1 oj−1 orandom oj+1 ok } simple suppose object p currently assign cluster represent medoid oj ( figure 104a b ) need reassign p different cluster oj replace orandom object p need reassign either orandom cluster represent oi ( = j ) whichever closest example figure 104 ( ) p closest oi therefore reassign oi figure 104 ( b ) however p closest orandom reassign orandom instead p currently assign cluster represent object oi = j 
456 chapter 10 cluster analysis basic concept method oi p oj orandom ( ) reassign oi oi oj p oi oj oi oj p orandom ( b ) reassign orandom ( c ) change orandom p orandom datum object cluster center swap swap ( ) reassign orandom figure 104 four case cost function k-medoid cluster object remain assign cluster represent oi long still closer oi orandom ( figure 104c ) otherwise reassign orandom ( figure 104d ) time reassignment occur difference absolute error e contribute cost function therefore cost function calculate difference absolute-error value current representative object replace nonrepresentative object total cost swap sum cost incur nonrepresentative object total cost negative oj replace swap orandom actual absolute-error e reduce total cost positive current representative object oj consider acceptable nothing change iteration “ method robust—k-mean k-medoid ” k-medoid method robust k-mean presence noise outlier medoid less influenced outlier extreme value mean however complexity iteration k-medoid algorithm ( k ( n − k ) 2 ) large value n k computation become costly much costly k-mean method method require user specify k number cluster “ scale k-medoid method ” typical k-medoid partition algorithm like pam ( figure 105 ) work effectively small datum set scale well large datum set deal larger datum set sampling-based method call clara ( cluster large application ) used instead take whole datum set consideration clara used random sample datum set pam algorithm apply compute best medoid sample ideally sample closely represent original datum set many case large sample work well create object equal probability select sample representative object ( medoid ) choose likely similar would choose whole datum set clara build clustering multiple random sample return best cluster output complexity compute medoid random sample ( ks 2 + k ( n − k ) ) size sample k number cluster n total number object clara deal larger datum set pam effectiveness clara depend sample size notice pam search best k-medoid among give datum set whereas clara search best k-medoid among select sample datum set clara find good cluster best sample medoid far best k-medoid object 
103 hierarchical method 457 algorithm k-medoid pam k-medoid algorithm partition base medoid central object input k number cluster datum set contain n object output set k cluster method ( 1 ) arbitrarily choose k object initial representative object seed ( 2 ) repeat ( 3 ) assign remain object cluster nearest representative object ( 4 ) randomly select nonrepresentative object orandom ( 5 ) compute total cost swap representative object oj orandom ( 6 ) < 0 swap oj orandom form new set k representative object ( 7 ) change figure 105 pam k-medoid partition algorithm one best k-medoid select sampling clara never find best cluster ( ask provide example demonstrate exercise ) “ might improve quality scalability clara ” recall search better medoid pam examine every object datum set every current medoid whereas clara confine candidate medoid random sample datum set randomize algorithm call claran ( cluster large application base upon randomize search ) present trade-off cost effectiveness used sample obtain cluster first randomly select k object datum set current medoid randomly select current medoid x object one current medoid replace x improve absolute-error criterion yes replacement make claran conduct randomize search l time set current medoid l step consider local optimum claran repeat randomize process time return best local optimal final result 103 hierarchical method partition method meet basic cluster requirement organize set object number exclusive group situation may want partition datum group different level hierarchy hierarchical cluster method work grouping datum object hierarchy “ tree ” cluster represent datum object form hierarchy useful datum summarization visualization example manager human resource allelectronic 
458 chapter 10 cluster analysis basic concept method may organize employee major group executive manager staff partition group smaller subgroup instance general group staff divide subgroup senior officer officer trainee group form hierarchy easily summarize characterize datum organized hierarchy used find say average salary manager officer consider handwritten character recognition another example set handwriting sample may first partition general group group correspond unique character group partition subgroup since character may written multiple substantially different way necessary hierarchical partition continue recursively desire granularity reach previous example although partition datum hierarchically assume datum hierarchical structure ( eg manager level allelectronic hierarchy staff ) use hierarchy summarize represent underlie datum compress way hierarchy particularly useful datum visualization alternatively application may believe datum bear underlie hierarchical structure want discover example hierarchical cluster may uncover hierarchy allelectronic employee structure say salary study evolution hierarchical cluster may group animal accord biological feature uncover evolutionary path hierarchy species another example grouping configuration strategic game ( eg chess checker ) hierarchical way may help develop game strategy used train player section study hierarchical cluster method section 1031 begin discussion agglomerative versus divisive hierarchical cluster organize object hierarchy used bottom-up top-down strategy respectively agglomerative method start individual object cluster iteratively merged form larger cluster conversely divisive method initially let give object form one cluster iteratively split smaller cluster hierarchical cluster method encounter difficulty regard selection merge split point decision critical group object merged split process next step operate newly generate cluster neither undo do previously perform object swap cluster thus merge split decision well choose may lead low-quality cluster moreover method scale well decision merge split need examine evaluate many object cluster promising direction improve cluster quality hierarchical method integrate hierarchical cluster cluster technique result multiple-phase ( multiphase ) cluster introduce two method namely birch chameleon birch ( section 1033 ) begin partition object hierarchically used tree structure leaf low-level nonleaf node view “ microcluster ” depend resolution scale apply 
103 hierarchical method 459 cluster algorithms perform macrocluster microcluster chameleon ( section 1034 ) explore dynamic modele hierarchical cluster several orthogonal way categorize hierarchical cluster method instance may categorize algorithmic method probabilistic method bayesian method agglomerative divisive multiphase method algorithmic meaning consider datum object deterministic compute cluster accord deterministic distance object probabilistic method use probabilistic model capture cluster measure quality cluster fitness model discuss probabilistic hierarchical cluster section bayesian method compute distribution possible clustering instead output single deterministic cluster datum set return group cluster structure probability conditional give datum bayesian method consider advanced topic discuss book 1031 agglomerative versus divisive hierarchical cluster hierarchical cluster method either agglomerative divisive depend whether hierarchical decomposition form bottom-up ( merge ) topdown ( splitting ) fashion let ’ closer look strategy agglomerative hierarchical cluster method used bottom-up strategy typically start let object form cluster iteratively merge cluster larger larger cluster object single cluster certain termination condition satisfied single cluster become hierarchy ’ root merge step find two cluster closest ( accord similarity measure ) combine two form one cluster two cluster merged per iteration cluster contain least one object agglomerative method require n iteration divisive hierarchical cluster method employ top-down strategy start place object one cluster hierarchy ’ root divide root cluster several smaller subcluster recursively partition cluster smaller one partition process continue cluster lowest level coherent enough—either contain one object object within cluster sufficiently similar either agglomerative divisive hierarchical cluster user specify desire number cluster termination condition example 103 agglomerative versus divisive hierarchical cluster figure 106 show application agne ( agglomerative nest ) agglomerative hierarchical cluster method diana ( divisive analysis ) divisive hierarchical cluster method datum set five object { b c e } initially agne agglomerative method place object cluster cluster merged step-by-step accord criterion example cluster c1 c2 may merged object c1 object c2 form minimum euclidean distance two object 
chapter 10 cluster analysis basic concept method agglomerative ( agne ) step 0 step 1 step 2 step 3 step 4 ab b abcde c cde de e step 4 step 3 step 2 step 1 divisive ( diana ) step 0 figure 106 agglomerative divisive hierarchical cluster datum object { b c e } level l=0 b c e 10 l=1 l=2 06 l=3 04 l=4 02 08 similarity scale 460 00 figure 107 dendrogram representation hierarchical cluster datum object { b c e } different cluster single-linkage approach cluster represent object cluster similarity two cluster measure similarity closest pair datum point belong different cluster cluster-merge process repeat object eventually merged form one cluster diana divisive method proceed contrast way object used form one initial cluster cluster split accord principle maximum euclidean distance closest neighboring object cluster cluster-split process repeat eventually new cluster contain single object tree structure call dendrogram commonly used represent process hierarchical cluster show object group together ( agglomerative method ) partition ( divisive method ) step-by-step figure 107 show dendrogram five object present figure 106 l = 0 show five object singleton cluster level l = 1 object b group together form 
103 hierarchical method 461 first cluster stay together subsequent level also use vertical axis show similarity scale cluster example similarity two group object { b } { c e } roughly 016 merged together form single cluster challenge divisive method partition large cluster several smaller one example 2n−1 − 1 possible way partition set n object two exclusive subset n number object n large computationally prohibitive examine possibility consequently divisive method typically used heuristic partition lead inaccurate result sake efficiency divisive method typically backtrack partition decision make cluster partition alternative partition cluster consider due challenge divisive method many agglomerative method divisive method 1032 distance measure algorithmic method whether used agglomerative method divisive method core need measure distance two cluster cluster generally set object four widely used measure distance cluster follow p − p0 | distance two object point p p0 mi mean cluster ci ni number object ci also know linkage measure minimum distance distmin ( ci cj ) = maximum distance distmax ( ci cj ) = mean distance average distance min { p − p0 | } ( 103 ) max { p − p0 | } ( 104 ) p∈ci p0 ∈cj p∈ci p0 ∈cj distmean ( ci cj ) = mi − mj | distavg ( ci cj ) = 1 ni nj x ( 105 ) p − p0 | ( 106 ) p∈ci p0 ∈cj algorithm used minimum distance dmin ( ci cj ) measure distance cluster sometimes call nearest-neighbor cluster algorithm moreover cluster process terminate distance nearest cluster exceed user-defined threshold call single-linkage algorithm view datum point node graph edge form path node cluster merge two cluster ci cj correspond add edge nearest pair node ci cj edge link cluster always go distinct cluster result graph generate tree thus agglomerative hierarchical cluster algorithm used minimum distance measure also call 
462 chapter 10 cluster analysis basic concept method minimal span tree algorithm span tree graph tree connect vertex minimal span tree one least sum edge weight algorithm used maximum distance dmax ( ci cj ) measure distance cluster sometimes call farthest-neighbor cluster algorithm cluster process terminate maximum distance nearest cluster exceed user-defined threshold call complete-linkage algorithm view datum point node graph edge link node think cluster complete subgraph edge connect node cluster distance two cluster determine distant node two cluster farthest-neighbor algorithms tend minimize increase diameter cluster iteration true cluster rather compact approximately equal size method produce high-quality cluster otherwise cluster produce meaningless previous minimum maximum measure represent two extreme measure distance cluster tend overly sensitive outlier noisy datum use mean average distance compromise minimum maximum distance overcome outlier sensitivity problem whereas mean distance simplest compute average distance advantageous handle categoric well numeric datum computation mean vector categoric datum difficult impossible define example 104 single versus complete linkage let us apply hierarchical cluster datum set figure 108 ( ) figure 108 ( b ) show dendrogram used single linkage figure 108 ( c ) show case used complete linkage edge cluster { b j h } { c g f e } omitted ease presentation example show used single linkage find hierarchical cluster defined local proximity whereas complete linkage tend find cluster opt global closeness variation four essential linkage measure discuss example measure distance two cluster distance centroid ( ie central object ) cluster 1033 birch multiphase hierarchical cluster used cluster feature tree balanced iterative reduce cluster used hierarchy ( birch ) design cluster large amount numeric datum integrate hierarchical cluster ( initial microcluster stage ) cluster method iterative partition ( later macrocluster stage ) overcome two difficulty agglomerative cluster method ( 1 ) scalability ( 2 ) inability undo do previous step birch used notion cluster feature summarize cluster cluster feature tree ( cf-tree ) represent cluster hierarchy structure help 
103 hierarchical method b c 463 e j h g f ( ) datum set b c e j h g f b c e f g h j c ( b ) cluster used single linkage b c e j h g f b h j e f g ( c ) cluster used complete linkage figure 108 hierarchical cluster used single complete linkage cluster method achieve good speed scalability large even stream databasis also make effective incremental dynamic cluster incoming object consider cluster n d-dimensional datum object point cluster feature ( cf ) cluster 3-d vector summarize information cluster object defined cf = hn ls ssi ( 107 ) p ls linear n point ( ie ni=1 xi ) ss square sum pn sum datum point ( ie i=1 xi 2 ) cluster feature essentially summary statistic give cluster used cluster feature easily derive many useful statistic cluster example cluster ’ centroid x0 radius r diameter n p x0 = i=1 n xi = ls n ( 108 ) 
464 chapter 10 cluster analysis basic concept method = = v u n ux u ( xi − x0 ) 2 u i=1 n = v ux n x n u u ( xi − xj ) 2 u i=1 j=1 n ( n − 1 ) nss − 2ls2 + nls n2 = 2nss − 2ls2 n ( n − 1 ) ( 109 ) ( 1010 ) r average distance member object centroid average pairwise distance within cluster r reflect tightness cluster around centroid summarize cluster used cluster feature avoid store detailed information individual object point instead need constant size space store cluster feature key birch efficiency space moreover cluster feature additive two disjoint cluster c1 c2 cluster feature cf1 = hn1 ls1 ss1 cf2 = hn2 ls2 ss2 respectively cluster feature cluster form merge c1 c2 simply cf1 + cf2 = hn1 + n2 ls1 + ls2 ss1 + ss2 ( 1011 ) example 105 cluster feature suppose three point ( 2 5 ) ( 3 2 ) ( 4 3 ) cluster c1 cluster feature c1 cf1 = h3 ( 2 + 3 + 4 5 + 2 + 3 ) ( 22 + 32 + 42 52 + 22 + 32 ) = h3 ( 9 10 ) ( 29 38 ) suppose c1 disjoint second cluster c2 cf2 = h3 ( 35 36 ) ( 417 440 ) cluster feature new cluster c3 form merge c1 c2 derive add cf1 cf2 cf3 = h3 + 3 ( 9 + 35 10 + 36 ) ( 29 + 417 38 + 440 ) = h6 ( 44 46 ) ( 446 478 ) cf-tree height-balanced tree store cluster feature hierarchical cluster example show figure definition nonleaf node tree descendant “ ” nonleaf node store sum cfs child thus summarize cluster information child cf-tree two parameter branch factor b threshold t branch factor specify maximum number child per nonleaf node threshold parameter specify maximum diameter subcluster store leaf node tree two parameter implicitly control result tree ’ size give limit amount main memory important consideration birch minimize time require output ( o ) birch apply multiphase cluster technique single scan datum set yield basic good cluster 
103 hierarchical method cf1 cf11 cf12 cf2 cf1k cfk 465 root level first level figure 109 cf-tree structure one additional scan optionally used improve quality primary phase phase 1 birch scan database build initial in-memory cf-tree view multilevel compression datum try preserve datum ’ inherent cluster structure phase 2 birch apply ( select ) cluster algorithm cluster leaf node cf-tree remove sparse cluster outlier group dense cluster larger one phase 1 cf-tree build dynamically object insert thus method incremental object insert closest leaf entry ( subcluster ) diameter subcluster store leaf node insertion larger threshold value leaf node possibly node split insertion new object information object pass toward root tree size cf-tree change modify threshold size memory need store cf-tree larger size main memory larger threshold value specify cf-tree rebuild rebuild process perform build new tree leaf node old tree thus process rebuild tree do without necessity reread object point similar insertion node split construction b+-tree therefore build tree datum read heuristic method introduce deal outlier improve quality cf-tree additional scan datum cf-tree build cluster algorithm typical partition algorithm used cf-tree phase 2 “ effective birch ” time complexity algorithm ( n ) n number object cluster experiment show linear scalability algorithm respect number object good quality cluster datum however since node cf-tree hold limit number entry due size cf-tree node always correspond user may consider natural cluster moreover cluster spherical shape birch perform well used notion radius diameter control boundary cluster 
466 chapter 10 cluster analysis basic concept method idea cluster feature cf-tree apply beyond birch idea borrow many other tackle problem cluster stream dynamic datum 1034 chameleon multiphase hierarchical cluster used dynamic modele chameleon hierarchical cluster algorithm used dynamic modele determine similarity pair cluster chameleon cluster similarity assessed base ( 1 ) well connect object within cluster ( 2 ) proximity cluster two cluster merged interconnectivity high close together thus chameleon depend static user-supplied model automatically adapt internal characteristic cluster merged merge process facilitate discovery natural homogeneous cluster apply datum type long similarity function specify figure 1010 illustrate chameleon work chameleon used k-nearest-neighbor graph approach construct sparse graph vertex graph represent datum object exist edge two vertex ( object ) one object among k-most similar object edge weight reflect similarity object chameleon used graph partition algorithm partition k-nearest-neighbor graph large number relatively small subcluster minimize edge cut cluster c partition subcluster ci cj minimize weight edge would cut c bisect ci cj assess absolute interconnectivity cluster ci cj chameleon used agglomerative hierarchical cluster algorithm iteratively merge subcluster base similarity determine pair similar subcluster take account interconnectivity closeness cluster specifically chameleon determine similarity pair cluster ci cj accord relative interconnectivity ri ( ci cj ) relative closeness rc ( ci cj ) relative interconnectivity ri ( ci cj ) two cluster ci cj defined absolute interconnectivity ci cj normalize respect k-nearest-neighbor graph datum set construct sparse graph partition graph final cluster merge partition figure 1010 chameleon hierarchical cluster base k-nearest neighbor dynamic modele source base karypis han kumar [ khk99 ] 
103 hierarchical method 467 internal interconnectivity two cluster ci cj ri ( ci cj ) = ec { ci cj } | 1 2 ( ecci | + eccj | ) ( 1012 ) ec { ci cj } edge cut previously defined cluster contain ci cj similarly ecci ( eccj ) minimum sum cut edge partition ci ( cj ) two roughly equal part relative closeness rc ( ci cj ) pair cluster ci cj absolute closeness ci cj normalize respect internal closeness two cluster ci cj defined rc ( ci cj ) = sec { ci cj } ci | ci cj | ec ci c | j + ci c sec cj | ( 1013 ) sec { ci cj } average weight edge connect vertex ci vertex cj sec ci ( sec cj ) average weight edge belong mincut bisector cluster ci ( cj ) chameleon show greater power discover arbitrarily shape cluster high quality several well-known algorithms birch densitybased dbscan ( section 1041 ) however process cost high-dimensional datum may require ( n2 ) time n object worst case 1035 probabilistic hierarchical cluster algorithmic hierarchical cluster method used linkage measure tend easy understand often efficient cluster commonly used many cluster analysis application however algorithmic hierarchical cluster method suffer several drawback first choose good distance measure hierarchical cluster often far trivial second apply algorithmic method datum object miss attribute value case datum partially observed ( ie attribute value object miss ) easy apply algorithmic hierarchical cluster method distance computation conduct third algorithmic hierarchical cluster method heuristic step locally search good splitting decision consequently optimization goal result cluster hierarchy unclear probabilistic hierarchical cluster aim overcome disadvantage used probabilistic model measure distance cluster one way look cluster problem regard set datum object cluster sample underlie datum generation mechanism analyze formally generative model example conduct cluster analysis set marketing survey assume survey collect sample opinion possible customer datum generation mechanism probability 
468 chapter 10 cluster analysis basic concept method distribution opinion respect different customer obtain directly completely task cluster estimate generative model accurately possible used observed datum object cluster practice assume datum generative model adopt common distribution function gaussian distribution bernoulli distribution govern parameter task learn generative model reduce find parameter value model best fit observed datum set example 106 generative model suppose give set 1-d point x = { x1 xn } cluster analysis let us assume datum point generate gaussian distribution n ( µ σ 2 ) = √ 2 1 2π σ 2 e − ( x−µ ) 2 2σ ( 1014 ) parameter µ ( mean ) σ 2 ( variance ) probability point xi ∈ x generate model ( x −µ ) 2 1 − e 2σ 2 p ( xi µ σ 2 ) = √ 2π σ 2 ( 1015 ) consequently likelihood x generate model l ( n ( µ σ 2 ) x ) = p ( x|µ σ 2 ) = n i=1 √ 1 2π σ 2 e − ( xi −µ ) 2 2σ 2 ( 1016 ) task learn generative model find parameter µ σ 2 likelihood l ( n ( µ σ 2 ) x ) maximize find n ( µ0 σ02 ) = arg max { l ( n ( µ σ 2 ) x ) } ( 1017 ) max { l ( n ( µ σ 2 ) x ) } call maximum likelihood give set object quality cluster form object measure maximum likelihood set object partition cluster c1 cm quality measure q ( { c1 cm } ) = i=1 p ( ci ) ( 1018 ) 
103 hierarchical method 469 p ( ) maximum likelihood merge two cluster cj1 cj2 cluster cj1 ∪ cj2 change quality overall cluster q ( ( { c1 cm } − { cj1 cj2 } ) ∪ { cj1 ∪ cj2 } ) − q ( { c1 cm } ) qm p ( ci ) · p ( cj1 ∪ cj2 ) = i=1 − p ( ci ) p ( cj1 ) p ( cj2 ) i=1 = i=1  p ( cj1 ∪ cj2 ) −1 p ( ci ) p ( cj1 ) p ( cj2 )  ( 1019 ) q choose merge two cluster hierarchical cluster i=1 p ( ci ) constant pair cluster therefore give cluster c1 c2 distance measure dist ( ci cj ) = − log p ( c1 ∪ c2 ) p ( c1 ) p ( c2 ) ( 1020 ) probabilistic hierarchical cluster method adopt agglomerative cluster framework use probabilistic model ( eq 1020 ) measure distance cluster upon close observation eq ( 1019 ) see merge two cluster may p ( c ∪c ) always lead improvement cluster quality p ( cj j1 ) p ( cj2j ) may less 1 2 example assume gaussian distribution function used model figure although merge cluster c1 c2 result cluster better fit gaussian distribution merge cluster c3 c4 lower cluster quality gaussian function fit merged cluster well base observation probabilistic hierarchical cluster scheme start one cluster per object merge two cluster ci cj distance negative iteration try find ci cj maximize p ( c ∪c ) p ( c ∪c ) j j log p ( ci ) p ( c iteration continue long log p ( ci ) p ( c > 0 long j ) j ) improvement cluster quality pseudocode give figure 1012 probabilistic hierarchical cluster method easy understand generally efficiency algorithmic agglomerative hierarchical cluster method fact share framework probabilistic model interpretable sometimes less flexible distance metric probabilistic model handle partially observed datum example give multidimensional datum set object miss value dimension learn gaussian model dimension independently used observed value dimension result cluster hierarchy accomplish optimization goal fitting datum select probabilistic model drawback used probabilistic hierarchical cluster output one hierarchy respect choose probabilistic model handle uncertainty cluster hierarchy give datum set may exist multiple hierarchy 
470 chapter 10 cluster analysis basic concept method c1 c2 ( ) c3 c4 ( b ) ( c ) figure 1011 merge cluster probabilistic hierarchical cluster ( ) merge cluster c1 c2 lead increase overall cluster quality merge cluster ( b ) c3 ( c ) c4 algorithm probabilistic hierarchical cluster algorithm input = { o1 } datum set contain n object output hierarchy cluster method ( 1 ) create cluster object ci = { oi } 1 ≤ ≤ n ( 2 ) = 1 n p ( c ∪c ) ( 3 ) j find pair cluster ci cj ci cj = arg maxi6=j log p ( c ) p ( c ) ( 4 ) j log p ( c ) p ( c ) > 0 merge ci cj ( 5 ) else stop p ( c ∪c ) j j figure 1012 probabilistic hierarchical cluster algorithm fit observed datum neither algorithmic approach probabilistic approach find distribution hierarchy recently bayesian tree-structure model develop handle problem bayesian sophisticated probabilistic cluster method consider advanced topic cover book 
104 density-based method 104 471 density-based method partition hierarchical method design find spherical-shap cluster difficulty find cluster arbitrary shape “ ” shape oval cluster figure give datum would likely inaccurately identify convex region noise outlier include cluster find cluster arbitrary shape alternatively model cluster dense region datum space separated sparse region main strategy behind density-based cluster method discover cluster nonspherical shape section learn basic technique density-based cluster study three representative method namely dbscan ( section 1041 ) optic ( section 1042 ) denclue ( section 1043 ) 1041 dbscan density-based cluster base connect region high density “ find dense region density-based cluster ” density object measure number object close o dbscan ( density-based spatial cluster application noise ) find core object object dense neighborhood connect core object neighborhood form dense region cluster “ dbscan quantify neighborhood object ” user-specified parameter  > 0 used specify radius neighborhood consider every object -neighborhood object space within radius  center due fix neighborhood size parameterized  density neighborhood measure simply number object neighborhood determine whether neighborhood dense dbscan used another user-specified figure 1013 cluster arbitrary shape 
472 chapter 10 cluster analysis basic concept method parameter minpt specify density threshold dense region object core object -neighborhood object contain least minpt object core object pillar dense region give set object identify core object respect give parameter  minpt cluster task therein reduce used core object neighborhood form dense region dense region cluster core object q object p say p directly density-reachable q ( respect  minpt ) p within -neighborhood q clearly object p directly density-reachable another object q q core object p -neighborhood q used directly density-reachable relation core object “ bring ” object -neighborhood dense region “ assemble large dense region used small dense region center core object ” dbscan p density-reachable q ( respect  minpt ) chain object p1 pn p1 = q pn = p pi+1 directly density-reachable pi respect  minpt 1 ≤ ≤ n pi ∈ d note density-reachability equivalence relation symmetric o1 o2 core object o1 density-reachable o2 o2 density-reachable o1 however o2 core object o1 o1 may density-reachable o2 vice versa connect core object well neighbor dense region dbscan used notion density-connectedness two object p1 p2 ∈ density-connect respect  minpt object q ∈ p1 p2 densityreachable q respect  minpt unlike density-reachability densityconnectedness equivalence relation easy show object o1 o2 o3 o1 o2 density-connect o2 o3 density-connect o1 o3 example 107 density-reachability density-connectivity consider figure 1014 give  represent radius circle say let minpt = 3 labele point p r core object -neighborhood contain least three point object q directly density-reachable m object directly density-reachable p vice versa object q ( indirectly ) density-reachable p q directly densityreachable directly density-reachable p however p densityreachable q q core object similarly r density-reachable density-reachable r thus r density-connect use closure density-connectedness find connect dense region cluster close set density-based cluster subset c ⊆ cluster ( 1 ) two object o1 o2 ∈ c o1 o2 density-connect ( 2 ) exist object ∈ c another object o0 ∈ ( − c ) o0 densityconnect 
104 density-based method 473 q p r figure 1014 density-reachability density-connectivity density-based cluster source base ester kriegel sander xu [ eksx96 ] “ dbscan find cluster ” initially object give datum set marked “ ” dbscan randomly select unvisite object p mark p “ visit ” check whether -neighborhood p contain least minpt object p marked noise point otherwise new cluster c create p object -neighborhood p add candidate set n dbscan iteratively add c object n belong cluster process object p0 n carry label “ unvisite ” dbscan mark “ visit ” check -neighborhood -neighborhood p0 least minpt object object -neighborhood p0 add n dbscan continue add object c c longer expand n empty time cluster c complete thus output find next cluster dbscan randomly select unvisite object remain one cluster process continue object visit pseudocode dbscan algorithm give figure 1015 spatial index used computational complexity dbscan ( n log n ) n number database object otherwise complexity ( n2 ) appropriate setting user-defined parameter  minpt algorithm effective find arbitrary-shap cluster 1042 optic order point identify cluster structure although dbscan cluster object give input parameter  ( maximum radius neighborhood ) minpt ( minimum number point require neighborhood core object ) encumber user responsibility select parameter value lead discovery acceptable cluster problem associate many cluster algorithms parameter setting 
474 chapter 10 cluster analysis basic concept method algorithm dbscan density-based cluster algorithm input datum set contain n object  radius parameter minpt neighborhood density threshold output set density-based cluster method ( 1 ) mark object unvisite ( 2 ) ( 3 ) randomly select unvisite object p ( 4 ) mark p visit ( 5 ) -neighborhood p least minpt object ( 6 ) create new cluster c add p c ( 7 ) let n set object -neighborhood p ( 8 ) point p0 n ( 9 ) p0 unvisite ( 10 ) mark p0 visit ( 11 ) -neighborhood p0 least minpt point add point n ( 12 ) p0 yet member cluster add p0 c ( 13 ) end ( 14 ) output c ( 15 ) else mark p noise ( 16 ) object unvisite figure 1015 dbscan algorithm usually empirically set difficult determine especially real-world highdimensional datum set algorithms sensitive parameter value slightly different setting may lead different clustering datum moreover real-world high-dimensional datum set often skewer distribution intrinsic cluster structure may well characterize single set global density parameter note density-based cluster monotonic respect neighborhood threshold dbscan fix minpt value two neighborhood threshold 1 < 2 cluster c respect 1 minpt must subset cluster c 0 respect 2 minpt mean two object density-based cluster must also cluster lower density requirement overcome difficulty used one set global parameter cluster analysis cluster analysis method call optic propose optic explicitly produce datum set cluster instead output cluster order linear list 
104 density-based method 475 object analysis represent density-based cluster structure datum object denser cluster list closer cluster order order equivalent density-based cluster obtain wide range parameter setting thus optic require user provide specific density threshold cluster order used extract basic cluster information ( eg cluster center arbitrary-shap cluster ) derive intrinsic cluster structure well provide visualization cluster construct different clustering simultaneously object processed specific order order select object density-reachable respect lowest  value cluster higher density ( lower  ) finished first base idea optic need two important piece information per object core-distance object p smallest value  0  0 neighborhood p least minpt object  0 minimum distance threshold make p core object p core object respect  minpt core-distance p undefined reachability-distance object p q minimum radius value make p density-reachable q accord definition density-reachability q core object p must neighborhood q therefore reachability-distance q p max { core-distance ( q ) dist ( p q ) } q core object respect  minpt reachability-distance p q undefined object p may directly reachable multiple core object therefore p may multiple reachability-distance respect different core object smallest reachability-distance p particular interest give shortest path p connect dense cluster example 108 core-distance reachability-distance figure 1016 illustrate concept coredistance reachability-distance suppose  = 6 mm minpt = coredistance p distance  0 p fourth closest datum object p reachability-distance q1 p core-distance p ( ie  0 = 3 mm ) greater euclidean distance p q1 reachability-distance q2 respect p euclidean distance p q2 greater core-distance p optic compute order object give database object database store core-distance suitable reachability-distance optic maintain list call orderseed generate output order object orderseed sort reachability-distance respective closest core object smallest reachability-distance object optic begin arbitrary object input database current object p retrieve -neighborhood p determine core-distance set reachability-distance undefined current object p written output 
476 chapter 10 cluster analysis basic concept method = 6 mm p = 3 mm = 6 mm  p q1 q2 core-distance p reachability-distance ( p q1 ) = = 3 mm reachability-distance ( p q2 ) = dist ( p q2 ) figure 1016 optic terminology source base ankerst breunig kriegel sander [ abks99 ] p core object optic simply move next object orderseed list ( input database orderseed empty ) p core object object q -neighborhood p optic update reachability-distance p insert q orderseed q yet processed iteration continue input fully consume orderseed empty datum set ’ cluster order represent graphically help visualize understand cluster structure datum set example figure 1017 reachability plot simple 2-d datum set present general overview datum structure cluster datum object plot cluster order ( horizontal axis ) together respective reachability-distance ( vertical axis ) three gaussian “ bump ” plot reflect three cluster datum set method also develop view cluster structure high-dimensional datum various level detail structure optic algorithm similar dbscan consequently two algorithms time complexity complexity ( n log n ) spatial index used ( n2 ) otherwise n number object 1043 denclue cluster base density distribution function density estimation core issue density-based cluster method denclue ( density-based cluster ) cluster method base set density distribution function first give background density estimation describe denclue algorithm probability statistic density estimation estimation unobservable underlie probability density function base set observed datum context density-based cluster unobservable underlie probability density function true distribution population possible object analyze observed datum set regard random sample population 
104 density-based method 477 reachability-distance undefined cluster order object figure 1017 cluster order optic source adapt ankerst breunig kriegel sander [ abks99 ] 1 2 figure 1018 subtlety density estimation dbscan optic increase neighborhood radius slightly 1 2 result much higher density dbscan optic density calculate count number object neighborhood defined radius parameter  density estimate highly sensitive radius value used example figure 1018 density change significantly radius increase small amount overcome problem kernel density estimation used nonparametric density estimation approach statistic general idea behind kernel density estimation simple treat observed object indicator 
478 chapter 10 cluster analysis basic concept method high-probability density surround region probability density point depend distance point observed object formally let x1 xn independent identically distribute sample random variable f kernel density approximation probability density function   n x − xi 1 x ( 1021 ) k fˆh ( x ) = nh h i=1 k ( ) kernel h bandwidth serve smooth parameter kernel regard function modele influence sample point within neighborhood technically kernel k ( ) isra non-negative real-valu integrable func+∞ tion satisfy two requirement −∞ k ( u ) du = 1 k ( −u ) = k ( u ) value u frequently used kernel standard gaussian function mean 0 variance 1   x − xi 1 − ( x − 2xi ) 2 2h k ( 1022 ) √ e h 2π denclue used gaussian kernel estimate density base give set object cluster point x∗ call density attractor local maximum estimate density function avoid trivial local maximum point denclue used noise threshold ξ consider density attractor x∗ fˆ ( x∗ ) ≥ ξ nontrivial density attractor center cluster object analysis assign cluster density attractor used stepwise hill-climb procedure object x hill-climb procedure start x guide gradient estimate density function density attractor x compute x0 = x xj+1 = xj + δ ∇ fˆ ( xj ) ∇ fˆ ( xj ) | ( 1023 ) δ parameter control speed convergence ∇ fˆ ( x ) = hd+2 n 1   x − xi ( x − x ) k i=1 h pn ( 1024 ) hill-climb procedure stop step k > 0 fˆ ( xk+1 ) < fˆ ( xk ) assign x density attractor x∗ = xk object x outlier noise converge hillclimb procedure local maximum x∗ fˆ ( x∗ ) < ξ cluster denclue set density attractor x set input object c object c assign density attractor x exist path every pair density attractor density ξ used multiple density attractor connect path denclue find cluster arbitrary shape 
105 grid-based method 479 denclue several advantage regard generalization several well-known cluster method single-linkage approach dbscan moreover denclue invariant noise kernel density estimation effectively reduce influence noise uniformly distribute noise input datum 105 grid-based method cluster method discuss far data-driven—they partition set object adapt distribution object embedding space alternatively grid-based cluster method take space-driven approach partition embedding space cell independent distribution input object grid-based cluster approach used multiresolution grid datum structure quantize object space finite number cell form grid structure operation cluster perform main advantage approach fast process time typically independent number datum object yet dependent number cell dimension quantized space section illustrate grid-based cluster used two typical example sting ( section 1051 ) explore statistical information store grid cell clique ( section 1052 ) represent - density-based approach subspace cluster high-dimensional datum space 1051 sting statistical information grid sting grid-based multiresolution cluster technique embedding spatial area input object divide rectangular cell space divide hierarchical recursive way several level rectangular cell correspond different level resolution form hierarchical structure cell high level partition form number cell next lower level statistical information regard attribute grid cell mean maximum minimum value precompute store statistical parameter statistical parameter useful query process datum analysis task figure 1019 show hierarchical structure sting cluster statistical parameter higher-level cell easily compute parameter lower-level cell parameter include follow attribute-independent parameter count attribute-dependent parameter mean stdev ( standard deviation ) min ( minimum ) max ( maximum ) type distribution attribute value cell follow normal uniform exponential none ( distribution unknown ) attribute select measure analysis price house object datum load database parameter count mean stdev min max bottom-level cell calculate directly datum value distribution may either assign user distribution type know 
480 chapter 10 cluster analysis basic concept method first layer ( – 1 ) st layer ith layer figure 1019 hierarchical structure sting cluster beforehand obtain hypothesis test χ 2 test type distribution higher-level cell compute base majority distribution type corresponding lower-level cell conjunction threshold filter process distribution lower-level cell disagree fail threshold test distribution type high-level cell set none “ statistical information useful query answer ” statistical parameter used top-down grid-based manner follow first layer within hierarchical structure determine query-answer process start layer typically contain small number cell cell current layer compute confidence interval ( estimate probability range ) reflect cell ’ relevancy give query irrelevant cell remove consideration process next lower level examine remain relevant cell process repeat bottom layer reach time query specification meet region relevant cell satisfy query return otherwise datum fall relevant cell retrieve processed meet query ’ requirement interesting property sting approach cluster result dbscan granularity approach 0 ( ie toward low-level datum ) word used count cell size information dense cluster identify approximately used sting therefore sting also regard density-based cluster method “ advantage sting offer cluster method ” sting offer several advantage ( 1 ) grid-based computation query-independent statistical information store cell represent summary information datum grid cell independent query ( 2 ) grid structure facilitate parallel process incremental update ( 3 ) method ’ efficiency major advantage sting go database compute statistical parameter cell hence time complexity generate cluster ( n ) n total number object generate hierarchical structure query process time 
105 grid-based method 481 ( g ) g total number grid cell lowest level usually much smaller n sting used multiresolution approach cluster analysis quality sting cluster depend granularity lowest level grid structure granularity fine cost process increase substantially however bottom level grid structure coarse may reduce quality cluster analysis moreover sting consider spatial relationship child neighboring cell construction parent cell result shape result cluster isothetic cluster boundary either horizontal vertical diagonal boundary detected may lower quality accuracy cluster despite fast process time technique 1052 clique apriori-like subspace cluster method datum object often ten attribute many may irrelevant value attribute may vary considerably factor make difficult locate cluster span entire datum space may meaningful instead search cluster within different subspace datum example consider healthinformatic application patient record contain extensive attribute describe personal information numerous symptom condition family history find nontrivial group patient even attribute strongly agree unlikely bird flu patient instance age gender job attribute may vary dramatically within wide range value thus difficult find cluster within entire datum space instead search subspace may find cluster similar patient lower-dimensional space ( eg patient similar one respect symptom like high fever cough runny nose age 3 16 ) clique ( cluster quest ) simple grid-based method find densitybased cluster subspace clique partition dimension nonoverlapping interval thereby partition entire embedding space datum object cell used density threshold identify dense cell sparse one cell dense number object map exceed density threshold main strategy behind clique identify candidate search space used monotonicity dense cell respect dimensionality base apriori property used frequent pattern association rule mining ( chapter 6 ) context cluster subspace monotonicity say follow k-dimensional cell c ( k > 1 ) least l point every ( k − 1 ) dimensional projection c cell ( k − 1 ) dimensional subspace least l point consider figure 1020 embedding datum space contain three dimension age salary vacation 2-d cell say subspace form age salary contain l point projection cell every dimension age salary respectively contain least l point clique perform cluster two step first step clique partition d-dimensional datum space nonoverlapping rectangular unit identify dense unit among clique find dense cell subspace 
482 chapter 10 cluster analysis basic concept method 7 salary ( $ 10000 ) 6 5 4 3 2 1 0 20 30 40 50 60 age 30 40 50 60 age 7 vacation ( week ) 6 5 4 3 2 1 vacation 0 20 50 age sa la ry 30 figure 1020 dense unit find respect age dimension salary vacation intersected provide candidate search space dense unit higher dimensionality 
106 evaluation cluster 483 clique partition every dimension interval identify interval contain least l point l density threshold clique iteratively join two k-dimensional dense cell c1 c2 subspace ( di1 dik ) ( dj1 djk ) respectively di1 = dj1 dik−1 = djk−1 c1 c2 share interval dimension join operation generate new ( k + 1 ) dimensional candidate cell c space ( di1 dik−1 dik djk ) clique check whether number point c pass density threshold iteration terminate candidate generate candidate cell dense second step clique used dense cell subspace assemble cluster arbitrary shape idea apply minimum description length ( mdl ) principle ( chapter 8 ) use maximal region cover connect dense cell maximal region hyperrectangle every cell fall region dense region extend dimension subspace find best description cluster general np-hard thus clique adopt simple greedy approach start arbitrary dense cell find maximal region cover cell work remain dense cell yet cover greedy method terminate dense cell cover “ effective clique ” clique automatically find subspace highest dimensionality high-density cluster exist subspace insensitive order input object presume canonical datum distribution scale linearly size input good scalability number dimension datum increase however obtain meaningful cluster dependent proper tune grid size ( stable structure ) density threshold difficult practice grid size density threshold used across combination dimension datum set thus accuracy cluster result may degraded expense method ’ simplicity moreover give dense region projection region onto lower-dimensionality subspace also dense result large overlap among report dense region furthermore difficult find cluster rather different density within different dimensional subspace several extension approach follow similar philosophy example think grid set fix bin instead used fix bin dimension use adaptive data-driven strategy dynamically determine bin dimension base datum distribution statistic alternatively instead used density threshold may use entropy ( chapter 8 ) measure quality subspace cluster 106 evaluation cluster learn cluster know several popular cluster method may ask “ try cluster method datum set evaluate whether cluster result good ” general cluster evaluation assess 
484 chapter 10 cluster analysis basic concept method feasibility cluster analysis datum set quality result generate cluster method major task cluster evaluation include follow assess cluster tendency task give datum set assess whether nonrandom structure exist datum blindly apply cluster method datum set return cluster however cluster mine may mislead cluster analysis datum set meaningful nonrandom structure datum determine number cluster datum set algorithms k-mean require number cluster datum set parameter moreover number cluster regard interesting important summary statistic datum set therefore desirable estimate number even cluster algorithm used derive detailed cluster measure cluster quality apply cluster method datum set want assess good result cluster number measure used method measure well cluster fit datum set other measure well cluster match ground truth truth available also measure score clustering thus compare two set cluster result datum set rest section discuss three topic 1061 assess cluster tendency cluster tendency assessment determine whether give datum set non-random structure may lead meaningful cluster consider datum set non-random structure set uniformly distribute point datum space even though cluster algorithm may return cluster datum cluster random meaningful example 109 cluster require nonuniform distribution datum figure 1021 show datum set uniformly distribute 2-d datum space although cluster algorithm may still artificially partition point group group unlikely mean anything significant application due uniform distribution datum “ assess cluster tendency datum set ” intuitively try measure probability datum set generate uniform datum distribution achieve used statistical test spatial randomness illustrate idea let ’ look simple yet effective statistic call hopkin statistic hopkin statistic spatial statistic test spatial randomness variable distribute space give datum set regard sample 
106 evaluation cluster 485 figure 1021 datum set uniformly distribute datum space random variable want determine far away uniformly distribute datum space calculate hopkin statistic follow sample n point p1 pn uniformly d point probability include sample point pi find nearest neighbor pi ( 1 ≤ ≤ n ) let xi distance pi nearest neighbor d xi = min { dist ( pi v ) } v∈d ( 1025 ) sample n point q1 qn uniformly d qi ( 1 ≤ ≤ n ) find nearest neighbor qi − { qi } let yi distance qi nearest neighbor − { qi } yi = min { dist ( qi v ) } v∈d v6=qi ( 1026 ) calculate hopkin statistic h pn h = pn i=1 xi i=1 yi + pn i=1 yi ( 1027 ) “ hopkin statistic tell us likely datum set follow pn uniform distribution datum space ” uniformly distribute i=1 yi pn x would close thus h would 05 however i=1 p p highly skewer ni=1 yi would substantially smaller ni=1 xi expectation thus h would close 0 
486 chapter 10 cluster analysis basic concept method null hypothesis homogeneous hypothesis—that uniformly distribute thus contain meaningful cluster nonhomogeneous hypothesis ( ie uniformly distribute thus contain cluster ) alternative hypothesis conduct hopkin statistic test iteratively used 05 threshold reject alternative hypothesis h > 05 unlikely statistically significant cluster 1062 determine number cluster determine “ right ” number cluster datum set important cluster algorithms like k-mean require parameter also appropriate number cluster control proper granularity cluster analysis regard find good balance compressibility accuracy cluster analysis consider two extreme case treat entire datum set cluster would maximize compression datum cluster analysis value hand treat object datum set cluster give finest cluster resolution ( ie accurate due zero distance object corresponding cluster center ) method like k-mean even achieve best cost however one object per cluster enable datum summarization determine number cluster far easy often “ right ” number ambiguous figure right number cluster often depend distribution ’ shape scale datum set well cluster resolution require user many possible way estimate number cluster briefly introduce simple yet popular effective method q simple method set number cluster n2 datum set n √ point expectation cluster 2n point elbow method base observation increase number cluster help reduce sum within-cluster variance cluster cluster allow one capture finer group datum object similar however marginal effect reduce sum within-cluster variance may drop many cluster form splitting cohesive cluster two give small reduction consequently heuristic select right number cluster use turn point curve sum within-cluster variance respect number cluster technically give number k > 0 form k cluster datum set question used cluster algorithm like k-mean calculate sum within-cluster variance var ( k ) plot curve var respect k first ( significant ) turn point curve suggest “ right ” number advanced method determine number cluster used information criterium information theoretic approach please refer bibliographic note information ( section 109 ) 
106 evaluation cluster 487 “ right ” number cluster datum set also determine crossvalidation technique often used classification ( chapter 8 ) first divide give datum set part next use − 1 part build cluster model use remain part test quality cluster example point test set find closest centroid consequently use sum square distance point test set closest centroid measure well cluster model fit test set integer k > 0 repeat process time derive clustering k cluster used part turn test set average quality measure take overall quality measure compare overall quality measure respect different value k find number cluster best fit datum 1063 measure cluster quality suppose assessed cluster tendency give datum set may also try predetermine number cluster set apply one multiple cluster method obtain clustering datum set “ good cluster generate method compare clustering generate different method ” method choose measure quality cluster general method categorize two group accord whether ground truth available ground truth ideal cluster often build used human expert ground truth available used extrinsic method compare cluster group truth measure ground truth unavailable use intrinsic method evaluate goodness cluster consider well cluster separated ground truth consider supervision form “ cluster ” hence extrinsic method also know supervised method intrinsic method unsupervised method let ’ look simple method category extrinsic method ground truth available compare cluster assess cluster thus core task extrinsic method assign score q ( c cg ) cluster c give ground truth cg whether extrinsic method effective largely depend measure q used general measure q cluster quality effective satisfy follow four essential criterium cluster homogeneity require pure cluster cluster better cluster suppose ground truth say object datum set belong category l1 ln consider cluster c1 wherein cluster c ∈ c1 contain object two category li lj ( 1 ≤ < j ≤ n ) also 
488 chapter 10 cluster analysis basic concept method consider cluster c2 identical c1 except c2 split two cluster contain object li lj respectively cluster quality measure q respect cluster homogeneity give higher score c2 c1 q ( c2 cg ) > q ( c1 cg ) cluster completeness counterpart cluster homogeneity cluster completeness require cluster two object belong category accord ground truth assign cluster cluster completeness require cluster assign object belong category ( accord ground truth ) cluster consider cluster c1 contain cluster c1 c2 member belong category accord ground truth let cluster c2 identical c1 except c1 c2 merged one cluster c2 cluster quality measure q respect cluster completeness give higher score c2 q ( c2 cg ) > q ( c1 cg ) rag bag many practical scenario often “ rag bag ” category contain object merged object category often call “ miscellaneous ” “ ” rag bag criterion state putt heterogeneous object pure cluster penalize putt rag bag consider cluster c1 cluster c ∈ c1 object c except one denote belong category accord ground truth consider cluster c2 identical c1 except assign cluster c 0 = c c2 c 0 contain object various category accord ground truth thus noisy word c 0 c2 rag bag cluster quality measure q respect rag bag criterion give higher score c2 q ( c2 cg ) > q ( c1 cg ) small cluster preservation small category split small piece cluster small piece may likely become noise thus small category discover cluster small cluster preservation criterion state splitting small category piece harmful splitting large category piece consider extreme case let datum set n + 2 object accord ground truth n object denote o1 belong one category two object denote on+1 on+2 belong another category suppose cluster c1 three cluster c1 = { o1 } c2 = { on+1 } c3 = { on+2 } let cluster c2 three cluster namely c1 = { o1 on−1 } c2 = { } c3 = { on+1 on+2 } word c1 split small category c2 split big category cluster quality measure q preserve small cluster give higher score c2 q ( c2 cg ) > q ( c1 cg ) many cluster quality measure satisfy four criterium introduce bcube precision recall metric satisfy four criterium bcube evaluate precision recall every object cluster give datum set accord ground truth precision object indicate many object cluster belong category object recall 
106 evaluation cluster 489 object reflect many object category assign cluster formally let = { o1 } set object c cluster d let l ( oi ) ( 1 ≤ ≤ n ) category oi give ground truth c ( oi ) cluster id oi c two object oi oj ( 1 ≤ j ≤ n = j ) correctness relation oi oj cluster c give ( 1 l ( oi ) = l ( oj ) ⇔ c ( oi ) = c ( oj ) correctness ( oi oj ) = 0 otherwise ( 1028 ) bcube precision defined x n x oj i6=j c ( oi ) c ( oj ) precision bcube = correctness ( oi oj ) k { oj i = j c ( oi ) = c ( oj ) } k i=1 n ( 1029 ) bcube recall defined x n x oj i6=j l ( oi ) l ( oj ) recall bcube = i=1 correctness ( oi oj ) k { oj i = j l ( oi ) = l ( oj ) } k n ( 1030 ) intrinsic method ground truth datum set available use intrinsic method assess cluster quality general intrinsic method evaluate cluster examine well cluster separated compact cluster many intrinsic method advantage similarity metric object datum set silhouette coefficient measure datum set n object suppose partition k cluster c1 ck object ∈ calculate ( ) average distance object cluster belong similarly b ( ) minimum average distance cluster belong formally suppose ∈ ci ( 1 ≤ ≤ k ) p ( ) = o0 ∈ci o6=o0 dist ( ) ci | − 1 0 ( 1031 ) 
490 chapter 10 cluster analysis basic concept method ( p b ( ) = min cj 1≤j≤k j6=i 0 ) o0 ∈cj dist ( ) cj | ( 1032 ) silhouette coefficient defined ( ) = b ( ) − ( ) max { ( ) b ( ) } ( 1033 ) value silhouette coefficient −1 value ( ) reflect compactness cluster belong smaller value compact cluster value b ( ) capture degree separated cluster larger b ( ) separated cluster therefore silhouette coefficient value approach 1 cluster contain compact far away cluster preferable case however silhouette coefficient value negative ( ie b ( ) < ( ) ) mean expectation closer object another cluster object cluster many case bad situation avoid measure cluster ’ fitness within cluster compute average silhouette coefficient value object cluster measure quality cluster use average silhouette coefficient value object datum set silhouette coefficient intrinsic measure also used elbow method heuristically derive number cluster datum set replace sum within-cluster variance 107 summary cluster collection datum object similar one another within cluster dissimilar object cluster process grouping set physical abstract object class similar object call cluster cluster analysis extensive application include business intelligence image pattern recognition web search biology security cluster analysis used standalone datum mining tool gain insight datum distribution preprocess step datum mining algorithms operate detected cluster cluster dynamic field research datum mining related unsupervised learn machine learn cluster challenge field typical requirement include scalability ability deal different type datum attribute discovery cluster arbitrary shape minimal requirement domain knowledge determine input parameter ability deal noisy datum incremental cluster 
108 exercise 491 insensitivity input order capability cluster high-dimensionality datum constraint-based cluster well interpretability usability many cluster algorithms develop categorize several orthogonal aspect regard partition criterium separation cluster similarity measure used cluster space chapter discuss major fundamental cluster method follow category partition method hierarchical method density-based method grid-based method algorithms may belong one category partition method first create initial set k partition parameter k number partition construct used iterative relocation technique attempt improve partition move object one group another typical partition method include k-mean k-medoid claran hierarchical method create hierarchical decomposition give set datum object method classify either agglomerative ( bottom-up ) divisive ( top-down ) base hierarchical decomposition form compensate rigidity merge split quality hierarchical agglomeration improve analyze object linkage hierarchical partition ( eg chameleon ) first perform microcluster ( grouping object “ microcluster ” ) operate microcluster cluster technique iterative relocation ( birch ) density-based method cluster object base notion density grow cluster either accord density neighborhood object ( eg dbscan ) accord density function ( eg denclue ) optic density-based method generate augment order datum ’ cluster structure grid-based method first quantize object space finite number cell form grid structure perform cluster grid structure sting typical example grid-based method base statistical information store grid cell clique grid-based subspace cluster algorithm cluster evaluation assess feasibility cluster analysis datum set quality result generate cluster method task include assess cluster tendency determine number cluster measure cluster quality 108 exercise 101 briefly describe give example follow approach cluster partition method hierarchical method density-based method grid-based method 
492 chapter 10 cluster analysis basic concept method 102 suppose datum mining task cluster point ( ( x ) represent location ) three cluster point a1 ( 2 10 ) a2 ( 2 5 ) a3 ( 8 4 ) b1 ( 5 8 ) b2 ( 7 5 ) b3 ( 6 4 ) c1 ( 1 2 ) c2 ( 4 9 ) distance function euclidean distance suppose initially assign a1 b1 c1 center cluster respectively use k-mean algorithm show ( ) three cluster center first round execution ( b ) final three cluster 103 use example show k-mean algorithm may find global optimum optimize within-cluster variation 104 k-mean algorithm interesting note choose initial cluster center carefully may able speed algorithm ’ convergence also guarantee quality final cluster + algorithm variant k-mean choose initial center follow first select one center uniformly random object datum set iteratively object p choose center choose object new center object choose random probability proportional dist ( p ) 2 dist ( p ) distance p closest center already choose iteration continue k center select explain method speed convergence k-mean algorithm also guarantee quality final cluster result 105 provide pseudocode object reassignment step pam algorithm 106 k-mean k-medoid algorithms perform effective cluster ( ) illustrate strength weakness k-mean comparison k-medoid ( b ) illustrate strength weakness scheme comparison hierarchical cluster scheme ( eg agne ) 107 prove dbscan density-connectedness equivalence relation 108 prove dbscan fix minpt value two neighborhood threshold 1 < 2 cluster c respect 1 minpt must subset cluster c 0 respect 2 minpt 109 provide pseudocode optic algorithm 1010 birch encounter difficulty find cluster arbitrary shape optic propose modification birch help find cluster arbitrary shape 1011 provide pseudocode step clique find dense cell subspace 
108 exercise 493 1012 present condition density-based cluster suitable partitioning-based cluster hierarchical cluster give application example support argument 1013 give example specific cluster method integrate example one cluster algorithm used preprocess step another addition provide reasoning integration two method may sometimes lead improve cluster quality efficiency 1014 cluster recognize important datum mining task broad application give one application example follow case ( ) application used cluster major datum mining function ( b ) application used cluster preprocess tool datum preparation datum mining task 1015 datum cube multidimensional databasis contain nominal ordinal numeric datum hierarchical aggregate form base learn cluster method design cluster method find cluster large datum cube effectively efficiently 1016 describe follow cluster algorithms term follow criterium ( 1 ) shape cluster determine ( 2 ) input parameter must specify ( 3 ) limitation ( ) ( b ) ( c ) ( ) ( e ) ( f ) k-mean k-medoid clara birch chameleon dbscan 1017 human eye fast effective judge quality cluster method 2-d datum design datum visualization method may help human visualize datum cluster judge cluster quality 3-d datum even higher-dimensional datum 1018 suppose allocate number automatic teller machine ( atms ) give region satisfy number constraint household workplace may cluster typically one atm assign per cluster cluster however may constrain two factor ( 1 ) obstacle object ( ie bridge river highway affect atm accessibility ) ( 2 ) additional user-specified constraint atm serve least 10000 household cluster algorithm k-mean modify quality cluster constraint 1019 constraint-based cluster aside minimum number customer cluster ( atm allocation ) constraint many kind 
494 chapter 10 cluster analysis basic concept method constraint example constraint can form maximum number customer per cluster average income customer per cluster maximum distance every two cluster categorize kind constraint impose cluster produce discuss perform cluster efficiently kind constraint 1020 design privacy-preserve cluster method datum owner would able ask third party mine datum quality cluster without worry potential inappropriate disclosure certain private sensitive information store datum 1021 show bcube metric satisfy four essential requirement extrinsic cluster evaluation method 109 bibliographic note cluster extensively study 40 year across many discipline due broad application book pattern classification machine learn contain chapter cluster analysis unsupervised learn several textbook dedicate method cluster analysis include hartigan [ har75 ] jain dube [ jd88 ] kaufman rousseeuw [ kr90 ] arabie hubert de sorte [ ahs96 ] also many survey article different aspect cluster method recent one include jain murty flynn [ jmf99 ] parson haque liu [ phl04 ] jain [ jai10 ] partition method k-mean algorithm first introduce lloyd [ llo57 ] macqueen [ mac67 ] arthur vassilvitskii [ av07 ] present + algorithm filter algorithm used spatial hierarchical datum index speed computation cluster mean give kanungo mount netanyahu et al [ + 02 ] k-medoid algorithms pam clara propose kaufman rousseeuw [ kr90 ] k-mode ( cluster nominal datum ) k-prototype ( cluster hybrid datum ) algorithms propose huang [ hua98 ] k-mode cluster algorithm also propose independently chaturvedi green carroll [ cgc94 cgc01 ] claran algorithm propose ng han [ nh94 ] ester kriegel xu [ ekx95 ] propose technique improvement performance claran used efficient spatial access method r∗-tree focuse technique k-means-based scalable cluster algorithm propose bradley fayyad reina [ bfr98 ] early survey agglomerative hierarchical cluster algorithms conduct day edelsbrunner [ de84 ] agglomerative hierarchical cluster agne divisive hierarchical cluster diana introduce kaufman rousseeuw [ kr90 ] interesting direction improve cluster quality hierarchical cluster method integrate hierarchical cluster distance-based iterative relocation nonhierarchical cluster method example birch zhang ramakrishnan livny [ zrl96 ] first perform hierarchical cluster 
109 bibliographic note 495 cf-tree apply technique hierarchical cluster also perform sophisticated linkage analysis transformation nearest-neighbor analysis cure guha rastogi shim [ grs98 ] rock ( cluster nominal attribute ) guha rastogi shim [ grs99 ] chameleon karypis han kumar [ khk99 ] probabilistic hierarchical cluster framework follow normal linkage algorithms used probabilistic model define cluster similarity develop friedman [ fri03 ] heller ghahramani [ hg05 ] density-based cluster method dbscan propose ester kriegel sander xu [ eksx96 ] ankerst breunig kriegel sander [ abks99 ] develop optic cluster-order method facilitate density-based cluster without worry parameter specification denclue algorithm base set density distribution function propose hinneburg keim [ hk98 ] hinneburg gabriel [ hg07 ] develop denclue 20 include new hill-climb procedure gaussian kernel adjust step size automatically sting grid-based multiresolution approach collect statistical information grid cell propose wang yang muntz [ wym97 ] wavecluster develop sheikholeslami chatterjee zhang [ scz98 ] multiresolution cluster approach transform original feature space wavelet transform scalable method cluster nominal datum study gibson kleinberg raghavan [ gkr98 ] guha rastogi shim [ grs99 ] ganti gehrke ramakrishnan [ ggr99 ] also many cluster paradigm example fuzzy cluster method discuss kaufman rousseeuw [ kr90 ] bezdek [ bez81 ] bezdek pal [ bp92 ] high-dimensional cluster apriori-based dimension-growth subspace cluster algorithm call clique propose agrawal gehrke gunopulos raghavan [ aggr98 ] integrate density-based grid-based cluster method recent study proceed cluster stream datum babcock badu datar et al [ + 02 ] k-median-based datum stream cluster algorithm propose guha mishra motwani ’ callaghan [ gmmo00 ] ’ callaghan et al [ + 02 ] method cluster evolve datum stream propose aggarwal han wang yu [ ahwy03 ] framework project cluster high-dimensional datum stream propose aggarwal han wang yu [ ahwy04a ] cluster evaluation discuss monograph survey article jain dube [ jd88 ] halkidi batistakis vazirgiannis [ hbv01 ] extrinsic method cluster quality evaluation extensively explore recent study include meilǎ [ mei03 mei05 ] amigó gonzalo artile verdejo [ agav09 ] four essential criterium introduce chapter formulate amigó gonzalo artile verdejo [ agav09 ] individual criterium also mentioned earlier example meilǎ [ mei03 ] rosenberg hirschberg [ rh07 ] bagga baldwin [ bb98 ] introduce bcube metric silhouette coefficient describe kaufman rousseeuw [ kr90 ] 
11 advanced cluster analysis learn fundamental cluster analysis chapter chapter discuss advanced topic cluster analysis specifically investigate four major perspective probabilistic model-based cluster section 111 introduce general framework method derive cluster object assign probability belong cluster probabilistic model-based cluster widely used many datum mining application text mining cluster high-dimensional datum dimensionality high conventional distance measure dominate noise section 112 introduce fundamental method cluster analysis high-dimensional datum cluster graph network datum graph network datum increasingly popular application online social network world wide web digital library section 113 study key issue cluster graph network datum include similarity measurement cluster method cluster constraint discussion far assume constraint cluster application however various constraint may exist constraint may rise background knowledge spatial distribution object learn conduct cluster analysis different kind constraint section 114 end chapter good grasp issue technique regard advanced cluster analysis 111 probabilistic model-based cluster cluster analysis method discuss far datum object assign one number cluster cluster assignment rule require application assign customer marketing manager however datum mining concept technique doi b978-0-12-381479-100011-3 c 2012 elsevier right re-serve 497 
498 chapter 11 advanced cluster analysis application rigid requirement may desirable section demonstrate need fuzzy flexible cluster assignment application introduce general method compute probabilistic cluster assignment “ situation may datum object belong one cluster ” consider example 111 example 111 cluster product reviews allelectronic online store customer purchase online also create reviews product every product receive reviews instead product may many reviews many other none moreover review may involve multiple product thus review editor allelectronic task cluster reviews ideally cluster topic example group product service issue highly related assign review one cluster exclusively would work well task suppose cluster “ camera camcorder ” another “ ” review talk compatibility camcorder computer review relate cluster however exclusively belong either cluster would like use cluster method allow review belong one cluster review indeed involve one topic reflect strength review belong cluster want assignment review cluster carry weight represent partial membership scenario object may belong multiple cluster occur often many application illustrated example 112 example 112 cluster study user search intent allelectronic online store record customer browse purchasing behavior log important datum mining task use log datum categorize understand user search intent example consider user session ( short period user interact online store ) user search product make comparison among different product look customer support information cluster analysis help difficult predefine user behavior pattern thoroughly cluster contain similar user browse trajectory may represent similar user behavior however every session belong one cluster example suppose user session involve purchase digital camera form one cluster user session compare laptop computer form another cluster user one session make order digital camera time compare several laptop computer session belong cluster extent section systematically study theme cluster allow object belong one cluster start notion fuzzy cluster section generalize concept probabilistic model-based cluster section section 1113 introduce expectation-maximization algorithm general framework mining cluster 
111 probabilistic model-based cluster 499 1111 fuzzy cluster give set object x = { x1 xn } fuzzy set subset x allow object x membership degree 0 formally fuzzy set modeled function fs x → [ 0 1 ] example 113 fuzzy set digital camera unit sell popular camera allelectronic use follow formula compute degree popularity digital camera give sale pop ( ) = ( 1 1000 1000 unit sell ( < 1000 ) unit sell ( 111 ) function pop ( ) define fuzzy set popular digital camera example suppose sale digital camera allelectronic show table fuzzy set popular digital camera { ( 005 ) b ( 1 ) c ( 086 ) ( 027 ) } degree membership written parenthesis apply fuzzy set idea cluster give set object cluster fuzzy set object cluster call fuzzy cluster consequently cluster contain multiple fuzzy cluster formally give set object o1 fuzzy cluster k fuzzy cluster c1 ck represent used partition matrix = [ wij ] ( 1 ≤ ≤ n 1 ≤ j ≤ k ) wij membership degree oi fuzzy cluster cj partition matrix satisfy follow three requirement object oi cluster cj 0 ≤ wij ≤ requirement enforce fuzzy cluster fuzzy set object oi k x wij = requirement ensure every object - j=1 pate cluster equivalently table 111 set digital camera sale allelectronic camera sale ( unit ) b c 50 1320 860 270 
500 chapter 11 advanced cluster analysis cluster cj 0 < n x wij < n requirement ensure every cluster i=1 least one object membership value nonzero example 114 fuzzy cluster suppose allelectronic online store six reviews keyword contain reviews list table 112 group reviews two fuzzy cluster c1 c2 c1 “ digital camera ” “ lens ” c2 “ ” partition matrix  1 1  1  = 2 3  0 0  0 0  0  1  3 1 1 use keyword “ digital camera ” “ lens ” feature cluster c1 “ computer ” feature cluster c2 review ri cluster cj ( 1 ≤ ≤ 6 1 ≤ j ≤ 2 ) wij defined wij = ri ∩ cj | ri ∩ cj | = ri ∩ ( c1 ∪ c2 ) | ri ∩ { digital camera lens computer } | fuzzy cluster review r4 belong cluster c1 c2 membership degree 23 31 respectively “ evaluate well fuzzy cluster describe datum set ” consider set object o1 fuzzy cluster c k cluster c1 ck let = [ wij ] ( 1 ≤ ≤ n 1 ≤ j ≤ k ) partition matrix let c1 ck center cluster c1 ck respectively center defined either mean medoid way specific application discuss chapter 10 distance similarity object center cluster object assign used measure well table 112 set reviews keyword used review id keyword r1 r2 r3 r4 r5 r6 digital camera lens digital camera lens digital camera lens computer computer cpu computer computer game 
111 probabilistic model-based cluster 501 object belong cluster idea extend fuzzy cluster object oi cluster cj wij > 0 dist ( oi cj ) measure well oi represent cj thus belong cluster cj object participate one cluster sum distance corresponding cluster center weight degree membership capture well object fit cluster formally object oi sum square error ( sse ) give sse ( oi ) = k x p wij dist ( oi cj ) 2 ( 112 ) j=1 parameter p ( p ≥ 1 ) control influence degree membership larger value p larger influence degree membership orthogonally sse cluster cj sse ( cj ) = n x p wij dist ( oi cj ) 2 ( 113 ) i=1 finally sse cluster defined sse ( c ) = n x k x p wij dist ( oi cj ) 2 ( 114 ) i=1 j=1 sse used measure well fuzzy cluster fit datum set fuzzy cluster also call soft cluster allow object belong one cluster easy see traditional ( rigid ) cluster enforce object belong one cluster exclusively special case fuzzy cluster defer discussion compute fuzzy cluster section 1113 1112 probabilistic model-based cluster “ fuzzy cluster ( section 1111 ) provide flexibility allow object participate multiple cluster general framework specify clustering object may participate multiple cluster probabilistic way ” section introduce general notion probabilistic model-based cluster answer question discuss chapter 10 conduct cluster analysis datum set assume object datum set fact belong different inherent category recall cluster tendency analysis ( section 1061 ) used examine whether datum set contain object may lead meaningful cluster inherent category hide datum latent mean directly observed instead infer used datum observed example topic hide set reviews allelectronic online store latent one read topic directly however topic infer reviews review one multiple topic 
502 chapter 11 advanced cluster analysis therefore goal cluster analysis find hide category datum set subject cluster analysis regard sample possible instance hide category without category label cluster derive cluster analysis infer used datum set design approach hide category statistically assume hide category distribution datum space mathematically represent used probability density function ( distribution function ) call hide category probabilistic cluster probabilistic cluster c probability density function f point datum space f ( ) relative likelihood instance c appear example 115 probabilistic cluster suppose digital camera sell allelectronic divide two category c1 consumer line ( eg point-and-shoot camera ) c2 professional line ( eg single-len reflex camera ) respective probability density function f1 f2 show figure 111 respect attribute price price value say $ 1000 f1 ( 1000 ) relative likelihood price consumer-line camera $ 1000 similarly f2 ( 1000 ) relative likelihood price professional-line camera $ 1000 probability density function f1 f2 observed directly instead allelectronic infer distribution analyze price digital camera sell moreover camera often come well-determine category ( eg “ consumer line ” “ professional line ” ) instead category typically base user background knowledge vary example camera prosumer segment may regard high end consumer line customer low end professional line other analyst allelectronic consider category probabilistic cluster conduct cluster analysis price camera approach category probability consumer line professional line price 1000 figure 111 probability density function two probabilistic cluster 
111 probabilistic model-based cluster 503 suppose want find k probabilistic cluster c1 ck cluster analysis datum set n object regard finite sample possible instance cluster conceptually assume form follow cluster cj ( 1 ≤ j ≤ k ) associate probability ωj instance sample cluster often assume ω1 ωk give part problem set p kj=1 ωj = 1 ensure object generate k cluster parameter ωj capture background knowledge relative population cluster cj run follow two step generate object d step execute n time total generate n object o1 choose cluster cj accord probability ω1 ωk choose instance cj accord probability density function fj datum generation process basic assumption mixture model formally mixture model assume set observed object mixture instance multiple probabilistic cluster conceptually observed object generate independently two step first choose probabilistic cluster accord probability cluster choose sample accord probability density function choose cluster give datum set k number cluster require task probabilistic model-based cluster analysis infer set k probabilistic cluster likely generate used datum generation process important question remain measure likelihood set k probabilistic cluster probability generate observed datum set consider set c k probabilistic cluster c1 ck probability density function f1 fk respectively probability ω1 ωk object probability generate cluster cj ( 1 ≤ j ≤ k ) give p ( o|cj ) = ωj fj ( ) therefore probability generate set c cluster p ( o|c ) = k x ωj fj ( ) ( 115 ) j=1 since object assume generate independently datum set = { o1 } n object p ( d|c ) = n i=1 p ( oi c ) = k n x ωj fj ( oi ) ( 116 ) i=1 j=1 clear task probabilistic model-based cluster analysis datum set find set c k probabilistic cluster p ( d|c ) maximize maximize p ( d|c ) often intractable general probability density function 
504 chapter 11 advanced cluster analysis cluster take arbitrarily complicate form make probabilistic model-based cluster computationally feasible often compromise assume probability density function parameterized distribution formally let o1 n observed object 21 2k parameter k distribution denote = { o1 } 2 = { 21 2k } respectively object oi ∈ ( 1 ≤ ≤ n ) eq ( 115 ) rewrite p ( oi 2 ) = k x ωj pj ( oi 2j ) ( 117 ) j=1 pj ( oi 2j ) probability oi generate jth distribution used parameter 2j consequently eq ( 116 ) rewrite p ( o|2 ) = n x k ωj pj ( oi 2j ) ( 118 ) i=1 j=1 used parameterized probability distribution model task probabilistic model-based cluster analysis infer set parameter 2 maximize eq ( 118 ) example 116 univariate gaussian mixture model let ’ use univariate gaussian distribution example assume probability density function cluster follow 1-d gaussian distribution suppose k cluster two parameter probability density function cluster center µj standard deviation σj ( 1 ≤ j ≤ k ) denote parameter 2j = ( µj σj ) 2 = { 21 2k } let datum set = { o1 } oi ( 1 ≤ ≤ n ) real number point oi ∈ 1 e p ( oi 2j ) = √ 2π σj − ( oi −µj ) 2 2σ 2 ( 119 ) assume cluster probability ω1 = ω2 = · · · = ωk = k1 plug eq ( 119 ) eq ( 117 ) k 2 ( oi −µj ) 1x 1 − p ( oi 2 ) = e 2σ 2 √ k 2π σj ( 1110 ) j=1 apply eq ( 118 ) n p ( o|2 ) = k 2 ( oi −µj ) 1 yx 1 − e 2σ 2 √ k 2π σj ( 1111 ) i=1 j=1 task probabilistic model-based cluster analysis used univariate gaussian mixture model infer 2 eq ( 1111 ) maximize 
111 probabilistic model-based cluster 505 1113 expectation-maximization algorithm “ compute fuzzy clustering probabilistic model-based clustering ” section introduce principled approach let ’ start review k-mean cluster problem k-mean algorithm study chapter 10 easily show k-mean cluster special case fuzzy cluster ( exercise 111 ) k-mean algorithm iterate cluster improve iteration consist two step expectation step ( e-step ) give current cluster center object assign cluster center closest object object expect belong closest cluster maximization step ( m-step ) give cluster assignment cluster algorithm adjust center sum distance object assign cluster new center minimize similarity object assign cluster maximize generalize two-step method tackle fuzzy cluster probabilistic model-based cluster general expectation-maximization ( em ) algorithm framework approach maximum likelihood maximum posteriori estimate parameter statistical model context fuzzy probabilistic model-based cluster em algorithm start initial set parameter iterate cluster improve cluster converge change sufficiently small ( less preset threshold ) iteration also consist two step expectation step assign object cluster accord current fuzzy cluster parameter probabilistic cluster maximization step find new cluster parameter maximize sse fuzzy cluster ( eq 114 ) expect likelihood probabilistic model-based cluster example 117 fuzzy cluster used em algorithm consider six point figure 112 coordinate point also show let ’ compute two fuzzy cluster used em algorithm randomly select two point say c1 = c2 = b initial center two cluster first iteration conduct expectation step maximization step follow e-step point calculate membership degree cluster point assign c1 c2 membership weight 1 dist ( c1 ) 2 1 1 + 2 dist ( c1 ) dist ( c2 ) 2 = dist ( c2 ) 2 dist ( c1 ) 2 dist ( c1 ) 2 + dist ( c2 ) 2 dist ( c1 ) 2 + dist ( c2 ) 2 
506 chapter 11 advanced cluster analysis e ( 18 11 ) b ( 4 10 ) ( 14 8 ) c ( 9 6 ) f ( 21 7 ) ( 3 3 ) x figure 112 datum set fuzzy cluster table 113 intermediate result first three iteration example 117 ’ em algorithm iteration 1 2 3 e-step ` 1 0 = 0 1 ` 073 mt = 027 ` 080 mt = 020 048 052 042 058 m-step 041 059 # 047 053 049 051 091 009 026 074 033 067 076 024 099 001 002 098 014 086 c1 = ( 847 512 ) c2 = ( 1042 899 ) # 042 058 # 023 077 c1 = ( 851 611 ) c2 = ( 1442 869 ) c1 = ( 640 624 ) c2 = ( 1655 864 ) respectively dist ( ) euclidean distance rationale close c1 dist ( c1 ) small membership degree respect c1 high also normalize membership degree sum degree object equal 1 point wa c1 = 1 wa c2 = exclusively belong c1 41 = 048 point b wb c1 = 0 wb c2 = point c wc c1 = 45+41 45 wc c2 = 45+41 = degree membership point show partition matrix table 113 m-step recalculate centroid accord partition matrix minimize sse give eq ( 114 ) new centroid adjust x 2 wo c j point cj = ( 1112 ) x 2 wo c j point j = 1 2 
111 probabilistic model-based cluster 507 example 12 × 3 + 02 × 4 + 0482 × 9 + 0422 × 14 + 0412 × 18 + 0472 × 21 12 + 02 + 0482 + 0422 + 0412 + 0472  12 × 3 + 02 × 10 + 0482 × 6 + 0422 × 8 + 0412 × 11 + 0472 × 7 12 + 02 + 0482 + 0422 + 0412 + 0472  c1 = = ( 847 512 )  c2 = 02 × 3 + 12 × 4 + 0522 × 9 + 0582 × 14 + 0592 × 18 + 0532 × 21 02 + 12 + 0522 + 0582 + 0592 + 0532  02 × 3 + 12 × 10 + 0522 × 6 + 0582 × 8 + 0592 × 11 + 0532 × 7 02 + 12 + 0522 + 0582 + 0592 + 0532 = ( 1042 899 ) repeat iteration iteration contain e-step m-step table 113 show result first three iteration algorithm stop cluster center converge change small enough “ apply em algorithm compute probabilistic model-based cluster ” let ’ use univariate gaussian mixture model ( example 116 ) illustrate example 118 used em algorithm mixture model give set object = { o1 } want mine set parameter 2 = { 21 2k } p ( o|2 ) eq ( 1111 ) maximize 2j = ( µj σj ) mean standard deviation respectively jth univariate gaussian distribution ( 1 ≤ j ≤ k ) apply em algorithm assign random value parameter 2 initial value iteratively conduct e-step m-step follow parameter converge change sufficiently small e-step object oi ∈ ( 1 ≤ ≤ n ) calculate probability oi belong distribution p ( oi 2j ) p ( 2j oi 2 ) = pk l=1 p ( oi 2l ) ( 1113 ) m-step adjust parameter 2 expect likelihood p ( o|2 ) eq ( 1111 ) maximize achieve set pn n p ( 2j oi 2 ) 1x 1 i=1 oi p ( 2j oi 2 ) µj = oi pn = pn k k l=1 p ( 2j ol 2 ) i=1 p ( 2j oi 2 ) i=1 ( 1114 ) 
508 chapter 11 advanced cluster analysis σj = sp n 2 i=1 p ( 2j oi 2 ) ( oi − uj ) pn i=1 p ( 2j oi 2 ) ( 1115 ) many application probabilistic model-based cluster show effective general partition method fuzzy cluster method distinct advantage appropriate statistical model used capture latent cluster em algorithm commonly used handle many learn problem datum mining statistic due simplicity note general em algorithm may converge optimal solution may instead converge local maximum many heuristic explore avoid example can run em process multiple time used different random initial value furthermore em algorithm costly number distribution large datum set contain observed datum point 112 cluster high-dimensional datum cluster method study far work well dimensionality high less 10 attribute however important application high dimensionality “ conduct cluster analysis high-dimensional datum ” section study approach cluster high-dimensional datum section 1121 start overview major challenge approach used method high-dimensional datum cluster divide two category subspace cluster method ( section 1122 ) dimensionality reduction method ( section 1123 ) 1121 cluster high-dimensional datum problem challenge major methodology present specific method cluster high-dimensional datum let ’ first demonstrate need cluster analysis high-dimensional datum used example examine challenge call new method categorize major method accord whether search cluster subspace original space whether create new lower-dimensionality space search cluster application datum object may describe 10 attribute object refer high-dimensional datum space example 119 high-dimensional datum cluster allelectronic keep track product purchase every customer customer-relationship manager want cluster customer group accord purchase allelectronic 
112 cluster high-dimensional datum 509 table 114 customer purchase datum customer p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 ada bob cathy 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 customer purchase datum high dimensionality allelectronic carry ten thousand product therefore customer ’ purchase profile vector product carry company ten thousand dimension “ traditional distance measure frequently used low-dimensional cluster analysis also effective high-dimensional datum ” consider customer table 114 10 product p1 p10 used demonstration customer purchase product 1 set corresponding bit otherwise 0 appear let ’ calculate euclidean distance ( eq 216 ) among ada bob cathy easy see dist ( ada bob ) = dist ( bob cathy ) = dist ( ada cathy ) = √ 2 accord euclidean distance three customer equivalently similar ( dissimilar ) however close look tell us ada similar cathy bob ada cathy share one common purchase item p1 show example 119 traditional distance measure ineffective high-dimensional datum distance measure may dominate noise many dimension therefore cluster full high-dimensional space unreliable find cluster may meaningful “ kind cluster meaningful high-dimensional datum ” cluster analysis high-dimensional datum still want group similar object together however datum space often big messy additional challenge need find cluster cluster set attribute manifest cluster word cluster high-dimensional datum often defined used small set attribute instead full datum space essentially cluster high-dimensional datum return group object cluster ( conventional cluster analysis ) addition cluster set attribute characterize cluster example table 114 characterize similarity ada cathy p1 may return attribute ada cathy purchase p1 cluster high-dimensional datum search cluster space exist thus two major kind method subspace cluster approach search cluster exist subspace give high-dimensional datum space subspace defined used subset attribute full space subspace cluster approach discuss section 1122 
510 chapter 11 advanced cluster analysis dimensionality reduction approach try construct much lower-dimensional space search cluster space often method may construct new dimension combine dimension original datum dimensionality reduction method topic section 1124 general cluster high-dimensional datum raise several new challenge addition conventional cluster major issue create appropriate model cluster high-dimensional datum unlike conventional cluster low-dimensional space cluster hide high-dimensional datum often significantly smaller example cluster customer-purchase datum would expect many user similar purchase pattern search small meaningful cluster like find needle haystack show conventional distance measure ineffective instead often consider various sophisticated technique model correlation consistency among object subspace typically exponential number possible subspace dimensionality reduction option thus optimal solution often computationally prohibitive example original datum space 1000 dimension want 1000 find cluster dimensionality 10 = 263 × 1023 possible 10 subspace 1122 subspace cluster method “ find subspace cluster high-dimensional datum ” many method propose generally categorize three major group subspace search method correlation-based cluster method bicluster method subspace search method subspace search method search various subspace cluster cluster subset object similar subspace similarity often capture conventional measure distance density example clique algorithm introduce section 1052 subspace cluster method enumerate subspace cluster subspace dimensionality-increas order apply antimonotonicity prune subspace cluster may exist major challenge subspace search method face search series subspace effectively efficiently generally two kind strategy bottom-up approach start low-dimensional subspace search higherdimensional subspace may cluster higher-dimensional 
112 cluster high-dimensional datum 511 subspace various prune technique explore reduce number higherdimensional subspace need search clique example bottom-up approach top-down approach start full space search smaller smaller subspace recursively top-down approach effective locality assumption hold require subspace cluster determine local neighborhood example 1110 proclus top-down subspace approach proclus k-medoid-like method first generate k potential cluster center high-dimensional datum set used sample datum set refine subspace cluster iteratively iteration current k-medoid proclus consider local neighborhood medoid whole datum set identify subspace cluster minimize standard deviation distance point neighborhood medoid dimension subspace medoid determine point datum set assign closest medoid accord corresponding subspace cluster possible outlier identify next iteration new medoid replace exist one improve cluster quality correlation-based cluster method subspace search method search cluster similarity measure used conventional metric like distance density correlation-based approach discover cluster defined advanced correlation model example 1111 correlation-based approach used pca example pca-based approach first apply pca ( principal component analysis see chapter 3 ) derive set new uncorrelated dimension mine cluster new space subspace addition pca space transformation may used hough transform fractal dimension additional detail subspace search method correlation-based cluster method please refer bibliographic note ( section 117 ) bicluster method application want cluster object attribute simultaneously result cluster know bicluster meet four requirement ( 1 ) small set object participate cluster ( 2 ) cluster involve small number attribute ( 3 ) object may participate multiple cluster participate cluster ( 4 ) attribute may involved multiple cluster involved cluster section 1123 discuss bicluster detail 
512 chapter 11 advanced cluster analysis 1123 bicluster cluster analysis discuss far cluster object accord attribute value object attribute treat way however application object attribute defined symmetric way datum analysis involve search datum matrix submatrix show unique pattern cluster kind cluster technique belong category bicluster section first introduce two motivate application example biclustering— gene expression recommender system learn different type bicluster last present bicluster method application example bicluster technique first propose address need analyze gene expression datum gene unit passing-on trait live organism offspr typically gene reside segment dna gene critical live thing specify protein functional rna chain hold information build maintain live organism ’ cell pass genetic trait offspr synthesis functional gene product either rna protein rely process gene expression genotype genetic makeup cell organism individual phenotype observable characteristic organism gene expression fundamental level genetic genotype cause phenotype used dna chip ( also know dna microarray ) biological engineering technique measure expression level large number ( possibly ) organism ’ gene number different experimental condition condition may correspond different time point experiment sample different organ roughly speaking gene expression datum dna microarray datum conceptually condition matrix row correspond one gene column correspond one sample condition element matrix real number record expression level gene specific condition figure 113 show illustration cluster viewpoint interesting issue gene expression datum matrix analyze two dimensions—the gene dimension condition dimension analyze gene dimension treat gene object treat condition attribute mining gene dimension may find pattern share multiple gene cluster gene group example may find group gene express similarly highly interesting bioinformatic find pathway analyze condition dimension treat condition object treat gene attribute way may find pattern condition cluster condition group example may find difference gene expression compare group tumor sample nontumor sample 
112 cluster high-dimensional datum 513 condition gene w11 w12 w1m w21 w22 w2m w31 w32 w3m wn1 wn2 wnm figure 113 microarrary datum matrix example 1112 gene expression gene expression matrix popular bioinformatic research development example important task classify new gene used expression datum gene gene know class symmetrically may classify new sample ( eg new patient ) used expression datum sample sample know class ( eg tumor nontumor ) task invaluable understand mechanism disease clinical treatment see many gene expression datum mining problem highly related cluster analysis however challenge instead cluster one dimension ( eg gene condition ) many case need cluster two dimension simultaneously ( eg gene condition ) moreover unlike cluster model discuss far cluster gene expression datum matrix submatrix usually follow characteristic small set gene participate cluster cluster involve small subset condition gene may participate multiple cluster may participate cluster condition may involved multiple cluster may involved cluster find cluster condition matrix need new cluster technique meet follow requirement bicluster cluster gene defined used subset condition cluster condition defined used subset gene 
514 chapter 11 advanced cluster analysis cluster neither exclusive ( eg one gene participate multiple cluster ) exhaustive ( eg gene may participate cluster ) bicluster useful bioinformatic also application well consider recommender system example example 1113 used bicluster recommender system allelectronic collect datum customer ’ evaluation product used datum recommend product customer datum modeled customer-product matrix row represent customer column represent product element matrix represent customer ’ evaluation product may score ( eg like like somewhat like ) purchase behavior ( eg buy ) figure 114 illustrate structure customer-product matrix analyze two dimension customer dimension product dimension treat customer object product attribute allelectronic find customer group similar preference purchase pattern used product object customer attribute allelectronic mine product group similar customer interest moreover allelectronic mine cluster customer product simultaneously cluster contain subset customer involve subset product example allelectronic highly interested find group customer like group product cluster submatrix customer-product matrix element high value used cluster allelectronic make recommendation two direction first company recommend product new customer similar customer cluster second company recommend customer new product similar involved cluster bicluster gene expression datum matrix bicluster customerproduct matrix usually follow characteristic small set customer participate cluster cluster involve small subset product customer participate multiple cluster may participate cluster customer w11 w21 ··· wn1 product w12 · · · w22 · · · ··· ··· wn2 · · · figure 114 customer–product matrix w1m w2m ··· wnm 
112 cluster high-dimensional datum 515 product may involved multiple cluster may involved cluster bicluster apply customer-product matrix mine cluster satisfying requirement type bicluster “ model bicluster mine ” let ’ start basic notation sake simplicity use “ gene ” “ condition ” refer two dimension discussion discussion easily extend application example simply replace “ gene ” “ condition ” “ customer ” “ product ” tackle customer-product bicluster problem let = { a1 } set gene b = { b1 bm } set condition let e = [ eij ] gene expression datum matrix gene-condition matrix 1 ≤ ≤ n 1 ≤ j ≤ m submatrix × j defined subset ⊆ gene subset j ⊆ b condition example matrix show figure 115 { a1 a33 a86 } × { b6 b12 b36 b99 } submatrix bicluster submatrix gene condition follow consistent pattern define different type bicluster base pattern simplest case submatrix × j ( ⊆ j ⊆ b ) bicluster constant value ∈ j ∈ j eij = c c constant example submatrix { a1 a33 a86 } × { b6 b12 b36 b99 } figure 115 bicluster constant value bicluster interesting row constant value though different row may different value bicluster constant value row submatrix × j ∈ j ∈ j eij = c + αi αi adjustment row i example figure 116 show bicluster constant value row symmetrically bicluster constant value column submatrix × j ∈ j ∈ j eij = c + βj βj adjustment column j a1 ··· a33 ··· a86 ··· ··· ··· ··· ··· ··· ··· ··· b6 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b12 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b36 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b99 · · · 60 · · · ··· ··· 60 · · · ··· ··· 60 · · · ··· ··· figure 115 gene-condition matrix submatrix bicluster 
516 chapter 11 advanced cluster analysis generally bicluster interesting row change synchronize way respect column vice versa mathematically bicluster coherent value ( also know pattern-based cluster ) submatrix × j ∈ j ∈ j eij = c + αi + βj αi βj adjustment row column j respectively example figure 117 show bicluster coherent value show × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 moreover instead used addition define bicluster coherent value used multiplication eij = c · αi · βj clearly bicluster constant value row column special case bicluster coherent value application may interested - down-regulate change across gene condition without constrain exact value bicluster coherent evolution row submatrix × j i1 i2 ∈ j1 j2 ∈ j ( ei1 j1 − ei1 j2 ) ( ei2 j1 − ei2 j2 ) ≥ example figure 118 show bicluster coherent evolution row symmetrically define bicluster coherent evolution column next study mine bicluster 10 20 50 0 10 20 50 0 10 20 50 0 10 20 50 0 10 20 50 0 figure 116 bicluster constant value row 10 20 50 0 50 60 90 40 30 40 70 20 70 80 110 60 20 30 60 10 figure 117 bicluster coherent value 10 20 50 0 50 100 100 80 30 50 90 20 70 1000 120 100 20 30 80 10 figure 118 bicluster coherent evolution row 
112 cluster high-dimensional datum 517 bicluster method previous specification type bicluster consider ideal case real datum set perfect bicluster rarely exist exist usually small instead random noise affect reading eij thus prevent bicluster nature appear perfect shape two major type method discover bicluster datum may come noise optimization-based method conduct iterative search iteration submatrix highest significance score identify bicluster process terminate user-specified condition meet due cost concern computation greedy search often employ find local optimal bicluster enumeration method use tolerance threshold specify degree noise allow bicluster mine try enumerate submatrix bicluster satisfy requirement use δ-cluster maple algorithms example illustrate idea optimization used δ-cluster algorithm submatrix × j mean ith row 1 x eij = eij | ( 1116 ) j∈j symmetrically mean jth column 1 x eij = eij | ( 1117 ) i∈i mean element submatrix 1 x 1 x 1 x eij = eij = eij eij = | | | i∈i j∈j i∈i ( 1118 ) j∈j quality submatrix bicluster measure mean-squared residue value 1 x h ( × j ) = ( eij − eij − eij + eij ) 2 ( 1119 ) | i∈i j∈j submatrix × j δ-bicluster h ( × j ) ≤ δ δ ≥ 0 threshold δ = 0 × j perfect bicluster coherent value set δ > 0 user specify tolerance average noise per element perfect bicluster eq ( 1119 ) residue element residue ( eij ) = eij − eij − eij + eij ( 1120 ) maximal δ-bicluster δ-bicluster × j exist another δ-bicluster 0 × j 0 ⊆ 0 j ⊆ j 0 least one inequality hold find 
518 chapter 11 advanced cluster analysis maximal δ-bicluster largest size computationally costly therefore use heuristic greedy search method obtain local optimal cluster algorithm work two phase deletion phase start whole matrix mean-squared residue matrix δ iteratively remove row column iteration row compute mean-squared residue 1 x ( ) = ( eij − eij − eij + eij ) 2 ( 1121 ) | j∈j moreover column j compute mean-squared residue 1 x ( eij − eij − eij + eij ) 2 ( j ) = | ( 1122 ) i∈i remove row column largest mean-squared residue end phase obtain submatrix × j δ-bicluster however submatrix may maximal addition phase iteratively expand δ-bicluster × j obtain deletion phase long δ-bicluster requirement maintain iteration consider row column involved current bicluster × j calculate mean-squared residue row column smallest mean-squared residue add current δ-bicluster greedy algorithm find one δ-bicluster find multiple bicluster heavy overlap run algorithm multiple time execution δ-bicluster output replace element output bicluster random number although greedy algorithm may find neither optimal bicluster bicluster fast even large matrix enumerate bicluster used maple mentioned submatrix × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 2 × 2 submatrix × j define p-score ei1 j1 ei1 j2 p-score = | ( ei1 j1 − ei2 j1 ) − ( ei1 j2 − ei2 j2 ) | ( 1123 ) ei2 j1 ei2 j2 submatrix × j δ-pcluster ( pattern-based cluster ) p-score every 2 × 2 submatrix × j δ δ ≥ 0 threshold specify user ’ tolerance noise perfect bicluster p-score control noise every element bicluster mean-squared residue capture average noise interesting property δ-pcluster × j δ-pcluster every x × ( x ≥ 2 ) submatrix × j also δ-pcluster monotonicity enable 
112 cluster high-dimensional datum 519 us obtain succinct representation nonredundant δ-pcluster δ-pcluster maximal row column add cluster maintain δ-pcluster property avoid redundancy instead find δ-pcluster need compute maximal δ-pcluster maple algorithm enumerate maximal δ-pcluster systematically enumerate every combination condition used set enumeration tree depthfirst search enumeration framework pattern-growth method frequent pattern mining ( chapter 6 ) consider gene expression datum condition combination j maple find maximal subset gene × j δ-pcluster × j submatrix another δ-pcluster × j maximal δ-pcluster may huge number condition combination maple prune many unfruitful combination used monotonicity δ-pcluster condition combination j exist set gene × j δ-pcluster need consider superset j moreover consider × j candidate δ-pcluster every ( | − 1 ) subset j 0 j × j 0 δ-pcluster maple also employ several prune technique speed search retain completeness return maximal δ-pcluster example examine current δ-pcluster × j maple collect gene condition may add expand cluster candidate gene condition together j form submatrix δ-pcluster already find search × j superset j prune interested reader may refer bibliographic note additional information maple algorithm ( section 117 ) interesting observation search maximal δ-pcluster maple somewhat similar mining frequent close itemset consequently maple borrow depth-first search framework idea prune technique pattern-growth method frequent pattern mining example frequent pattern mining cluster analysis may share similar technique idea advantage maple algorithms enumerate bicluster guarantee completeness result miss overlapping bicluster however challenge enumeration algorithms may become time consume matrix become large customer-purchase matrix hundred thousand customer million product 1124 dimensionality reduction method spectral cluster subspace cluster method try find cluster subspace original datum space situation effective construct new space instead used subspace original datum motivation behind dimensionality reduction method cluster high-dimensional datum example 1114 cluster derive space consider three cluster point figure possible cluster point subspace original space x × 
520 chapter 11 advanced cluster analysis − 0707x + 0707y x figure 119 cluster derive space may effective three cluster would end project onto overlapping area x √ √ 2 2 axe instead construct new dimension − 2 x + 2 ( show dash line figure ) project point onto new dimension three cluster become apparent although example 1114 involve two dimension idea construct new space ( cluster structure hide datum become well manifest ) extend high-dimensional datum preferably newly construct space low dimensionality many dimensionality reduction method straightforward approach apply feature selection extraction method datum set discuss chapter however method may able detect cluster structure therefore method combine feature extraction cluster prefer section introduce spectral cluster group method effective highdimensional datum application figure 1110 show general framework spectral cluster approach ng-jordan-weiss algorithm spectral cluster method let ’ look step framework also note special condition apply ng-jordan-weiss algorithm example give set object o1 distance pair object dist ( oi oj ) ( 1 ≤ j ≤ n ) desire number k cluster spectral cluster approach work follow used distance measure calculate affinity matrix w wij = e − dist ( oi oj ) σ2 σ scaling parameter control fast affinity wij decrease dist ( oi oj ) increase ng-jordan-weiss algorithm wii set 0 
112 cluster high-dimensional datum datum affinity matrix [ wij ] compute lead k eigenvector cluster new space 521 project back cluster original datum av = λv = f ( w ) figure 1110 framework spectral cluster approach source adapt slide 8 http micued08 azran used affinity matrix w derive matrix = f ( w ) way do vary ng-jordan-weiss algorithm define matrix diagonal matrix dii sum ith row w dii = n x wij ( 1124 ) j=1 set 1 1 = d− 2 wd− 2 ( 1125 ) find k lead eigenvector a recall eigenvector square matrix nonzero vector remain proportional original vector multiply matrix mathematically vector v eigenvector matrix av = λv λ call corresponding eigenvalue step derive k new dimension base affinity matrix w typically k much smaller dimensionality original datum ng-jordan-weiss algorithm compute k eigenvector largest eigenvalue x1 xk used k lead eigenvector project original datum new space defined k lead eigenvector run cluster algorithm k-mean find k cluster ng-jordan-weiss algorithm stack k largest eigenvector column form matrix x = [ x1 x2 · · · xk ] ∈ rn×k algorithm form matrix renormalize row x unit length xij yij = qp k 2 j=1 xij ( 1126 ) algorithm treat row point k-dimensional space rk run k-mean ( algorithm serve partition purpose ) cluster point k cluster 
522 chapter 11 advanced cluster analysis v = [ v1 v2 v3 ] w 05 u = [ u1 u2 u3 ] 0 −05 0 10 20 30 40 50 60 1 05 0 −05 −1 0 10 20 30 40 50 60 1 05 05 0 0 0 04 02 0 −02 0 10 10 20 20 30 30 40 40 50 50 60 60 0 1 05 0 −05 0 10 10 20 20 30 30 40 40 50 50 60 60 figure 1111 new dimension cluster result ng-jordan-weiss algorithm source adapt slide 9 http micued08 azran assign original datum point cluster accord transform point assign cluster obtain step 4 ng-jordan-weiss algorithm original object oi assign jth cluster matrix ’ row assign jth cluster result step 4 spectral cluster method dimensionality new space set desire number cluster set expect new dimension able manifest cluster example 1115 ng-jordan-weiss algorithm consider set point figure datum set affinity matrix three largest eigenvector normalize vector show note three new dimension ( form three largest eigenvector ) cluster easily detected spectral cluster effective high-dimensional application image process theoretically work well certain condition apply scalability however challenge compute eigenvector large matrix costly spectral cluster combine cluster method bicluster additional information dimensionality reduction cluster method kernel pca find bibliographic note ( section 117 ) 113 cluster graph network datum cluster analysis graph network datum extract valuable knowledge information datum increasingly popular many application discuss application challenge cluster graph network datum section similarity measure form cluster give section learn graph cluster method section 1133 general term graph network used interchangeably rest section mainly use term graph 
113 cluster graph network datum 523 1131 application challenge customer relationship manager allelectronic notice lot datum relate customer purchase behavior preferably modeled used graph example 1116 bipartite graph customer purchase behavior allelectronic represent bipartite graph bipartite graph vertex divide two disjoint set edge connect vertex one set vertex set allelectronic customer purchase datum one set vertex represent customer one customer per vertex set represent product one product per vertex edge connect customer product represent purchase product customer figure 1112 show illustration “ kind knowledge obtain cluster analysis customer-product bipartite graph ” cluster customer customer buy similar set product place one group customer relationship manager make product recommendation example suppose ada belong customer cluster customer purchase digital camera last 12 month ada yet purchase one manager decide recommend digital camera alternatively cluster product product purchase similar set customer group together cluster information also used product recommendation example digital camera high-speed flash memory card belong product cluster customer purchase digital camera recommend high-speed flash memory card bipartite graph widely used many application consider another example example 1117 web search engine web search engine search log archive record user query corresponding click-through information ( click-through information tell us page give result search user click ) query click-through information represent used bipartite graph two set customer product figure 1112 bipartite graph represent customer-purchase datum 
524 chapter 11 advanced cluster analysis vertex correspond query web page respectively edge link query web page user click web page ask query valuable information obtain cluster analysis query–web page bipartite graph instance may identify query pose different language mean thing click-through information query similar another example web page web form direct graph also know web graph web page vertex hyperlink edge point source page destination page cluster analysis web graph disclose community find hub authoritative web page detect web spam addition bipartite graph cluster analysis also apply type graph include general graph elaborate example 1118 example 1118 social network social network social structure represent graph vertex individual organization link interdependency vertex represent friendship common interest collaborative activity allelectronic ’ customer form social network customer vertex edge link two customer know customer relationship manager interested find useful information derive allelectronic ’ social network cluster analysis obtain cluster network customer cluster know friend common customer within cluster may influence one another regard purchase decision make moreover communication channel design inform “ head ” cluster ( ie “ best ” connect person cluster ) promotional information spread quickly thus may use customer cluster promote sale allelectronic another example author scientific publication form social network author vertex two author connect edge coauthor publication network general weight graph edge two author carry weight represent strength collaboration many publication two author ( end vertex ) coauthor cluster coauthor network provide insight community author pattern collaboration “ challenge specific cluster analysis graph network datum ” cluster method discuss far object represent used set attribute unique feature graph network datum object ( vertex ) relationship ( edge ) give dimension attribute explicitly defined conduct cluster analysis graph network datum two major new challenge “ measure similarity two object graph accordingly ” typically use conventional distance measure euclidean distance instead need develop new measure quantify similarity 
113 cluster graph network datum 525 measure often metric thus raise new challenge regard development efficient cluster method similarity measure graph discuss section 1132 “ design cluster model method effective graph network datum ” graph network datum often complicate carry topological structure sophisticated traditional cluster analysis application many graph datum set large web graph contain least ten billion web page publicly indexable web graph also sparse average vertex connect small number vertex graph discover accurate useful knowledge hide deep datum good cluster method accommodate factor cluster method graph network datum introduce section 1133 1132 similarity measure “ measure similarity distance two vertex graph ” discussion examine two type measure geodesic distance distance base random walk geodesic distance simple measure distance two vertex graph shortest path vertex formally geodesic distance two vertex length term number edge shortest path vertex two vertex connect graph geodesic distance defined infinite used geodesic distance define several useful measurement graph analysis cluster give graph g = ( v e ) v set vertex e set edge define follow vertext v ∈ v eccentricity v denote eccen ( v ) largest geodesic distance v vertex u ∈ v − { v } eccentricity v capture far away v remotest vertex graph radius graph g minimum eccentricity vertex r = min eccen ( v ) v∈v ( 1127 ) radius capture distance “ central point ” “ farthest border ” graph diameter graph g maximum eccentricity vertex = max eccen ( v ) v∈v diameter represent largest distance pair vertex peripheral vertex vertex achieve diameter ( 1128 ) 
526 chapter 11 advanced cluster analysis b c e figure 1113 graph g vertex c e peripheral example 1119 measurement base geodesic distance consider graph g figure eccentricity 2 eccen ( ) = 2 eccen ( b ) = 2 eccen ( c ) = eccen ( ) = eccen ( e ) = thus radius g 2 diameter note necessary = 2 × r vertex c e peripheral vertex simrank similarity base random walk structural context application geodesic distance may inappropriate measure similarity vertex graph introduce simrank similarity measure base random walk structural context graph mathematics random walk trajectory consist take successive random step example 1120 similarity person social network let ’ consider measure similarity two vertex allelectronic customer social network example 1118 similarity explain closeness two participant network close two person term relationship represent social network “ well geodesic distance measure similarity closeness network ” suppose ada bob two customer network network undirected geodesic distance ( ie length shortest path ada bob ) shortest path message pass ada bob vice versa however information useful allelectronic ’ customer relationship management company typically want send specific message one customer another therefore geodesic distance suit application “ similarity mean social network ” consider two way define similarity two customer consider similar one another similar neighbor social network heuristic intuitive practice two person receive recommendation good number common friend often make similar decision kind similarity base local structure ( ie neighborhood ) vertex thus call structural context–based similarity 
113 cluster graph network datum 527 suppose allelectronic send promotional information ada bob social network ada bob may randomly forward information friend ( neighbor ) network closeness ada bob measure likelihood customer simultaneously receive promotional information originally send ada bob kind similarity base random walk reachability network thus refer similarity base random walk let ’ closer look meant similarity base structural context similarity base random walk intuition behind similarity base structural context two vertex graph similar connect similar vertex measure similarity need define notion individual neighborhood direct graph g = ( v e ) v set vertex e ⊆ v × v set edge vertex v ∈ v individual in-neighborhood v defined ( 1129 ) ( v ) = { | ( u v ) ∈ e } symmetrically define individual out-neighborhood v ( 1130 ) ( v ) = { | ( v w ) ∈ e } follow intuition illustrated example 1120 define simrank structural-context similarity value 0 1 pair vertex vertex v ∈ v similarity vertex ( v v ) = 1 neighborhood identical vertex u v ∈ v u = v define x x c ( u v ) = ( x ) ( 1131 ) i ( u ) i ( v ) | x∈i ( u ) y∈i ( v ) c constant 0 vertex may in-neighbor thus define eq ( 1131 ) 0 either ( u ) ( v ) ∅ parameter c specify rate decay similarity propagate across edge “ compute simrank ” straightforward method iteratively evaluate eq ( 1131 ) fix point reach let si ( u v ) simrank score calculate ith round begin set ( 0 u = v s0 ( u v ) = ( 1132 ) 1 u = v use eq ( 1131 ) compute si+1 si si+1 ( u v ) = x c i ( u ) i ( v ) | x x∈i ( u ) y∈i ( v ) si ( x ) ( 1133 ) 
528 chapter 11 advanced cluster analysis show lim si ( u v ) = ( u v ) additional method approximate i→∞ simrank give bibliographic note ( section 117 ) let ’ consider similarity base random walk direct graph strongly connect two node u v path u v another path v u strongly connect graph g = ( v e ) two vertex u v ∈ v define expect distance u v ( u v ) = x u p [ ] l ( ) ( 1134 ) v u v path start u end v may contain cycle reach v end travele tour = w1 → w2 → · · · → wk length l ( ) = k − probability tour defined ( q k−1 1 i=1 o ( wi ) | l ( ) > 0 ( 1135 ) p [ ] = 0 l ( ) = 0 measure probability vertex w receive message originated simultaneously u v extend expect distance notion expect meeting distance x ( u v ) = p [ ] l ( ) ( 1136 ) ( x x ) ( u v ) ( u v ) ( x x ) pair tour u x v x length used constant c 0 1 define expect meeting probability p ( u v ) = x ( u v ) p [ ] c l ( ) ( 1137 ) ( x x ) similarity measure base random walk parameter c specify probability continue walk step trajectory show ( u v ) = p ( u v ) two vertex u v simrank base structural context random walk 1133 graph cluster method let ’ consider conduct cluster graph first describe intuition behind graph cluster discuss two general category graph cluster method find cluster graph imagine cut graph piece piece cluster vertex within cluster well connect vertex different cluster connect much weaker way formally graph g = ( v e ) 
113 cluster graph network datum 529 cut c = ( ) partition set vertex v g v = ∪ ∩ = ∅ cut set cut set edge { ( u v ) ∈ e|u ∈ v ∈ } size cut number edge cut set weight graph size cut sum weight edge cut set “ kind cut good derive cluster graph ” graph theory network application minimum cut importance cut minimum cut ’ size greater cut ’ size polynomial time algorithms compute minimum cut graph use algorithms graph cluster example 1121 cut cluster consider graph g figure graph two cluster { b c e f } { g h j k } one outlier vertex l consider cut c1 = ( { b c e f g h j k } { l } ) one edge namely ( e l ) cross two partition create c1 therefore cut set c1 { ( e l ) } size c1 1 ( note size cut connect graph smaller 1 ) minimum cut c1 lead good cluster separate outlier vertex l rest graph cut c2 = ( { b c e f l } { g h j k } ) lead much better cluster c1 edge cut set c2 connect two “ natural cluster ” graph specifically edge ( h ) ( e k ) cut set edge connect h e k belong one cluster example 1121 indicate used minimum cut unlikely lead good cluster better choose cut vertex u involved edge cut set edge connect u belong one cluster formally let deg ( u ) degree u number edge connect u sparsity cut c = ( ) defined = cut size min { | | } ( 1138 ) sparsest cut c2 b c g f h e k minimum cut c1 l figure 1114 graph g two cut j 
530 chapter 11 advanced cluster analysis cut sparsest sparsity greater sparsity cut may one sparsest cut example 1121 figure 1114 c2 sparsest cut used sparsity objective function sparsest cut try minimize number edge cross partition balance partition size consider cluster graph g = ( v e ) partition graph k cluster modularity cluster assess quality cluster defined = k x i=1   di 2 li − | | ( 1139 ) li number edge vertex ith cluster di sum degree vertex ith cluster modularity cluster graph difference fraction edge fall individual cluster fraction would graph vertex randomly connect optimal cluster graph maximize modularity theoretically many graph cluster problem regard find good cut sparsest cut graph practice however number challenge exist high computational cost many graph cut problem computationally expensive sparsest cut problem example np-hard therefore find optimal solution large graph often impossible good trade-off scalability quality achieve sophisticated graph graph sophisticated one describe involve weight or cycle high dimensionality graph many vertex similarity matrix vertex represent vector ( row matrix ) dimensionality number vertex graph therefore graph cluster method must handle high dimensionality sparsity large graph often sparse meaning vertex average connect small number vertex similarity matrix large sparse graph also sparse two kind method cluster graph datum address challenge one used cluster method high-dimensional datum design specifically cluster graph first group method base generic cluster method highdimensional datum extract similarity matrix graph used similarity measure discuss section generic cluster method apply similarity matrix discover cluster cluster method 
113 cluster graph network datum 531 high-dimensional datum typically employ example many scenario similarity matrix obtain spectral cluster method ( section 1124 ) apply spectral cluster approximate optimal graph cut solution additional information please refer bibliographic note ( section 117 ) second group method specific graph search graph find well-connected component cluster let ’ look method call scan ( structural cluster algorithm network ) example give undirected graph g = ( v e ) vertex u ∈ v neighborhood u 0 ( u ) = { | ( u v ) ∈ e } ∪ { u } used idea structural-context similarity scan measure similarity two vertex u v ∈ v normalize common neighborhood size 0 ( u ) ∩ 0 ( v ) | σ ( u v ) = √ 0 ( u ) 0 ( v ) | ( 1140 ) larger value compute similar two vertex scan used similarity threshold ε define cluster membership vertex u ∈ v ε-neighborhood u defined nε ( u ) = { v ∈ 0 ( u ) σ ( u v ) ≥ ε } ε-neighborhood u contain neighbor u structural-context similarity u least ε scan core vertex vertex inside cluster u ∈ v core vertex nε ( u ) | ≥ µ µ popularity threshold scan grow cluster core vertex vertex v ε-neighborhood core u v assign cluster u process grow cluster continue cluster grow process similar density-based cluster method dbscan ( chapter 10 ) formally vertex v directly reach core u v ∈ nε ( u ) transitively vertex v reach core u exist vertex w1 wn w1 reach u wi reach wi−1 1 < ≤ n v reach wn moreover two vertex u v ∈ v may may core say connect exist core w u v reach w vertex cluster connect cluster maximum set vertex every pair set connect vertex may belong cluster vertex u hub neighborhood 0 ( u ) u contain vertex one cluster vertex belong cluster hub outlier scan algorithm show figure search framework closely resemble cluster-find process dbscan scan find cut graph cluster set vertex connect base transitive similarity structural context advantage scan time complexity linear respect number edge large sparse graph number edge scale number vertex therefore scan expect good scalability cluster large graph 
532 chapter 11 advanced cluster analysis algorithm scan cluster graph datum input graph g = ( v e ) similarity threshold ε population threshold µ output set cluster method set vertex v unlabeled unlabeled vertex u u core generate new cluster-id c insert v ∈ nε ( u ) queue q q = w ← first vertex q r ← set vertex directly reach w ∈ r unlabeled labele nonmember assign current cluster-id c endif unlabeled insert queue q endif endfor remove w q end else label u nonmember endif endfor vertex u labele nonmember ∃x ∈ 0 ( u ) x different cluster-id label u hub else label u outlier endif endfor figure 1115 scan algorithm cluster analysis graph datum 114 cluster constraint user often background knowledge want integrate cluster analysis may also application-specific requirement information modeled cluster constraint approach topic cluster constraint two step section 1141 categorize type constraint cluster graph datum method cluster constraint introduce section 1142 
114 cluster constraint 533 1141 categorization constraint section study categorize constraint used cluster analysis specifically categorize constraint accord subject set strongly constraint enforce discuss chapter 10 cluster analysis involve three essential aspect object instance cluster cluster group object similarity among object therefore first method discuss categorize constraint accord apply thus three type constraint instance constraint cluster constraint similarity measurement constraint instance constraint instance specify pair set instance group cluster analysis two common type constraint category include must-link constraint must-link constraint specify two object x x group one cluster output cluster analysis must-link constraint transitive must-link ( x ) must-link ( z ) must-link ( x z ) link constraint link constraint opposite must-link constraint link constraint specify two object x output cluster analysis x belong different cluster link constraint entail link ( x ) must-link ( x x 0 ) must-link ( 0 ) link ( x 0 0 ) constraint instance defined used specific instance alternatively also defined used instance variable attribute instance example constraint constraint ( x ) must-link ( x ) dist ( x ) ≤  used distance object specify must-link constraint constraint cluster constraint cluster specify requirement cluster possibly used attribute cluster example constraint may specify minimum number object cluster maximum diameter cluster shape cluster ( eg convex ) number cluster specify partition cluster method regard constraint cluster constraint similarity measurement often similarity measure euclidean distance used measure similarity object cluster analysis application exception apply constraint similarity measurement specify requirement similarity calculation must respect example cluster person move object plaza euclidean distance used give 
534 chapter 11 advanced cluster analysis walking distance two point constraint similarity measurement trajectory implement shortest distance cross wall one way express constraint depend category example specify constraint cluster constraint1 diameter cluster larger requirement also expressed used constraint instance constraint10 link ( x ) dist ( x ) > ( 1141 ) example 1122 constraint instance cluster similarity measurement allelectronic cluster customer group customer assign customer relationship manager suppose want specify customer address place group would allow comprehensive service family expressed used must-link constraint instance constraintfamily ( x ) must-link ( x ) xaddress = yaddress allelectronic eight customer relationship manager ensure similar workload place constraint cluster eight cluster cluster least 10 % customer 15 % customer calculate spatial distance two customer used drive distance two however two customer live different country use flight distance instead constraint similarity measurement another way categorize cluster constraint consider firmly constraint respect constraint hard cluster violate constraint unacceptable constraint soft cluster violate constraint preferable acceptable better solution find soft constraint also call preference example 1123 hard soft constraint allelectronic constraintfamily example 1122 hard constraint splitting family different cluster can prevent company provide comprehensive service family lead poor customer satisfaction constraint number cluster ( correspond number customer relationship manager company ) also hard example 1122 also constraint balance size cluster satisfying constraint strongly prefer company flexible willing assign senior capable customer relationship manager oversee larger cluster therefore constraint soft ideally specific datum set set constraint clustering satisfy constraint however possible may cluster datum set 
114 cluster constraint 535 satisfy constraint trivially two constraint set conflict cluster satisfy time example 1124 conflict constraint consider constraint must-link ( x ) dist ( x ) < 5 link ( x ) dist ( x ) > 3 datum set two object x dist ( x ) = 4 cluster satisfy constraint simultaneously consider two constraint must-link ( x ) dist ( x ) < 5 must-link ( x ) dist ( x ) < 3 second constraint redundant give first moreover datum set distance two object least 5 every possible cluster object satisfy constraint “ measure quality usefulness set constraint ” general consider either informativeness coherence informativeness amount information carry constraint beyond cluster model give datum set cluster method set constraint c informativeness c respect measure fraction constraint c unsatisfied cluster compute d higher informativeness specific requirement background knowledge constraint carry coherence set constraint degree agreement among constraint measure redundancy among constraint 1142 method cluster constraint although categorize cluster constraint application may different constraint specific form consequently various technique need handle specific constraint section discuss general principle handle hard soft constraint handle hard constraint general strategy handle hard constraint strictly respect constraint cluster assignment process illustrate idea use partition cluster example 
536 chapter 11 advanced cluster analysis give datum set set constraint instance ( ie must-link link constraint ) extend k-mean method satisfy constraint cop-k-mean algorithm work follow generate superinstance must-link constraint compute transitive closure must-link constraint must-link constraint treat equivalence relation closure give one multiple subset object object subset must assign one cluster represent subset replace object subset mean superinstance also carry weight number object represent step must-link constraint always satisfied conduct modify k-mean cluster recall k-mean object assign closest center nearest-center assignment violate link constraint respect link constraint modify center assignment process k-mean nearest feasible center assignment object assign center sequence step make sure assignment far violate link constraint object assign nearest center assignment respect link constraint cop-k-mean ensure constraint violate every step require backtracking greedy algorithm generate cluster satisfy constraint provide conflict exist among constraint handle soft constraint cluster soft constraint optimization problem cluster violate soft constraint penalty impose cluster therefore optimization goal cluster contain two part optimize cluster quality minimize constraint violation penalty overall objective function combination cluster quality score penalty score illustrate use partition cluster example give datum set set soft constraint instance cvqe ( constrain vector quantization error ) algorithm conduct k-mean cluster enforce constraint violation penalty objective function used cvqe sum distance used k-mean adjust constraint violation penalty calculate follow penalty must-link violation must-link constraint object x assign two different center c1 c2 respectively constraint violate result dist ( c1 c2 ) distance c1 c2 add objective function penalty penalty link violation link constraint object x assign common center c constraint violate 
114 cluster constraint 537 distance dist ( c c 0 ) c c 0 add objective function penalty speeding constrain cluster constraint similarity measurement lead heavy cost cluster consider follow cluster obstacle problem cluster person move object plaza euclidean distance used measure walking distance two point however constraint similarity measurement trajectory implement shortest distance cross wall ( section 1141 ) obstacle may occur object distance two object may derive geometric computation ( eg involve triangulation ) computational cost high large number object obstacle involved cluster obstacle problem represent used graphical notation first point p visible another point q region r straight line join p q intersect obstacle visibility graph graph vg = ( v e ) vertex obstacle corresponding node v two node v1 v2 v joined edge e corresponding vertex represent visible let vg 0 = ( v 0 e 0 ) visibility graph create vg add two additional point p q v 0 e 0 contain edge join two point v 0 two point mutually visible shortest path two point p q subpath vg 0 show figure 1116 ( ) see begin edge p either v1 v2 v3 go path vg end edge either v4 v5 q reduce cost distance computation two pair object point several preprocess optimization technique used one method group point close together microcluster do first triangulating region r triangle grouping nearby point triangle microcluster used method similar birch dbscan show figure 1116 ( b ) process microcluster rather individual point overall computation reduce precomputation perform build two v4 v1 p v2 o1 v3 o2 vg q v5 vg ( ) ( b ) figure 1116 cluster obstacle object ( o1 o2 ) ( ) visibility graph ( b ) triangulation region microcluster source adapt tung hou han [ thh01 ] 
538 chapter 11 advanced cluster analysis kind join index base computation shortest path ( 1 ) vv index pair obstacle vertex ( 2 ) mv index pair microcluster obstacle vertex use index help optimize overall performance used precomputation optimization strategy distance two point ( granularity level microcluster ) compute efficiently thus cluster process perform manner similar typical efficient k-medoid algorithm claran achieve good cluster quality large datum set 115 summary conventional cluster analysis object assign one cluster exclusively however application need assign object one cluster fuzzy probabilistic way fuzzy cluster probabilistic model-based cluster allow object belong one cluster partition matrix record membership degree object belong cluster probabilistic model-based cluster assume cluster parameterized distribution used datum cluster observed sample estimate parameter cluster mixture model assume set observed object mixture instance multiple probabilistic cluster conceptually observed object generate independently first choose probabilistic cluster accord probability cluster choose sample accord probability density function choose cluster expectation-maximization algorithm framework approach maximum likelihood maximum posteriori estimate parameter statistical model expectation-maximization algorithms used compute fuzzy cluster probabilistic model-based cluster high-dimensional datum pose several challenge cluster analysis include model high-dimensional cluster search cluster two major category cluster method high-dimensional datum subspace cluster method dimensionality reduction method subspace cluster method search cluster subspace original space example include subspace search method correlation-based cluster method bicluster method dimensionality reduction method create new space lower dimensionality search cluster bicluster method cluster object attribute simultaneously type bicluster include bicluster constant value constant value column coherent value coherent evolution column two major type bicluster method optimization-based method enumeration method 
116 exercise 539 spectral cluster dimensionality reduction method general idea construct new dimension used affinity matrix cluster graph network datum many application social network analysis challenge include measure similarity object graph design cluster model method graph network datum geodesic distance number edge two vertex graph used measure similarity alternatively similarity graph social network measure used structural context random walk simrank similarity measure base structural context random walk graph cluster modeled compute graph cut sparsest cut may lead good cluster modularity used measure cluster quality scan graph cluster algorithm search graph identify well-connected component cluster constraint used express application-specific requirement background knowledge cluster analysis constraint cluster categorize constraint instance cluster similarity measurement constraint instance include must-link link constraint constraint hard soft hard constraint cluster enforce strictly respect constraint cluster assignment process cluster soft constraint consider optimization problem heuristic used speed constrain cluster 116 exercise 111 traditional cluster method rigid require object belong exclusively one cluster explain special case fuzzy cluster may use k-mean example 112 allelectronic carry 1000 product p1 p1000 consider customer ada bob cathy ada bob purchase three product common p1 p2 p3 997 product ada bob independently purchase seven randomly cathy purchase 10 product randomly select 1000 product euclidean distance probability dist ( ada bob ) > dist ( ada cathy ) jaccard similarity ( chapter 2 ) used learn example 113 show × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 114 compare maple algorithm ( section 1123 ) frequent close itemset mining algorithm closet ( pei han mao [ phm00 ] ) major similarity difference 
540 chapter 11 advanced cluster analysis 115 simrank similarity measure cluster graph network datum ( ) prove lim si ( u v ) = ( u v ) simrank computation i→∞ ( b ) show ( u v ) = p ( u v ) simrank 116 large sparse graph average node low degree similarity matrix used simrank still sparse sense deliberate answer 117 compare scan algorithm ( section 1133 ) dbscan ( section 1041 ) similarity difference 118 consider partition cluster follow constraint cluster number object cluster must nk ( 1 − δ ) nk ( 1 + δ ) n total number object datum set k number cluster desire δ [ 0 1 ) parameter extend k-mean method handle constraint discuss situation constraint hard soft 117 bibliographic note höppner klawonn kruse runkler [ hkkr99 ] provide thorough discussion fuzzy cluster fuzzy c-mean algorithm ( example 117 base ) propose bezdek [ bez81 ] fraley raftery [ fr02 ] give comprehensive overview model-based cluster analysis probabilistic model mclachlan basford [ mb88 ] present systematic introduction mixture model application cluster analysis dempster laird rubin [ dlr77 ] recognize first introduce em algorithm give name however idea em algorithm “ propose many time special circumstance ” admit dempster laird rubin [ dlr77 ] wu [ wu83 ] give correct analysis em algorithm mixture model em algorithms used extensively many datum mining application introduction model-based cluster mixture model em algorithms find recent textbook machine learn statistical learning—for example bishop [ bis06 ] marsland [ mar09 ] alpaydin [ alp11 ] increase dimensionality severe effect distance function indicated beyer et al [ bgrs99 ] also dramatic impact various technique classification cluster semisupervised learn ( radovanović nanopoulos ivanović [ rni09 ] ) kriegel kröger zimek [ kkz09 ] present comprehensive survey method cluster high-dimensional datum clique algorithm develop agrawal gehrke gunopulos raghavan [ aggr98 ] proclus algorithm propose aggawal procopiuc wolf et al [ + 99 ] technique bicluster initially propose hartigan [ har72 ] term bicluster coin mirkin [ mir98 ] cheng church [ cc00 ] introduce 
117 bibliographic note 541 bicluster gene expression datum analysis many study bicluster model method notion δ-pcluster introduce wang wang yang yu [ wwyy02 ] informative survey see madeira oliveira [ mo04 ] tanay sharan shamir [ tss04 ] chapter introduce δ-cluster algorithm cheng church [ cc00 ] maple pei zhang cho et al [ + 03 ] example optimization-based method enumeration method bicluster respectively donath hoffman [ dh73 ] fiedler [ fie73 ] pioneer spectral cluster chapter use algorithm propose ng jordan weis [ njw01 ] example thorough tutorial spectral cluster see luxburg [ lux07 ] cluster graph network datum important fast-growing topic schaeffer [ sch07 ] provide survey simrank measure similarity develop jeh widom [ jw02a ] xu et al [ xyfs07 ] propose scan algorithm arora rao vazirani [ arv09 ] discuss sparsest cut approximation algorithms cluster constraint extensively study davidson wagstaff basu [ dwb06 ] propose measure informativeness coherence copk-mean algorithm give wagstaff et al [ wcrs01 ] cvqe algorithm propose davidson ravi [ dr05 ] tung han lakshmanan ng [ thln01 ] present framework constraint-based cluster base user-specified constraint efficient method constraint-based spatial cluster existence physical obstacle constraint propose tung hou han [ thh01 ] 
13 datum mining trend research frontier young research field datum mining make significant progress cover broad spectrum application since 1980s today datum mining used vast array area numerous commercial datum mining system service available many challenge however still remain final chapter introduce mining complex datum type prelude in-depth study reader may choose addition focus trend research frontier datum mining section 131 present overview methodology mining complex datum type extend concept task introduce book mining include mining time-series sequential pattern biological sequence graph network spatiotemporal datum include geospatial datum moving-object datum cyber-physical system datum multimedium datum text datum web datum datum stream section 132 briefly introduce approach datum mining include statistical method theoretical foundation visual audio datum mining section 133 learn datum mining application business science include financial retail telecommunication industry science engineering recommender system social impact datum mining discuss section 134 include ubiquitous invisible datum mining privacy-preserve datum mining finally section 135 speculate current expect datum mining trend arise response new challenge field 131 mining complex datum type section outline major development research effort mining complex datum type complex datum type summarize figure section 1311 cover mining sequence datum time-series symbolic sequence biological sequence section 1312 discuss mining graph social information network section 1313 address mining kind datum include spatial datum spatiotemporal datum moving-object datum cyber-physical system datum multimedium datum text datum datum mining concept technique doi b978-0-12-381479-100013-7 c 2012 elsevier right re-serve 585 
586 chapter 13 datum mining trend research frontier c p l e x p e f sequence datum graph network mining kind datum time-series datum ( eg stock market datum ) symbolic sequence ( eg customer shopping sequence web click stream ) biological sequence ( eg dna protein sequence ) homogeneous ( link type ) heterogeneous ( link different type ) example graph social information network etc spatial datum spatiotemporal datum cyber-physical system datum multimedium datum text datum web datum datum stream figure 131 complex datum type mining web datum datum stream due broad scope theme section present high-level overview topic discuss in-depth book 1311 mining sequence datum time-series symbolic sequence biological sequence sequence order list event sequence may categorize three group base characteristic event describe ( 1 ) time-series datum ( 2 ) symbolic sequence datum ( 3 ) biological sequence let ’ consider type time-series datum sequence datum consist long sequence numeric datum record equal time interval ( eg per minute per hour per day ) time-series datum generate many natural economic process stock market scientific medical natural observation symbolic sequence datum consist long sequence event nominal datum typically observed equal time interval many sequence gap ( ie lapse record event ) matter much example include customer shopping sequence web click stream well sequence event science engineering natural social development biological sequence include dna protein sequence sequence typically long carry important complicate hide semantic meaning gap usually important let ’ look datum mining sequence datum type 
131 mining complex datum type 587 similarity search time-series datum time-series datum set consist sequence numeric value obtain repeat measurement time value typically measure equal time interval ( eg every minute hour day ) time-series databasis popular many application stock market analysis economic sale forecasting budgetary analysis utility study inventory study yield projection workload projection process quality control also useful study natural phenomena ( eg atmosphere temperature wind earthquake ) scientific engineering experiment medical treatment unlike normal database query find datum match give query exactly similarity search find datum sequence differ slightly give query sequence many time-series similarity query require subsequence match find set sequence contain subsequence similar give query sequence similarity search often necessary first perform datum dimensionality reduction transformation time-series datum typical dimensionality reduction technique include ( 1 ) discrete fourier transform ( dft ) ( 2 ) discrete wavelet transform ( dwt ) ( 3 ) singular value decomposition ( svd ) base principle component analysis ( pca ) touch concept chapter 3 thorough explanation beyond scope book go great detail technique datum signal map signal transform space small subset “ strongest ” transform coefficient save feature feature form feature space projection transform space index construct original transform time-series datum speed search query-based similarity search technique include normalization transformation atomic match ( ie find pair gap-free window small length similar ) window stitching ( ie stitching similar window form pair large similar subsequence allow gap atomic match ) subsequence order ( ie linearly order subsequence match determine whether enough similar piece exist ) numerous software package exist similarity search time-series datum recently researcher propose transform time-series datum piecewise aggregate approximation datum view sequence symbolic representation problem similarity search transform one match subsequence symbolic sequence datum identify motif ( ie frequently occur sequential pattern ) build index hashing mechanism efficient search base motif experiment show approach fast simple comparable search quality dft dwt dimensionality reduction method regression trend analysis time-series datum regression analysis time-series datum study substantially field statistic signal analysis however one may often need go beyond pure regression 
chapter 13 datum mining trend research frontier price 588 allelectronic stock 10-day move average time figure 132 time-series datum stock price allelectronic time trend show dash curve calculate move average analysis perform trend analysis many practical application trend analysis build integrate model used follow four major component movement characterize time-series datum trend long-term movement indicate general direction time-series graph move time example used weight move average least square method find trend curf dash curve indicated figure 132 cyclic movement long-term oscillation trend line curve seasonal variation nearly identical pattern time series appear follow corresponding season successive year holiday shopping season effective trend analysis datum often need “ deseasonalize ” base seasonal index compute autocorrelation random movement characterize sporadic change due chance event labor dispute announce personnel change within company trend analysis also used time-series forecasting find mathematical function approximately generate historic pattern time series used make long-term short-term prediction future value arima ( auto-regressive integrate move average ) long-memory time-series modele autoregression popular method analysis sequential pattern mining symbolic sequence symbolic sequence consist order set element event record without concrete notion time many application involve datum 
131 mining complex datum type 589 symbolic sequence customer shopping sequence web click stream program execution sequence biological sequence sequence event science engineering natural social development biological sequence carry complicate semantic meaning pose many challenge research issue investigation conduct field bioinformatic sequential pattern mining focuse extensively mining symbolic sequence sequential pattern frequent subsequence exist single sequence set sequence sequence α = ha1 a2 · · · subsequence another sequence β = hb1 b2 · · · bm exist integer 1 ≤ j1 < j2 < · · · < jn ≤ a1 ⊆ bj1 a2 ⊆ bj2 ⊆ bjn example α = h { ab } di β = h { abc } { } { de } ai b c e item α subsequence mining sequential pattern consist mining set subsequence frequent one sequence set sequence many scalable algorithms develop result extensive study area alternatively mine set close sequential pattern sequential pattern close exist sequential pattern 0 proper subsequence 0 0 ( frequency ) support s similar frequent pattern mining counterpart also study efficient mining multidimensional multilevel sequential pattern constraint-based frequent pattern mining user-specified constraint used reduce search space sequential pattern mining derive pattern interest user refer constraint-based sequential pattern mining moreover may relax constraint enforce additional constraint problem sequential pattern mining derive different kind pattern sequence datum example enforce gap constraint pattern derive contain consecutive subsequence subsequence small gap alternatively may derive periodic sequential pattern fold event proper-size window find recur subsequence window another approach derive partial order pattern relax requirement strict sequential order mining subsequence pattern besides mining partial order pattern sequential pattern mining methodology also extend mining tree lattice episode order pattern sequence classification classification method perform model construction base feature vector however sequence explicit feature even sophisticated feature selection technique dimensionality potential feature still high sequential nature feature difficult capture make sequence classification challenge task sequence classification method organized three category ( 1 ) featurebased classification transform sequence feature vector apply conventional classification method ( 2 ) sequence distance–based classification distance function measure similarity sequence determine 
590 chapter 13 datum mining trend research frontier quality classification significantly ( 3 ) model-based classification used hide markov model ( hmm ) statistical model classify sequence time-series numeric-valu datum feature selection technique symbolic sequence easily apply time-series datum without discretization however discretization cause information loss recently propose time-series shapelet method used time-series subsequence maximally represent class feature achieve quality classification result alignment biological sequence biological sequence generally refer sequence nucleotide amino acid biological sequence analysis compare align index analyze biological sequence thus play crucial role bioinformatic modern biology sequence alignment base fact live organism related evolution imply nucleotide ( dna rna ) protein sequence species closer evolution exhibit similarity alignment process line sequence achieve maximal identity level also express degree similarity sequence two sequence homologous share common ancestor degree similarity obtain sequence alignment useful determine possibility homology two sequence alignment also help determine relative position multiple species evolution tree call phylogenetic tree problem alignment biological sequence describe follow give two input biological sequence identify similar sequence long conserve subsequence number sequence align exactly two problem know pairwise sequence alignment otherwise multiple sequence alignment sequence compare align either nucleotide ( rna ) amino acid ( protein ) nucleotide two symbol align identical however amino acid two symbol align identical one derive substitution likely occur nature two kind alignment local alignment global alignment former mean portion sequence align whereas latter require alignment entire length sequence either nucleotide amino acid insertion deletion substitution occur nature different probability substitution matrix used represent probability substitution nucleotide amino acid probability insertion deletion usually use gap character − indicate position preferable align two symbol evaluate quality alignment score mechanism typically defined usually count identical similar symbol positive score gap negative one algebraic sum score take alignment measure goal alignment achieve maximal score among possible alignment however expensive ( exactly np-hard problem ) find optimal alignment therefore various heuristic method develop find suboptimal alignment 
131 mining complex datum type 591 dynamic programming approach commonly used sequence alignment among many available analysis package blast ( basic local alignment search tool ) one popular tool biosequence analysis hide markov model biological sequence analysis give biological sequence biologist would like analyze sequence represent represent structure statistical regularity sequence class biologist construct various probabilistic model markov chain hide markov model model probability state depend previous state therefore particularly useful analysis biological sequence datum common method construct hide markov model forward algorithm viterbi algorithm baum-welch algorithm give sequence symbol x forward algorithm find probability obtain x model viterbi algorithm find probable path ( corresponding x ) model whereas baum-welch algorithm learn adjust model parameter best explain set training sequence 1312 mining graph network graph represent general class structure set sequence lattice tree broad range graph application web social network information network biological network bioinformatic chemical informatic computer vision multimedium text retrieval hence graph network mining become increasingly important heavily research overview follow major theme ( 1 ) graph pattern mining ( 2 ) statistical modele network ( 3 ) datum clean integration validation network analysis ( 4 ) cluster classification graph homogeneous network ( 5 ) cluster ranking classification heterogeneous network ( 6 ) role discovery link prediction information network ( 7 ) similarity search olap information network ( 8 ) evolution information network graph pattern mining graph pattern mining mining frequent subgraph ( also call ( sub ) graph pattern ) one set graph method mining graph pattern categorize apriori-based pattern growth–base approach alternatively mine set close graph graph g close exist proper supergraph g 0 carry support count g moreover many variant graph pattern include approximate frequent graph coherent graph dense graph user-specified constraint push deep graph pattern mining process improve mining efficiency graph pattern mining many interesting application example used generate compact effective graph index structure base concept 
592 chapter 13 datum mining trend research frontier frequent discriminative graph pattern approximate structure similarity search achieve explore graph index structure multiple graph feature moreover classification graph also perform effectively used frequent discriminative subgraph feature statistical modele network network consist set node corresponding object associate set property set edge ( link ) connect node represent relationship object network homogeneous node link type friend network coauthor network web page network network heterogeneous node link different type publication network ( link together author conference paper content ) health-care network ( link together doctor nurse patient disease treatment ) researcher propose multiple statistical model modele homogeneous network well-known generative model random graph model ( ie erdös-rényi model ) watts-strogatz model scale-free model scalefree model assume network follow power law distribution ( also know pareto distribution heavy-tailed distribution ) large-scale social network small-world phenomenon observed network characterize high degree local cluster small fraction node ( ie node interconnect one another ) degree separation remain node social network exhibit certain evolutionary characteristic tend follow densification power law state network become increasingly dense time shrink diameter another characteristic effective diameter often decrease network grow node out-degree in-degree typically follow heavytailed distribution datum clean integration validation information network analysis real-world datum often incomplete noisy uncertain unreliable information redundancy may exist among multiple piece datum interconnect large network information redundancy explore network perform quality datum clean datum integration information validation trustability analysis network analysis example distinguish author share name examine networked connection heterogeneous object coauthor publication venue term addition identify inaccurate author information present bookseller explore network build base author information provide multiple bookseller sophisticated information network analysis method develop direction many case portion datum serve “ training ” relatively clean reliable datum consensus datum multiple information 
131 mining complex datum type 593 provider used help consolidate remain unreliable portion datum reduce costly effort labele datum hand training massive dynamic real-world datum set cluster classification graph homogeneous network large graph network cohesive structure often hide among massive interconnect node link cluster analysis method develop large network uncover network structure discover hide community hub outlier base network topological structure associate property various kind network cluster method develop categorize either partition hierarchical density-based algorithms moreover give human-labele training datum discovery network structure guide human-specify heuristic constraint supervised classification semi-supervised classification network recent hot topic datum mining research community cluster ranking classification heterogeneous network heterogeneous network contain interconnect node link different type interconnect structure contain rich information used mutually enhance node link propagate knowledge one type another cluster ranking heterogeneous network perform hand-inhand context highly rank link cluster may contribute lower-rank counterpart evaluation cohesiveness cluster cluster may help consolidate high ranking link dedicate cluster mutual enhancement ranking cluster prompt development algorithm call rankclus moreover user may specify different ranking rule present labele link certain datum type knowledge one type propagate type propagation reach link type via heterogeneous-type connection algorithms develop supervised learn semi-supervised learn heterogeneous network role discovery link prediction information network exist many hide role relationship among different link heterogeneous network example include advisor–advisee leader–follower relationship research publication network discover hide role relationship expert specify constraint base background knowledge enforce constraint may help crosscheck validation large interconnect network information redundancy network often used help weed link follow constraint 
594 chapter 13 datum mining trend research frontier similarly link prediction perform base assessment ranking expect relationship among candidate link example may predict paper author may write read cite base author ’ recent publication history trend research similar topic study often require analyze proximity network link trend connection similar neighbor roughly speaking person refer link prediction link mining however link mining cover additional task include link-based object classification object type prediction link type prediction link existence prediction link cardinality estimation object reconciliation ( predict whether two object fact ) also include group detection ( cluster object ) well subgraph identification ( find characteristic subgraph within network ) metadata mining ( uncover schema-type information regard unstructured datum ) similarity search olap information network similarity search primitive operation database web search engine heterogeneous information network consist multityped interconnect object example include bibliographic network social medium network two object consider similar link similar way multityped object general object similarity within network determine base network structure object property similarity measure moreover network cluster hierarchical network structure help organize object network identify subcommunity well facilitate similarity search furthermore similarity defined differently per user consider different linkage path derive various similarity semantic network know path-based similarity organize network base notion similarity cluster generate multiple hierarchy within network online analytical process ( olap ) perform example drill dice information network base different level abstraction different angle view olap operation may generate multiple interrelate network relationship among network may disclose interesting hide semantic evolution social information network network dynamic constantly evolve detect evolve community evolve regularity anomaly homogeneous heterogeneous network help person better understand structural evolution network predict trend irregularity evolve network homogeneous network evolve community discover subnetwork consist object type set friend coauthor however heterogeneous network community discover subnetwork consist object different type connect set paper author venue term also derive set evolve object type like evolve author theme 
131 mining complex datum type 595 1313 mining kind datum addition sequence graph many kind semi-structure unstructured datum spatiotemporal multimedium hypertext datum interesting application datum carry various kind semantic either store dynamically stream system call specialize datum mining methodology thus mining multiple kind datum include spatial datum spatiotemporal datum cyber-physical system datum multimedium datum text datum web datum datum stream increasingly important task datum mining subsection overview methodology mining kind datum mining spatial datum spatial datum mining discover pattern knowledge spatial datum spatial datum many case refer geospace-related datum store geospatial datum repository datum “ vector ” “ raster ” format form imagery geo-reference multimedium recently large geographic datum warehouse construct integrate thematic geographically reference datum multiple source construct spatial datum cube contain spatial dimension measure support spatial olap multidimensional spatial datum analysis spatial datum mining perform spatial datum warehouse spatial databasis geospatial datum repository popular topic geographic knowledge discovery spatial datum mining include mining spatial association co-location pattern spatial cluster spatial classification spatial modele spatial trend outlier analysis mining spatiotemporal datum move object spatiotemporal datum datum relate space time spatiotemporal datum mining refer process discover pattern knowledge spatiotemporal datum typical example spatiotemporal datum mining include discover evolutionary history city land uncover weather pattern predict earthquake hurricane determine global warm trend spatiotemporal datum mining become increasingly important far-reaching implication give popularity mobile phone gps device internet-based map service weather service digital earth well satellite rfid sensor wireless video technology among many kind spatiotemporal datum moving-object datum ( ie datum move object ) especially important example animal scientist attach telemetry equipment wildlife analyze ecological behavior mobility manager emb gps car better monitor guide vehicle meteorologist use weather satellite radar observe hurricane massive-scale moving-object datum become rich complex ubiquitous example moving-object datum mining include mining movement pattern multiple move object ( ie discovery relationship among multiple move object move cluster leader follower merge convoy swarm pincer well collective movement pattern ) example 
596 chapter 13 datum mining trend research frontier moving-object datum mining include mining periodic pattern one set move object mining trajectory pattern cluster model outlier mining cyber-physical system datum cyber-physical system ( cp ) typically consist large number interact physical information component cp system may interconnect form large heterogeneous cyber-physical network example cyber-physical network include patient care system link patient monitoring system network medical information emergency handle system transportation system link transportation monitoring network consist many sensor video camera traffic information control system battlefield commander system link reconnaissance network battlefield information analysis system clearly cyber-physical system network ubiquitous form critical component modern information infrastructure datum generate cyber-physical system dynamic volatile noisy inconsistent interdependent contain rich spatiotemporal information critically important real-time decision make comparison typical spatiotemporal datum mining mining cyber-physical datum require link current situation large information base perform real-time calculation return prompt response research area include rare-event detection anomaly analysis cyber-physical datum stream reliability trustworthiness cyber-physical datum analysis effective spatiotemporal datum analysis cyber-physical network integration stream datum mining real-time automate control process mining multimedium datum multimedium datum mining discovery interesting pattern multimedium databasis store manage large collection multimedium object include image datum video datum audio datum well sequence datum hypertext datum contain text text markup linkage multimedium datum mining interdisciplinary field integrate image process understand computer vision datum mining pattern recognition issue multimedium datum mining include content-based retrieval similarity search generalization multidimensional analysis multimedium datum cube contain additional dimension measure multimedium information topic multimedium mining include classification prediction analysis mining association video audio datum mining ( section 1323 ) mining text datum text mining interdisciplinary field draw information retrieval datum mining machine learn statistic computational linguistic substantial portion information store text news article technical paper book digital library email message blog web page hence research text mining active important goal derive high-quality information text 
131 mining complex datum type 597 typically do discovery pattern trend mean statistical pattern learn topic modele statistical language modele text mining usually require structuring input text ( eg parse along addition derive linguistic feature removal other subsequent insertion database ) follow derive pattern within structure datum evaluation interpretation output “ high quality ” text mining usually refer combination relevance novelty interestingness typical text mining task include text categorization text cluster entity extraction production granular taxonomy sentiment analysis document summarization entity-relation modele ( ie learn relation name entity ) example include multilingual datum mining multidimensional text analysis contextual text mining trust evolution analysis text datum well text mining application security biomedical literature analysis online medium analysis analytical customer relationship management various kind text mining analysis software tool available academic institution open-source forum industry text mining often also used wordnet sematic web wikipedia information source enhance understand mining text datum mining web datum world wide web serve huge widely distribute global information center news advertisement consumer information financial management education government e-commerce contain rich dynamic collection information web page content hypertext structure multimedium hyperlink information access usage information provide fertile source datum mining web mining application datum mining technique discover pattern structure knowledge web accord analysis target web mining organized three main area web content mining web structure mining web usage mining web content mining analyze web content text multimedium datum structure datum ( within web page link across web page ) do understand content web page provide scalable informative keyword-based page indexing concept resolution web page relevance ranking web page content summary valuable information related web search analysis web page reside either surface web deep web surface web portion web index typical search engine deep web ( hide web ) refer web content part surface web content provide underlie database engine web content mining study extensively researcher search engine web service company web content mining build link across multiple web page individual therefore potential inappropriately disclose personal information study privacy-preserve datum mining address concern development technique protect personal privacy web web structure mining process used graph network mining theory method analyze node connection structure web extract pattern hyperlink hyperlink structural component connect 
598 chapter 13 datum mining trend research frontier web page another location also mine document structure within page ( eg analyze treelike structure page structure describe html xml tag usage ) kind web structure mining help us understand web content may also help transform web content relatively structure datum set web usage mining process extract useful information ( eg user click stream ) server log find pattern related general particular group user understand user ’ search pattern trend association predict user look internet help improve search efficiency effectiveness well promote product related information different group user right time web search company routinely conduct web usage mining improve quality service mining datum stream stream datum refer datum flow system vast volume change dynamically possibly infinite contain multidimensional feature datum store traditional database system moreover system may able read stream sequential order pose great challenge effective mining stream datum substantial research lead progress development efficient method mining datum stream area mining frequent sequential pattern multidimensional analysis ( eg construction stream cube ) classification cluster outlier analysis online detection rare event datum stream general philosophy develop single-scan a-few-scan algorithms used limit compute storage capability include collect information stream datum slide window tilt time window ( recent datum register finest granularity distant datum register coarser granularity ) explore technique like microcluster limit aggregation approximation many application stream datum mining explored—for example real-time detection anomaly computer network traffic botnet text stream video stream power-grid flow web search sensor network cyber-physical system 132 methodology datum mining due broad scope datum mining large variety datum mining methodology methodology datum mining thoroughly cover book section briefly discuss several interesting methodology fully address previous chapter methodology list figure 133 1321 statistical datum mining datum mining technique describe book primarily draw computer science discipline include datum mining machine learn datum warehousing algorithms design efficient handle huge amount datum 
132 methodology datum mining h e r n n g e h l g e statistical datum mining foundation datum mining visual audio datum mining 599 regression generalized linear model analysis variance mixed-effect model factor analysis discriminant analysis survival analysis datum reduction datum compression probability statistical theory microeconomic view pattern discovery inductive database datum visualization datum mining result visualization datum mining process visualization interactive visual datum mining audio datum mining figure 133 datum mining methodology typically multidimensional possibly various complex type however many well-established statistical technique datum analysis particularly numeric datum technique apply extensively scientific datum ( eg datum experiment physics engineering manufacturing psychology medicine ) well datum economic social science technique principal component analysis ( chapter 3 ) cluster ( chapter 10 11 ) already address book thorough discussion major statistical method datum analysis beyond scope book however several method mentioned sake completeness pointer technique provide bibliographic note ( section 138 ) regression general method used predict value response ( dependent ) variable one predictor ( independent ) variable variable numeric various form regression linear multiple weight polynomial nonparametric robust ( robust method useful error fail satisfy normalcy condition datum contain significant outlier ) generalized linear model model generalization ( generalized additive model ) allow categorical ( nominal ) response variable ( transformation 
600 chapter 13 datum mining trend research frontier ) related set predictor variable manner similar modele numeric response variable used linear regression generalized linear model include logistic regression poisson regression analysis variance technique analyze experimental datum two population describe numeric response variable one categorical variable ( factor ) general anova ( single-factor analysis variance ) problem involve comparison k population treatment mean determine least two mean different complex anova problem also exist mixed-effect model model analyze group data—data classify accord one grouping variable typically describe relationship response variable covariate datum group accord one factor common area application include multilevel datum repeat measure datum block design longitudinal datum factor analysis method used determine variable combine generate give factor example many psychiatric datum possible measure certain factor interest directly ( eg intelligence ) however often possible measure quantity ( eg student test score ) reflect factor interest none variable designate dependent discriminant analysis technique used predict categorical response variable unlike generalized linear model assume independent variable follow multivariate normal distribution procedure attempt determine several discriminant function ( linear combination independent variable ) discriminate among group defined response variable discriminant analysis commonly used social science survival analysis several well-established statistical technique exist survival analysis technique originally design predict probability patient undergo medical treatment would survive least time t method survival analysis however also commonly apply manufacturing setting estimate life span industrial equipment popular method include kaplanmeier estimate survival cox proportional hazard regression model extension quality control various statistic used prepare chart quality control shewhart chart cusum chart ( display group summary statistic ) statistic include mean standard deviation range count move average move standard deviation move range 1322 view datum mining foundation research theoretical foundation datum mining yet mature solid systematic theoretical foundation important help provide coherent 
132 methodology datum mining 601 framework development evaluation practice datum mining technology several theory basis datum mining include follow datum reduction theory basis datum mining reduce datum representation datum reduction trade accuracy speed response need obtain quick approximate answer query large databasis datum reduction technique include singular value decomposition ( drive element behind principal component analysis ) wavelet regression log-linear model histogram cluster sampling construction index tree datum compression accord theory basis datum mining compress give datum encode term bit association rule decision tree cluster encode base minimum description length principle state “ best ” theory infer datum set one minimize length theory datum encode used theory predictor datum encode typically bit probability statistical theory accord theory basis datum mining discover joint probability distribution random variable example bayesian belief network hierarchical bayesian model microeconomic view microeconomic view consider datum mining task find pattern interesting extent used decision-make process enterprise ( eg regard marketing strategy production plan ) view one utility pattern consider interesting act enterprise regard face optimization problem object maximize utility value decision theory datum mining become nonlinear optimization problem pattern discovery inductive databasis theory basis datum mining discover pattern occur datum association classification model sequential pattern area machine learn neural network association mining sequential pattern mining cluster several subfield contribute theory knowledge base view database consist datum pattern user interact system query datum theory ( ie pattern ) knowledge base knowledge base actually inductive database theory mutually exclusive example pattern discovery also see form datum reduction datum compression ideally theoretical framework able model typical datum mining task ( eg association classification cluster ) probabilistic nature able handle different form datum consider iterative interactive essence datum mining effort require establish well-defined framework datum mining satisfy requirement 
602 chapter 13 datum mining trend research frontier 1323 visual audio datum mining visual datum mining discover implicit useful knowledge large datum set used datum or knowledge visualization technique human visual system controlled eye brain latter thought powerful highly parallel process reasoning engine contain large knowledge base visual datum mining essentially combine power component make highly attractive effective tool comprehension datum distribution pattern cluster outlier datum visual datum mining view integration two discipline datum visualization datum mining also closely related computer graphic multimedium system human–computer interaction pattern recognition high-performance compute general datum visualization datum mining integrate follow way datum visualization datum database datum warehouse view different granularity abstraction level different combination attribute dimension datum present various visual form boxplot 3-d cube datum distribution chart curf surface link graph show datum visualization section chapter figure 134 135 statsoft show figure 134 boxplot show multiple variable combination statsoft source wwwstatsoftcom 
132 methodology datum mining 603 figure 135 multidimensional datum distribution analysis statsoft source wwwstatsoftcom datum distribution multidimensional space visual display help give user clear impression overview datum characteristic large datum set datum mining result visualization visualization datum mining result presentation result knowledge obtain datum mining visual form form may include scatter plot boxplot ( chapter 2 ) well decision tree association rule cluster outlier generalized rule example scatter plot show figure 136 sas enterprise miner figure 137 mineset used plane associate set pillar describe set association rule mine database figure 138 also mineset present decision tree figure 139 ibm intelligent miner present set cluster property associate datum mining process visualization type visualization present various process datum mining visual form user see datum extract database datum warehouse extract well select datum clean integrate preprocessed mine moreover may also show method select datum mining result store may view figure 1310 show visual presentation datum mining process clementine datum mining system 
604 chapter 13 datum mining trend research frontier figure 136 visualization datum mining result sas enterprise miner interactive visual datum mining ( interactive ) visual datum mining visualization tool used datum mining process help user make smart datum mining decision example datum distribution set attribute display used colored sector ( whole space represent circle ) display help user determine sector first select classification good split point sector may example show figure 1311 output perception-based classification ( pbc ) system develop university munich audio datum mining used audio signal indicate pattern datum feature datum mining result although visual datum mining may disclose interesting pattern used graphical display require user concentrate watch pattern identify interesting novel feature within sometimes quite tiresome pattern transform sound music instead watch picture listen pitch rhythm tune melody identify anything interesting unusual may relieve burden visual concentration 
132 methodology datum mining figure 137 visualization association rule mineset figure 138 visualization decision tree mineset 605 
606 chapter 13 datum mining trend research frontier figure 139 visualization cluster grouping ibm intelligent miner figure 1310 visualization datum mining process clementine 
133 datum mining application 607 figure 1311 perception-based classification interactive visual mining approach relax visual mining therefore audio datum mining interesting complement visual mining 133 datum mining application book study principle method mining relational datum datum warehouse complex datum type datum mining relatively young discipline wide diverse application still nontrivial gap general principle datum mining application-specific effective datum mining tool section examine several application domain list figure discuss customize datum mining method tool develop application 1331 datum mining financial datum analysis bank financial institution offer wide variety banking investment credit service ( latter include business mortgage automobile loan credit card ) also offer insurance stock investment service 
608 chapter 13 datum mining trend research frontier financial datum analysis retail telecommunication industry science engineering datum mining application intrusion detection prevention recommender system figure 1312 common datum mining application domain financial datum collect banking financial industry often relatively complete reliable high quality facilitate systematic datum analysis datum mining present typical case design construction datum warehouse multidimensional datum analysis datum mining like many application datum warehouse need construct banking financial datum multidimensional datum analysis method used analyze general property datum example company ’ financial officer may want view debt revenue change month region sector factor along maximum minimum total average trend deviation statistical information datum warehouse datum cube ( include advanced datum cube concept multifeature discovery-driven regression prediction datum cube ) characterization class comparison cluster outlier analysis play important role financial datum analysis mining loan payment prediction customer credit policy analysis loan payment prediction customer credit analysis critical business bank many factor strongly weakly influence loan payment performance customer credit rating datum mining method attribute selection attribute relevance ranking may help identify important factor eliminate irrelevant one example factor related risk loan payment include loan-to-value ratio term loan debt ratio ( total amount monthly debt versus total monthly income ) payment-to-income ratio customer income level education level residence region credit history analysis customer payment history may find say payment-to-income ratio dominant factor education level debt ratio bank may decide adjust loan-grant policy 
133 datum mining application 609 grant loan customer whose application previously deny whose profile show relatively low risk accord critical factor analysis classification cluster customer target marketing classification cluster method used customer group identification target marketing example use classification identify crucial factor may influence customer ’ decision regard banking customer similar behavior regard loan payment may identify multidimensional cluster technique help identify customer group associate new customer appropriate customer group facilitate target marketing detection money launder financial crime detect money launder financial crime important integrate information multiple heterogeneous databasis ( eg bank transaction databasis federal state crime history databasis ) long potentially related study multiple datum analysis tool used detect unusual pattern large amount cash flow certain period certain group customer useful tool include datum visualization tool ( display transaction activity used graph time group customer ) linkage information network analysis tool ( identify link among different customer activity ) classification tool ( filter unrelated attribute rank highly related one ) cluster tool ( group different case ) outlier analysis tool ( detect unusual amount fund transfer activity ) sequential pattern analysis tool ( characterize unusual access sequence ) tool may identify important relationship pattern activity help investigator focus suspicious case detailed examination 1332 datum mining retail telecommunication industry retail industry well-fit application area datum mining since collect huge amount datum sale customer shopping history good transportation consumption service quantity datum collect continue expand rapidly especially due increase availability ease popularity business conduct web e-commerce today major chain store also web site customer make purchase online business amazoncom ( wwwamazoncom ) exist solely online without brick-and-mortar ( ie physical ) store location retail datum provide rich source datum mining retail datum mining help identify customer buy behavior discover customer shopping pattern trend improve quality customer service achieve better customer retention satisfaction enhance good consumption ratio design effective good transportation distribution policy reduce cost business example datum mining retail industry outlined follow design construction datum warehouse retail datum cover wide spectrum ( include sale customer employee good transportation consumption 
610 chapter 13 datum mining trend research frontier service ) many way design datum warehouse industry level detail include vary substantially outcome preliminary datum mining exercise used help guide design development datum warehouse structure involve decide dimension level include preprocess perform facilitate effective datum mining multidimensional analysis sale customer product time region retail industry require timely information regard customer need product sale trend fashion well quality cost profit service commodity therefore important provide powerful multidimensional analysis visualization tool include construction sophisticated datum cube accord need datum analysis advanced datum cube structure introduce chapter 5 useful retail datum analysis facilitate analysis multidimensional aggregate complex condition analysis effectiveness sale campaign retail industry conduct sale campaign used advertisement coupon various kind discount bonuse promote product attract customer careful analysis effectiveness sale campaign help improve company profit multidimensional analysis used purpose compare amount sale number transaction contain sale item sale period versus contain item sale campaign moreover association analysis may disclose item likely purchase together item sale especially comparison sale campaign customer retention—analysis customer loyalty use customer loyalty card information register sequence purchase particular customer customer loyalty purchase trend analyze systematically good purchase different period customer group sequence sequential pattern mining used investigate change customer consumption loyalty suggest adjustment pricing variety good help retain customer attract new one product recommendation cross-referencing item mining association sale record may discover customer buy digital camera likely buy another set item information used form product recommendation collaborative recommender system ( section 1335 ) use datum mining technique make personalize product recommendation live customer transaction base opinion customer product recommendation also advertised sale receipt weekly flyer web help improve customer service aid customer select item increase sale similarly information “ hot item week ” attractive deal display together associative information promote sale fraudulent analysis identification unusual pattern fraudulent activity cost retail industry million dollar per year important ( 1 ) identify potentially fraudulent user atypical usage pattern ( 2 ) detect attempt gain fraudulent entry unauthorized access individual organizational 
133 datum mining application 611 account ( 3 ) discover unusual pattern may need special attention many pattern discover multidimensional analysis cluster analysis outlier analysis another industry handle huge amount datum telecommunication industry quickly evolved offer local long-distance telephone service provide many comprehensive communication service include cellular phone smart phone internet access email text message image computer web datum transmission datum traffic integration telecommunication computer network internet numerous mean communication compute way change face telecommunication compute create great demand datum mining help understand business dynamic identify telecommunication pattern catch fraudulent activity make better use resource improve service quality datum mining task telecommunication share many similarity retail industry common task include construct large-scale datum warehouse perform multidimensional visualization olap in-depth analysis trend customer pattern sequential pattern task contribute business improvement cost reduction customer retention fraud analysis sharpen edge competition many datum mining task customize datum mining tool telecommunication flourishing expect play increasingly important role business datum mining popularly used many industry insurance manufacturing health care well analysis governmental institutional administration datum although industry characteristic datum set application demand share many common principle methodology therefore effective mining one industry may gain experience methodology transfer industrial application 1333 datum mining science engineering past many scientific datum analysis task tend handle relatively small homogeneous datum set datum typically analyze used “ formulate hypothesis build model evaluate result ” paradigm case statistical technique typically employ analysis ( see section 1321 ) massive datum collection storage technology recently change landscape scientific datum analysis today scientific datum amassed much higher speed lower cost result accumulation huge volume high-dimensional datum stream datum heterogenous datum contain rich spatial temporal information consequently scientific application shift “ hypothesize-and-test ” paradigm toward “ collect store datum mine new hypothesis confirm datum experimentation ” process shift bring new challenge datum mining vast amount datum collect scientific domain ( include geoscience astronomy meteorology geology biological science ) used sophisticated 
612 chapter 13 datum mining trend research frontier telescope multispectral high-resolution remote satellite sensor global position system new generation biological datum collection analysis technology large datum set also generate due fast numeric simulation various field climate ecosystem modele chemical engineering fluid dynamic structural mechanic look challenge bring emerge scientific application datum mining datum warehouse datum preprocess datum preprocess datum warehouse critical information exchange datum mining create warehouse often require find mean resolve inconsistent incompatible datum collect multiple environment different time period require reconcile semantic reference system geometry measurement accuracy precision method need integrate datum heterogeneous source identify event instance consider climate ecosystem datum spatial temporal require cross-referencing geospatial datum major problem analyze datum many event spatial domain temporal domain example el nino event occur every four seven year previous datum might collect systematically today method also need efficient computation sophisticated spatial aggregate handle spatial-related datum stream mining complex datum type scientific datum set heterogeneous nature typically involve semi-structure unstructured datum multimedium datum georeference stream datum well datum sophisticated deeply hide semantic ( eg genomic proteomic datum ) robust dedicate analysis method need handle spatiotemporal datum biological datum related concept hierarchy complex semantic relationship example bioinformatic research problem identify regulatory influence gene gene regulation refer gene cell switch ( ) determine cell ’ function different biological process involve different set gene act together precisely regulate pattern thus understand biological process need identify participate gene regulator require development sophisticated datum mining method analyze large biological datum set clue regulatory influence specific gene find dna segment ( “ regulatory sequence ” ) mediate influence graph-based network-based mining often difficult impossible model several physical phenomena process due limitation exist modele approach alternatively labele graph network may used capture many spatial topological geometric biological relational characteristic present scientific datum set graph network modele object mine represent vertex graph edge vertex represent relationship object example graph used model chemical structure biological pathway datum generate numeric 
133 datum mining application 613 simulation fluid-flow simulation success graph network modele however depend improvement scalability efficiency many graph-based datum mining task classification frequent pattern mining cluster visualization tool domain-specific knowledge high-level graphical user interface visualization tool require scientific datum mining system integrate exist domain-specific datum information system guide researcher general user search pattern interpret visualize discover pattern used discover knowledge decision make datum mining engineering share many similarity datum mining science practice often collect massive amount datum require datum preprocess datum warehousing scalable mining complex type datum typically use visualization make good use graph network moreover many engineering process need real-time response mining datum stream real time often become critical component massive amount human communication datum pour daily life communication exist many form include news blog article web page online discussion product reviews twitter message advertisement communication web various kind social network hence datum mining social science social study become increasingly popular moreover user reader feedback regard product speech article analyze deduce general opinion sentiment view society analysis result used predict trend improve work help decision make computer science generate unique kind datum example computer program long execution often generate huge-size trace computer network complex structure network flow dynamic massive sensor network may generate large amount datum varied reliability computer system databasis suffer various kind attack data access may raise security privacy concern unique kind datum provide fertile land datum mining datum mining computer science used help monitor system status improve system performance isolate software bug detect software plagiarism analyze computer system fault uncover network intrusion recognize system malfunction datum mining software system engineering operate static dynamic ( ie stream-based ) datum depend whether system dump trace beforehand postanalysis must react real time handle online datum various method develop domain integrate extend method machine learn datum mining system engineering pattern recognition statistic datum mining computer science active rich domain datum miner unique challenge require development sophisticated scalable real-time datum mining system engineering method 
614 chapter 13 datum mining trend research frontier 1334 datum mining intrusion detection prevention security computer system datum continual risk extensive growth internet increase availability tool trick intrude attack network prompt intrusion detection prevention become critical component networked system intrusion defined set action threaten integrity confidentiality availability network resource ( eg user account file system system kernel ) intrusion detection system intrusion prevention system monitor network traffic or system execution malicious activity however former produce report whereas latter place in-line able actively block intrusion detected main function intrusion prevention system identify malicious activity log information say activity attempt stop activity report activity majority intrusion detection prevention system use either signaturebased detection anomaly-based detection signature-based detection method detection utilize signature attack pattern preconfigured predetermine domain expert signature-based intrusion prevention system monitor network traffic match signature match find intrusion detection system report anomaly intrusion prevention system take additional appropriate action note since system usually quite dynamic signature need update laboriously whenever new software version arrive change network configuration situation occur another drawback detection mechanism identify case match signature unable detect new previously unknown intrusion trick anomaly-based detection method build model normal network behavior ( call profile ) used detect new pattern significantly deviate profile deviation may represent actual intrusion simply new behavior need add profile main advantage anomaly detection may detect novel intrusion yet observed typically human analyst must sort deviation ascertain represent real intrusion limit factor anomaly detection high percentage false positive new pattern intrusion add set signature enhance signature-based detection datum mining method help intrusion detection prevention system enhance performance various way follow new datum mining algorithms intrusion detection datum mining algorithms used signature-based anomaly-based detection signature-based detection training datum labele either “ normal ” “ ” classifier derive detect know intrusion research area 
133 datum mining application 615 include application classification algorithms association rule mining cost-sensitive modele anomaly-based detection build model normal behavior automatically detect significant deviation method include application cluster outlier analysis classification algorithms statistical approach technique used must efficient scalable capable handle network datum high volume dimensionality heterogeneity association correlation discriminative pattern analysis help select build discriminative classifier association correlation discriminative pattern mining apply find relationship system attribute describe network datum information provide insight regard selection useful attribute intrusion detection new attribute derive aggregate datum may also helpful summary count traffic match particular pattern analysis stream datum due transient dynamic nature intrusion malicious attack crucial perform intrusion detection datum stream environment moreover event may normal consider malicious view part sequence event thus necessary study sequence event frequently encounter together find sequential pattern identify outlier datum mining method find evolve cluster build dynamic classification model datum stream also necessary real-time intrusion detection distribute datum mining intrusion launch several different location target many different destination distribute datum mining method may used analyze network datum several network location detect distribute attack visualization query tool visualization tool available view anomalous pattern detected tool may include feature view association discriminative pattern cluster outlier intrusion detection system also graphical user interface allow security analyst pose query regard network datum intrusion detection result summary computer system continual risk break security datum mining technology used develop strong intrusion detection prevention system may employ signature-based anomaly-based detection 1335 datum mining recommender system today ’ consumer face million good service shopping online recommender system help consumer make product recommendation likely interest user book cds movie restaurant online news article service recommender system may use either contentbased approach collaborative approach hybrid approach combine content-based collaborative method 
616 chapter 13 datum mining trend research frontier content-based approach recommend item similar item user prefer query past rely product feature textual item description collaborative approach ( collaborative filter approach ) may consider user ’ social environment recommend item base opinion customer similar taste preference user recommender system use broad range technique information retrieval statistic machine learn datum mining search similarity among item customer preference consider example 131 example 131 scenario used recommender system suppose visit web site online bookstore ( eg amazon ) intention purchasing book want read type name book first time visit web site browse even make purchase last christmas web store remember previous visit store click stream information information regard past purchase system display description price book specify compare interest customer similar interest recommend additional book title say “ customer buy book specify also buy title ” survey list see another title spark interest decide purchase one well suppose go another online store intention purchasing digital camera system suggest additional item consider base previously mine sequential pattern “ customer buy kind digital camera likely buy particular brand printer memory card photo editing software within three ” decide buy camera without additional item week later receive coupon store regard additional item advantage recommender system provide personalization customer e-commerce promote one-to-one marketing amazon pioneer use collaborative recommender system offer “ personalize store every customer ” part marketing strategy personalization benefit consumer company involved accurate model customer company gain better understand customer need serve need result greater success regard cross-selling related product upsel product affinity one-to-one promotion larger basket customer retention recommendation problem consider set c user set item let u utility function measure usefulness item user c utility commonly represent rating initially defined item previously rate user example join movie recommendation system user typically ask rate several movie space c × possible user item huge recommendation system able extrapolate know unknown rating predict item–user combination item highest predict utility user recommend user 
133 datum mining application 617 “ utility item estimate user ” content-based method estimate base utility assign user item similar many system focus recommend item contain textual information web site article news message look commonality among item movie may look similar genre director actor article may look similar term content-based method root information theory make use keyword ( describe item ) user profile contain information user ’ taste need profile may obtain explicitly ( eg questionnaire ) learn user ’ transactional behavior time collaborative recommender system try predict utility item user u base item previously rate user similar u example recommend book collaborative recommender system try find user history agree u ( eg tend buy similar book give similar rating book ) collaborative recommender system either memory ( heuristic ) base model base memory-based method essentially use heuristic make rating prediction base entire collection item previously rate user unknown rating item–user combination estimate aggregate rating similar user item typically k-nearest-neighbor approach used find k user ( neighbor ) similar target user u various approach used compute similarity user popular approach use either pearson ’ correlation coefficient ( section 332 ) cosine similarity ( section 247 ) weight aggregate used adjust fact different user may use rating scale differently model-based collaborative recommender system use collection rating learn model used make rating prediction example probabilistic model cluster ( find cluster like-minded customer ) bayesian network machine learn technique used recommender system face major challenge scalability ensure quality recommendation consumer example regard scalability collaborative recommender system must able search million potential neighbor real time site used browse pattern indication product preference may thousand datum point customer ensure quality recommendation essential gain consumer ’ trust consumer follow system recommendation end liking product less likely use recommender system classification system recommender system make two type error false negative false positive false negative product system fail recommend although consumer would like false positive product recommend consumer like false positive less desirable annoy anger consumer content-based recommender system limit feature used describe item recommend 
618 chapter 13 datum mining trend research frontier another challenge content-based collaborative recommender system deal new user buy history yet available hybrid approach integrate content-based collaborative method achieve improve recommendation netflix prize open competition hold online dvd-rental service payout $ 1000000 best recommender algorithm predict user rating film base previous rating competition study show predictive accuracy recommender system substantially improve blending multiple predictor especially used ensemble many substantially different method rather refine single technique collaborative recommender system form intelligent query answer consist analyze intent query provide generalized neighborhood associate information relevant query example rather simply return book description price response customer ’ query return additional information related query explicitly ask ( eg book evaluation comment recommendation book sale statistic ) provide intelligent answer query 134 datum mining society us datum mining part daily life although may often unaware presence section 1341 look several example “ ubiquitous invisible ” datum mining affect everyday thing product stock local supermarket ad see surfing internet crime prevention datum mining offer individual many benefit improve customer service satisfaction well lifestyle general however also serious implication regard one ’ right privacy datum security issue topic section 1342 1341 ubiquitous invisible datum mining datum mining present many aspect daily life whether realize affect shop work search information even influence leisure time health well-being section look example ubiquitous ( ever-present ) datum mining several example also represent invisible datum mining “ smart ” software search engine customer-adaptive web service ( eg used recommender algorithms ) “ intelligent ” database system email manager ticket master incorporate datum mining functional component often unbeknownst user grocery store print personalize coupon customer receipt online store recommend additional item base customer interest datum mining innovatively influenced buy way shop experience shopping one example wal-mart hundred million customer visit ten thousand store every week wal-mart allow supplier access datum 
134 datum mining society 619 product perform analysis used datum mining software allow supplier identify customer buy pattern different store control inventory product placement identify new merchandize opportunity affect item ( many ) end store ’ shelves—something think next time wander aisle wal-mart datum mining shape online shopping experience many shopper routinely turn online store purchase book music movie toy recommender system discuss section 1335 offer personalize product recommendation base opinion customer amazoncom forefront used personalize datum mining–based approach marketing strategy observed traditional brick-and-mortar store hardest part get customer store customer likely buy something since cost go another store high therefore marketing brick-and-mortar store tend emphasize draw customer rather actual in-store customer experience contrast online store customer “ walk ” enter another online store click mouse amazoncom capitalize difference offer “ personalize store every ” use several datum mining technique identify customer ’ like make reliable recommendation topic shopping suppose lot buy credit card nowadays unusual receive phone call one ’ credit card company regard suspicious unusual pattern spending credit card company use datum mining detect fraudulent usage save billion dollar year many company increasingly use datum mining customer relationship management ( crm ) help provide customize personal service address individual customer ’ need lieu mass marketing study browse purchasing pattern web store company tailor advertisement promotion customer profile customer less likely annoyed unwanted mass mailing junk mail action result substantial cost saving company customer benefit likely notified offer actually interest result less waste personal time greater satisfaction datum mining greatly influenced way person use computer search information work get internet example decide check email unbeknownst several annoying email already delete thank spam filter used classification algorithms recognize spam process email go google ( wwwgooglecom ) provide access information billion web page index server google one popular widely used internet search engine used google search information become way life many person google popular even become new verb english language meaning “ search ( something ) internet used google search engine extension comprehensive search ” 1 decide type keyword 1 http open-dictionarycom 
620 chapter 13 datum mining trend research frontier topic interest google return list web site topic mine index organized set datum mining algorithms include pagerank moreover type “ boston new york ” google show bus train schedule boston new york however minor change “ boston paris ” lead flight schedule boston paris smart offering information service likely base frequent pattern mine click stream many previous query view result google query various ad pop relate query google ’ strategy tailor advertising match user ’ interest one typical service explore every internet search provider also make happier less likely pester irrelevant ad datum mining omnipresent see daily-encounter example can go scenario many case datum mining invisible user may unaware examine result return datum mining click actually fed new datum datum mining function datum mining become improve accept technology continue research development need many area mentioned challenge throughout book include efficiency scalability increase user interaction incorporation background knowledge visualization technique effective method find interesting pattern improve handle complex datum type stream datum realtime datum mining web mining addition integration datum mining exist business scientific technology provide domain-specific datum mining tool contribute advancement technology success datum mining solution tailor e-commerce application opposed generic datum mining system example 1342 privacy security social impact datum mining information accessible electronic form available web increasingly powerful datum mining tool develop put use increase concern datum mining may pose threat privacy datum security however important note many datum mining application even touch personal datum prominent example include application involve natural resource prediction flood drought meteorology astronomy geography geology biology scientific engineering datum furthermore study datum mining research focus development scalable algorithms involve personal datum focus datum mining technology discovery general statistically significant pattern specific information regard individual sense believe real privacy concern unconstrained access individual record especially access privacy-sensitive information credit card transaction record health-care record personal financial record biological trait justice investigation ethnicity datum mining application involve personal datum many case simple method remove sensitive id datum may protect privacy individual nevertheless privacy concern exist wherever 
134 datum mining society 621 personally identifiable information collect store digital form datum mining program able access datum even datum preparation improper nonexistent disclosure control root cause privacy issue handle concern numerous datum security-enhancing technique develop addition great deal recent effort develop privacypreserve datum mining method section look advance protect privacy datum security datum mining “ secure privacy individual collect mining datum ” many datum security–enhancing technique develop help protect datum databasis employ multilevel security model classify restrict datum accord various security level user permit access authorize level show however user execute specific query authorize security level still infer sensitive information similar possibility occur datum mining encryption another technique individual datum item may encode may involve blind signature ( build public key encryption ) biometric encryption ( eg image person ’ iris fingerprint used encode personal information ) anonymous databasis ( permit consolidation various databasis limit access personal information need know personal information encrypt store different location ) intrusion detection another active area research help protect privacy personal datum privacy-preserve datum mining area datum mining research response privacy protection datum mining also know privacy-enhance privacysensitive datum mining deal obtain valid datum mining result without disclose underlie sensitive datum value privacy-preserve datum mining method use form transformation datum perform privacy preservation typically method reduce granularity representation preserve privacy example may generalize datum individual customer customer group reduction granularity cause loss information possibly usefulness datum mining result natural trade-off information loss privacy privacy-preserve datum mining method classify follow category randomization method method add noise datum mask attribute value record noise add sufficiently large individual record value especially sensitive one re-cover however add skillfully final result datum mining basically preserve technique design derive aggregate distribution perturbed datum subsequently datum mining technique develop work aggregate distribution k-anonymity l-diversity method method alter individual record uniquely identify k-anonymity method granularity datum representation reduce sufficiently give record map onto least k record datum used technique like generalization suppression k-anonymity method weak homogeneity 
622 chapter 13 datum mining trend research frontier sensitive value within group value may infer alter record l-diversity model design handle weakness enforce intragroup diversity sensitive value ensure anonymization goal make sufficiently difficult adversary use combination record attribute exactly identify individual record distribute privacy preservation large datum set can partition distribute either horizontally ( ie datum set partition different subset record distribute across multiple site ) vertically ( ie datum set partition distribute attribute ) even combination individual site may want share entire datum set may consent limit information sharing use variety protocol overall effect method maintain privacy individual object derive aggregate result datum downgrading effectiveness datum mining result many case even though datum may available output datum mining ( eg association rule classification model ) may result violation privacy solution can downgrade effectiveness datum mining either modify datum mining result hiding association rule slightly distort classification model recently researcher propose new idea privacy-preserve datum mining notion differential privacy general idea two datum set close one another ( ie differ tiny datum set single element ) give differentially private algorithm behave approximately datum set definition give strong guarantee presence absence tiny datum set ( eg represent individual ) affect final output query significantly base notion set differential privacy-preserve datum mining algorithms develop research direction ongoing expect powerful privacy-preserve datum publish datum mining algorithms near future like technology datum mining misuse however must lose sight benefit datum mining research bring range insight gain medical scientific application increase customer satisfaction help company better suit client ’ need expect computer scientist policy expert counterterrorism expert continue work social scientist lawyer company consumer take responsibility build solution ensure datum privacy protection security way may continue reap benefit datum mining term time money saving discovery new knowledge 135 datum mining trend diversity datum datum mining task datum mining approach pose many challenge research issue datum mining development efficient effective datum 
135 datum mining trend 623 mining method system service interactive integrate datum mining environment key area study use datum mining technique solve large sophisticated application problem important task datum mining researcher datum mining system application developer section describe trend datum mining reflect pursuit challenge application exploration early datum mining application put lot effort help business gain competitive edge exploration datum mining business continue expand e-commerce e-marketing become mainstream retail industry datum mining increasingly used exploration application area web text analysis financial analysis industry government biomedicine science emerge application area include datum mining counterterrorism mobile ( wireless ) datum mining generic datum mining system may limitation deal application-specific problem may see trend toward development application-specific datum mining system tool well invisible datum mining function embed various kind service scalable interactive datum mining method contrast traditional datum analysis method datum mining must able handle huge amount datum efficiently possible interactively amount datum collect continue increase rapidly scalable algorithms individual integrate datum mining function become essential one important direction toward improve overall efficiency mining process increase user interaction constraint-based mining provide user add control allow specification use constraint guide datum mining system search interesting pattern knowledge integration datum mining search engine database system datum warehouse system cloud compute system search engine database system datum warehouse system cloud compute system mainstream information process compute system important ensure datum mining serve essential datum analysis component smoothly integrate information process environment datum mining service tightly couple system seamless unify framework invisible function ensure datum availability datum mining portability scalability high performance integrate information process environment multidimensional datum analysis exploration mining social information network mining social information network link analysis critical task network ubiquitous complex development scalable effective knowledge discovery method application large number network datum essential outlined section 1312 mining spatiotemporal moving-object cyber-physical system cyberphysical system well spatiotemporal datum mount rapidly due 
624 chapter 13 datum mining trend research frontier popular use cellular phone gps sensor wireless equipment outlined section 1313 many challenge research issue realize real-time effective knowledge discovery datum mining multimedium text web datum outlined section 1313 mining kind datum recent focus datum mining research great progress make yet still many open issue solve mining biological biomedical datum unique combination complexity richness size importance biological biomedical datum warrant special attention datum mining mining dna protein sequence mining highdimensional microarray datum biological pathway network analysis topic field area biological datum mining research include mining biomedical literature link analysis across heterogeneous biological datum information integration biological datum datum mining datum mining software engineering system engineering software program large computer system become increasingly bulky size sophisticated complexity tend originate integration multiple component develop different implementation team trend make increasingly challenge task ensure software robustness reliability analysis execution buggy software program essentially datum mining process—trace datum generate program execution may disclose important pattern outlier can lead eventual automate discovery software bug expect development datum mining methodology system debug enhance software robustness bring new vigor system engineering visual audio datum mining visual audio datum mining effective way integrate human ’ visual audio system discover knowledge huge amount datum systematic development technique facilitate promotion human participation effective efficient datum analysis distribute datum mining real-time datum stream mining traditional datum mining method design work centralize location work well many distribute compute environment present today ( eg internet intranet local area network high-speed wireless network sensor network cloud compute ) advance distribute datum mining method expect moreover many application involve stream datum ( eg e-commerce web mining stock analysis intrusion detection mobile datum mining datum mining counterterrorism ) require dynamic datum mining model build real time additional research need direction privacy protection information security datum mining abundance personal confidential information available electronic form couple increasingly powerful datum mining tool pose threat datum privacy security grow interest datum mining counterterrorism also add concern 
136 summary 625 development privacy-preserve datum mining method foresee collaboration technologist social scientist law expert government company need produce rigorous privacy security protection mechanism datum publish datum mining confidence look forward next generation datum mining technology benefit bring 136 summary mining complex datum type pose challenge issue many dedicate line research development chapter present high-level overview mining complex datum type include mining sequence datum time series symbolic sequence biological sequence mining graph network mining kind datum include spatiotemporal cyber-physical system datum multimedium text web datum datum stream several well-established statistical method propose datum analysis regression generalized linear model analysis variance mixed-effect model factor analysis discriminant analysis survival analysis quality control full coverage statistical datum analysis method beyond scope book interested reader refer statistical literature cite bibliographic note ( section 138 ) researcher strive build theoretical foundation datum mining several interesting proposal appear base datum reduction datum compression probability statistic theory microeconomic theory pattern discovery–based inductive databasis visual datum mining integrate datum mining datum visualization discover implicit useful knowledge large datum set visual datum mining include datum visualization datum mining result visualization datum mining process visualization interactive visual datum mining audio datum mining used audio signal indicate datum pattern feature datum mining result many customize datum mining tool develop domain-specific application include finance retail telecommunication industry science engineering intrusion detection prevention recommender system application domain-based study integrate domain-specific knowledge datum analysis technique provide mission-specific datum mining solution ubiquitous datum mining constant presence datum mining many aspect daily life influence shop work search information use computer well leisure time health well-being invisible datum mining “ smart ” software search engine customer-adaptive web service 
626 chapter 13 datum mining trend research frontier ( eg used recommender algorithms ) email manager incorporate datum mining functional component often unbeknownst user major social concern datum mining issue privacy datum security privacy-preserve datum mining deal obtain valid datum mining result without disclose underlie sensitive value goal ensure privacy protection security preserve overall quality datum mining result datum mining trend include effort toward exploration new application area improve scalable interactive constraint-based mining method integration datum mining web service database warehousing cloud compute system mining social information network trend include mining spatiotemporal cyber-physical system datum biological datum system engineering datum multimedium text datum addition web mining distribute real-time datum stream mining visual audio mining privacy security datum mining 137 exercise 131 sequence datum ubiquitous diverse application chapter present general overview sequential pattern mining sequence classification sequence similarity search trend analysis biological sequence alignment modele however cover sequence cluster present overview method sequence cluster 132 chapter present overview sequence pattern mining graph pattern mining method mining tree pattern partial order pattern also study research summarize method mining structure pattern include sequence tree graph partial order relationship examine kind structural pattern mining cover research propose application create new mining problem 133 many study analyze homogeneous information network ( eg social network consist friend link friend ) however many application involve heterogeneous information network ( ie network link multiple type object research paper conference author topic ) major difference methodology mining heterogeneous information network method homogeneous counterpart 134 research describe datum mining application present chapter discuss different form datum mining used application 135 establishment theoretical foundation important datum mining name describe main theoretical foundation propose datum mining comment satisfy ( fail satisfy ) requirement ideal theoretical framework datum mining 
137 exercise 627 136 ( research project ) build theory datum mining require set theoretical framework major datum mining function explain framework take one theory example ( eg datum compression theory ) examine major datum mining function fit framework function fit well current theoretical framework propose way extend framework explain function 137 strong linkage statistical datum analysis datum mining person think datum mining automate scalable method statistical datum analysis agree disagree perception present one statistical analysis method automate or scale nicely integration current datum mining methodology 138 difference visual datum mining datum visualization datum visualization may suffer datum abundance problem example easy visually discover interesting property network connection social network huge complex dense connection propose visualization method may help person see network topology interesting feature social network 139 propose implementation method audio datum mining integrate audio visual datum mining bring fun power datum mining possible develop video datum mining method state scenario solution make integrate audiovisual mining effective 1310 general-purpose computer domain-independent relational database system become large market last several decade however many person feel generic datum mining system prevail datum mining market think datum mining focus effort develop domain-independent datum mining tool develop domain-specific datum mining solution present reasoning 1311 recommender system way differ customer productbased cluster system differ typical classification predictive modele system outline one method collaborative filter discuss work limitation practice 1312 suppose local bank datum mining system bank study debit card usage pattern notice make many transaction home renovation store bank decide contact offer information regard special loan home improvement ( ) discuss may conflict right privacy ( b ) describe another situation feel datum mining infringe privacy ( c ) describe privacy-preserve datum mining method may allow bank perform customer pattern analysis without infringe customer ’ right privacy ( ) example datum mining can used help society think way can used may detrimental society 
628 chapter 13 datum mining trend research frontier 1313 major challenge face bring datum mining research market illustrate one datum mining research issue view may strong impact market society discuss approach research issue 1314 base view challenge research problem datum mining give number year good number researcher implementor would plan make good progress toward effective solution problem 1315 base experience knowledge suggest new frontier datum mining mentioned chapter 138 bibliographic note mining complex datum type many research paper book cover various theme list recent book well-cite survey research article reference time-series analysis study statistic computer science community decade many textbook box jenkin reinsel [ bjr08 ] brockwell davis [ bd02 ] chatfield [ cha03b ] hamilton [ ham94 ] shumway stoffer [ ss05 ] fast subsequence match method time-series databasis present faloutsos ranganathan manolopoulos [ frm94 ] agrawal lin sawhney shim [ alss95 ] develop method fast similarity search presence noise scaling translation time-series databasis shasha zhu present overview method high-performance discovery time series [ sz04 ] sequential pattern mining method study many researcher include agrawal srikant [ as95 ] zaki [ zak01 ] pei han mortazavi-asl et al [ + 04 ] yan han afshar [ yha03 ] study sequence classification include ji bailey dong [ jbd05 ] ye keogh [ yk09 ] survey xing pei keogh [ xpk10 ] dong pei [ dp07 ] provide overview sequence datum mining method method analysis biological sequence include markov chain hide markov model introduce many book tutorial waterman [ wat95 ] setubal meidanis [ sm97 ] durbin eddy krogh mitchison [ dekm98 ] baldi brunak [ bb01 ] krane raymer [ kr03 ] rabiner [ rab89 ] jone pevzner [ jp04 ] baxevanis ouellette [ bo04 ] information blast ( see also korf yandell bedell [ kyb03 ] ) find ncbi web site graph pattern mining study extensively include holder cook djoko [ hcd94 ] inokuchi washio motoda [ iwm98 ] kuramochi karypis [ kk01 ] yan han [ yh02 yh03a ] borgelt berthold [ bb02 ] huan wang bandyopadhyay et al [ + 04 ] gaston tool nijssen kok [ nk04 ] 
138 bibliographic note 629 great deal research social information network analysis include newman [ new10 ] easley kleinberg [ ek10 ] yu han faloutsos [ yhf10 ] wasserman faust [ wf94 ] watt [ wat03 ] newman barabasi watt [ nbw06 ] statistical modele network study popularly albert barbasi [ ab99 ] watt [ wat03 ] faloutsos faloutsos faloutsos [ fff99 ] kumar raghavan rajagopalan et al [ + 00 ] leskovec kleinberg faloutsos [ lkf05 ] datum clean integration validation information network analysis study many include bhattacharya getoor [ bg04 ] yin han yu [ yhy07 yhy08 ] cluster ranking classification network study extensively include brin page [ bp98 ] chakrabarti dom indyk [ cdi98 ] kleinberg [ kle99 ] getoor friedman koller taskar [ gfkt01 ] newman m girvan [ ng04 ] yin han yang yu [ yhyy04 ] yin han yu [ yhy05 ] xu yuruk feng schweiger [ xyfs07 ] kuli basu dhillon mooney [ kbdm09 ] sun han zhao et al [ + 09 ] neville gallaher eliassi-rad [ nge-r09 ] ji sun danilevsky et al [ + 10 ] role discovery link prediction information network study extensively well krebs [ kre02 ] kubica moore schneider [ kms03 ] liben-nowell kleinberg [ l-nk03 ] wang han jia et al [ + 10 ] similarity search olap information network study many include tian hankin patel [ thp08 ] chen yan zhu et al [ + 08 ] evolution social information network study many researcher chakrabarti kumar tomkin [ ckt06 ] chi song zhou et al [ + 07 ] tang liu zhang nazeri [ tlzn08 ] xu zhang yu long [ xzyl08 ] kim han [ kh09 ] sun tang han [ + 10 ] spatial spatiotemporal datum mining study extensively collection paper miller han [ mh09 ] introduce textbook shekhar chawla [ sc03 ] hsu lee wang [ hlw07 ] spatial cluster algorithms study extensively chapter 10 11 book research conduct spatial warehouse olap stefanovic han koperski [ shk00 ] spatial spatiotemporal datum mining koperski han [ kh95 ] mamouli cao kollio hadjieleftheriou et al [ + 04 ] tsoukatos gunopulos [ tg01 ] hadjieleftheriou kollio gunopulos tsotra [ hkgt03 ] mining moving-object datum study many vlachos gunopulos kollio [ vgk02 ] tao faloutsos papadia liu [ tfpl04 ] li han kim gonzalez [ lhkg07 ] lee han whang [ lhw07 ] li ding han et al [ + 10 ] bibliography temporal spatial spatiotemporal datum mining research see collection roddick hornsby spiliopoulou [ rhs01 ] multimedium datum mining deep root image process pattern recognition study extensively many textbook include gonzalez wood [ gw07 ] russ [ rus06 ] duda hart stork [ dhs01 ] z zhang r zhang [ zz09 ] search mining multimedium datum study many ( see eg fayyad smyth [ fs93 ] faloutsos lin [ fl95 ] natsev rastogi 
630 chapter 13 datum mining trend research frontier shim [ nrs99 ] zaı̈ane han zhu [ zhz00 ] ) overview image mining method give hsu lee zhang [ hlz02 ] text datum analysis study extensively information retrieval many textbook survey article croft metzler strohman [ cms09 ] s buttcher c clarke g cormack [ bcc10 ] man raghavan schutze [ mrs08 ] grossman frieder [ gr04 ] baeza-yate riberio-neto [ byrn11 ] zhai [ zha08 ] feldman sanger [ fs06 ] berry [ ber03 ] weis indurkhya zhang damerau [ wizd04 ] text mining fast-developing field numerous paper publish recent year cover many topic topic model ( eg blei lafferty [ bl09 ] ) sentiment analysis ( eg pang lee [ pl07 ] ) contextual text mining ( eg mei zhai [ mz06 ] ) web mining another focuse theme book like chakrabarti [ cha03a ] liu [ liu06 ] berry [ ber03 ] web mining substantially improve search engine influential milestone work brin page [ bp98 ] kleinberg [ kle99 ] chakrabarti dom kumar et al [ + 99 ] kleinberg tomkin [ kt99 ] numerous result generate since search log mining ( eg silvestri [ sil10 ] ) blog mining ( eg mei liu su zhai [ mlsz06 ] ) mining online forum ( eg cong wang lin et al [ + 08 ] ) book survey stream datum system stream datum process include babu widom [ bw01 ] babcock babu datar et al [ + 02 ] muthukrishnan [ mut05 ] aggarwal [ agg06 ] stream datum mining research cover stream cube model ( eg chen dong han et al [ + 02 ] ) stream frequent pattern mining ( eg manku motwani [ mm02 ] karp papadimitriou shenker [ kps03 ] ) stream classification ( eg domingo hulten [ dh00 ] wang fan yu han [ wfyh03 ] aggarwal han wang yu [ ahwy04b ] ) stream cluster ( eg guha mishra motwani ’ callaghan [ gmmo00 ] aggarwal han wang yu [ ahwy03 ] ) many book discuss datum mining application financial datum analysis financial modele see example benninga [ ben08 ] higgin [ hig08 ] retail datum mining customer relationship management see example book berry linoff [ bl04 ] berson smith thearle [ bst99 ] telecommunication-related datum mining see example horak [ hor08 ] also book scientific datum analysis grossman kamath kegelmeyer et al [ + 01 ] kamath [ kam09 ] issue theoretical foundation datum mining address many researcher example mannila present summary study foundation datum mining [ man00 ] datum reduction view datum mining summarize new jersey datum reduction report barbará dumouchel faloutos et al [ + 97 ] datum compression view find study minimum description length principle grunwald rissanen [ gr07 ] pattern discovery point view datum mining address numerous machine learn datum mining study range association mining decision tree induction sequential pattern mining cluster probability theory point view popular statistic machine learn literature 
138 bibliographic note 631 bayesian network hierarchical bayesian model chapter 9 probabilistic graph model ( eg koller friedman [ kf09 ] ) kleinberg papadimitriou raghavan [ kpr98 ] present microeconomic view treat datum mining optimization problem study inductive database view include imielinski mannila [ im96 ] de raedt gun nijssen [ rgn10 ] statistical method datum analysis describe many book hastie tibshirani friedman [ htf09 ] freedman pisani purf [ fpp07 ] devore [ dev03 ] kutner nachtsheim neter li [ knnl04 ] dobson [ dob01 ] breiman friedman olshen stone [ bfos84 ] pinheiro bate [ pb00 ] johnson wichern [ jw02b ] huberty [ hub94 ] shumway stoffer [ ss05 ] miller [ mil98 ] visual datum mining popular book visual display datum information include tufte [ tuf90 tuf97 tuf01 ] summary technique visualize datum present cleveland [ cle93 ] dedicate visual datum mining book visual datum mining technique tool datum visualization mining soukup davidson [ sd02 ] book information visualization datum mining knowledge discovery edit fayyad grinstein wierse [ fgw01 ] contain collection article visual datum mining method ubiquitous invisible datum mining discuss many text include john [ joh99 ] article book edit kargupta joshi sivakumar yesha [ kjsy04 ] book business @ speed thought succeed digital economy gate [ gat00 ] discuss e-commerce customer relationship management provide interesting perspective datum mining future mena [ men03 ] informative book use datum mining detect prevent crime cover many form criminal activity range fraud detection money launder insurance crime identity crime intrusion detection datum mining issue regard privacy datum security address popularly literature book privacy security datum mining include thuraisingham [ thu04 ] aggarwal yu [ ay08 ] vaidya clifton zhu [ vcz10 ] fung wang fu yu [ fwfy10 ] research article include agrawal srikant [ as00 ] evfimievski srikant agrawal gehrke [ esag02 ] vaidya clifton [ vc03 ] differential privacy introduce dwork [ dwo06 ] study many hay rastogi miklau suciu [ hrms10 ] many discussion trend research direction datum mining various forum several book collection article issue kargupta han yu et al [ + 08 ] 

cluster 
user insight interaction cluster  visual insight one picture worth thousand word  human eye high-speed processor link rich knowledge-base  human provide intuitive insight hd-eye visualize hd cluster  semi-supervised insight pass user ’ insight intention system  user-see user provide number labele example approximately represent category interest  multi-view ensemble-based insight  multi-view cluster multiple clustering represent different perspective  multiple cluster result ensembled provide robust solution  validation-based insight evaluation quality cluster generate  2 may use case study specific measure pre-existing label 
recommend reading  major reference book cluster analysis  jiawei han micheline kamber jian pei datum mining concept technique morgan kaufmann 3rd ed 2011 ( chapter 10 & 11 )  charu aggarwal chandran k reddy ( ) datum cluster algorithms application crc press 2014  moham j zaki wagner meira datum mining analysis fundamental concept algorithms cambridge university press 2014  reference paper lecture  3 charu aggarwal introduction cluster analysis aggarwal reddy ( ) datum cluster algorithms application ( chapter 1 ) crc press 2014 

two vector cosine similarity 
cosine similarity two vector  document represent bag term long vector attribute record frequency particular term ( word keyword phrase ) document  vector object gene feature micro-array  application information retrieval biologic taxonomy gene feature mapping etc  cosine measure d1 d2 two vector ( eg term-frequency vector ) d1 • 2 co ( d1 2 ) = | d1 | × | 2 | • indicate vector dot product | length vector 2 
example calculate cosine similarity calculate cosine similarity • 1 2 co ( d1 2 ) = | d1 | × | 2 | • indicate vector dot product | length vector  ex find similarity document 1 2 d2 = ( 3 0 2 0 1 1 0 1 0 1 ) d1 = ( 5 0 3 0 2 0 0 2 0 0 )  first calculate vector dot product d1•d2 = 5 x 3 + 0 x 0 + 3 x 2 + 0 x 0 + 2 x 1 + 0 x 1 + 0 x 1 + 2 x 1 + 0 x 0 + 0 x 1 = 25  calculate | |  | d1 = 5× 5 + 0 × 0 + 3× 3 + 0 × 0 + 2 × 2 + 0 × 0 + 0 × 0 + 2 × 2 + 0 × 0 + 0 × 0 = 6481 | 2 = 3 × 3 + 0 × 0 + 2 × 2 + 0 × 0 + 1× 1 + 1× 1 + 0 × 0 + 1×1 + 0 × 0 + 1×1 = 412  3 calculate cosine similarity co ( d1 d2 ) = ( 6481 x 412 ) = 094 

10 9 8 7 6 5 4 3 2 1 
author jiawei han bliss professor engineering department computer science university illinois urbana-champaign receive numerous award contribution research knowledge discovery datum mining include acm sigkdd innovation award ( 2004 ) ieee computer society technical achievement award ( 2005 ) ieee w wallace mcdowell award ( 2009 ) fellow acm ieee serve founding editor-in-chief acm transaction knowledge discovery datum ( 2006–2011 ) editorial board member several journal include ieee transaction knowledge datum engineering datum mining knowledge discovery micheline kamber master ’ degree computer science ( specialize artificial intelligence ) concordium university montreal quebec nserc scholar work researcher mcgill university simon fraser university switzerland background datum mining passion writing easyto-understand term help make text favorite professional instructor student jian pei currently associate professor school compute science simon fraser university british columbia receive degree compute science simon fraser university 2002 dr jiawei han ’ supervision publish prolifically premier academic forum datum mining databasis web search information retrieval actively serve academic community publication receive thousand citation several prestigious award associate editor several datum mining datum analytic journal xxxv 
2 get know datum ’ tempting jump straight mining first need get datum ready involve closer look attribute datum value real-world datum typically noisy enormous volume ( often several gigabyte ) may originate hodgepodge heterogenous source chapter get familiar datum knowledge datum useful datum preprocess ( see chapter 3 ) first major task datum mining process want know follow type attribute field make datum kind value attribute attribute discrete continuous-valu datum look like value distribute way visualize datum get better sense spot outlier measure similarity datum object respect other gain insight datum help subsequent analysis “ learn datum ’ helpful datum preprocess ” begin section 21 study various attribute type include nominal attribute binary attribute ordinal attribute numeric attribute basic statistical description used learn attribute ’ value describe section 22 give temperature attribute example determine mean ( average value ) median ( middle value ) mode ( common value ) measure central tendency give us idea “ middle ” center distribution know basic statistic regard attribute make easier fill miss value smooth noisy value spot outlier datum preprocess knowledge attribute attribute value also help fix inconsistency incur datum integration plot measure central tendency show us datum symmetric skewer quantile plot histogram scatter plot graphic display basic statistical description useful datum preprocess provide insight area mining field datum visualization provide many additional technique view datum graphical mean help identify relation trend biase “ hide ” unstructured datum set technique may simple scatter-plot matrix ( datum mining concept technique doi b978-0-12-381479-100002-2 c 2012 elsevier right re-serve 39 
40 chapter 2 get know datum two attribute map onto 2-d grid ) sophisticated method treemaps ( hierarchical partition screen display base attribute value ) datum visualization technique describe section 23 finally may want examine similar ( dissimilar ) datum object example suppose database datum object patient describe symptom may want find similarity dissimilarity individual patient information allow us find cluster like patient within datum set dissimilarity object may also used detect outlier datum perform nearest-neighbor classification ( cluster topic chapter 10 11 nearest-neighbor classification discuss chapter 9 ) many measure assess similarity dissimilarity general measure refer proximity measure think proximity two object function distance attribute value although proximity also calculate base probability rather actual distance measure datum proximity describe section 24 summary end chapter know different attribute type basic statistical measure describe central tendency dispersion ( spread ) attribute datum also know technique visualize attribute distribution compute similarity dissimilarity object 21 datum object attribute type datum set make datum object datum object represent entity—in sale database object may customer store item sale medical database object may patient university database object may student professor course datum object typically describe attribute datum object also refer sample example instance datum point object datum object store database datum tuple row database correspond datum object column correspond attribute section define attribute look various attribute type 211 attribute attribute datum field represent characteristic feature datum object noun attribute dimension feature variable often used interchangeably literature term dimension commonly used datum warehousing machine learn literature tend use term feature statistician prefer term variable datum mining database professional commonly use term attribute well attribute describe customer object include example customer id name address observed value give attribute know observation set attribute used describe give object call attribute vector ( feature vector ) distribution datum involve one attribute ( variable ) call univariate bivariate distribution involve two attribute 
21 datum object attribute type 41 type attribute determine set possible values—nominal binary ordinal numeric—the attribute follow subsection introduce type 212 nominal attribute nominal mean “ relate ” value nominal attribute symbol name thing value represent kind category code state nominal attribute also refer categorical value meaningful order computer science value also know enumeration example 21 nominal attribute suppose hair color marital status two attribute describe person object application possible value hair color black brown blond red auburn gray white attribute marital status take value single married divorce widow hair color marital status nominal attribute another example nominal attribute occupation value teacher dentist programmer farmer although say value nominal attribute symbol “ name thing ” possible represent symbol “ name ” number hair color instance assign code 0 black 1 brown another example customor id possible value numeric however case number intend used quantitatively mathematical operation value nominal attribute meaningful make sense subtract one customer id number another unlike say subtract age value another ( age numeric attribute ) even though nominal attribute may integer value consider numeric attribute integer meant used quantitatively say numeric attribute section 215 nominal attribute value meaningful order quantitative make sense find mean ( average ) value median ( middle ) value attribute give set object one thing interest however attribute ’ commonly occur value value know mode one measure central tendency learn measure central tendency section 22 213 binary attribute binary attribute nominal attribute two category state 0 1 0 typically mean attribute absent 1 mean present binary attribute refer boolean two state correspond true false example 22 binary attribute give attribute smoker describe patient object 1 indicate patient smoke 0 indicate patient similarly suppose 
42 chapter 2 get know datum patient undergo medical test two possible outcome attribute medical test binary value 1 mean result test patient positive 0 mean result negative binary attribute symmetric state equally valuable carry weight preference outcome code 0 one example can attribute gender state male female binary attribute asymmetric outcome state equally important positive negative outcome medical test hiv convention code important outcome usually rarest one 1 ( eg hiv positive ) 0 ( eg hiv negative ) 214 ordinal attribute ordinal attribute attribute possible value meaningful order ranking among magnitude successive value know example 23 ordinal attribute suppose drink size correspond size drink available fast-food restaurant nominal attribute three possible value small medium large value meaningful sequence ( correspond increase drink size ) however tell value much bigger say medium large example ordinal attribute include grade ( eg + a− + ) professional rank professional rank enumerate sequential order example assistant associate full professor private private first class specialist corporal sergeant army rank ordinal attribute useful register subjective assessment quality measure objectively thus ordinal attribute often used survey rating one survey participant ask rate satisfied customer customer satisfaction follow ordinal category 0 dissatisfied 1 somewhat dissatisfied 2 neutral 3 satisfied 4 satisfied ordinal attribute may also obtain discretization numeric quantity splitting value range finite number order category describe chapter 3 datum reduction central tendency ordinal attribute represent mode median ( middle value order sequence ) mean defined note nominal binary ordinal attribute qualitative describe feature object without give actual size quantity value qualitative attribute typically word represent category integer used represent computer code category opposed measurable quantity ( eg 0 small drink size 1 medium 2 large ) follow subsection look numeric attribute provide quantitative measurement object 
21 datum object attribute type 215 43 numeric attribute numeric attribute quantitative measurable quantity represent integer real value numeric attribute interval-scaled ratio-scale interval-scaled attribute interval-scaled attribute measure scale equal-size unit value interval-scaled attribute order positive 0 negative thus addition provide ranking value attribute allow us compare quantify difference value example 24 interval-scaled attribute temperature attribute interval-scaled suppose outdoor temperature value number different day day object order value obtain ranking object respect temperature addition quantify difference value example temperature 20◦ c five degree higher temperature 15◦ c calendar date another example instance year 2002 2010 eight year apart temperature celsius fahrenheit true zero-point neither 0◦ c 0◦ f indicate “ ” ( celsius scale example unit measurement 100 difference melt temperature boil temperature water atmospheric pressure ) although compute difference temperature value talk one temperature value multiple another without true zero say instance 10◦ c twice warm 5◦ c speak value term ratio similarly true zero-point calendar date ( year 0 correspond begin time ) bring us ratio-scale attribute true zero-point exit interval-scaled attribute numeric compute mean value addition median mode measure central tendency ratio-scale attribute ratio-scale attribute numeric attribute inherent zero-point measurement ratio-scale speak value multiple ( ratio ) another value addition value order also compute difference value well mean median mode example 25 ratio-scale attribute unlike temperature celsius fahrenheit kelvin ( k ) temperature scale consider true zero-point ( 0◦ k = −27315◦ c ) point particle comprise matter zero kinetic energy example ratio-scale attribute include count attribute year experience ( eg object employee ) number word ( eg object document ) additional example include attribute measure weight height latitude longitude 
44 chapter 2 get know datum coordinate ( eg cluster house ) monetary quantity ( eg 100 time richer $ 100 $ 1 ) 216 discrete versus continuous attribute presentation organized attribute nominal binary ordinal numeric type many way organize attribute type type mutually exclusive classification algorithms develop field machine learn often talk attribute either discrete continuous type may processed differently discrete attribute finite countably infinite set value may may represent integer attribute hair color smoker medical test drink size finite number value discrete note discrete attribute may numeric value 0 1 binary attribute value 0 110 attribute age attribute countably infinite set possible value infinite value put one-to-one correspondence natural number example attribute customer id countably infinite number customer grow infinity reality actual set value countable ( value put one-to-one correspondence set integer ) zip code another example attribute discrete continuous term numeric attribute continuous attribute often used interchangeably literature ( confuse classic sense continuous value real number whereas numeric value either integer real number ) practice real value represent used finite number digit continuous attribute typically represent floating-point variable 22 basic statistical description datum datum preprocess successful essential overall picture datum basic statistical description used identify property datum highlight datum value treat noise outlier section discuss three area basic statistical description start measure central tendency ( section 221 ) measure location middle center datum distribution intuitively speaking give attribute value fall particular discuss mean median mode midrange addition assess central tendency datum set also would like idea dispersion datum datum spread common datum dispersion measure range quartile interquartile range five-number summary boxplot variance standard deviation datum measure useful identify outlier describe section 222 finally use many graphic display basic statistical description visually inspect datum ( section 223 ) statistical graphical datum presentation software 
22 basic statistical description datum 45 package include bar chart pie chart line graph popular display datum summary distribution include quantile plot quantile–quantile plot histogram scatter plot 221 measure central tendency mean median mode section look various way measure central tendency datum suppose attribute x like salary record set object let x1 x2 xn set n observed value observation x value may also refer datum set ( x ) plot observation salary would value fall give us idea central tendency datum measure central tendency include mean median mode midrange common effective numeric measure “ center ” set datum ( arithmetic ) mean let x1 x2 xn set n value observation numeric attribute x like salary mean set value n x x̄ = xi i=1 n = x1 + x2 + · · · + xn n ( 21 ) correspond built-in aggregate function average ( avg ( ) sql ) provide relational database system example 26 mean suppose follow value salary ( thousand dollar ) show increase order 30 36 47 50 52 52 56 60 63 70 70 used eq ( 21 ) 30 + 36 + 47 + 50 + 52 + 52 + 56 + 60 + 63 + 70 + 70 + 110 12 696 = = 58 12 x̄ = thus mean salary $ 58000 sometimes value xi set may associate weight wi = 1 n weight reflect significance importance occurrence frequency attach respective value case compute n x x̄ = wi xi i=1 n x = w1 x1 + w2 x2 + · · · + wn xn w1 + w2 + · · · + wn wi i=1 call weight arithmetic mean weight average ( 22 ) 
46 chapter 2 get know datum although mean singlemost useful quantity describe datum set always best way measure center datum major problem mean sensitivity extreme ( eg outlier ) value even small number extreme value corrupt mean example mean salary company may substantially push highly paid manager similarly mean score class exam can pull quite bit low score offset effect cause small number extreme value instead use trim mean mean obtain chop value high low extreme example sort value observed salary remove top bottom 2 % compute mean avoid trimming large portion ( 20 % ) end result loss valuable information skewer ( asymmetric ) datum better measure center datum median middle value set order datum value value separate higher half datum set lower half probability statistic median generally apply numeric datum however may extend concept ordinal datum suppose give datum set n value attribute x sort increase order n odd median middle value order set n even median unique two middlemost value value x numeric attribute case convention median take average two middlemost value example 27 median let ’ find median datum example datum already sort increase order even number observation ( ie 12 ) therefore median unique value within two middlemost value 52 56 ( within sixth seventh value list ) convention assign = 108 average two middlemost value median 52+56 2 2 = thus median $ 54000 suppose first 11 value list give odd number value median middlemost value sixth value list value $ 52000 median expensive compute large number observation numeric attribute however easily approximate value assume datum group interval accord xi datum value frequency ( ie number datum value ) interval know example employee may group accord annual salary interval $ 10–20000 $ 20–30000 let interval contain median frequency median interval approximate median entire datum set ( eg median salary ) interpolation used formula  p n 2 − freq l median = l1 + width ( 23 ) freqmedian l1 lower median interval n number value  pboundary entire datum set freq l sum frequency interval 
22 basic statistical description datum 47 lower median interval freqmedian frequency median interval width width median interval mode another measure central tendency mode set datum value occur frequently set therefore determine qualitative quantitative attribute possible greatest frequency correspond several different value result one mode datum set one two three mode respectively call unimodal bimodal trimodal general datum set two mode multimodal extreme datum value occur mode example 28 mode datum example 26 bimodal two mode $ 52000 $ 70000 unimodal numeric datum moderately skewer ( asymmetrical ) follow empirical relation mean − mode ≈ 3 × ( mean − median ) ( 24 ) imply mode unimodal frequency curf moderately skewer easily approximate mean median value know midrange also used assess central tendency numeric datum set average largest smallest value set measure easy compute used sql aggregate function max ( ) min ( ) example 29 midrange midrange datum example 26 30000+110000 2 = $ 70000 unimodal frequency curve perfect symmetric datum distribution mean median mode center value show figure 21 ( ) datum real application symmetric may instead either positively skewer mode occur value smaller median ( figure 21b ) negatively skewer mode occur value greater median ( figure 21c ) mean median mode mode mean median ( ) symmetric datum ( b ) positively skewer datum mean mode median ( c ) negatively skewer datum figure 21 mean median mode symmetric versus positively negatively skewer datum 
48 chapter 2 get know datum 222 measure dispersion datum range quartile variance standard deviation interquartile range look measure assess dispersion spread numeric datum measure include range quantile quartile percentile interquartile range five-number summary display boxplot useful identify outlier variance standard deviation also indicate spread datum distribution range quartile interquartile range start let ’ study range quantile quartile percentile interquartile range measure datum dispersion let x1 x2 xn set observation numeric attribute x range set difference largest ( max ( ) ) smallest ( min ( ) ) value suppose datum attribute x sort increase numeric order imagine pick certain datum point split datum distribution equal-size consecutive set figure datum point call quantile quantile point take regular interval datum distribution divide essentially equalsize consecutive set ( say “ essentially ” may datum value x divide datum exactly equal-sized subset readability refer equal ) kth q-quantile give datum distribution value x q datum value less x ( q − k ) q datum value x k integer 0 < k < q q − 1 q-quantile 2-quantile datum point divide lower upper half datum distribution correspond median 4-quantiles three datum point split datum distribution four equal part part represent one-fourth datum distribution commonly refer quartile 100-quantile commonly refer percentile divide datum distribution 100 equal-sized consecutive set median quartile percentile widely used form quantile 25 % q1 q2 q3 median 75th 25th percentile percentile figure 22 plot datum distribution attribute x quantile plot quartile three quartile divide distribution four equal-size consecutive subset second quartile correspond median 
22 basic statistical description datum 49 quartile give indication distribution ’ center spread shape first quartile denote q1 25th percentile cut lowest 25 % datum third quartile denote q3 75th percentile—it cut lowest 75 % ( highest 25 % ) datum second quartile 50th percentile median give center datum distribution distance first third quartile simple measure spread give range cover middle half datum distance call interquartile range ( iqr ) defined iqr = q3 − q1 ( 25 ) example 210 interquartile range quartile three value split sort datum set four equal part datum example 26 contain 12 observation already sort increase order thus quartile datum third sixth ninth value respectively sort list therefore q1 = $ 47000 q3 $ 63000 thus interquartile range iqr = 63 − 47 = $ 16000 ( note sixth value median $ 52000 although datum set two median since number datum value even ) five-number summary boxplot outlier single numeric measure spread ( eg iqr ) useful describe skewer distribution look symmetric skewer datum distribution figure 21 symmetric distribution median ( measure central tendency ) split datum equal-size half occur skewer distribution therefore informative also provide two quartile q1 q3 along median common rule thumb identify suspect outlier single value fall least 15 × iqr third quartile first quartile q1 median q3 together contain information endpoint ( eg tail ) datum fuller summary shape distribution obtain provide lowest highest datum value well know five-number summary five-number summary distribution consist median ( q2 ) quartile q1 q3 smallest largest individual observation written order minimum q1 median q3 maximum boxplot popular way visualize distribution boxplot incorporate five-number summary follow typically end box quartile box length interquartile range median marked line within box two line ( call whisker ) outside box extend smallest ( minimum ) largest ( maximum ) observation 
50 chapter 2 get know datum 220 200 180 160 unit price ( $ ) 140 120 100 80 60 40 20 branch 1 branch 2 branch 3 branch 4 figure 23 boxplot unit price datum item sell four branch allelectronic give time period deal moderate number observation worthwhile plot potential outlier individually boxplot whisker extend extreme low high observation value less 15 × iqr beyond quartile otherwise whisker terminate extreme observation occur within 15 × iqr quartile remain case plot individually boxplot used comparison several set compatible datum example 211 boxplot figure 23 show boxplot unit price datum item sell four branch allelectronic give time period branch 1 see median price item sell $ 80 q1 $ 60 q3 $ 100 notice two outlying observation branch plot individually value 175 202 15 time iqr 40 boxplot compute ( n log n ) time approximate boxplot compute linear sublinear time depend quality guarantee require variance standard deviation variance standard deviation measure datum dispersion indicate spread datum distribution low standard deviation mean datum observation tend close mean high standard deviation indicate datum spread large range value 
22 basic statistical description datum variance n observation x1 x2 xn numeric attribute x n n x x 1 1 σ2 = ( xi − x̄ ) 2 = xi2 − x̄ 2 n n i=1 51 ( 26 ) i=1 x̄ mean value observation defined eq ( 21 ) standard deviation σ observation square root variance σ 2 example 212 variance standard deviation example 26 find x̄ = $ 58000 used eq ( 21 ) mean determine variance standard deviation datum example set n = 12 use eq ( 26 ) obtain 1 ( 302 + 362 + 472 + 1102 ) − 582 12 ≈ 37917 √ σ ≈ 37917 ≈ 1947 σ2 = basic property standard deviation σ measure spread follow σ measure spread mean consider mean choose measure center σ = 0 spread observation value otherwise σ > 0 importantly observation unlikely several standard deviation away mathematically used chebyshev ’ inequality show  mean  least 1 − k12 × 100 % observation k standard deviation mean therefore standard deviation good indicator spread datum set computation variance standard deviation scalable large databasis 223 graphic display basic statistical description datum section study graphic display basic statistical description include quantile plot quantile–quantile plot histogram scatter plot graph helpful visual inspection datum useful datum preprocess first three show univariate distribution ( ie datum one attribute ) scatter plot show bivariate distribution ( ie involve two attribute ) quantile plot follow subsection cover common graphic display datum distribution quantile plot simple effective way first look univariate datum distribution first display datum give attribute ( allow user 
52 chapter 2 get know datum assess overall behavior unusual occurrence ) second plot quantile information ( see section 222 ) let xi = 1 n datum sort increase order x1 smallest observation xn largest ordinal numeric attribute x observation xi pair percentage fi indicate approximately fi × 100 % datum value xi say “ approximately ” may value exactly fraction fi datum xi note 025 percentile correspond quartile q1 050 percentile median 075 percentile q3 let fi = − 05 n ( 27 ) 1 number increase equal step n range 2n ( slightly 1 0 ) 1 − 2n ( slightly 1 ) quantile plot xi graph fi allow us compare different distribution base quantile example give quantile plot sale datum two different time period compare q1 median q3 fi value glance example 213 quantile plot figure 24 show quantile plot unit price datum table 21 quantile–quantile plot unit price ( $ ) quantile–quantile plot q-q plot graph quantile one univariate distribution corresponding quantile another powerful visualization tool allow user view whether shift go one distribution another suppose two set observation attribute variable unit price take two different branch location let x1 xn datum first branch y1 ym datum second datum set sort increase order = n ( ie number point set ) simply plot yi xi yi xi ( − 05 ) n quantile respective datum set < n ( ie second branch fewer observation first ) point q-q plot yi ( − 05 ) m quantile 140 120 100 80 60 40 20 0 000 q3 median q1 025 050 f-value 075 figure 24 quantile plot unit price datum table 21 100 
22 basic statistical description datum 53 table 21 set unit price datum item sell branch allelectronic unit price ( $ ) count item sell 40 43 47 − 74 75 78 − 115 117 120 275 300 250 − 360 515 540 − 320 270 350 branch 2 ( unit price $ ) 120 110 q3 100 median 90 80 70 q1 60 50 40 40 50 60 70 80 90 branch 1 ( unit price $ ) 100 110 120 figure 25 q-q plot unit price datum two allelectronic branch datum plot ( − 05 ) m quantile x datum computation typically involve interpolation example 214 quantile–quantile plot figure 25 show quantile–quantile plot unit price datum item sell two branch allelectronic give time period point correspond quantile datum set show unit price item sell branch 1 versus branch 2 quantile ( aid comparison straight line represent case give quantile unit price branch darker point correspond datum q1 median q3 respectively ) see example q1 unit price item sell branch 1 slightly less branch word 25 % item sell branch 1 less 
54 chapter 2 get know datum equal $ 60 25 % item sell branch 2 less equal $ 64 50th percentile ( marked median also q2 ) see 50 % item sell branch 1 less $ 78 50 % item branch 2 less $ 85 general note shift distribution branch 1 respect branch 2 unit price item sell branch 1 tend lower branch 2 histogram histogram ( frequency histogram ) least century old widely used “ histos ” mean pole mast “ gram ” mean chart histogram chart pole plot histogram graphical method summarize distribution give attribute x x nominal automobile model item type pole vertical bar draw know value x height bar indicate frequency ( ie count ) x value result graph commonly know bar chart x numeric term histogram prefer range value x partition disjoint consecutive subrange subrange refer bucket bin disjoint subset datum distribution x range bucket know width typically bucket equal width example price attribute value range $ 1 $ 200 ( round nearest dollar ) partition subrange 1 20 21 40 41 60 subrange bar draw height represent total count item observed within subrange histogram partition rule discuss chapter 3 datum reduction example 215 histogram figure 26 show histogram datum set table 21 bucket ( bin ) defined equal-width range represent $ 20 increment frequency count item sell although histogram widely used may effective quantile plot q-q plot boxplot method compare group univariate observation scatter plot datum correlation scatter plot one effective graphical method determine appear relationship pattern trend two numeric attribute construct scatter plot pair value treat pair coordinate algebraic sense plot point plane figure 27 show scatter plot set datum table 21 scatter plot useful method provide first look bivariate datum see cluster point outlier explore possibility correlation relationship two attribute x correlated one attribute imply correlation positive negative null ( uncorrelated ) figure 28 show example positive negative correlation two attribute plot point pattern slope 
22 basic statistical description datum 55 6000 count item sell 5000 4000 3000 2000 1000 0 40–59 60–79 80–99 unit price ( $ ) 100–119 120–139 figure 26 histogram table 21 datum set 700 item sell 600 500 400 300 200 100 0 0 20 40 60 80 unit price ( $ ) 100 120 140 figure 27 scatter plot table 21 datum set ( ) ( b ) figure 28 scatter plot used find ( ) positive ( b ) negative correlation attribute 
56 chapter 2 get know datum figure 29 three case observed correlation two plot attribute datum set lower left upper right mean value x increase value increase suggest positive correlation ( figure 28a ) pattern plot point slope upper left lower right value x increase value decrease suggest negative correlation ( figure 28b ) line best fit draw study correlation variable statistical test correlation give chapter 3 datum integration ( eq ( 33 ) ) figure 29 show three case correlation relationship two attribute give datum set section 232 show scatter plot extend n attribute result scatter-plot matrix conclusion basic datum description ( eg measure central tendency measure dispersion ) graphic statistical display ( eg quantile plot histogram scatter plot ) provide valuable insight overall behavior datum help identify noise outlier especially useful datum clean 23 datum visualization convey datum user effectively datum visualization aim communicate datum clearly effectively graphical representation datum visualization used extensively many applications—for example work report manage business operation tracking progress task popularly take advantage visualization technique discover datum relationship otherwise easily observable look raw datum nowadays person also use datum visualization create fun interesting graphic section briefly introduce basic concept datum visualization start multidimensional datum store relational databasis discuss several representative approach include pixel-oriented technique geometric projection technique icon-based technique hierarchical graph-based technique discuss visualization complex datum relation 
23 datum visualization 231 57 pixel-oriented visualization technique simple way visualize value dimension use pixel color pixel reflect dimension ’ value datum set dimension pixel-oriented technique create window screen one dimension dimension value record map pixel corresponding position window color pixel reflect corresponding value inside window datum value arrange global order share window global order may obtain sort datum record way ’ meaningful task hand example 216 pixel-oriented visualization allelectronic maintain customer information table consist four dimension income credit limit transaction volume age analyze correlation income attribute visualization sort customer income-ascending order use order lay customer datum four visualization window show figure pixel color choose smaller value lighter shading used pixelbased visualization easily observe follow credit limit increase income increase customer whose income middle range likely purchase allelectronic clear correlation income age pixel-oriented technique datum record also order query-dependent way example give point query sort record descend order similarity point query fill window layer datum record linear way may work well wide window first pixel row far away last pixel previous row though next global order moreover pixel next one window even though two next global order solve problem lay datum record space-filling curve ( ) income ( b ) credit_limit ( c ) transaction_volume ( ) age figure 210 pixel-oriented visualization four attribute sort customer income ascend order 
58 chapter 2 get know datum ( ) hilbert curve ( b ) gray code ( c ) z-curve figure 211 frequently used 2-d space-filling curf one datum record dim 6 dim 6 dim 5 dim 1 dim 4 dim 2 dim 3 ( ) dim 5 dim 1 dim 4 dim 2 dim 3 ( b ) figure 212 circle segment technique ( ) represent datum record circle segment ( b ) layer pixel circle segment fill window space-filling curve curve range cover entire n-dimensional unit hypercube since visualization window 2-d use 2-d space-filling curve figure 211 show frequently used 2-d space-filling curf note window rectangular example circle segment technique used window shape segment circle illustrated figure 212 technique ease comparison dimension dimension window locate side side form circle 232 geometric projection visualization technique drawback pixel-oriented visualization technique help us much understand distribution datum multidimensional space example show whether dense area multidimensional subspace geometric 
23 datum visualization 59 80 70 60 50 40 30 20 10 0 0 10 20 30 40 x 50 60 70 80 figure 213 visualization 2-d datum set used scatter plot source rareevent-geoinformatica06pdf projection technique help user find interesting projection multidimensional datum set central challenge geometric projection technique try address visualize high-dimensional space 2-d display scatter plot display 2-d datum point used cartesian coordinate third dimension add used different color shape represent different datum point figure 213 show example x two spatial attribute third dimension represent different shape visualization see point type “ + ” “ × ” tend colocate 3-d scatter plot used three axe cartesian coordinate system also used color display 4-d datum point ( figure 214 ) datum set four dimension scatter plot usually ineffective scatter-plot matrix technique useful extension scatter plot ndimensional datum set scatter-plot matrix n × n grid 2-d scatter plot provide visualization dimension every dimension figure 215 show example visualize iris datum set datum set consist 450 sample three species iris flower five dimension datum set length width sepal petal species scatter-plot matrix become less effective dimensionality increase another popular technique call parallel coordinate handle higher dimensionality visualize n-dimensional datum point parallel coordinate technique draw n equally space axe one dimension parallel one display axe 
60 chapter 2 get know datum figure 214 visualization 3-d datum set used scatter plot source http scatter plotjpg datum record represent polygonal line intersect axis point corresponding associate dimension value ( figure 216 ) major limitation parallel coordinate technique effectively show datum set many record even datum set several thousand record visual clutter overlap often reduce readability visualization make pattern hard find 233 icon-based visualization technique icon-based visualization technique use small icon represent multidimensional datum value look two popular icon-based technique chernoff face stick figure chernoff face introduce 1973 statistician herman chernoff display multidimensional datum 18 variable ( dimension ) cartoon human face ( figure 217 ) chernoff face help reveal trend datum component face eye ears mouth nose represent value dimension shape size placement orientation example dimension map follow facial characteristic eye size eye spacing nose length nose width mouth curvature mouth width mouth openness pupil size eyebrow slant eye eccentricity head eccentricity chernoff face make use ability human mind recognize small difference facial characteristic assimilate many facial characteristic 
23 datum visualization 10 30 50 70 0 10 61 20 80 70 sepal length ( mm ) 60 50 40 70 50 petal length ( mm ) 30 10 45 40 35 30 25 20 sepal width ( mm ) 25 20 15 10 5 0 petal width ( mm ) 40 50 60 70 80 iris species 20 setosa 30 versicolor 40 virginica figure 215 visualization iris datum set used scatter-plot matrix source http gsgscmatgif view large table datum tedious condense datum chernoff face make datum easier user digest way facilitate visualization regularity irregularity present datum although power relate multiple relationship limit another limitation specific datum value show furthermore facial feature vary perceive importance mean similarity two face ( represent two multidimensional datum point ) vary depend order dimension assign facial characteristic therefore mapping carefully choose eye size eyebrow slant find important asymmetrical chernoff face propose extension original technique since face vertical symmetry ( along y-axis ) left right side face identical waste space asymmetrical chernoff face double number facial characteristic thus allow 36 dimension display stick figure visualization technique map multidimensional datum five-piece stick figure figure four limb body two dimension map display ( x ) axe remain dimension map angle 
62 chapter 2 get know datum 10 5 x 0 –5 –10 ⫻1 ⫻2 ⫻3 ⫻4 ⫻5 ⫻6 ⫻7 ⫻8 ⫻9 ⫻10 figure 216 visualization used parallel coordinate source parallel coordithml figure 217 chernoff face face represent n-dimensional datum point ( n ≤ 18 ) or length limb figure 218 show census datum age income map display axe remain dimension ( gender education ) map stick figure datum item relatively dense respect two display dimension result visualization show texture pattern reflect datum trend 
23 datum visualization 63 figure 218 census datum represent used stick figure source professor g grinstein department computer science university massachusett lowell 234 hierarchical visualization technique visualization technique discuss far focus visualize multiple dimension simultaneously however large datum set high dimensionality would difficult visualize dimension time hierarchical visualization technique partition dimension subset ( ie subspace ) subspace visualize hierarchical manner “ worlds-within-world ” also know n-vision representative hierarchical visualization method suppose want visualize 6-d datum set dimension f x1 x5 want observe dimension f change respect dimension first fix value dimension x3 x4 x5 select value say c3 c4 c5 visualize f x1 x2 used 3-d plot call world show figure position origin inner world locate point ( c3 c4 c5 ) outer world another 3-d plot used dimension x3 x4 x5 user interactively change outer world location origin inner world user view result change inner world moreover user vary dimension used inner world outer world give dimension level world used method call “ worlds-withinworld ” another example hierarchical visualization method tree-maps display hierarchical datum set nest rectangle example figure 220 show tree-map visualize google news story news story organized seven category show large rectangle unique color within category ( ie rectangle top level ) news story partition smaller subcategory 
64 chapter 2 get know datum figure 219 “ worlds-within-world ” ( also know n-vision ) source http 1dipstick5gif 235 visualize complex datum relation early day visualization technique mainly numeric datum recently non-numeric datum text social network become available visualize analyze datum attract lot interest many new visualization technique dedicate kind datum example many person web tag various object picture blog entry product reviews tag cloud visualization statistic user-generated tag often tag cloud tag list alphabetically user-preferred order importance tag indicated font size color figure 221 show tag cloud visualize popular tag used web site tag cloud often used two way first tag cloud single item use size tag represent number time tag apply item different user second visualize tag statistic multiple item use size tag represent number item tag apply popularity tag addition complex datum complex relation among datum entry also raise challenge visualization example figure 222 used disease influence graph visualize correlation disease node graph disease size node proportional prevalence corresponding disease two node link edge corresponding disease strong correlation width edge proportional strength correlation pattern two corresponding disease 
24 measure datum similarity dissimilarity 65 figure 220 newsmap use tree-maps visualize google news headline story source wwwcsumd newsmappng summary visualization provide effective tool explore datum introduce several popular method essential idea behind many exist tool method moreover visualization used datum mining various aspect addition visualize datum visualization used represent datum mining process pattern obtain mining method user interaction datum visual datum mining important research development direction 24 measure datum similarity dissimilarity datum mining application cluster outlier analysis nearest-neighbor classification need way assess alike unalike object comparison one another example store may want search cluster customer object result group customer similar characteristic ( eg similar income area residence age ) information used marketing cluster 
66 chapter 2 get know datum figure 221 used tag cloud visualize popular web site tag source snapshot january 23 2010 high blood pressure ( hb ) allergy ( al ) st li overweight ( ov ) en high cholesterol level ( hc ) ki arthritis ( ar ) trouble see ( tr ) li risk diabetes ( ri ) asthma ( ) ca th diabetes ( di ) hayfever ( ha ) hc thyroid problem ( th ) di heart disease ( ) em tr ar hb cancer ( cn ) os sleep disorder ( sl ) ov eczema ( ec ) chronic bronchitis ( ch ) cn osteoporosis ( os ) prostate ( pr ) cardiovascular ( ca ) ps glaucoma ( gl ) ec pr stroke ( st ) liver condition ( li ) ch psa test abnormal ( ps ) kidney ( ki ) endometriosis ( en ) emphysema ( em ) ha al ri sl gl figure 222 disease influence graph person least 20 year old nhane datum set collection datum object object within cluster similar one another dissimilar object cluster outlier analysis also employ clustering-based technique identify potential outlier object highly dissimilar other knowledge object similarity also used nearest-neighbor classification scheme give object ( eg patient ) assign class label ( relate say diagnosis ) base similarity toward object model 
24 measure datum similarity dissimilarity 67 section present similarity dissimilarity measure refer measure proximity similarity dissimilarity related similarity measure two object j typically return value 0 object unalike higher similarity value greater similarity object ( typically value 1 indicate complete similarity object identical ) dissimilarity measure work opposite way return value 0 object ( therefore far dissimilar ) higher dissimilarity value dissimilar two object section 241 present two datum structure commonly used type application datum matrix ( used store datum object ) dissimilarity matrix ( used store dissimilarity value pair object ) also switch different notation datum object previously used chapter since deal object describe one attribute discuss object dissimilarity compute object describe nominal attribute ( section 242 ) binary attribute ( section 243 ) numeric attribute ( section 244 ) ordinal attribute ( section 245 ) combination attribute type ( section 246 ) section 247 provide similarity measure long sparse datum vector term-frequency vector represent document information retrieval know compute dissimilarity useful study attribute also reference later topic cluster ( chapter 10 11 ) outlier analysis ( chapter 12 ) nearest-neighbor classification ( chapter 9 ) 241 datum matrix versus dissimilarity matrix section 22 look way study central tendency dispersion spread observed value attribute x object one-dimensional describe single attribute section talk object describe multiple attribute therefore need change notation suppose n object ( eg person item course ) describe p attribute ( also call measurement feature age height weight gender ) object x1 = ( x11 x12 x1p ) x2 = ( x21 x22 x2p ) xij value object xi jth attribute brevity hereafter refer object xi object i object may tuple relational database also refer datum sample feature vector main memory-based cluster nearest-neighbor algorithms typically operate either follow two datum structure datum matrix ( object-by-attribute structure ) structure store n datum object form relational table n-by-p matrix ( n object ×p attribute )   x11 · · · x1f · · · x1p ··· ··· ··· ··· ···     ( 28 )  xi1 · · · xif · · · xip    ··· ··· ··· ··· ··· xn1 · · · xnf · · · xnp 
68 chapter 2 get know datum row correspond object part notation may use f index p attribute dissimilarity matrix ( object-by-object structure ) structure store collection proximity available pair n object often represent n-by-n table   0  d ( 2 1 ) 0      d ( 3 1 ) ( 3 2 ) 0 ( 29 )       ( n 1 ) ( n 2 ) · · · · · · 0 ( j ) measure dissimilarity “ difference ” object j general ( j ) non-negative number close 0 object j highly similar “ near ” become larger differ note ( ) = 0 difference object furthermore ( j ) = ( j ) ( readability show ( j ) entry matrix symmetric ) measure dissimilarity discuss throughout remainder chapter measure similarity often expressed function measure dissimilarity example nominal datum sim ( j ) = 1 − ( j ) ( 210 ) sim ( j ) similarity object j throughout rest chapter also comment measure similarity datum matrix make two entity “ thing ” namely row ( object ) column ( attribute ) therefore datum matrix often call two-mode matrix dissimilarity matrix contain one kind entity ( dissimilarity ) call one-mode matrix many cluster nearest-neighbor algorithms operate dissimilarity matrix datum form datum matrix transform dissimilarity matrix apply algorithms 242 proximity measure nominal attribute nominal attribute take two state ( section 212 ) example map color nominal attribute may say five state red yellow green pink blue let number state nominal attribute m state denote letter symbol set integer 1 2 m notice integer used datum handle represent specific order 
24 measure datum similarity dissimilarity 69 “ dissimilarity compute object describe nominal attribute ” dissimilarity two object j compute base ratio mismatch ( j ) = p−m p ( 211 ) number match ( ie number attribute j state ) p total number attribute describe object weight assign increase effect assign greater weight match attribute larger number state example 217 dissimilarity nominal attribute suppose sample datum table 22 except object-identifier attribute test-1 available test-1 nominal ( use test-2 test-3 later example ) let ’ compute dissimilarity matrix ( eq 29 )   0  d ( 2 1 ) 0      d ( 3 1 ) ( 3 2 ) 0 ( 4 1 ) ( 4 2 ) ( 4 3 ) 0 since one nominal attribute test-1 set p = 1 eq ( 211 ) ( j ) evaluate 0 object j match 1 object differ thus get  0 1   1 0  0 1 1 0 1     0 see object dissimilar except object 1 4 ( ie ( 4 1 ) = 0 ) table 22 sample datum table contain attribute mixed type object identifier test-1 ( nominal ) test-2 ( ordinal ) test-3 ( numeric ) 1 2 3 4 code code b code c code excellent fair good excellent 45 22 64 28 
70 chapter 2 get know datum alternatively similarity compute sim ( j ) = 1 − ( j ) = p ( 212 ) proximity object describe nominal attribute compute used alternative encode scheme nominal attribute encode used asymmetric binary attribute create new binary attribute state object give state value binary attribute represent state set 1 remain binary attribute set example encode nominal attribute map color binary attribute create five color previously list object color yellow yellow attribute set 1 remain four attribute set proximity measure form encode calculate used method discuss next subsection 243 proximity measure binary attribute let ’ look dissimilarity similarity measure object describe either symmetric asymmetric binary attribute recall binary attribute one two state 0 1 0 mean attribute absent 1 mean present ( section 213 ) give attribute smoker describe patient instance 1 indicate patient smoke 0 indicate patient treat binary attribute numeric mislead therefore method specific binary datum necessary compute dissimilarity “ compute dissimilarity two binary attribute ” one approach involve compute dissimilarity matrix give binary datum binary attribute thought weight 2 × 2 contingency table table 23 q number attribute equal 1 object j r number attribute equal 1 object equal 0 object j number attribute equal 0 object equal 1 object j number attribute equal 0 object j total number attribute p p = q + r + + recall symmetric binary attribute state equally valuable dissimilarity base symmetric binary attribute call symmetric binary dissimilarity object j describe symmetric binary attribute table 23 contingency table binary attribute object j object 1 0 sum 1 q q+s 0 r r t sum q+r s+t p 
24 measure datum similarity dissimilarity 71 dissimilarity j ( j ) = r s q+r s+t ( 213 ) asymmetric binary attribute two state equally important positive ( 1 ) negative ( 0 ) outcome disease test give two asymmetric binary attribute agreement two 1s ( positive match ) consider significant two 0s ( negative match ) therefore binary attribute often consider “ monary ” ( one state ) dissimilarity base attribute call asymmetric binary dissimilarity number negative match consider unimportant thus ignore follow computation ( j ) = r s q+r s ( 214 ) complementarily measure difference two binary attribute base notion similarity instead dissimilarity example asymmetric binary similarity object j compute sim ( j ) = q = 1 − ( j ) q+r s ( 215 ) coefficient sim ( j ) eq ( 215 ) call jaccard coefficient popularly reference literature symmetric asymmetric binary attribute occur datum set mixed attribute approach describe section 246 apply example 218 dissimilarity binary attribute suppose patient record table ( table 24 ) contain attribute name gender fever cough test-1 test-2 test-3 test-4 name object identifier gender symmetric attribute remain attribute asymmetric binary asymmetric attribute value let value ( yes ) p ( positive ) set 1 value n ( negative ) set suppose distance object table 24 relational table patient describe binary attribute name gender fever cough test-1 test-2 test-3 test-4 jack jim mary f n n p n p n n n n n p n n n 
72 chapter 2 get know datum ( patient ) compute base asymmetric attribute accord eq ( 214 ) distance pair three patients—jack mary jim—is ( jack jim ) = 1+1 = 067 1+1+1 ( jack mary ) = 0+1 = 033 2+0+1 ( jim mary ) = 1+2 = 075 1+1+2 measurement suggest jim mary unlikely similar disease highest dissimilarity value among three pair three patient jack mary likely similar disease 244 dissimilarity numeric datum minkowski distance section describe distance measure commonly used compute dissimilarity object describe numeric attribute measure include euclidean manhattan minkowski distance case datum normalize apply distance calculation involve transform datum fall within smaller common range [ −1 1 ] [ 00 10 ] consider height attribute example can measure either meter inch general express attribute smaller unit lead larger range attribute thus tend give attribute greater effect “ weight ” normalize datum attempt give attribute equal weight may may useful particular application method normalize datum discuss detail chapter 3 datum preprocess popular distance measure euclidean distance ( ie straight line “ crow fly ” ) let = ( xi1 xi2 xip ) j = ( xj1 xj2 xjp ) two object describe p numeric attribute euclidean distance object j defined q ( j ) = ( xi1 − xj1 ) 2 + ( xi2 − xj2 ) 2 + · · · + ( xip − xjp ) 2 ( 216 ) another well-known measure manhattan ( city block ) distance name distance block two point city ( 2 block 3 block total 5 block ) defined ( j ) = xi1 − xj1 | + xi2 − xj2 | + · · · + xip − xjp | ( 217 ) euclidean manhattan distance satisfy follow mathematical property non-negativity ( j ) ≥ 0 distance non-negative number identity indiscernible ( ) = 0 distance object 0 
24 measure datum similarity dissimilarity 73 symmetry ( j ) = ( j ) distance symmetric function triangle inequality ( j ) ≤ ( k ) + ( k j ) go directly object object j space make detour object k measure satisfy condition know metric please note non-negativity property imply three property example 219 euclidean distance manhattan distance let x1 = ( 1 2 ) x2 = ( 3 5 ) represent √ two object show figure euclidean distance two 22 + 32 = manhattan distance two 2 + 3 = 5 minkowski distance generalization euclidean manhattan distance defined q ( j ) = h xi1 − xj1 h + xi2 − xj2 h + · · · + xip − xjp h ( 218 ) h real number h ≥ 1 ( distance also call lp norm literature symbol p refer notation h keep p number attribute consistent rest chapter ) represent manhattan distance h = 1 ( ie l1 norm ) euclidean distance h = 2 ( ie l2 norm ) supremum distance ( also refer lmax l∞ norm chebyshev distance ) generalization minkowski distance h → ∞ compute find attribute f give maximum difference value two object difference supremum distance defined formally  1 h p x p h  ( j ) = lim xif − xjf | = max xif − xjf | ( 219 ) h→∞ f 1 f l∞ norm also know uniform norm x2 = ( 3 5 ) 5 4 euclidean distance = ( 22 + 32 ) 2 = 361 3 3 2 x1 = ( 1 2 ) manhattan distance 2+3=5 supremum distance 5–2=3 2 1 1 2 3 figure 223 euclidean manhattan supremum distance two object 
74 chapter 2 get know datum example 220 supremum distance let ’ use two object x1 = ( 1 2 ) x2 = ( 3 5 ) figure second attribute give greatest difference value object 5 − 2 = supremum distance object attribute assign weight accord perceive importance weight euclidean distance compute q ( j ) = w1 xi1 − xj1 2 + w2 xi2 − xj2 2 + · · · + wm xip − xjp 2 ( 220 ) weighting also apply distance measure well 245 proximity measure ordinal attribute value ordinal attribute meaningful order ranking yet magnitude successive value unknown ( section 214 ) example include sequence small medium large size attribute ordinal attribute may also obtain discretization numeric attribute splitting value range finite number category category organized rank range numeric attribute map ordinal attribute f mf state example range interval-scaled attribute temperature ( celsius ) organized follow state −30 −10 −10 10 10 30 represent category cold temperature moderate temperature warm temperature respectively let represent number possible state ordinal attribute order state define ranking 1 mf “ ordinal attribute handled ” treatment ordinal attribute quite similar numeric attribute compute dissimilarity object suppose f attribute set ordinal attribute describe n object dissimilarity computation respect f involve follow step value f ith object xif f mf order state represent ranking 1 mf replace xif corresponding rank rif ∈ { 1 mf } since ordinal attribute different number state often necessary map range attribute onto [ 00 10 ] attribute equal weight perform datum normalization replace rank rif ith object f th attribute zif = rif − 1 mf − 1 ( 221 ) dissimilarity compute used distance measure describe section 244 numeric attribute used zif represent f value ith object 
24 measure datum similarity dissimilarity 75 example 221 dissimilarity ordinal attribute suppose sample datum show earlier table 22 except time object-identifier continuous ordinal attribute test-2 available three state test-2 fair good excellent mf = step 1 replace value test-2 rank four object assign rank 3 1 2 3 respectively step 2 normalizes ranking mapping rank 1 00 rank 2 05 rank 3 step 3 use say euclidean distance ( eq 216 ) result follow dissimilarity matrix  0 10 0   05 05 0 0 10 05      0 therefore object 1 2 dissimilar object 2 4 ( ie ( 2 1 ) = 10 ( 4 2 ) = 10 ) make intuitive sense since object 1 4 excellent object 2 fair opposite end range value test-2 similarity value ordinal attribute interpreted dissimilarity sim ( j ) = 1 − ( j ) 246 dissimilarity attribute mixed type section 242 245 discuss compute dissimilarity object describe attribute type type may either nominal symmetric binary asymmetric binary numeric ordinal however many real databasis object describe mixture attribute type general database contain attribute type “ compute dissimilarity object mixed attribute type ” one approach group type attribute together perform separate datum mining ( eg cluster ) analysis type feasible analysis derive compatible result however real application unlikely separate analysis per attribute type generate compatible result preferable approach process attribute type together perform single analysis one technique combine different attribute single dissimilarity matrix bring meaningful attribute onto common scale interval [ 00 10 ] suppose datum set contain p attribute mixed type dissimilarity ( j ) object j defined ( f ) ( f ) f 1 δij dij pp ( f ) f 1 δij pp ( j ) = ( 222 ) 
76 chapter 2 get know datum ( f ) indicator δij = 0 either ( 1 ) xif xjf miss ( ie measurement attribute f object object j ) ( 2 ) xif = xjf = 0 attribute ( f ) f asymmetric binary otherwise δij = contribution attribute f ( f ) dissimilarity j ( ie dij ) compute dependent type ( f ) f numeric dij = attribute f xif −xjf | maxh xhf −minh xhf h run nonmissing object ( f ) ( f ) f nominal binary dij = 0 xif = xjf otherwise dij = 1 f ordinal compute rank rif zif = rif −1 mf −1 treat zif numeric step identical already see individual attribute type difference numeric attribute normalize value map interval [ 00 10 ] thus dissimilarity object compute even attribute describe object different type example 222 dissimilarity attribute mixed type let ’ compute dissimilarity matrix object table consider attribute different type example 217 221 work dissimilarity matrix individual attribute procedure follow test-1 ( nominal ) test-2 ( ordinal ) outlined earlier process attribute mixed type therefore use dissimilarity matrix obtain test-1 test-2 later compute eq ( 222 ) first however need compute dissimilarity matrix third attribute test-3 ( numeric ) ( 3 ) must compute dij follow case numeric attribute let maxh xh = 64 minh xh = difference two used eq ( 222 ) normalize value dissimilarity matrix result dissimilarity matrix test-3  0 055   045 040  0 100 014 0 086     0 use dissimilarity matrix three attribute computation ( f ) eq ( 222 ) indicator δij = 1 three attribute f get example ( 3 1 ) = 1 ( 1 ) 1 ( 050 ) 1 ( 045 ) 3 = result dissimilarity matrix obtain 
24 measure datum similarity dissimilarity 77 datum describe three attribute mixed type  0 085   065 013  0 083 071 0 079     0 table 22 intuitively guess object 1 4 similar base value test-1 test-2 confirm dissimilarity matrix ( 4 1 ) lowest value pair different object similarly matrix indicate object 1 2 least similar 247 cosine similarity document represent thousand attribute record frequency particular word ( keyword ) phrase document thus document object represent call term-frequency vector example table 25 see document1 contain five instance word team hockey occur three time word coach absent entire document indicated count value datum highly asymmetric term-frequency vector typically long sparse ( ie many 0 value ) application used structure include information retrieval text document cluster biological taxonomy gene feature mapping traditional distance measure study chapter work well sparse numeric datum example two term-frequency vector may many 0 value common meaning corresponding document share many word make similar need measure focus word two document common occurrence frequency word word need measure numeric datum ignore zero-match cosine similarity measure similarity used compare document say give ranking document respect give vector query word let x two vector comparison used cosine measure table 25 document vector term-frequency vector document team coach hockey baseball soccer penalty score win loss season document1 document2 document3 document4 5 3 0 0 0 0 7 1 3 2 0 0 0 0 2 0 2 1 1 1 0 1 0 2 0 0 0 2 2 1 3 0 0 0 0 3 0 1 0 0 
78 chapter 2 get know datum similarity function sim ( x ) = x·y | ( 223 ) | euclidean norm vector x = ( x1 x2 xp ) defined q x12 + x22 + · · · + xp2 conceptually length vector similarly | euclidean norm vector y measure compute cosine angle vector x y cosine value 0 mean two vector 90 degree ( orthogonal ) match closer cosine value 1 smaller angle greater match vector note cosine similarity measure obey property section 244 define metric measure refer nonmetric measure example 223 cosine similarity two term-frequency vector suppose x first two term-frequency vector table x = ( 5 0 3 0 2 0 0 2 0 0 ) = ( 3 0 2 0 1 1 0 1 0 1 ) similar x used eq ( 223 ) compute cosine similarity two vector get xt · = 5 × 3 + 0 × 0 + 3 × 2 + 0 × 0 + 2 × 1 + 0 × 1 + 0 × 0 + 2 × 1 + 0 × 0 + 0 × 1 = 25 p | = 52 + 02 + 32 + 02 + 22 + 02 + 02 + 22 + 02 + 02 = 648 p | = 32 + 02 + 22 + 02 + 12 + 12 + 02 + 12 + 02 + 12 = 412 sim ( x ) = 094 therefore used cosine similarity measure compare document would consider quite similar attribute binary-valu cosine similarity function interpreted term share feature attribute suppose object x possess ith attribute xi = xt · number attribute possessed ( ie share ) x | geometric mean number attribute possessed x number possessed y thus sim ( x ) measure relative possession common attribute simple variation cosine similarity precede scenario sim ( x ) = x·y x·x+y·y−x·y ( 224 ) ratio number attribute share x number attribute possessed x y function know tanimoto coefficient tanimoto distance frequently used information retrieval biology taxonomy 
26 exercise 25 79 summary datum set make datum object datum object represent entity datum object describe attribute attribute nominal binary ordinal numeric value nominal ( categorical ) attribute symbol name thing value represent kind category code state binary attribute nominal attribute two possible state ( 1 0 true false ) two state equally important attribute symmetric otherwise asymmetric ordinal attribute attribute possible value meaningful order ranking among magnitude successive value know numeric attribute quantitative ( ie measurable quantity ) represent integer real value numeric attribute type interval-scaled ratioscale value interval-scaled attribute measure fix equal unit ratio-scale attribute numeric attribute inherent zero-point measurement ratio-scale speak value order magnitude larger unit measurement basic statistical description provide analytical foundation datum preprocess basic statistical measure datum summarization include mean weight mean median mode measure central tendency datum range quantile quartile interquartile range variance standard deviation measure dispersion datum graphical representation ( eg boxplot quantile plot quantile– quantile plot histogram scatter plot ) facilitate visual inspection datum thus useful datum preprocess mining datum visualization technique may pixel-oriented geometric-based icon-based hierarchical method apply multidimensional relational datum additional technique propose visualization complex datum text social network measure object similarity dissimilarity used datum mining application cluster outlier analysis nearest-neighbor classification measure proximity compute attribute type study chapter combination attribute example include jaccard coefficient asymmetric binary attribute euclidean manhattan minkowski supremum distance numeric attribute application involve sparse numeric datum vector term-frequency vector cosine measure tanimoto coefficient often used assessment similarity 26 exercise 21 give three additional commonly used statistical measure already illustrated chapter characterization datum dispersion discuss compute efficiently large databasis 
80 chapter 2 get know datum 22 suppose datum analysis include attribute age age value datum tuple ( increase order ) 13 15 16 16 19 20 20 21 22 22 25 25 25 25 30 33 33 35 35 35 35 36 40 45 46 52 70 ( ) mean datum median ( b ) mode datum comment datum ’ modality ( ie bimodal trimodal etc ) ( c ) midrange datum ( ) find ( roughly ) first quartile ( q1 ) third quartile ( q3 ) datum ( e ) give five-number summary datum ( f ) show boxplot datum ( g ) quantile–quantile plot different quantile plot 23 suppose value give set datum group interval interval corresponding frequency follow age 1–5 6–15 16–20 21–50 51–80 81–110 frequency 200 450 300 1500 700 44 compute approximate median value datum 24 suppose hospital test age body fat datum 18 randomly select adult follow result age % fat 23 95 23 265 27 78 27 178 39 314 41 259 47 274 49 272 50 312 age % fat 52 346 54 425 54 288 56 334 57 302 58 341 58 329 60 412 61 357 ( ) calculate mean median standard deviation age % fat ( b ) draw boxplot age % fat ( c ) draw scatter plot q-q plot base two variable 25 briefly outline compute dissimilarity object describe follow ( ) nominal attribute ( b ) asymmetric binary attribute 
27 bibliographic note 81 ( c ) numeric attribute ( ) term-frequency vector 26 give two object represent tuple ( 22 1 42 10 ) ( 20 0 36 8 ) ( ) ( b ) ( c ) ( ) compute euclidean distance two object compute manhattan distance two object compute minkowski distance two object used q = 3 compute supremum distance two object 27 median one important holistic measure datum analysis propose several method median approximation analyze respective complexity different parameter setting decide extent real value approximate moreover suggest heuristic strategy balance accuracy complexity apply method give 28 important define select similarity measure datum analysis however commonly accept subjective similarity measure result vary depend similarity measure used nonetheless seemingly different similarity measure may equivalent transformation suppose follow 2-d datum set x1 x2 x3 x4 x5 a1 15 2 16 12 15 a2 17 19 18 15 10 ( ) consider datum 2-d datum point give new datum point x = ( 14 16 ) query rank database point base similarity query used euclidean distance manhattan distance supremum distance cosine similarity ( b ) normalize datum set make norm datum point equal use euclidean distance transform datum rank datum point 27 bibliographic note method descriptive datum summarization study statistic literature long onset computer good summary statistical descriptive datum mining method include freedman pisani purf [ fpp07 ] devore [ dev95 ] 
82 chapter 2 get know datum statistics-based visualization datum used boxplot quantile plot quantile–quantile plot scatter plot loess curf see cleveland [ cle93 ] pioneer work datum visualization technique describe visual display quantitative information [ tuf83 ] envision information [ tuf90 ] visual explanation image quantity evidence narrative [ tuf97 ] tufte addition graphic graphic information process bertin [ ber81 ] visualize datum cleveland [ cle93 ] information visualization datum mining knowledge discovery edit fayyad grinstein wierse [ fgw01 ] major conference symposium visualization include acm human factor compute system ( chi ) visualization international symposium information visualization research visualization also publish transaction visualization computer graphic journal computational graphical statistic ieee computer graphic application many graphical user interface visualization tool develop find various datum mining product several book datum mining ( eg datum mining solution westphal blaxton [ wb98 ] ) present many good example visual snapshot survey visualization technique see “ visual technique explore databasis ” keim [ kei97 ] similarity distance measure among various variable introduce many textbook study cluster analysis include hartigan [ har75 ] jain dube [ jd88 ] kaufman rousseeuw [ kr90 ] arabie hubert de soete [ ahs96 ] method combine attribute different type single dissimilarity matrix introduce kaufman rousseeuw [ kr90 ] 
10 cluster analysis basic concept method imagine director customer relationship allelectronic five manager work would like organize company ’ customer five group group assign different manager strategically would like customer group similar possible moreover two give customer different business pattern place group intention behind business strategy develop customer relationship campaign specifically target group base common feature share customer per group kind datum mining technique help accomplish task unlike classification class label ( group id ) customer unknown need discover grouping give large number customer many attribute describe customer profile costly even infeasible human study datum manually come way partition customer strategic group need cluster tool help cluster process grouping set datum object multiple group cluster object within cluster high similarity dissimilar object cluster dissimilarity similarity assessed base attribute value describe object often involve distance measures1 cluster datum mining tool root many application area biology security business intelligence web search chapter present basic concept method cluster analysis section 101 introduce topic study requirement cluster method massive amount datum various application learn several basic cluster technique organized follow category partition method ( section 102 ) hierarchical method ( section 103 ) density-based method ( section 104 ) grid-based method ( section 105 ) section 106 briefly discuss evaluate 1 datum similarity dissimilarity discuss detail section may want refer section quick review datum mining concept technique doi b978-0-12-381479-100010-1 c 2012 elsevier right re-serve 443 
444 chapter 10 cluster analysis basic concept method cluster method discussion advanced method cluster re-serve chapter 11 101 cluster analysis section set groundwork study cluster analysis section 1011 define cluster analysis present example useful section 1012 learn aspect compare cluster method well requirement cluster overview basic cluster technique present section 1013 1011 cluster analysis cluster analysis simply cluster process partition set datum object ( observation ) subset subset cluster object cluster similar one another yet dissimilar object cluster set cluster result cluster analysis refer cluster context different cluster method may generate different clustering datum set partition perform human cluster algorithm hence cluster useful lead discovery previously unknown group within datum cluster analysis widely used many application business intelligence image pattern recognition web search biology security business intelligence cluster used organize large number customer group customer within group share strong similar characteristic facilitate development business strategy enhance customer relationship management moreover consider consultant company large number project improve project management cluster apply partition project category base similarity project audit diagnosis ( improve project delivery outcome ) conduct effectively image recognition cluster used discover cluster “ subclass ” handwritten character recognition system suppose datum set handwritten digit digit labele either 1 2 3 note large variance way person write digit take number 2 example person may write small circle left bottom part other may use cluster determine subclass “ 2 ” represent variation way 2 written used multiple model base subclass improve overall recognition accuracy cluster also find many application web search example keyword search may often return large number hit ( ie page relevant search ) due extremely large number web page cluster used organize search result group present result concise easily accessible way moreover cluster technique develop cluster document topic commonly used information retrieval practice 
101 cluster analysis 445 datum mining function cluster analysis used standalone tool gain insight distribution datum observe characteristic cluster focus particular set cluster analysis alternatively may serve preprocess step algorithms characterization attribute subset selection classification would operate detected cluster select attribute feature cluster collection datum object similar one another within cluster dissimilar object cluster cluster datum object treat implicit class sense cluster sometimes call automatic classification critical difference cluster automatically find grouping distinct advantage cluster analysis cluster also call datum segmentation application cluster partition large datum set group accord similarity cluster also used outlier detection outlier ( value “ far away ” cluster ) may interesting common case application outlier detection include detection credit card fraud monitoring criminal activity electronic commerce example exceptional case credit card transaction expensive infrequent purchase may interest possible fraudulent activity outlier detection subject chapter 12 datum cluster vigorous development contribute area research include datum mining statistic machine learn spatial database technology information retrieval web search biology marketing many application area owing huge amount datum collect databasis cluster analysis recently become highly active topic datum mining research branch statistic cluster analysis extensively study main focus distance-based cluster analysis cluster analysis tool base k-mean k-medoid several method also build many statistical analysis software package system s-plus spss sas machine learn recall classification know supervised learn class label information give learn algorithm supervised tell class membership training tuple cluster know unsupervised learn class label information present reason cluster form learn observation rather learn example datum mining effort focuse find method efficient effective cluster analysis large databasis active theme research focus scalability cluster method effectiveness method cluster complex shape ( eg nonconvex ) type datum ( eg text graph image ) high-dimensional cluster technique ( eg cluster object thousand feature ) method cluster mixed numerical nominal datum large databasis 1012 requirement cluster analysis cluster challenge research field section learn requirement cluster datum mining tool well aspect used compare cluster method 
446 chapter 10 cluster analysis basic concept method follow typical requirement cluster datum mining scalability many cluster algorithms work well small datum set contain fewer several hundred datum object however large database may contain million even billion object particularly web search scenario cluster sample give large datum set may lead bias result therefore highly scalable cluster algorithms need ability deal different type attribute many algorithms design cluster numeric ( interval-based ) datum however application may require cluster datum type binary nominal ( categorical ) ordinal datum mixture datum type recently application need cluster technique complex datum type graph sequence image document discovery cluster arbitrary shape many cluster algorithms determine cluster base euclidean manhattan distance measure ( chapter 2 ) algorithms base distance measure tend find spherical cluster similar size density however cluster can shape consider sensor example often deploy environment surveillance cluster analysis sensor reading detect interesting phenomena may want use cluster find frontier run forest fire often spherical important develop algorithms detect cluster arbitrary shape requirement domain knowledge determine input parameter many cluster algorithms require user provide domain knowledge form input parameter desire number cluster consequently cluster result may sensitive parameter parameter often hard determine especially high-dimensionality datum set user yet grasp deep understand datum require specification domain knowledge burden user also make quality cluster difficult control ability deal noisy datum real-world datum set contain outlier or miss unknown erroneous datum sensor reading example often noisy—some reading may inaccurate due sense mechanism reading may erroneous due interference surround transient object cluster algorithms sensitive noise may produce poor-quality cluster therefore need cluster method robust noise incremental cluster insensitivity input order many application incremental update ( represent newer datum ) may arrive time cluster algorithms incorporate incremental update exist cluster structure instead recompute new cluster scratch cluster algorithms may also sensitive input datum order give set datum object cluster algorithms may return dramatically different clustering depend order object present incremental cluster algorithms algorithms insensitive input order need 
101 cluster analysis 447 capability cluster high-dimensionality datum datum set contain numerous dimension attribute cluster document example keyword regard dimension often thousand keyword cluster algorithms good handle low-dimensional datum datum set involve two three dimension find cluster datum object highdimensional space challenge especially consider datum sparse highly skewer constraint-based cluster real-world application may need perform cluster various kind constraint suppose job choose location give number new automatic teller machine ( atms ) city decide upon may cluster household consider constraint city ’ river highway network type number customer per cluster challenge task find datum group good cluster behavior satisfy specify constraint interpretability usability user want cluster result interpretable comprehensible usable cluster may need tie specific semantic interpretation application important study application goal may influence selection cluster feature cluster method follow orthogonal aspect cluster method compare partition criterium method object partition hierarchy exist among cluster cluster level conceptually method useful example partition customer group group manager alternatively method partition datum object hierarchically cluster form different semantic level example text mining may want organize corpus document multiple general topic “ politic ” “ sport ” may subtopic instance “ football ” “ basketball ” “ baseball ” “ hockey ” exist subtopic “ ” latter four subtopic lower level hierarchy “ sport ” separation cluster method partition datum object mutually exclusive cluster cluster customer group group take care one manager customer may belong one group situation cluster may exclusive datum object may belong one cluster example cluster document topic document may related multiple topic thus topic cluster may exclusive similarity measure method determine similarity two object distance distance defined euclidean space 
448 chapter 10 cluster analysis basic concept method road network vector space space method similarity may defined connectivity base density contiguity may rely absolute distance two object similarity measure play fundamental role design cluster method distance-based method often take advantage optimization technique - continuity-based method often find cluster arbitrary shape cluster space many cluster method search cluster within entire give datum space method useful low-dimensionality datum set highdimensional datum however many irrelevant attribute make similarity measurement unreliable consequently cluster find full space often meaningless ’ often better instead search cluster within different subspace datum set subspace cluster discover cluster subspace ( often low dimensionality ) manifest object similarity conclude cluster algorithms several requirement factor include scalability ability deal different type attribute noisy datum incremental update cluster arbitrary shape constraint interpretability usability also important addition cluster method differ respect partition level whether cluster mutually exclusive similarity measure used whether subspace cluster perform 1013 overview basic cluster method many cluster algorithms literature difficult provide crisp categorization cluster method category may overlap method may feature several category nevertheless useful present relatively organized picture cluster method general major fundamental cluster method classify follow category discuss rest chapter partition method give set n object partition method construct k partition datum partition represent cluster k ≤ n divide datum k group group must contain least one object word partition method conduct one-level partition datum set basic partition method typically adopt exclusive cluster separation object must belong exactly one group requirement may relax example fuzzy partition technique reference technique give bibliographic note ( section 109 ) partition method distance-based give k number partition construct partition method create initial partition used iterative relocation technique attempt improve partition move object one group another general criterion good partition object cluster “ close ” related whereas object different cluster “ far apart ” different various kind 
101 cluster analysis 449 criterium judge quality partition traditional partition method extend subspace cluster rather search full datum space useful many attribute datum sparse achieve global optimality partitioning-based cluster often computationally prohibitive potentially require exhaustive enumeration possible partition instead application adopt popular heuristic method greedy approach like k-mean k-medoid algorithms progressively improve cluster quality approach local optimum heuristic cluster method work well find spherical-shap cluster - medium-size databasis find cluster complex shape large datum set partitioning-based method need extend partitioning-based cluster method study depth section 102 hierarchical method hierarchical method create hierarchical decomposition give set datum object hierarchical method classify either agglomerative divisive base hierarchical decomposition form agglomerative approach also call bottom-up approach start object form separate group successively merge object group close one another group merged one ( topmost level hierarchy ) termination condition hold divisive approach also call top-down approach start object cluster successive iteration cluster split smaller cluster eventually object one cluster termination condition hold hierarchical cluster method distance-based - continuitybased various extension hierarchical method consider cluster subspace well hierarchical method suffer fact step ( merge split ) do never undo rigidity useful lead smaller computation cost worry combinatorial number different choice technique correct erroneous decision however method improve quality hierarchical cluster propose hierarchical cluster method study section 103 density-based method partition method cluster object base distance object method find spherical-shap cluster encounter difficulty discover cluster arbitrary shape cluster method develop base notion density general idea continue grow give cluster long density ( number object datum point ) “ neighborhood ” exceed threshold example datum point within give cluster neighborhood give radius contain least minimum number point method used filter noise outlier discover cluster arbitrary shape density-based method divide set object multiple exclusive cluster hierarchy cluster typically density-based method consider exclusive cluster consider fuzzy cluster moreover density-based method extend full space subspace cluster density-based cluster method study section 104 
450 chapter 10 cluster analysis basic concept method grid-based method grid-based method quantize object space finite number cell form grid structure cluster operation perform grid structure ( ie quantized space ) main advantage approach fast process time typically independent number datum object dependent number cell dimension quantized space used grid often efficient approach many spatial datum mining problem include cluster therefore grid-based method integrate cluster method density-based method hierarchical method gridbase cluster study section 105 method briefly summarize figure cluster algorithms integrate idea several cluster method sometimes difficult classify give algorithm uniquely belong one cluster method category furthermore application may cluster criterium require integration several cluster technique follow section examine cluster method detail advanced cluster method related issue discuss chapter general notation used follow let datum set n object cluster object describe variable variable also call attribute dimension method partition method general characteristic – find mutually exclusive cluster spherical shape – distance-based – may use mean medoid ( etc ) represent cluster center – effective - medium-size datum set hierarchical method – cluster hierarchical decomposition ( ie multiple level ) – correct erroneous merge split – may incorporate technique like microcluster consider object “ linkage ” density-based method – find arbitrarily shape cluster – cluster dense region object space separated low-density region – cluster density point must minimum number point within “ neighborhood ” – may filter outlier grid-based method – use multiresolution grid datum structure – fast process time ( typically independent number datum object yet dependent grid size ) figure 101 overview cluster method discuss chapter note algorithms may combine various method 
102 partition method 451 therefore may also refer point d-dimensional object space object represent bold italic font ( eg p ) 102 partition method simplest fundamental version cluster analysis partition organize object set several exclusive group cluster keep problem specification concise assume number cluster give background knowledge parameter start point partition method formally give datum set n object k number cluster form partition algorithm organize object k partition ( k ≤ n ) partition represent cluster cluster form optimize objective partition criterion dissimilarity function base distance object within cluster “ similar ” one another “ dissimilar ” object cluster term datum set attribute section learn well-known commonly used partition methods—k-mean ( section 1021 ) k-medoid ( section 1022 ) also learn several variation classic partition method scale handle large datum set 1021 k-mean centroid-based technique suppose datum set contain n object euclidean space partition method distribute object k cluster c1 ck ci ⊂ ci ∩ cj = ∅ ( 1 ≤ j ≤ k ) objective function used assess partition quality object within cluster similar one another dissimilar object cluster objective function aim high intracluster similarity low intercluster similarity centroid-based partition technique used centroid cluster ci represent cluster conceptually centroid cluster center point centroid defined various way mean medoid object ( point ) assign cluster difference object p ∈ ci ci representative cluster measure dist ( p ci ) dist ( x ) euclidean distance two point x y quality cluster ci measure withincluster variation sum square error object ci centroid ci defined = k x x dist ( p ci ) 2 ( 101 ) i=1 p∈ci e sum square error object datum set p point space represent give object ci centroid cluster ci ( p ci multidimensional ) word object cluster distance 
452 chapter 10 cluster analysis basic concept method object cluster center square distance sum objective function try make result k cluster compact separate possible optimize within-cluster variation computationally challenge worst case would enumerate number possible partitioning exponential number cluster check within-cluster variation value show problem np-hard general euclidean space even two cluster ( ie k = 2 ) moreover problem np-hard general number cluster k even 2-d euclidean space number cluster k dimensionality space fix problem solve time ( ndk+1 log n ) n number object overcome prohibitive computational cost exact solution greedy approach often used practice prime example k-mean algorithm simple commonly used “ k-mean algorithm work ” k-mean algorithm define centroid cluster mean value point within cluster proceed follow first randomly select k object initially represent cluster mean center remain object object assign cluster similar base euclidean distance object cluster mean k-mean algorithm iteratively improve within-cluster variation cluster compute new mean used object assign cluster previous iteration object reassign used update mean new cluster center iteration continue assignment stable cluster form current round form previous round k-mean procedure summarize figure 102 algorithm k-mean k-mean algorithm partition cluster ’ center represent mean value object cluster input k number cluster datum set contain n object output set k cluster method ( 1 ) arbitrarily choose k object initial cluster center ( 2 ) repeat ( 3 ) ( ) assign object cluster object similar base mean value object cluster ( 4 ) update cluster mean calculate mean value object cluster ( 5 ) change figure 102 k-mean partition algorithm 
102 partition method 453 + + + + + ( ) initial cluster + ( b ) iterate + + + ( c ) final cluster figure 103 cluster set object used k-mean method ( b ) update cluster center reassign object accordingly ( mean cluster marked + ) example 101 cluster k-mean partition consider set object locate 2-d space depict figure 103 ( ) let k = 3 user would like object partition three cluster accord algorithm figure 102 arbitrarily choose three object three initial cluster center cluster center marked + object assign cluster base cluster center nearest distribution form silhouette encircle dot curf show figure 103 ( ) next cluster center update mean value cluster recalculate base current object cluster used new cluster center object redistribute cluster base cluster center nearest redistribution form new silhouette encircle dash curf show figure 103 ( b ) process iterate lead figure 103 ( c ) process iteratively reassigning object cluster improve partition refer iterative relocation eventually reassignment object cluster occur process terminate result cluster return cluster process k-mean method guarantee converge global optimum often terminate local optimum result may depend initial random selection cluster center ( ask give example show exercise ) obtain good result practice common run k-mean algorithm multiple time different initial cluster center time complexity k-mean algorithm ( nkt ) n total number object k number cluster number iteration normally k n n therefore method relatively scalable efficient process large datum set several variant k-mean method differ selection initial k-mean calculation dissimilarity strategy calculate cluster mean 
454 chapter 10 cluster analysis basic concept method k-mean method apply mean set object defined may case application datum nominal attribute involved k-mode method variant k-mean extend k-mean paradigm cluster nominal datum replace mean cluster mode used new dissimilarity measure deal nominal object frequency-based method update mode cluster k-mean k-mode method integrate cluster datum mixed numeric nominal value necessity user specify k number cluster advance see disadvantage study overcome difficulty however provide approximate range k value used analytical technique determine best k compare cluster result obtain different k value k-mean method suitable discover cluster nonconvex shape cluster different size moreover sensitive noise outlier datum point small number datum substantially influence mean value “ make k-mean algorithm scalable ” one approach make k-mean method efficient large datum set use good-sized set sample cluster another employ filter approach used spatial hierarchical datum index save cost compute mean third approach explore microcluster idea first group nearby object “ microcluster ” perform k-mean cluster microcluster microcluster discuss section 103 1022 k-medoid representative object-based technique k-mean algorithm sensitive outlier object far away majority datum thus assign cluster dramatically distort mean value cluster inadvertently affect assignment object cluster effect particularly exacerbate due use squared-error function eq ( 101 ) observed example 102 example 102 drawback k-mean consider six point 1-d space value 1 2 3 8 9 10 25 respectively intuitively visual inspection may imagine point partition cluster { 1 2 3 } { 8 9 10 } point 25 exclude appear outlier would k-mean partition value apply k-mean used k = 2 eq ( 101 ) partition { { 1 2 3 } { 8 9 10 25 } } within-cluster variation ( 1 − 2 ) 2 + ( 2 − 2 ) 2 + ( 3 − 2 ) 2 + ( 8 − 13 ) 2 + ( 9 − 13 ) 2 + ( 10 − 13 ) 2 + ( 25 − 13 ) 2 = 196 give mean cluster { 1 2 3 } 2 mean { 8 9 10 25 } compare partition { { 1 2 3 8 } { 9 10 25 } } k-mean compute withincluster variation ( 1 − 35 ) 2 + ( 2 − 35 ) 2 + ( 3 − 35 ) 2 + ( 8 − 35 ) 2 + ( 9 − 1467 ) 2 + ( 10 − 1467 ) 2 + ( 25 − 1467 ) 2 = 18967 
102 partition method 455 give 35 mean cluster { 1 2 3 8 } 1467 mean cluster { 9 10 25 } latter partition lowest within-cluster variation therefore k-mean method assign value 8 cluster different contain 9 10 due outlier point moreover center second cluster 1467 substantially far member cluster “ modify k-mean algorithm diminish sensitivity outlier ” instead take mean value object cluster reference point pick actual object represent cluster used one representative object per cluster remain object assign cluster representative object similar partition method perform base principle minimize sum dissimilarity object p corresponding representative object absolute-error criterion used defined = k x x dist ( p oi ) ( 102 ) i=1 p∈ci e sum absolute error object p datum set oi representative object ci basis k-medoid method group n object k cluster minimize absolute error ( eq 102 ) k = 1 find exact median ( n2 ) time however k general positive number k-medoid problem np-hard partition around medoid ( pam ) algorithm ( see figure 105 later ) popular realization k-medoid cluster tackle problem iterative greedy way like k-mean algorithm initial representative object ( call seed ) choose arbitrarily consider whether replace representative object nonrepresentative object would improve cluster quality possible replacement try iterative process replace representative object object continue quality result cluster improve replacement quality measure cost function average dissimilarity object representative object cluster specifically let o1 ok current set representative object ( ie medoid ) determine whether nonrepresentative object denote orandom good replacement current medoid oj ( 1 ≤ j ≤ k ) calculate distance every object p closest object set { o1 oj−1 orandom oj+1 ok } use distance update cost function reassignment object { o1 oj−1 orandom oj+1 ok } simple suppose object p currently assign cluster represent medoid oj ( figure 104a b ) need reassign p different cluster oj replace orandom object p need reassign either orandom cluster represent oi ( = j ) whichever closest example figure 104 ( ) p closest oi therefore reassign oi figure 104 ( b ) however p closest orandom reassign orandom instead p currently assign cluster represent object oi = j 
456 chapter 10 cluster analysis basic concept method oi p oj orandom ( ) reassign oi oi oj p oi oj oi oj p orandom ( b ) reassign orandom ( c ) change orandom p orandom datum object cluster center swap swap ( ) reassign orandom figure 104 four case cost function k-medoid cluster object remain assign cluster represent oi long still closer oi orandom ( figure 104c ) otherwise reassign orandom ( figure 104d ) time reassignment occur difference absolute error e contribute cost function therefore cost function calculate difference absolute-error value current representative object replace nonrepresentative object total cost swap sum cost incur nonrepresentative object total cost negative oj replace swap orandom actual absolute-error e reduce total cost positive current representative object oj consider acceptable nothing change iteration “ method robust—k-mean k-medoid ” k-medoid method robust k-mean presence noise outlier medoid less influenced outlier extreme value mean however complexity iteration k-medoid algorithm ( k ( n − k ) 2 ) large value n k computation become costly much costly k-mean method method require user specify k number cluster “ scale k-medoid method ” typical k-medoid partition algorithm like pam ( figure 105 ) work effectively small datum set scale well large datum set deal larger datum set sampling-based method call clara ( cluster large application ) used instead take whole datum set consideration clara used random sample datum set pam algorithm apply compute best medoid sample ideally sample closely represent original datum set many case large sample work well create object equal probability select sample representative object ( medoid ) choose likely similar would choose whole datum set clara build clustering multiple random sample return best cluster output complexity compute medoid random sample ( ks 2 + k ( n − k ) ) size sample k number cluster n total number object clara deal larger datum set pam effectiveness clara depend sample size notice pam search best k-medoid among give datum set whereas clara search best k-medoid among select sample datum set clara find good cluster best sample medoid far best k-medoid object 
103 hierarchical method 457 algorithm k-medoid pam k-medoid algorithm partition base medoid central object input k number cluster datum set contain n object output set k cluster method ( 1 ) arbitrarily choose k object initial representative object seed ( 2 ) repeat ( 3 ) assign remain object cluster nearest representative object ( 4 ) randomly select nonrepresentative object orandom ( 5 ) compute total cost swap representative object oj orandom ( 6 ) < 0 swap oj orandom form new set k representative object ( 7 ) change figure 105 pam k-medoid partition algorithm one best k-medoid select sampling clara never find best cluster ( ask provide example demonstrate exercise ) “ might improve quality scalability clara ” recall search better medoid pam examine every object datum set every current medoid whereas clara confine candidate medoid random sample datum set randomize algorithm call claran ( cluster large application base upon randomize search ) present trade-off cost effectiveness used sample obtain cluster first randomly select k object datum set current medoid randomly select current medoid x object one current medoid replace x improve absolute-error criterion yes replacement make claran conduct randomize search l time set current medoid l step consider local optimum claran repeat randomize process time return best local optimal final result 103 hierarchical method partition method meet basic cluster requirement organize set object number exclusive group situation may want partition datum group different level hierarchy hierarchical cluster method work grouping datum object hierarchy “ tree ” cluster represent datum object form hierarchy useful datum summarization visualization example manager human resource allelectronic 
458 chapter 10 cluster analysis basic concept method may organize employee major group executive manager staff partition group smaller subgroup instance general group staff divide subgroup senior officer officer trainee group form hierarchy easily summarize characterize datum organized hierarchy used find say average salary manager officer consider handwritten character recognition another example set handwriting sample may first partition general group group correspond unique character group partition subgroup since character may written multiple substantially different way necessary hierarchical partition continue recursively desire granularity reach previous example although partition datum hierarchically assume datum hierarchical structure ( eg manager level allelectronic hierarchy staff ) use hierarchy summarize represent underlie datum compress way hierarchy particularly useful datum visualization alternatively application may believe datum bear underlie hierarchical structure want discover example hierarchical cluster may uncover hierarchy allelectronic employee structure say salary study evolution hierarchical cluster may group animal accord biological feature uncover evolutionary path hierarchy species another example grouping configuration strategic game ( eg chess checker ) hierarchical way may help develop game strategy used train player section study hierarchical cluster method section 1031 begin discussion agglomerative versus divisive hierarchical cluster organize object hierarchy used bottom-up top-down strategy respectively agglomerative method start individual object cluster iteratively merged form larger cluster conversely divisive method initially let give object form one cluster iteratively split smaller cluster hierarchical cluster method encounter difficulty regard selection merge split point decision critical group object merged split process next step operate newly generate cluster neither undo do previously perform object swap cluster thus merge split decision well choose may lead low-quality cluster moreover method scale well decision merge split need examine evaluate many object cluster promising direction improve cluster quality hierarchical method integrate hierarchical cluster cluster technique result multiple-phase ( multiphase ) cluster introduce two method namely birch chameleon birch ( section 1033 ) begin partition object hierarchically used tree structure leaf low-level nonleaf node view “ microcluster ” depend resolution scale apply 
103 hierarchical method 459 cluster algorithms perform macrocluster microcluster chameleon ( section 1034 ) explore dynamic modele hierarchical cluster several orthogonal way categorize hierarchical cluster method instance may categorize algorithmic method probabilistic method bayesian method agglomerative divisive multiphase method algorithmic meaning consider datum object deterministic compute cluster accord deterministic distance object probabilistic method use probabilistic model capture cluster measure quality cluster fitness model discuss probabilistic hierarchical cluster section bayesian method compute distribution possible clustering instead output single deterministic cluster datum set return group cluster structure probability conditional give datum bayesian method consider advanced topic discuss book 1031 agglomerative versus divisive hierarchical cluster hierarchical cluster method either agglomerative divisive depend whether hierarchical decomposition form bottom-up ( merge ) topdown ( splitting ) fashion let ’ closer look strategy agglomerative hierarchical cluster method used bottom-up strategy typically start let object form cluster iteratively merge cluster larger larger cluster object single cluster certain termination condition satisfied single cluster become hierarchy ’ root merge step find two cluster closest ( accord similarity measure ) combine two form one cluster two cluster merged per iteration cluster contain least one object agglomerative method require n iteration divisive hierarchical cluster method employ top-down strategy start place object one cluster hierarchy ’ root divide root cluster several smaller subcluster recursively partition cluster smaller one partition process continue cluster lowest level coherent enough—either contain one object object within cluster sufficiently similar either agglomerative divisive hierarchical cluster user specify desire number cluster termination condition example 103 agglomerative versus divisive hierarchical cluster figure 106 show application agne ( agglomerative nest ) agglomerative hierarchical cluster method diana ( divisive analysis ) divisive hierarchical cluster method datum set five object { b c e } initially agne agglomerative method place object cluster cluster merged step-by-step accord criterion example cluster c1 c2 may merged object c1 object c2 form minimum euclidean distance two object 
chapter 10 cluster analysis basic concept method agglomerative ( agne ) step 0 step 1 step 2 step 3 step 4 ab b abcde c cde de e step 4 step 3 step 2 step 1 divisive ( diana ) step 0 figure 106 agglomerative divisive hierarchical cluster datum object { b c e } level l=0 b c e 10 l=1 l=2 06 l=3 04 l=4 02 08 similarity scale 460 00 figure 107 dendrogram representation hierarchical cluster datum object { b c e } different cluster single-linkage approach cluster represent object cluster similarity two cluster measure similarity closest pair datum point belong different cluster cluster-merge process repeat object eventually merged form one cluster diana divisive method proceed contrast way object used form one initial cluster cluster split accord principle maximum euclidean distance closest neighboring object cluster cluster-split process repeat eventually new cluster contain single object tree structure call dendrogram commonly used represent process hierarchical cluster show object group together ( agglomerative method ) partition ( divisive method ) step-by-step figure 107 show dendrogram five object present figure 106 l = 0 show five object singleton cluster level l = 1 object b group together form 
103 hierarchical method 461 first cluster stay together subsequent level also use vertical axis show similarity scale cluster example similarity two group object { b } { c e } roughly 016 merged together form single cluster challenge divisive method partition large cluster several smaller one example 2n−1 − 1 possible way partition set n object two exclusive subset n number object n large computationally prohibitive examine possibility consequently divisive method typically used heuristic partition lead inaccurate result sake efficiency divisive method typically backtrack partition decision make cluster partition alternative partition cluster consider due challenge divisive method many agglomerative method divisive method 1032 distance measure algorithmic method whether used agglomerative method divisive method core need measure distance two cluster cluster generally set object four widely used measure distance cluster follow p − p0 | distance two object point p p0 mi mean cluster ci ni number object ci also know linkage measure minimum distance distmin ( ci cj ) = maximum distance distmax ( ci cj ) = mean distance average distance min { p − p0 | } ( 103 ) max { p − p0 | } ( 104 ) p∈ci p0 ∈cj p∈ci p0 ∈cj distmean ( ci cj ) = mi − mj | distavg ( ci cj ) = 1 ni nj x ( 105 ) p − p0 | ( 106 ) p∈ci p0 ∈cj algorithm used minimum distance dmin ( ci cj ) measure distance cluster sometimes call nearest-neighbor cluster algorithm moreover cluster process terminate distance nearest cluster exceed user-defined threshold call single-linkage algorithm view datum point node graph edge form path node cluster merge two cluster ci cj correspond add edge nearest pair node ci cj edge link cluster always go distinct cluster result graph generate tree thus agglomerative hierarchical cluster algorithm used minimum distance measure also call 
462 chapter 10 cluster analysis basic concept method minimal span tree algorithm span tree graph tree connect vertex minimal span tree one least sum edge weight algorithm used maximum distance dmax ( ci cj ) measure distance cluster sometimes call farthest-neighbor cluster algorithm cluster process terminate maximum distance nearest cluster exceed user-defined threshold call complete-linkage algorithm view datum point node graph edge link node think cluster complete subgraph edge connect node cluster distance two cluster determine distant node two cluster farthest-neighbor algorithms tend minimize increase diameter cluster iteration true cluster rather compact approximately equal size method produce high-quality cluster otherwise cluster produce meaningless previous minimum maximum measure represent two extreme measure distance cluster tend overly sensitive outlier noisy datum use mean average distance compromise minimum maximum distance overcome outlier sensitivity problem whereas mean distance simplest compute average distance advantageous handle categoric well numeric datum computation mean vector categoric datum difficult impossible define example 104 single versus complete linkage let us apply hierarchical cluster datum set figure 108 ( ) figure 108 ( b ) show dendrogram used single linkage figure 108 ( c ) show case used complete linkage edge cluster { b j h } { c g f e } omitted ease presentation example show used single linkage find hierarchical cluster defined local proximity whereas complete linkage tend find cluster opt global closeness variation four essential linkage measure discuss example measure distance two cluster distance centroid ( ie central object ) cluster 1033 birch multiphase hierarchical cluster used cluster feature tree balanced iterative reduce cluster used hierarchy ( birch ) design cluster large amount numeric datum integrate hierarchical cluster ( initial microcluster stage ) cluster method iterative partition ( later macrocluster stage ) overcome two difficulty agglomerative cluster method ( 1 ) scalability ( 2 ) inability undo do previous step birch used notion cluster feature summarize cluster cluster feature tree ( cf-tree ) represent cluster hierarchy structure help 
103 hierarchical method b c 463 e j h g f ( ) datum set b c e j h g f b c e f g h j c ( b ) cluster used single linkage b c e j h g f b h j e f g ( c ) cluster used complete linkage figure 108 hierarchical cluster used single complete linkage cluster method achieve good speed scalability large even stream databasis also make effective incremental dynamic cluster incoming object consider cluster n d-dimensional datum object point cluster feature ( cf ) cluster 3-d vector summarize information cluster object defined cf = hn ls ssi ( 107 ) p ls linear n point ( ie ni=1 xi ) ss square sum pn sum datum point ( ie i=1 xi 2 ) cluster feature essentially summary statistic give cluster used cluster feature easily derive many useful statistic cluster example cluster ’ centroid x0 radius r diameter n p x0 = i=1 n xi = ls n ( 108 ) 
464 chapter 10 cluster analysis basic concept method = = v u n ux u ( xi − x0 ) 2 u i=1 n = v ux n x n u u ( xi − xj ) 2 u i=1 j=1 n ( n − 1 ) nss − 2ls2 + nls n2 = 2nss − 2ls2 n ( n − 1 ) ( 109 ) ( 1010 ) r average distance member object centroid average pairwise distance within cluster r reflect tightness cluster around centroid summarize cluster used cluster feature avoid store detailed information individual object point instead need constant size space store cluster feature key birch efficiency space moreover cluster feature additive two disjoint cluster c1 c2 cluster feature cf1 = hn1 ls1 ss1 cf2 = hn2 ls2 ss2 respectively cluster feature cluster form merge c1 c2 simply cf1 + cf2 = hn1 + n2 ls1 + ls2 ss1 + ss2 ( 1011 ) example 105 cluster feature suppose three point ( 2 5 ) ( 3 2 ) ( 4 3 ) cluster c1 cluster feature c1 cf1 = h3 ( 2 + 3 + 4 5 + 2 + 3 ) ( 22 + 32 + 42 52 + 22 + 32 ) = h3 ( 9 10 ) ( 29 38 ) suppose c1 disjoint second cluster c2 cf2 = h3 ( 35 36 ) ( 417 440 ) cluster feature new cluster c3 form merge c1 c2 derive add cf1 cf2 cf3 = h3 + 3 ( 9 + 35 10 + 36 ) ( 29 + 417 38 + 440 ) = h6 ( 44 46 ) ( 446 478 ) cf-tree height-balanced tree store cluster feature hierarchical cluster example show figure definition nonleaf node tree descendant “ ” nonleaf node store sum cfs child thus summarize cluster information child cf-tree two parameter branch factor b threshold t branch factor specify maximum number child per nonleaf node threshold parameter specify maximum diameter subcluster store leaf node tree two parameter implicitly control result tree ’ size give limit amount main memory important consideration birch minimize time require output ( o ) birch apply multiphase cluster technique single scan datum set yield basic good cluster 
103 hierarchical method cf1 cf11 cf12 cf2 cf1k cfk 465 root level first level figure 109 cf-tree structure one additional scan optionally used improve quality primary phase phase 1 birch scan database build initial in-memory cf-tree view multilevel compression datum try preserve datum ’ inherent cluster structure phase 2 birch apply ( select ) cluster algorithm cluster leaf node cf-tree remove sparse cluster outlier group dense cluster larger one phase 1 cf-tree build dynamically object insert thus method incremental object insert closest leaf entry ( subcluster ) diameter subcluster store leaf node insertion larger threshold value leaf node possibly node split insertion new object information object pass toward root tree size cf-tree change modify threshold size memory need store cf-tree larger size main memory larger threshold value specify cf-tree rebuild rebuild process perform build new tree leaf node old tree thus process rebuild tree do without necessity reread object point similar insertion node split construction b+-tree therefore build tree datum read heuristic method introduce deal outlier improve quality cf-tree additional scan datum cf-tree build cluster algorithm typical partition algorithm used cf-tree phase 2 “ effective birch ” time complexity algorithm ( n ) n number object cluster experiment show linear scalability algorithm respect number object good quality cluster datum however since node cf-tree hold limit number entry due size cf-tree node always correspond user may consider natural cluster moreover cluster spherical shape birch perform well used notion radius diameter control boundary cluster 
466 chapter 10 cluster analysis basic concept method idea cluster feature cf-tree apply beyond birch idea borrow many other tackle problem cluster stream dynamic datum 1034 chameleon multiphase hierarchical cluster used dynamic modele chameleon hierarchical cluster algorithm used dynamic modele determine similarity pair cluster chameleon cluster similarity assessed base ( 1 ) well connect object within cluster ( 2 ) proximity cluster two cluster merged interconnectivity high close together thus chameleon depend static user-supplied model automatically adapt internal characteristic cluster merged merge process facilitate discovery natural homogeneous cluster apply datum type long similarity function specify figure 1010 illustrate chameleon work chameleon used k-nearest-neighbor graph approach construct sparse graph vertex graph represent datum object exist edge two vertex ( object ) one object among k-most similar object edge weight reflect similarity object chameleon used graph partition algorithm partition k-nearest-neighbor graph large number relatively small subcluster minimize edge cut cluster c partition subcluster ci cj minimize weight edge would cut c bisect ci cj assess absolute interconnectivity cluster ci cj chameleon used agglomerative hierarchical cluster algorithm iteratively merge subcluster base similarity determine pair similar subcluster take account interconnectivity closeness cluster specifically chameleon determine similarity pair cluster ci cj accord relative interconnectivity ri ( ci cj ) relative closeness rc ( ci cj ) relative interconnectivity ri ( ci cj ) two cluster ci cj defined absolute interconnectivity ci cj normalize respect k-nearest-neighbor graph datum set construct sparse graph partition graph final cluster merge partition figure 1010 chameleon hierarchical cluster base k-nearest neighbor dynamic modele source base karypis han kumar [ khk99 ] 
103 hierarchical method 467 internal interconnectivity two cluster ci cj ri ( ci cj ) = ec { ci cj } | 1 2 ( ecci | + eccj | ) ( 1012 ) ec { ci cj } edge cut previously defined cluster contain ci cj similarly ecci ( eccj ) minimum sum cut edge partition ci ( cj ) two roughly equal part relative closeness rc ( ci cj ) pair cluster ci cj absolute closeness ci cj normalize respect internal closeness two cluster ci cj defined rc ( ci cj ) = sec { ci cj } ci | ci cj | ec ci c | j + ci c sec cj | ( 1013 ) sec { ci cj } average weight edge connect vertex ci vertex cj sec ci ( sec cj ) average weight edge belong mincut bisector cluster ci ( cj ) chameleon show greater power discover arbitrarily shape cluster high quality several well-known algorithms birch densitybased dbscan ( section 1041 ) however process cost high-dimensional datum may require ( n2 ) time n object worst case 1035 probabilistic hierarchical cluster algorithmic hierarchical cluster method used linkage measure tend easy understand often efficient cluster commonly used many cluster analysis application however algorithmic hierarchical cluster method suffer several drawback first choose good distance measure hierarchical cluster often far trivial second apply algorithmic method datum object miss attribute value case datum partially observed ( ie attribute value object miss ) easy apply algorithmic hierarchical cluster method distance computation conduct third algorithmic hierarchical cluster method heuristic step locally search good splitting decision consequently optimization goal result cluster hierarchy unclear probabilistic hierarchical cluster aim overcome disadvantage used probabilistic model measure distance cluster one way look cluster problem regard set datum object cluster sample underlie datum generation mechanism analyze formally generative model example conduct cluster analysis set marketing survey assume survey collect sample opinion possible customer datum generation mechanism probability 
468 chapter 10 cluster analysis basic concept method distribution opinion respect different customer obtain directly completely task cluster estimate generative model accurately possible used observed datum object cluster practice assume datum generative model adopt common distribution function gaussian distribution bernoulli distribution govern parameter task learn generative model reduce find parameter value model best fit observed datum set example 106 generative model suppose give set 1-d point x = { x1 xn } cluster analysis let us assume datum point generate gaussian distribution n ( µ σ 2 ) = √ 2 1 2π σ 2 e − ( x−µ ) 2 2σ ( 1014 ) parameter µ ( mean ) σ 2 ( variance ) probability point xi ∈ x generate model ( x −µ ) 2 1 − e 2σ 2 p ( xi µ σ 2 ) = √ 2π σ 2 ( 1015 ) consequently likelihood x generate model l ( n ( µ σ 2 ) x ) = p ( x|µ σ 2 ) = n i=1 √ 1 2π σ 2 e − ( xi −µ ) 2 2σ 2 ( 1016 ) task learn generative model find parameter µ σ 2 likelihood l ( n ( µ σ 2 ) x ) maximize find n ( µ0 σ02 ) = arg max { l ( n ( µ σ 2 ) x ) } ( 1017 ) max { l ( n ( µ σ 2 ) x ) } call maximum likelihood give set object quality cluster form object measure maximum likelihood set object partition cluster c1 cm quality measure q ( { c1 cm } ) = i=1 p ( ci ) ( 1018 ) 
103 hierarchical method 469 p ( ) maximum likelihood merge two cluster cj1 cj2 cluster cj1 ∪ cj2 change quality overall cluster q ( ( { c1 cm } − { cj1 cj2 } ) ∪ { cj1 ∪ cj2 } ) − q ( { c1 cm } ) qm p ( ci ) · p ( cj1 ∪ cj2 ) = i=1 − p ( ci ) p ( cj1 ) p ( cj2 ) i=1 = i=1  p ( cj1 ∪ cj2 ) −1 p ( ci ) p ( cj1 ) p ( cj2 )  ( 1019 ) q choose merge two cluster hierarchical cluster i=1 p ( ci ) constant pair cluster therefore give cluster c1 c2 distance measure dist ( ci cj ) = − log p ( c1 ∪ c2 ) p ( c1 ) p ( c2 ) ( 1020 ) probabilistic hierarchical cluster method adopt agglomerative cluster framework use probabilistic model ( eq 1020 ) measure distance cluster upon close observation eq ( 1019 ) see merge two cluster may p ( c ∪c ) always lead improvement cluster quality p ( cj j1 ) p ( cj2j ) may less 1 2 example assume gaussian distribution function used model figure although merge cluster c1 c2 result cluster better fit gaussian distribution merge cluster c3 c4 lower cluster quality gaussian function fit merged cluster well base observation probabilistic hierarchical cluster scheme start one cluster per object merge two cluster ci cj distance negative iteration try find ci cj maximize p ( c ∪c ) p ( c ∪c ) j j log p ( ci ) p ( c iteration continue long log p ( ci ) p ( c > 0 long j ) j ) improvement cluster quality pseudocode give figure 1012 probabilistic hierarchical cluster method easy understand generally efficiency algorithmic agglomerative hierarchical cluster method fact share framework probabilistic model interpretable sometimes less flexible distance metric probabilistic model handle partially observed datum example give multidimensional datum set object miss value dimension learn gaussian model dimension independently used observed value dimension result cluster hierarchy accomplish optimization goal fitting datum select probabilistic model drawback used probabilistic hierarchical cluster output one hierarchy respect choose probabilistic model handle uncertainty cluster hierarchy give datum set may exist multiple hierarchy 
470 chapter 10 cluster analysis basic concept method c1 c2 ( ) c3 c4 ( b ) ( c ) figure 1011 merge cluster probabilistic hierarchical cluster ( ) merge cluster c1 c2 lead increase overall cluster quality merge cluster ( b ) c3 ( c ) c4 algorithm probabilistic hierarchical cluster algorithm input = { o1 } datum set contain n object output hierarchy cluster method ( 1 ) create cluster object ci = { oi } 1 ≤ ≤ n ( 2 ) = 1 n p ( c ∪c ) ( 3 ) j find pair cluster ci cj ci cj = arg maxi6=j log p ( c ) p ( c ) ( 4 ) j log p ( c ) p ( c ) > 0 merge ci cj ( 5 ) else stop p ( c ∪c ) j j figure 1012 probabilistic hierarchical cluster algorithm fit observed datum neither algorithmic approach probabilistic approach find distribution hierarchy recently bayesian tree-structure model develop handle problem bayesian sophisticated probabilistic cluster method consider advanced topic cover book 
104 density-based method 104 471 density-based method partition hierarchical method design find spherical-shap cluster difficulty find cluster arbitrary shape “ ” shape oval cluster figure give datum would likely inaccurately identify convex region noise outlier include cluster find cluster arbitrary shape alternatively model cluster dense region datum space separated sparse region main strategy behind density-based cluster method discover cluster nonspherical shape section learn basic technique density-based cluster study three representative method namely dbscan ( section 1041 ) optic ( section 1042 ) denclue ( section 1043 ) 1041 dbscan density-based cluster base connect region high density “ find dense region density-based cluster ” density object measure number object close o dbscan ( density-based spatial cluster application noise ) find core object object dense neighborhood connect core object neighborhood form dense region cluster “ dbscan quantify neighborhood object ” user-specified parameter  > 0 used specify radius neighborhood consider every object -neighborhood object space within radius  center due fix neighborhood size parameterized  density neighborhood measure simply number object neighborhood determine whether neighborhood dense dbscan used another user-specified figure 1013 cluster arbitrary shape 
472 chapter 10 cluster analysis basic concept method parameter minpt specify density threshold dense region object core object -neighborhood object contain least minpt object core object pillar dense region give set object identify core object respect give parameter  minpt cluster task therein reduce used core object neighborhood form dense region dense region cluster core object q object p say p directly density-reachable q ( respect  minpt ) p within -neighborhood q clearly object p directly density-reachable another object q q core object p -neighborhood q used directly density-reachable relation core object “ bring ” object -neighborhood dense region “ assemble large dense region used small dense region center core object ” dbscan p density-reachable q ( respect  minpt ) chain object p1 pn p1 = q pn = p pi+1 directly density-reachable pi respect  minpt 1 ≤ ≤ n pi ∈ d note density-reachability equivalence relation symmetric o1 o2 core object o1 density-reachable o2 o2 density-reachable o1 however o2 core object o1 o1 may density-reachable o2 vice versa connect core object well neighbor dense region dbscan used notion density-connectedness two object p1 p2 ∈ density-connect respect  minpt object q ∈ p1 p2 densityreachable q respect  minpt unlike density-reachability densityconnectedness equivalence relation easy show object o1 o2 o3 o1 o2 density-connect o2 o3 density-connect o1 o3 example 107 density-reachability density-connectivity consider figure 1014 give  represent radius circle say let minpt = 3 labele point p r core object -neighborhood contain least three point object q directly density-reachable m object directly density-reachable p vice versa object q ( indirectly ) density-reachable p q directly densityreachable directly density-reachable p however p densityreachable q q core object similarly r density-reachable density-reachable r thus r density-connect use closure density-connectedness find connect dense region cluster close set density-based cluster subset c ⊆ cluster ( 1 ) two object o1 o2 ∈ c o1 o2 density-connect ( 2 ) exist object ∈ c another object o0 ∈ ( − c ) o0 densityconnect 
104 density-based method 473 q p r figure 1014 density-reachability density-connectivity density-based cluster source base ester kriegel sander xu [ eksx96 ] “ dbscan find cluster ” initially object give datum set marked “ ” dbscan randomly select unvisite object p mark p “ visit ” check whether -neighborhood p contain least minpt object p marked noise point otherwise new cluster c create p object -neighborhood p add candidate set n dbscan iteratively add c object n belong cluster process object p0 n carry label “ unvisite ” dbscan mark “ visit ” check -neighborhood -neighborhood p0 least minpt object object -neighborhood p0 add n dbscan continue add object c c longer expand n empty time cluster c complete thus output find next cluster dbscan randomly select unvisite object remain one cluster process continue object visit pseudocode dbscan algorithm give figure 1015 spatial index used computational complexity dbscan ( n log n ) n number database object otherwise complexity ( n2 ) appropriate setting user-defined parameter  minpt algorithm effective find arbitrary-shap cluster 1042 optic order point identify cluster structure although dbscan cluster object give input parameter  ( maximum radius neighborhood ) minpt ( minimum number point require neighborhood core object ) encumber user responsibility select parameter value lead discovery acceptable cluster problem associate many cluster algorithms parameter setting 
474 chapter 10 cluster analysis basic concept method algorithm dbscan density-based cluster algorithm input datum set contain n object  radius parameter minpt neighborhood density threshold output set density-based cluster method ( 1 ) mark object unvisite ( 2 ) ( 3 ) randomly select unvisite object p ( 4 ) mark p visit ( 5 ) -neighborhood p least minpt object ( 6 ) create new cluster c add p c ( 7 ) let n set object -neighborhood p ( 8 ) point p0 n ( 9 ) p0 unvisite ( 10 ) mark p0 visit ( 11 ) -neighborhood p0 least minpt point add point n ( 12 ) p0 yet member cluster add p0 c ( 13 ) end ( 14 ) output c ( 15 ) else mark p noise ( 16 ) object unvisite figure 1015 dbscan algorithm usually empirically set difficult determine especially real-world highdimensional datum set algorithms sensitive parameter value slightly different setting may lead different clustering datum moreover real-world high-dimensional datum set often skewer distribution intrinsic cluster structure may well characterize single set global density parameter note density-based cluster monotonic respect neighborhood threshold dbscan fix minpt value two neighborhood threshold 1 < 2 cluster c respect 1 minpt must subset cluster c 0 respect 2 minpt mean two object density-based cluster must also cluster lower density requirement overcome difficulty used one set global parameter cluster analysis cluster analysis method call optic propose optic explicitly produce datum set cluster instead output cluster order linear list 
104 density-based method 475 object analysis represent density-based cluster structure datum object denser cluster list closer cluster order order equivalent density-based cluster obtain wide range parameter setting thus optic require user provide specific density threshold cluster order used extract basic cluster information ( eg cluster center arbitrary-shap cluster ) derive intrinsic cluster structure well provide visualization cluster construct different clustering simultaneously object processed specific order order select object density-reachable respect lowest  value cluster higher density ( lower  ) finished first base idea optic need two important piece information per object core-distance object p smallest value  0  0 neighborhood p least minpt object  0 minimum distance threshold make p core object p core object respect  minpt core-distance p undefined reachability-distance object p q minimum radius value make p density-reachable q accord definition density-reachability q core object p must neighborhood q therefore reachability-distance q p max { core-distance ( q ) dist ( p q ) } q core object respect  minpt reachability-distance p q undefined object p may directly reachable multiple core object therefore p may multiple reachability-distance respect different core object smallest reachability-distance p particular interest give shortest path p connect dense cluster example 108 core-distance reachability-distance figure 1016 illustrate concept coredistance reachability-distance suppose  = 6 mm minpt = coredistance p distance  0 p fourth closest datum object p reachability-distance q1 p core-distance p ( ie  0 = 3 mm ) greater euclidean distance p q1 reachability-distance q2 respect p euclidean distance p q2 greater core-distance p optic compute order object give database object database store core-distance suitable reachability-distance optic maintain list call orderseed generate output order object orderseed sort reachability-distance respective closest core object smallest reachability-distance object optic begin arbitrary object input database current object p retrieve -neighborhood p determine core-distance set reachability-distance undefined current object p written output 
476 chapter 10 cluster analysis basic concept method = 6 mm p = 3 mm = 6 mm  p q1 q2 core-distance p reachability-distance ( p q1 ) = = 3 mm reachability-distance ( p q2 ) = dist ( p q2 ) figure 1016 optic terminology source base ankerst breunig kriegel sander [ abks99 ] p core object optic simply move next object orderseed list ( input database orderseed empty ) p core object object q -neighborhood p optic update reachability-distance p insert q orderseed q yet processed iteration continue input fully consume orderseed empty datum set ’ cluster order represent graphically help visualize understand cluster structure datum set example figure 1017 reachability plot simple 2-d datum set present general overview datum structure cluster datum object plot cluster order ( horizontal axis ) together respective reachability-distance ( vertical axis ) three gaussian “ bump ” plot reflect three cluster datum set method also develop view cluster structure high-dimensional datum various level detail structure optic algorithm similar dbscan consequently two algorithms time complexity complexity ( n log n ) spatial index used ( n2 ) otherwise n number object 1043 denclue cluster base density distribution function density estimation core issue density-based cluster method denclue ( density-based cluster ) cluster method base set density distribution function first give background density estimation describe denclue algorithm probability statistic density estimation estimation unobservable underlie probability density function base set observed datum context density-based cluster unobservable underlie probability density function true distribution population possible object analyze observed datum set regard random sample population 
104 density-based method 477 reachability-distance undefined cluster order object figure 1017 cluster order optic source adapt ankerst breunig kriegel sander [ abks99 ] 1 2 figure 1018 subtlety density estimation dbscan optic increase neighborhood radius slightly 1 2 result much higher density dbscan optic density calculate count number object neighborhood defined radius parameter  density estimate highly sensitive radius value used example figure 1018 density change significantly radius increase small amount overcome problem kernel density estimation used nonparametric density estimation approach statistic general idea behind kernel density estimation simple treat observed object indicator 
478 chapter 10 cluster analysis basic concept method high-probability density surround region probability density point depend distance point observed object formally let x1 xn independent identically distribute sample random variable f kernel density approximation probability density function   n x − xi 1 x ( 1021 ) k fˆh ( x ) = nh h i=1 k ( ) kernel h bandwidth serve smooth parameter kernel regard function modele influence sample point within neighborhood technically kernel k ( ) isra non-negative real-valu integrable func+∞ tion satisfy two requirement −∞ k ( u ) du = 1 k ( −u ) = k ( u ) value u frequently used kernel standard gaussian function mean 0 variance 1   x − xi 1 − ( x − 2xi ) 2 2h k ( 1022 ) √ e h 2π denclue used gaussian kernel estimate density base give set object cluster point x∗ call density attractor local maximum estimate density function avoid trivial local maximum point denclue used noise threshold ξ consider density attractor x∗ fˆ ( x∗ ) ≥ ξ nontrivial density attractor center cluster object analysis assign cluster density attractor used stepwise hill-climb procedure object x hill-climb procedure start x guide gradient estimate density function density attractor x compute x0 = x xj+1 = xj + δ ∇ fˆ ( xj ) ∇ fˆ ( xj ) | ( 1023 ) δ parameter control speed convergence ∇ fˆ ( x ) = hd+2 n 1   x − xi ( x − x ) k i=1 h pn ( 1024 ) hill-climb procedure stop step k > 0 fˆ ( xk+1 ) < fˆ ( xk ) assign x density attractor x∗ = xk object x outlier noise converge hillclimb procedure local maximum x∗ fˆ ( x∗ ) < ξ cluster denclue set density attractor x set input object c object c assign density attractor x exist path every pair density attractor density ξ used multiple density attractor connect path denclue find cluster arbitrary shape 
105 grid-based method 479 denclue several advantage regard generalization several well-known cluster method single-linkage approach dbscan moreover denclue invariant noise kernel density estimation effectively reduce influence noise uniformly distribute noise input datum 105 grid-based method cluster method discuss far data-driven—they partition set object adapt distribution object embedding space alternatively grid-based cluster method take space-driven approach partition embedding space cell independent distribution input object grid-based cluster approach used multiresolution grid datum structure quantize object space finite number cell form grid structure operation cluster perform main advantage approach fast process time typically independent number datum object yet dependent number cell dimension quantized space section illustrate grid-based cluster used two typical example sting ( section 1051 ) explore statistical information store grid cell clique ( section 1052 ) represent - density-based approach subspace cluster high-dimensional datum space 1051 sting statistical information grid sting grid-based multiresolution cluster technique embedding spatial area input object divide rectangular cell space divide hierarchical recursive way several level rectangular cell correspond different level resolution form hierarchical structure cell high level partition form number cell next lower level statistical information regard attribute grid cell mean maximum minimum value precompute store statistical parameter statistical parameter useful query process datum analysis task figure 1019 show hierarchical structure sting cluster statistical parameter higher-level cell easily compute parameter lower-level cell parameter include follow attribute-independent parameter count attribute-dependent parameter mean stdev ( standard deviation ) min ( minimum ) max ( maximum ) type distribution attribute value cell follow normal uniform exponential none ( distribution unknown ) attribute select measure analysis price house object datum load database parameter count mean stdev min max bottom-level cell calculate directly datum value distribution may either assign user distribution type know 
480 chapter 10 cluster analysis basic concept method first layer ( – 1 ) st layer ith layer figure 1019 hierarchical structure sting cluster beforehand obtain hypothesis test χ 2 test type distribution higher-level cell compute base majority distribution type corresponding lower-level cell conjunction threshold filter process distribution lower-level cell disagree fail threshold test distribution type high-level cell set none “ statistical information useful query answer ” statistical parameter used top-down grid-based manner follow first layer within hierarchical structure determine query-answer process start layer typically contain small number cell cell current layer compute confidence interval ( estimate probability range ) reflect cell ’ relevancy give query irrelevant cell remove consideration process next lower level examine remain relevant cell process repeat bottom layer reach time query specification meet region relevant cell satisfy query return otherwise datum fall relevant cell retrieve processed meet query ’ requirement interesting property sting approach cluster result dbscan granularity approach 0 ( ie toward low-level datum ) word used count cell size information dense cluster identify approximately used sting therefore sting also regard density-based cluster method “ advantage sting offer cluster method ” sting offer several advantage ( 1 ) grid-based computation query-independent statistical information store cell represent summary information datum grid cell independent query ( 2 ) grid structure facilitate parallel process incremental update ( 3 ) method ’ efficiency major advantage sting go database compute statistical parameter cell hence time complexity generate cluster ( n ) n total number object generate hierarchical structure query process time 
105 grid-based method 481 ( g ) g total number grid cell lowest level usually much smaller n sting used multiresolution approach cluster analysis quality sting cluster depend granularity lowest level grid structure granularity fine cost process increase substantially however bottom level grid structure coarse may reduce quality cluster analysis moreover sting consider spatial relationship child neighboring cell construction parent cell result shape result cluster isothetic cluster boundary either horizontal vertical diagonal boundary detected may lower quality accuracy cluster despite fast process time technique 1052 clique apriori-like subspace cluster method datum object often ten attribute many may irrelevant value attribute may vary considerably factor make difficult locate cluster span entire datum space may meaningful instead search cluster within different subspace datum example consider healthinformatic application patient record contain extensive attribute describe personal information numerous symptom condition family history find nontrivial group patient even attribute strongly agree unlikely bird flu patient instance age gender job attribute may vary dramatically within wide range value thus difficult find cluster within entire datum space instead search subspace may find cluster similar patient lower-dimensional space ( eg patient similar one respect symptom like high fever cough runny nose age 3 16 ) clique ( cluster quest ) simple grid-based method find densitybased cluster subspace clique partition dimension nonoverlapping interval thereby partition entire embedding space datum object cell used density threshold identify dense cell sparse one cell dense number object map exceed density threshold main strategy behind clique identify candidate search space used monotonicity dense cell respect dimensionality base apriori property used frequent pattern association rule mining ( chapter 6 ) context cluster subspace monotonicity say follow k-dimensional cell c ( k > 1 ) least l point every ( k − 1 ) dimensional projection c cell ( k − 1 ) dimensional subspace least l point consider figure 1020 embedding datum space contain three dimension age salary vacation 2-d cell say subspace form age salary contain l point projection cell every dimension age salary respectively contain least l point clique perform cluster two step first step clique partition d-dimensional datum space nonoverlapping rectangular unit identify dense unit among clique find dense cell subspace 
482 chapter 10 cluster analysis basic concept method 7 salary ( $ 10000 ) 6 5 4 3 2 1 0 20 30 40 50 60 age 30 40 50 60 age 7 vacation ( week ) 6 5 4 3 2 1 vacation 0 20 50 age sa la ry 30 figure 1020 dense unit find respect age dimension salary vacation intersected provide candidate search space dense unit higher dimensionality 
106 evaluation cluster 483 clique partition every dimension interval identify interval contain least l point l density threshold clique iteratively join two k-dimensional dense cell c1 c2 subspace ( di1 dik ) ( dj1 djk ) respectively di1 = dj1 dik−1 = djk−1 c1 c2 share interval dimension join operation generate new ( k + 1 ) dimensional candidate cell c space ( di1 dik−1 dik djk ) clique check whether number point c pass density threshold iteration terminate candidate generate candidate cell dense second step clique used dense cell subspace assemble cluster arbitrary shape idea apply minimum description length ( mdl ) principle ( chapter 8 ) use maximal region cover connect dense cell maximal region hyperrectangle every cell fall region dense region extend dimension subspace find best description cluster general np-hard thus clique adopt simple greedy approach start arbitrary dense cell find maximal region cover cell work remain dense cell yet cover greedy method terminate dense cell cover “ effective clique ” clique automatically find subspace highest dimensionality high-density cluster exist subspace insensitive order input object presume canonical datum distribution scale linearly size input good scalability number dimension datum increase however obtain meaningful cluster dependent proper tune grid size ( stable structure ) density threshold difficult practice grid size density threshold used across combination dimension datum set thus accuracy cluster result may degraded expense method ’ simplicity moreover give dense region projection region onto lower-dimensionality subspace also dense result large overlap among report dense region furthermore difficult find cluster rather different density within different dimensional subspace several extension approach follow similar philosophy example think grid set fix bin instead used fix bin dimension use adaptive data-driven strategy dynamically determine bin dimension base datum distribution statistic alternatively instead used density threshold may use entropy ( chapter 8 ) measure quality subspace cluster 106 evaluation cluster learn cluster know several popular cluster method may ask “ try cluster method datum set evaluate whether cluster result good ” general cluster evaluation assess 
484 chapter 10 cluster analysis basic concept method feasibility cluster analysis datum set quality result generate cluster method major task cluster evaluation include follow assess cluster tendency task give datum set assess whether nonrandom structure exist datum blindly apply cluster method datum set return cluster however cluster mine may mislead cluster analysis datum set meaningful nonrandom structure datum determine number cluster datum set algorithms k-mean require number cluster datum set parameter moreover number cluster regard interesting important summary statistic datum set therefore desirable estimate number even cluster algorithm used derive detailed cluster measure cluster quality apply cluster method datum set want assess good result cluster number measure used method measure well cluster fit datum set other measure well cluster match ground truth truth available also measure score clustering thus compare two set cluster result datum set rest section discuss three topic 1061 assess cluster tendency cluster tendency assessment determine whether give datum set non-random structure may lead meaningful cluster consider datum set non-random structure set uniformly distribute point datum space even though cluster algorithm may return cluster datum cluster random meaningful example 109 cluster require nonuniform distribution datum figure 1021 show datum set uniformly distribute 2-d datum space although cluster algorithm may still artificially partition point group group unlikely mean anything significant application due uniform distribution datum “ assess cluster tendency datum set ” intuitively try measure probability datum set generate uniform datum distribution achieve used statistical test spatial randomness illustrate idea let ’ look simple yet effective statistic call hopkin statistic hopkin statistic spatial statistic test spatial randomness variable distribute space give datum set regard sample 
106 evaluation cluster 485 figure 1021 datum set uniformly distribute datum space random variable want determine far away uniformly distribute datum space calculate hopkin statistic follow sample n point p1 pn uniformly d point probability include sample point pi find nearest neighbor pi ( 1 ≤ ≤ n ) let xi distance pi nearest neighbor d xi = min { dist ( pi v ) } v∈d ( 1025 ) sample n point q1 qn uniformly d qi ( 1 ≤ ≤ n ) find nearest neighbor qi − { qi } let yi distance qi nearest neighbor − { qi } yi = min { dist ( qi v ) } v∈d v6=qi ( 1026 ) calculate hopkin statistic h pn h = pn i=1 xi i=1 yi + pn i=1 yi ( 1027 ) “ hopkin statistic tell us likely datum set follow pn uniform distribution datum space ” uniformly distribute i=1 yi pn x would close thus h would 05 however i=1 p p highly skewer ni=1 yi would substantially smaller ni=1 xi expectation thus h would close 0 
486 chapter 10 cluster analysis basic concept method null hypothesis homogeneous hypothesis—that uniformly distribute thus contain meaningful cluster nonhomogeneous hypothesis ( ie uniformly distribute thus contain cluster ) alternative hypothesis conduct hopkin statistic test iteratively used 05 threshold reject alternative hypothesis h > 05 unlikely statistically significant cluster 1062 determine number cluster determine “ right ” number cluster datum set important cluster algorithms like k-mean require parameter also appropriate number cluster control proper granularity cluster analysis regard find good balance compressibility accuracy cluster analysis consider two extreme case treat entire datum set cluster would maximize compression datum cluster analysis value hand treat object datum set cluster give finest cluster resolution ( ie accurate due zero distance object corresponding cluster center ) method like k-mean even achieve best cost however one object per cluster enable datum summarization determine number cluster far easy often “ right ” number ambiguous figure right number cluster often depend distribution ’ shape scale datum set well cluster resolution require user many possible way estimate number cluster briefly introduce simple yet popular effective method q simple method set number cluster n2 datum set n √ point expectation cluster 2n point elbow method base observation increase number cluster help reduce sum within-cluster variance cluster cluster allow one capture finer group datum object similar however marginal effect reduce sum within-cluster variance may drop many cluster form splitting cohesive cluster two give small reduction consequently heuristic select right number cluster use turn point curve sum within-cluster variance respect number cluster technically give number k > 0 form k cluster datum set question used cluster algorithm like k-mean calculate sum within-cluster variance var ( k ) plot curve var respect k first ( significant ) turn point curve suggest “ right ” number advanced method determine number cluster used information criterium information theoretic approach please refer bibliographic note information ( section 109 ) 
106 evaluation cluster 487 “ right ” number cluster datum set also determine crossvalidation technique often used classification ( chapter 8 ) first divide give datum set part next use − 1 part build cluster model use remain part test quality cluster example point test set find closest centroid consequently use sum square distance point test set closest centroid measure well cluster model fit test set integer k > 0 repeat process time derive clustering k cluster used part turn test set average quality measure take overall quality measure compare overall quality measure respect different value k find number cluster best fit datum 1063 measure cluster quality suppose assessed cluster tendency give datum set may also try predetermine number cluster set apply one multiple cluster method obtain clustering datum set “ good cluster generate method compare clustering generate different method ” method choose measure quality cluster general method categorize two group accord whether ground truth available ground truth ideal cluster often build used human expert ground truth available used extrinsic method compare cluster group truth measure ground truth unavailable use intrinsic method evaluate goodness cluster consider well cluster separated ground truth consider supervision form “ cluster ” hence extrinsic method also know supervised method intrinsic method unsupervised method let ’ look simple method category extrinsic method ground truth available compare cluster assess cluster thus core task extrinsic method assign score q ( c cg ) cluster c give ground truth cg whether extrinsic method effective largely depend measure q used general measure q cluster quality effective satisfy follow four essential criterium cluster homogeneity require pure cluster cluster better cluster suppose ground truth say object datum set belong category l1 ln consider cluster c1 wherein cluster c ∈ c1 contain object two category li lj ( 1 ≤ < j ≤ n ) also 
488 chapter 10 cluster analysis basic concept method consider cluster c2 identical c1 except c2 split two cluster contain object li lj respectively cluster quality measure q respect cluster homogeneity give higher score c2 c1 q ( c2 cg ) > q ( c1 cg ) cluster completeness counterpart cluster homogeneity cluster completeness require cluster two object belong category accord ground truth assign cluster cluster completeness require cluster assign object belong category ( accord ground truth ) cluster consider cluster c1 contain cluster c1 c2 member belong category accord ground truth let cluster c2 identical c1 except c1 c2 merged one cluster c2 cluster quality measure q respect cluster completeness give higher score c2 q ( c2 cg ) > q ( c1 cg ) rag bag many practical scenario often “ rag bag ” category contain object merged object category often call “ miscellaneous ” “ ” rag bag criterion state putt heterogeneous object pure cluster penalize putt rag bag consider cluster c1 cluster c ∈ c1 object c except one denote belong category accord ground truth consider cluster c2 identical c1 except assign cluster c 0 = c c2 c 0 contain object various category accord ground truth thus noisy word c 0 c2 rag bag cluster quality measure q respect rag bag criterion give higher score c2 q ( c2 cg ) > q ( c1 cg ) small cluster preservation small category split small piece cluster small piece may likely become noise thus small category discover cluster small cluster preservation criterion state splitting small category piece harmful splitting large category piece consider extreme case let datum set n + 2 object accord ground truth n object denote o1 belong one category two object denote on+1 on+2 belong another category suppose cluster c1 three cluster c1 = { o1 } c2 = { on+1 } c3 = { on+2 } let cluster c2 three cluster namely c1 = { o1 on−1 } c2 = { } c3 = { on+1 on+2 } word c1 split small category c2 split big category cluster quality measure q preserve small cluster give higher score c2 q ( c2 cg ) > q ( c1 cg ) many cluster quality measure satisfy four criterium introduce bcube precision recall metric satisfy four criterium bcube evaluate precision recall every object cluster give datum set accord ground truth precision object indicate many object cluster belong category object recall 
106 evaluation cluster 489 object reflect many object category assign cluster formally let = { o1 } set object c cluster d let l ( oi ) ( 1 ≤ ≤ n ) category oi give ground truth c ( oi ) cluster id oi c two object oi oj ( 1 ≤ j ≤ n = j ) correctness relation oi oj cluster c give ( 1 l ( oi ) = l ( oj ) ⇔ c ( oi ) = c ( oj ) correctness ( oi oj ) = 0 otherwise ( 1028 ) bcube precision defined x n x oj i6=j c ( oi ) c ( oj ) precision bcube = correctness ( oi oj ) k { oj i = j c ( oi ) = c ( oj ) } k i=1 n ( 1029 ) bcube recall defined x n x oj i6=j l ( oi ) l ( oj ) recall bcube = i=1 correctness ( oi oj ) k { oj i = j l ( oi ) = l ( oj ) } k n ( 1030 ) intrinsic method ground truth datum set available use intrinsic method assess cluster quality general intrinsic method evaluate cluster examine well cluster separated compact cluster many intrinsic method advantage similarity metric object datum set silhouette coefficient measure datum set n object suppose partition k cluster c1 ck object ∈ calculate ( ) average distance object cluster belong similarly b ( ) minimum average distance cluster belong formally suppose ∈ ci ( 1 ≤ ≤ k ) p ( ) = o0 ∈ci o6=o0 dist ( ) ci | − 1 0 ( 1031 ) 
490 chapter 10 cluster analysis basic concept method ( p b ( ) = min cj 1≤j≤k j6=i 0 ) o0 ∈cj dist ( ) cj | ( 1032 ) silhouette coefficient defined ( ) = b ( ) − ( ) max { ( ) b ( ) } ( 1033 ) value silhouette coefficient −1 value ( ) reflect compactness cluster belong smaller value compact cluster value b ( ) capture degree separated cluster larger b ( ) separated cluster therefore silhouette coefficient value approach 1 cluster contain compact far away cluster preferable case however silhouette coefficient value negative ( ie b ( ) < ( ) ) mean expectation closer object another cluster object cluster many case bad situation avoid measure cluster ’ fitness within cluster compute average silhouette coefficient value object cluster measure quality cluster use average silhouette coefficient value object datum set silhouette coefficient intrinsic measure also used elbow method heuristically derive number cluster datum set replace sum within-cluster variance 107 summary cluster collection datum object similar one another within cluster dissimilar object cluster process grouping set physical abstract object class similar object call cluster cluster analysis extensive application include business intelligence image pattern recognition web search biology security cluster analysis used standalone datum mining tool gain insight datum distribution preprocess step datum mining algorithms operate detected cluster cluster dynamic field research datum mining related unsupervised learn machine learn cluster challenge field typical requirement include scalability ability deal different type datum attribute discovery cluster arbitrary shape minimal requirement domain knowledge determine input parameter ability deal noisy datum incremental cluster 
108 exercise 491 insensitivity input order capability cluster high-dimensionality datum constraint-based cluster well interpretability usability many cluster algorithms develop categorize several orthogonal aspect regard partition criterium separation cluster similarity measure used cluster space chapter discuss major fundamental cluster method follow category partition method hierarchical method density-based method grid-based method algorithms may belong one category partition method first create initial set k partition parameter k number partition construct used iterative relocation technique attempt improve partition move object one group another typical partition method include k-mean k-medoid claran hierarchical method create hierarchical decomposition give set datum object method classify either agglomerative ( bottom-up ) divisive ( top-down ) base hierarchical decomposition form compensate rigidity merge split quality hierarchical agglomeration improve analyze object linkage hierarchical partition ( eg chameleon ) first perform microcluster ( grouping object “ microcluster ” ) operate microcluster cluster technique iterative relocation ( birch ) density-based method cluster object base notion density grow cluster either accord density neighborhood object ( eg dbscan ) accord density function ( eg denclue ) optic density-based method generate augment order datum ’ cluster structure grid-based method first quantize object space finite number cell form grid structure perform cluster grid structure sting typical example grid-based method base statistical information store grid cell clique grid-based subspace cluster algorithm cluster evaluation assess feasibility cluster analysis datum set quality result generate cluster method task include assess cluster tendency determine number cluster measure cluster quality 108 exercise 101 briefly describe give example follow approach cluster partition method hierarchical method density-based method grid-based method 
492 chapter 10 cluster analysis basic concept method 102 suppose datum mining task cluster point ( ( x ) represent location ) three cluster point a1 ( 2 10 ) a2 ( 2 5 ) a3 ( 8 4 ) b1 ( 5 8 ) b2 ( 7 5 ) b3 ( 6 4 ) c1 ( 1 2 ) c2 ( 4 9 ) distance function euclidean distance suppose initially assign a1 b1 c1 center cluster respectively use k-mean algorithm show ( ) three cluster center first round execution ( b ) final three cluster 103 use example show k-mean algorithm may find global optimum optimize within-cluster variation 104 k-mean algorithm interesting note choose initial cluster center carefully may able speed algorithm ’ convergence also guarantee quality final cluster + algorithm variant k-mean choose initial center follow first select one center uniformly random object datum set iteratively object p choose center choose object new center object choose random probability proportional dist ( p ) 2 dist ( p ) distance p closest center already choose iteration continue k center select explain method speed convergence k-mean algorithm also guarantee quality final cluster result 105 provide pseudocode object reassignment step pam algorithm 106 k-mean k-medoid algorithms perform effective cluster ( ) illustrate strength weakness k-mean comparison k-medoid ( b ) illustrate strength weakness scheme comparison hierarchical cluster scheme ( eg agne ) 107 prove dbscan density-connectedness equivalence relation 108 prove dbscan fix minpt value two neighborhood threshold 1 < 2 cluster c respect 1 minpt must subset cluster c 0 respect 2 minpt 109 provide pseudocode optic algorithm 1010 birch encounter difficulty find cluster arbitrary shape optic propose modification birch help find cluster arbitrary shape 1011 provide pseudocode step clique find dense cell subspace 
108 exercise 493 1012 present condition density-based cluster suitable partitioning-based cluster hierarchical cluster give application example support argument 1013 give example specific cluster method integrate example one cluster algorithm used preprocess step another addition provide reasoning integration two method may sometimes lead improve cluster quality efficiency 1014 cluster recognize important datum mining task broad application give one application example follow case ( ) application used cluster major datum mining function ( b ) application used cluster preprocess tool datum preparation datum mining task 1015 datum cube multidimensional databasis contain nominal ordinal numeric datum hierarchical aggregate form base learn cluster method design cluster method find cluster large datum cube effectively efficiently 1016 describe follow cluster algorithms term follow criterium ( 1 ) shape cluster determine ( 2 ) input parameter must specify ( 3 ) limitation ( ) ( b ) ( c ) ( ) ( e ) ( f ) k-mean k-medoid clara birch chameleon dbscan 1017 human eye fast effective judge quality cluster method 2-d datum design datum visualization method may help human visualize datum cluster judge cluster quality 3-d datum even higher-dimensional datum 1018 suppose allocate number automatic teller machine ( atms ) give region satisfy number constraint household workplace may cluster typically one atm assign per cluster cluster however may constrain two factor ( 1 ) obstacle object ( ie bridge river highway affect atm accessibility ) ( 2 ) additional user-specified constraint atm serve least 10000 household cluster algorithm k-mean modify quality cluster constraint 1019 constraint-based cluster aside minimum number customer cluster ( atm allocation ) constraint many kind 
494 chapter 10 cluster analysis basic concept method constraint example constraint can form maximum number customer per cluster average income customer per cluster maximum distance every two cluster categorize kind constraint impose cluster produce discuss perform cluster efficiently kind constraint 1020 design privacy-preserve cluster method datum owner would able ask third party mine datum quality cluster without worry potential inappropriate disclosure certain private sensitive information store datum 1021 show bcube metric satisfy four essential requirement extrinsic cluster evaluation method 109 bibliographic note cluster extensively study 40 year across many discipline due broad application book pattern classification machine learn contain chapter cluster analysis unsupervised learn several textbook dedicate method cluster analysis include hartigan [ har75 ] jain dube [ jd88 ] kaufman rousseeuw [ kr90 ] arabie hubert de sorte [ ahs96 ] also many survey article different aspect cluster method recent one include jain murty flynn [ jmf99 ] parson haque liu [ phl04 ] jain [ jai10 ] partition method k-mean algorithm first introduce lloyd [ llo57 ] macqueen [ mac67 ] arthur vassilvitskii [ av07 ] present + algorithm filter algorithm used spatial hierarchical datum index speed computation cluster mean give kanungo mount netanyahu et al [ + 02 ] k-medoid algorithms pam clara propose kaufman rousseeuw [ kr90 ] k-mode ( cluster nominal datum ) k-prototype ( cluster hybrid datum ) algorithms propose huang [ hua98 ] k-mode cluster algorithm also propose independently chaturvedi green carroll [ cgc94 cgc01 ] claran algorithm propose ng han [ nh94 ] ester kriegel xu [ ekx95 ] propose technique improvement performance claran used efficient spatial access method r∗-tree focuse technique k-means-based scalable cluster algorithm propose bradley fayyad reina [ bfr98 ] early survey agglomerative hierarchical cluster algorithms conduct day edelsbrunner [ de84 ] agglomerative hierarchical cluster agne divisive hierarchical cluster diana introduce kaufman rousseeuw [ kr90 ] interesting direction improve cluster quality hierarchical cluster method integrate hierarchical cluster distance-based iterative relocation nonhierarchical cluster method example birch zhang ramakrishnan livny [ zrl96 ] first perform hierarchical cluster 
109 bibliographic note 495 cf-tree apply technique hierarchical cluster also perform sophisticated linkage analysis transformation nearest-neighbor analysis cure guha rastogi shim [ grs98 ] rock ( cluster nominal attribute ) guha rastogi shim [ grs99 ] chameleon karypis han kumar [ khk99 ] probabilistic hierarchical cluster framework follow normal linkage algorithms used probabilistic model define cluster similarity develop friedman [ fri03 ] heller ghahramani [ hg05 ] density-based cluster method dbscan propose ester kriegel sander xu [ eksx96 ] ankerst breunig kriegel sander [ abks99 ] develop optic cluster-order method facilitate density-based cluster without worry parameter specification denclue algorithm base set density distribution function propose hinneburg keim [ hk98 ] hinneburg gabriel [ hg07 ] develop denclue 20 include new hill-climb procedure gaussian kernel adjust step size automatically sting grid-based multiresolution approach collect statistical information grid cell propose wang yang muntz [ wym97 ] wavecluster develop sheikholeslami chatterjee zhang [ scz98 ] multiresolution cluster approach transform original feature space wavelet transform scalable method cluster nominal datum study gibson kleinberg raghavan [ gkr98 ] guha rastogi shim [ grs99 ] ganti gehrke ramakrishnan [ ggr99 ] also many cluster paradigm example fuzzy cluster method discuss kaufman rousseeuw [ kr90 ] bezdek [ bez81 ] bezdek pal [ bp92 ] high-dimensional cluster apriori-based dimension-growth subspace cluster algorithm call clique propose agrawal gehrke gunopulos raghavan [ aggr98 ] integrate density-based grid-based cluster method recent study proceed cluster stream datum babcock badu datar et al [ + 02 ] k-median-based datum stream cluster algorithm propose guha mishra motwani ’ callaghan [ gmmo00 ] ’ callaghan et al [ + 02 ] method cluster evolve datum stream propose aggarwal han wang yu [ ahwy03 ] framework project cluster high-dimensional datum stream propose aggarwal han wang yu [ ahwy04a ] cluster evaluation discuss monograph survey article jain dube [ jd88 ] halkidi batistakis vazirgiannis [ hbv01 ] extrinsic method cluster quality evaluation extensively explore recent study include meilǎ [ mei03 mei05 ] amigó gonzalo artile verdejo [ agav09 ] four essential criterium introduce chapter formulate amigó gonzalo artile verdejo [ agav09 ] individual criterium also mentioned earlier example meilǎ [ mei03 ] rosenberg hirschberg [ rh07 ] bagga baldwin [ bb98 ] introduce bcube metric silhouette coefficient describe kaufman rousseeuw [ kr90 ] 
11 advanced cluster analysis learn fundamental cluster analysis chapter chapter discuss advanced topic cluster analysis specifically investigate four major perspective probabilistic model-based cluster section 111 introduce general framework method derive cluster object assign probability belong cluster probabilistic model-based cluster widely used many datum mining application text mining cluster high-dimensional datum dimensionality high conventional distance measure dominate noise section 112 introduce fundamental method cluster analysis high-dimensional datum cluster graph network datum graph network datum increasingly popular application online social network world wide web digital library section 113 study key issue cluster graph network datum include similarity measurement cluster method cluster constraint discussion far assume constraint cluster application however various constraint may exist constraint may rise background knowledge spatial distribution object learn conduct cluster analysis different kind constraint section 114 end chapter good grasp issue technique regard advanced cluster analysis 111 probabilistic model-based cluster cluster analysis method discuss far datum object assign one number cluster cluster assignment rule require application assign customer marketing manager however datum mining concept technique doi b978-0-12-381479-100011-3 c 2012 elsevier right re-serve 497 
498 chapter 11 advanced cluster analysis application rigid requirement may desirable section demonstrate need fuzzy flexible cluster assignment application introduce general method compute probabilistic cluster assignment “ situation may datum object belong one cluster ” consider example 111 example 111 cluster product reviews allelectronic online store customer purchase online also create reviews product every product receive reviews instead product may many reviews many other none moreover review may involve multiple product thus review editor allelectronic task cluster reviews ideally cluster topic example group product service issue highly related assign review one cluster exclusively would work well task suppose cluster “ camera camcorder ” another “ ” review talk compatibility camcorder computer review relate cluster however exclusively belong either cluster would like use cluster method allow review belong one cluster review indeed involve one topic reflect strength review belong cluster want assignment review cluster carry weight represent partial membership scenario object may belong multiple cluster occur often many application illustrated example 112 example 112 cluster study user search intent allelectronic online store record customer browse purchasing behavior log important datum mining task use log datum categorize understand user search intent example consider user session ( short period user interact online store ) user search product make comparison among different product look customer support information cluster analysis help difficult predefine user behavior pattern thoroughly cluster contain similar user browse trajectory may represent similar user behavior however every session belong one cluster example suppose user session involve purchase digital camera form one cluster user session compare laptop computer form another cluster user one session make order digital camera time compare several laptop computer session belong cluster extent section systematically study theme cluster allow object belong one cluster start notion fuzzy cluster section generalize concept probabilistic model-based cluster section section 1113 introduce expectation-maximization algorithm general framework mining cluster 
111 probabilistic model-based cluster 499 1111 fuzzy cluster give set object x = { x1 xn } fuzzy set subset x allow object x membership degree 0 formally fuzzy set modeled function fs x → [ 0 1 ] example 113 fuzzy set digital camera unit sell popular camera allelectronic use follow formula compute degree popularity digital camera give sale pop ( ) = ( 1 1000 1000 unit sell ( < 1000 ) unit sell ( 111 ) function pop ( ) define fuzzy set popular digital camera example suppose sale digital camera allelectronic show table fuzzy set popular digital camera { ( 005 ) b ( 1 ) c ( 086 ) ( 027 ) } degree membership written parenthesis apply fuzzy set idea cluster give set object cluster fuzzy set object cluster call fuzzy cluster consequently cluster contain multiple fuzzy cluster formally give set object o1 fuzzy cluster k fuzzy cluster c1 ck represent used partition matrix = [ wij ] ( 1 ≤ ≤ n 1 ≤ j ≤ k ) wij membership degree oi fuzzy cluster cj partition matrix satisfy follow three requirement object oi cluster cj 0 ≤ wij ≤ requirement enforce fuzzy cluster fuzzy set object oi k x wij = requirement ensure every object - j=1 pate cluster equivalently table 111 set digital camera sale allelectronic camera sale ( unit ) b c 50 1320 860 270 
500 chapter 11 advanced cluster analysis cluster cj 0 < n x wij < n requirement ensure every cluster i=1 least one object membership value nonzero example 114 fuzzy cluster suppose allelectronic online store six reviews keyword contain reviews list table 112 group reviews two fuzzy cluster c1 c2 c1 “ digital camera ” “ lens ” c2 “ ” partition matrix  1 1  1  = 2 3  0 0  0 0  0  1  3 1 1 use keyword “ digital camera ” “ lens ” feature cluster c1 “ computer ” feature cluster c2 review ri cluster cj ( 1 ≤ ≤ 6 1 ≤ j ≤ 2 ) wij defined wij = ri ∩ cj | ri ∩ cj | = ri ∩ ( c1 ∪ c2 ) | ri ∩ { digital camera lens computer } | fuzzy cluster review r4 belong cluster c1 c2 membership degree 23 31 respectively “ evaluate well fuzzy cluster describe datum set ” consider set object o1 fuzzy cluster c k cluster c1 ck let = [ wij ] ( 1 ≤ ≤ n 1 ≤ j ≤ k ) partition matrix let c1 ck center cluster c1 ck respectively center defined either mean medoid way specific application discuss chapter 10 distance similarity object center cluster object assign used measure well table 112 set reviews keyword used review id keyword r1 r2 r3 r4 r5 r6 digital camera lens digital camera lens digital camera lens computer computer cpu computer computer game 
111 probabilistic model-based cluster 501 object belong cluster idea extend fuzzy cluster object oi cluster cj wij > 0 dist ( oi cj ) measure well oi represent cj thus belong cluster cj object participate one cluster sum distance corresponding cluster center weight degree membership capture well object fit cluster formally object oi sum square error ( sse ) give sse ( oi ) = k x p wij dist ( oi cj ) 2 ( 112 ) j=1 parameter p ( p ≥ 1 ) control influence degree membership larger value p larger influence degree membership orthogonally sse cluster cj sse ( cj ) = n x p wij dist ( oi cj ) 2 ( 113 ) i=1 finally sse cluster defined sse ( c ) = n x k x p wij dist ( oi cj ) 2 ( 114 ) i=1 j=1 sse used measure well fuzzy cluster fit datum set fuzzy cluster also call soft cluster allow object belong one cluster easy see traditional ( rigid ) cluster enforce object belong one cluster exclusively special case fuzzy cluster defer discussion compute fuzzy cluster section 1113 1112 probabilistic model-based cluster “ fuzzy cluster ( section 1111 ) provide flexibility allow object participate multiple cluster general framework specify clustering object may participate multiple cluster probabilistic way ” section introduce general notion probabilistic model-based cluster answer question discuss chapter 10 conduct cluster analysis datum set assume object datum set fact belong different inherent category recall cluster tendency analysis ( section 1061 ) used examine whether datum set contain object may lead meaningful cluster inherent category hide datum latent mean directly observed instead infer used datum observed example topic hide set reviews allelectronic online store latent one read topic directly however topic infer reviews review one multiple topic 
502 chapter 11 advanced cluster analysis therefore goal cluster analysis find hide category datum set subject cluster analysis regard sample possible instance hide category without category label cluster derive cluster analysis infer used datum set design approach hide category statistically assume hide category distribution datum space mathematically represent used probability density function ( distribution function ) call hide category probabilistic cluster probabilistic cluster c probability density function f point datum space f ( ) relative likelihood instance c appear example 115 probabilistic cluster suppose digital camera sell allelectronic divide two category c1 consumer line ( eg point-and-shoot camera ) c2 professional line ( eg single-len reflex camera ) respective probability density function f1 f2 show figure 111 respect attribute price price value say $ 1000 f1 ( 1000 ) relative likelihood price consumer-line camera $ 1000 similarly f2 ( 1000 ) relative likelihood price professional-line camera $ 1000 probability density function f1 f2 observed directly instead allelectronic infer distribution analyze price digital camera sell moreover camera often come well-determine category ( eg “ consumer line ” “ professional line ” ) instead category typically base user background knowledge vary example camera prosumer segment may regard high end consumer line customer low end professional line other analyst allelectronic consider category probabilistic cluster conduct cluster analysis price camera approach category probability consumer line professional line price 1000 figure 111 probability density function two probabilistic cluster 
111 probabilistic model-based cluster 503 suppose want find k probabilistic cluster c1 ck cluster analysis datum set n object regard finite sample possible instance cluster conceptually assume form follow cluster cj ( 1 ≤ j ≤ k ) associate probability ωj instance sample cluster often assume ω1 ωk give part problem set p kj=1 ωj = 1 ensure object generate k cluster parameter ωj capture background knowledge relative population cluster cj run follow two step generate object d step execute n time total generate n object o1 choose cluster cj accord probability ω1 ωk choose instance cj accord probability density function fj datum generation process basic assumption mixture model formally mixture model assume set observed object mixture instance multiple probabilistic cluster conceptually observed object generate independently two step first choose probabilistic cluster accord probability cluster choose sample accord probability density function choose cluster give datum set k number cluster require task probabilistic model-based cluster analysis infer set k probabilistic cluster likely generate used datum generation process important question remain measure likelihood set k probabilistic cluster probability generate observed datum set consider set c k probabilistic cluster c1 ck probability density function f1 fk respectively probability ω1 ωk object probability generate cluster cj ( 1 ≤ j ≤ k ) give p ( o|cj ) = ωj fj ( ) therefore probability generate set c cluster p ( o|c ) = k x ωj fj ( ) ( 115 ) j=1 since object assume generate independently datum set = { o1 } n object p ( d|c ) = n i=1 p ( oi c ) = k n x ωj fj ( oi ) ( 116 ) i=1 j=1 clear task probabilistic model-based cluster analysis datum set find set c k probabilistic cluster p ( d|c ) maximize maximize p ( d|c ) often intractable general probability density function 
504 chapter 11 advanced cluster analysis cluster take arbitrarily complicate form make probabilistic model-based cluster computationally feasible often compromise assume probability density function parameterized distribution formally let o1 n observed object 21 2k parameter k distribution denote = { o1 } 2 = { 21 2k } respectively object oi ∈ ( 1 ≤ ≤ n ) eq ( 115 ) rewrite p ( oi 2 ) = k x ωj pj ( oi 2j ) ( 117 ) j=1 pj ( oi 2j ) probability oi generate jth distribution used parameter 2j consequently eq ( 116 ) rewrite p ( o|2 ) = n x k ωj pj ( oi 2j ) ( 118 ) i=1 j=1 used parameterized probability distribution model task probabilistic model-based cluster analysis infer set parameter 2 maximize eq ( 118 ) example 116 univariate gaussian mixture model let ’ use univariate gaussian distribution example assume probability density function cluster follow 1-d gaussian distribution suppose k cluster two parameter probability density function cluster center µj standard deviation σj ( 1 ≤ j ≤ k ) denote parameter 2j = ( µj σj ) 2 = { 21 2k } let datum set = { o1 } oi ( 1 ≤ ≤ n ) real number point oi ∈ 1 e p ( oi 2j ) = √ 2π σj − ( oi −µj ) 2 2σ 2 ( 119 ) assume cluster probability ω1 = ω2 = · · · = ωk = k1 plug eq ( 119 ) eq ( 117 ) k 2 ( oi −µj ) 1x 1 − p ( oi 2 ) = e 2σ 2 √ k 2π σj ( 1110 ) j=1 apply eq ( 118 ) n p ( o|2 ) = k 2 ( oi −µj ) 1 yx 1 − e 2σ 2 √ k 2π σj ( 1111 ) i=1 j=1 task probabilistic model-based cluster analysis used univariate gaussian mixture model infer 2 eq ( 1111 ) maximize 
111 probabilistic model-based cluster 505 1113 expectation-maximization algorithm “ compute fuzzy clustering probabilistic model-based clustering ” section introduce principled approach let ’ start review k-mean cluster problem k-mean algorithm study chapter 10 easily show k-mean cluster special case fuzzy cluster ( exercise 111 ) k-mean algorithm iterate cluster improve iteration consist two step expectation step ( e-step ) give current cluster center object assign cluster center closest object object expect belong closest cluster maximization step ( m-step ) give cluster assignment cluster algorithm adjust center sum distance object assign cluster new center minimize similarity object assign cluster maximize generalize two-step method tackle fuzzy cluster probabilistic model-based cluster general expectation-maximization ( em ) algorithm framework approach maximum likelihood maximum posteriori estimate parameter statistical model context fuzzy probabilistic model-based cluster em algorithm start initial set parameter iterate cluster improve cluster converge change sufficiently small ( less preset threshold ) iteration also consist two step expectation step assign object cluster accord current fuzzy cluster parameter probabilistic cluster maximization step find new cluster parameter maximize sse fuzzy cluster ( eq 114 ) expect likelihood probabilistic model-based cluster example 117 fuzzy cluster used em algorithm consider six point figure 112 coordinate point also show let ’ compute two fuzzy cluster used em algorithm randomly select two point say c1 = c2 = b initial center two cluster first iteration conduct expectation step maximization step follow e-step point calculate membership degree cluster point assign c1 c2 membership weight 1 dist ( c1 ) 2 1 1 + 2 dist ( c1 ) dist ( c2 ) 2 = dist ( c2 ) 2 dist ( c1 ) 2 dist ( c1 ) 2 + dist ( c2 ) 2 dist ( c1 ) 2 + dist ( c2 ) 2 
506 chapter 11 advanced cluster analysis e ( 18 11 ) b ( 4 10 ) ( 14 8 ) c ( 9 6 ) f ( 21 7 ) ( 3 3 ) x figure 112 datum set fuzzy cluster table 113 intermediate result first three iteration example 117 ’ em algorithm iteration 1 2 3 e-step ` 1 0 = 0 1 ` 073 mt = 027 ` 080 mt = 020 048 052 042 058 m-step 041 059 # 047 053 049 051 091 009 026 074 033 067 076 024 099 001 002 098 014 086 c1 = ( 847 512 ) c2 = ( 1042 899 ) # 042 058 # 023 077 c1 = ( 851 611 ) c2 = ( 1442 869 ) c1 = ( 640 624 ) c2 = ( 1655 864 ) respectively dist ( ) euclidean distance rationale close c1 dist ( c1 ) small membership degree respect c1 high also normalize membership degree sum degree object equal 1 point wa c1 = 1 wa c2 = exclusively belong c1 41 = 048 point b wb c1 = 0 wb c2 = point c wc c1 = 45+41 45 wc c2 = 45+41 = degree membership point show partition matrix table 113 m-step recalculate centroid accord partition matrix minimize sse give eq ( 114 ) new centroid adjust x 2 wo c j point cj = ( 1112 ) x 2 wo c j point j = 1 2 
111 probabilistic model-based cluster 507 example 12 × 3 + 02 × 4 + 0482 × 9 + 0422 × 14 + 0412 × 18 + 0472 × 21 12 + 02 + 0482 + 0422 + 0412 + 0472  12 × 3 + 02 × 10 + 0482 × 6 + 0422 × 8 + 0412 × 11 + 0472 × 7 12 + 02 + 0482 + 0422 + 0412 + 0472  c1 = = ( 847 512 )  c2 = 02 × 3 + 12 × 4 + 0522 × 9 + 0582 × 14 + 0592 × 18 + 0532 × 21 02 + 12 + 0522 + 0582 + 0592 + 0532  02 × 3 + 12 × 10 + 0522 × 6 + 0582 × 8 + 0592 × 11 + 0532 × 7 02 + 12 + 0522 + 0582 + 0592 + 0532 = ( 1042 899 ) repeat iteration iteration contain e-step m-step table 113 show result first three iteration algorithm stop cluster center converge change small enough “ apply em algorithm compute probabilistic model-based cluster ” let ’ use univariate gaussian mixture model ( example 116 ) illustrate example 118 used em algorithm mixture model give set object = { o1 } want mine set parameter 2 = { 21 2k } p ( o|2 ) eq ( 1111 ) maximize 2j = ( µj σj ) mean standard deviation respectively jth univariate gaussian distribution ( 1 ≤ j ≤ k ) apply em algorithm assign random value parameter 2 initial value iteratively conduct e-step m-step follow parameter converge change sufficiently small e-step object oi ∈ ( 1 ≤ ≤ n ) calculate probability oi belong distribution p ( oi 2j ) p ( 2j oi 2 ) = pk l=1 p ( oi 2l ) ( 1113 ) m-step adjust parameter 2 expect likelihood p ( o|2 ) eq ( 1111 ) maximize achieve set pn n p ( 2j oi 2 ) 1x 1 i=1 oi p ( 2j oi 2 ) µj = oi pn = pn k k l=1 p ( 2j ol 2 ) i=1 p ( 2j oi 2 ) i=1 ( 1114 ) 
508 chapter 11 advanced cluster analysis σj = sp n 2 i=1 p ( 2j oi 2 ) ( oi − uj ) pn i=1 p ( 2j oi 2 ) ( 1115 ) many application probabilistic model-based cluster show effective general partition method fuzzy cluster method distinct advantage appropriate statistical model used capture latent cluster em algorithm commonly used handle many learn problem datum mining statistic due simplicity note general em algorithm may converge optimal solution may instead converge local maximum many heuristic explore avoid example can run em process multiple time used different random initial value furthermore em algorithm costly number distribution large datum set contain observed datum point 112 cluster high-dimensional datum cluster method study far work well dimensionality high less 10 attribute however important application high dimensionality “ conduct cluster analysis high-dimensional datum ” section study approach cluster high-dimensional datum section 1121 start overview major challenge approach used method high-dimensional datum cluster divide two category subspace cluster method ( section 1122 ) dimensionality reduction method ( section 1123 ) 1121 cluster high-dimensional datum problem challenge major methodology present specific method cluster high-dimensional datum let ’ first demonstrate need cluster analysis high-dimensional datum used example examine challenge call new method categorize major method accord whether search cluster subspace original space whether create new lower-dimensionality space search cluster application datum object may describe 10 attribute object refer high-dimensional datum space example 119 high-dimensional datum cluster allelectronic keep track product purchase every customer customer-relationship manager want cluster customer group accord purchase allelectronic 
112 cluster high-dimensional datum 509 table 114 customer purchase datum customer p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 ada bob cathy 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 customer purchase datum high dimensionality allelectronic carry ten thousand product therefore customer ’ purchase profile vector product carry company ten thousand dimension “ traditional distance measure frequently used low-dimensional cluster analysis also effective high-dimensional datum ” consider customer table 114 10 product p1 p10 used demonstration customer purchase product 1 set corresponding bit otherwise 0 appear let ’ calculate euclidean distance ( eq 216 ) among ada bob cathy easy see dist ( ada bob ) = dist ( bob cathy ) = dist ( ada cathy ) = √ 2 accord euclidean distance three customer equivalently similar ( dissimilar ) however close look tell us ada similar cathy bob ada cathy share one common purchase item p1 show example 119 traditional distance measure ineffective high-dimensional datum distance measure may dominate noise many dimension therefore cluster full high-dimensional space unreliable find cluster may meaningful “ kind cluster meaningful high-dimensional datum ” cluster analysis high-dimensional datum still want group similar object together however datum space often big messy additional challenge need find cluster cluster set attribute manifest cluster word cluster high-dimensional datum often defined used small set attribute instead full datum space essentially cluster high-dimensional datum return group object cluster ( conventional cluster analysis ) addition cluster set attribute characterize cluster example table 114 characterize similarity ada cathy p1 may return attribute ada cathy purchase p1 cluster high-dimensional datum search cluster space exist thus two major kind method subspace cluster approach search cluster exist subspace give high-dimensional datum space subspace defined used subset attribute full space subspace cluster approach discuss section 1122 
510 chapter 11 advanced cluster analysis dimensionality reduction approach try construct much lower-dimensional space search cluster space often method may construct new dimension combine dimension original datum dimensionality reduction method topic section 1124 general cluster high-dimensional datum raise several new challenge addition conventional cluster major issue create appropriate model cluster high-dimensional datum unlike conventional cluster low-dimensional space cluster hide high-dimensional datum often significantly smaller example cluster customer-purchase datum would expect many user similar purchase pattern search small meaningful cluster like find needle haystack show conventional distance measure ineffective instead often consider various sophisticated technique model correlation consistency among object subspace typically exponential number possible subspace dimensionality reduction option thus optimal solution often computationally prohibitive example original datum space 1000 dimension want 1000 find cluster dimensionality 10 = 263 × 1023 possible 10 subspace 1122 subspace cluster method “ find subspace cluster high-dimensional datum ” many method propose generally categorize three major group subspace search method correlation-based cluster method bicluster method subspace search method subspace search method search various subspace cluster cluster subset object similar subspace similarity often capture conventional measure distance density example clique algorithm introduce section 1052 subspace cluster method enumerate subspace cluster subspace dimensionality-increas order apply antimonotonicity prune subspace cluster may exist major challenge subspace search method face search series subspace effectively efficiently generally two kind strategy bottom-up approach start low-dimensional subspace search higherdimensional subspace may cluster higher-dimensional 
112 cluster high-dimensional datum 511 subspace various prune technique explore reduce number higherdimensional subspace need search clique example bottom-up approach top-down approach start full space search smaller smaller subspace recursively top-down approach effective locality assumption hold require subspace cluster determine local neighborhood example 1110 proclus top-down subspace approach proclus k-medoid-like method first generate k potential cluster center high-dimensional datum set used sample datum set refine subspace cluster iteratively iteration current k-medoid proclus consider local neighborhood medoid whole datum set identify subspace cluster minimize standard deviation distance point neighborhood medoid dimension subspace medoid determine point datum set assign closest medoid accord corresponding subspace cluster possible outlier identify next iteration new medoid replace exist one improve cluster quality correlation-based cluster method subspace search method search cluster similarity measure used conventional metric like distance density correlation-based approach discover cluster defined advanced correlation model example 1111 correlation-based approach used pca example pca-based approach first apply pca ( principal component analysis see chapter 3 ) derive set new uncorrelated dimension mine cluster new space subspace addition pca space transformation may used hough transform fractal dimension additional detail subspace search method correlation-based cluster method please refer bibliographic note ( section 117 ) bicluster method application want cluster object attribute simultaneously result cluster know bicluster meet four requirement ( 1 ) small set object participate cluster ( 2 ) cluster involve small number attribute ( 3 ) object may participate multiple cluster participate cluster ( 4 ) attribute may involved multiple cluster involved cluster section 1123 discuss bicluster detail 
512 chapter 11 advanced cluster analysis 1123 bicluster cluster analysis discuss far cluster object accord attribute value object attribute treat way however application object attribute defined symmetric way datum analysis involve search datum matrix submatrix show unique pattern cluster kind cluster technique belong category bicluster section first introduce two motivate application example biclustering— gene expression recommender system learn different type bicluster last present bicluster method application example bicluster technique first propose address need analyze gene expression datum gene unit passing-on trait live organism offspr typically gene reside segment dna gene critical live thing specify protein functional rna chain hold information build maintain live organism ’ cell pass genetic trait offspr synthesis functional gene product either rna protein rely process gene expression genotype genetic makeup cell organism individual phenotype observable characteristic organism gene expression fundamental level genetic genotype cause phenotype used dna chip ( also know dna microarray ) biological engineering technique measure expression level large number ( possibly ) organism ’ gene number different experimental condition condition may correspond different time point experiment sample different organ roughly speaking gene expression datum dna microarray datum conceptually condition matrix row correspond one gene column correspond one sample condition element matrix real number record expression level gene specific condition figure 113 show illustration cluster viewpoint interesting issue gene expression datum matrix analyze two dimensions—the gene dimension condition dimension analyze gene dimension treat gene object treat condition attribute mining gene dimension may find pattern share multiple gene cluster gene group example may find group gene express similarly highly interesting bioinformatic find pathway analyze condition dimension treat condition object treat gene attribute way may find pattern condition cluster condition group example may find difference gene expression compare group tumor sample nontumor sample 
112 cluster high-dimensional datum 513 condition gene w11 w12 w1m w21 w22 w2m w31 w32 w3m wn1 wn2 wnm figure 113 microarrary datum matrix example 1112 gene expression gene expression matrix popular bioinformatic research development example important task classify new gene used expression datum gene gene know class symmetrically may classify new sample ( eg new patient ) used expression datum sample sample know class ( eg tumor nontumor ) task invaluable understand mechanism disease clinical treatment see many gene expression datum mining problem highly related cluster analysis however challenge instead cluster one dimension ( eg gene condition ) many case need cluster two dimension simultaneously ( eg gene condition ) moreover unlike cluster model discuss far cluster gene expression datum matrix submatrix usually follow characteristic small set gene participate cluster cluster involve small subset condition gene may participate multiple cluster may participate cluster condition may involved multiple cluster may involved cluster find cluster condition matrix need new cluster technique meet follow requirement bicluster cluster gene defined used subset condition cluster condition defined used subset gene 
514 chapter 11 advanced cluster analysis cluster neither exclusive ( eg one gene participate multiple cluster ) exhaustive ( eg gene may participate cluster ) bicluster useful bioinformatic also application well consider recommender system example example 1113 used bicluster recommender system allelectronic collect datum customer ’ evaluation product used datum recommend product customer datum modeled customer-product matrix row represent customer column represent product element matrix represent customer ’ evaluation product may score ( eg like like somewhat like ) purchase behavior ( eg buy ) figure 114 illustrate structure customer-product matrix analyze two dimension customer dimension product dimension treat customer object product attribute allelectronic find customer group similar preference purchase pattern used product object customer attribute allelectronic mine product group similar customer interest moreover allelectronic mine cluster customer product simultaneously cluster contain subset customer involve subset product example allelectronic highly interested find group customer like group product cluster submatrix customer-product matrix element high value used cluster allelectronic make recommendation two direction first company recommend product new customer similar customer cluster second company recommend customer new product similar involved cluster bicluster gene expression datum matrix bicluster customerproduct matrix usually follow characteristic small set customer participate cluster cluster involve small subset product customer participate multiple cluster may participate cluster customer w11 w21 ··· wn1 product w12 · · · w22 · · · ··· ··· wn2 · · · figure 114 customer–product matrix w1m w2m ··· wnm 
112 cluster high-dimensional datum 515 product may involved multiple cluster may involved cluster bicluster apply customer-product matrix mine cluster satisfying requirement type bicluster “ model bicluster mine ” let ’ start basic notation sake simplicity use “ gene ” “ condition ” refer two dimension discussion discussion easily extend application example simply replace “ gene ” “ condition ” “ customer ” “ product ” tackle customer-product bicluster problem let = { a1 } set gene b = { b1 bm } set condition let e = [ eij ] gene expression datum matrix gene-condition matrix 1 ≤ ≤ n 1 ≤ j ≤ m submatrix × j defined subset ⊆ gene subset j ⊆ b condition example matrix show figure 115 { a1 a33 a86 } × { b6 b12 b36 b99 } submatrix bicluster submatrix gene condition follow consistent pattern define different type bicluster base pattern simplest case submatrix × j ( ⊆ j ⊆ b ) bicluster constant value ∈ j ∈ j eij = c c constant example submatrix { a1 a33 a86 } × { b6 b12 b36 b99 } figure 115 bicluster constant value bicluster interesting row constant value though different row may different value bicluster constant value row submatrix × j ∈ j ∈ j eij = c + αi αi adjustment row i example figure 116 show bicluster constant value row symmetrically bicluster constant value column submatrix × j ∈ j ∈ j eij = c + βj βj adjustment column j a1 ··· a33 ··· a86 ··· ··· ··· ··· ··· ··· ··· ··· b6 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b12 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b36 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b99 · · · 60 · · · ··· ··· 60 · · · ··· ··· 60 · · · ··· ··· figure 115 gene-condition matrix submatrix bicluster 
516 chapter 11 advanced cluster analysis generally bicluster interesting row change synchronize way respect column vice versa mathematically bicluster coherent value ( also know pattern-based cluster ) submatrix × j ∈ j ∈ j eij = c + αi + βj αi βj adjustment row column j respectively example figure 117 show bicluster coherent value show × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 moreover instead used addition define bicluster coherent value used multiplication eij = c · αi · βj clearly bicluster constant value row column special case bicluster coherent value application may interested - down-regulate change across gene condition without constrain exact value bicluster coherent evolution row submatrix × j i1 i2 ∈ j1 j2 ∈ j ( ei1 j1 − ei1 j2 ) ( ei2 j1 − ei2 j2 ) ≥ example figure 118 show bicluster coherent evolution row symmetrically define bicluster coherent evolution column next study mine bicluster 10 20 50 0 10 20 50 0 10 20 50 0 10 20 50 0 10 20 50 0 figure 116 bicluster constant value row 10 20 50 0 50 60 90 40 30 40 70 20 70 80 110 60 20 30 60 10 figure 117 bicluster coherent value 10 20 50 0 50 100 100 80 30 50 90 20 70 1000 120 100 20 30 80 10 figure 118 bicluster coherent evolution row 
112 cluster high-dimensional datum 517 bicluster method previous specification type bicluster consider ideal case real datum set perfect bicluster rarely exist exist usually small instead random noise affect reading eij thus prevent bicluster nature appear perfect shape two major type method discover bicluster datum may come noise optimization-based method conduct iterative search iteration submatrix highest significance score identify bicluster process terminate user-specified condition meet due cost concern computation greedy search often employ find local optimal bicluster enumeration method use tolerance threshold specify degree noise allow bicluster mine try enumerate submatrix bicluster satisfy requirement use δ-cluster maple algorithms example illustrate idea optimization used δ-cluster algorithm submatrix × j mean ith row 1 x eij = eij | ( 1116 ) j∈j symmetrically mean jth column 1 x eij = eij | ( 1117 ) i∈i mean element submatrix 1 x 1 x 1 x eij = eij = eij eij = | | | i∈i j∈j i∈i ( 1118 ) j∈j quality submatrix bicluster measure mean-squared residue value 1 x h ( × j ) = ( eij − eij − eij + eij ) 2 ( 1119 ) | i∈i j∈j submatrix × j δ-bicluster h ( × j ) ≤ δ δ ≥ 0 threshold δ = 0 × j perfect bicluster coherent value set δ > 0 user specify tolerance average noise per element perfect bicluster eq ( 1119 ) residue element residue ( eij ) = eij − eij − eij + eij ( 1120 ) maximal δ-bicluster δ-bicluster × j exist another δ-bicluster 0 × j 0 ⊆ 0 j ⊆ j 0 least one inequality hold find 
518 chapter 11 advanced cluster analysis maximal δ-bicluster largest size computationally costly therefore use heuristic greedy search method obtain local optimal cluster algorithm work two phase deletion phase start whole matrix mean-squared residue matrix δ iteratively remove row column iteration row compute mean-squared residue 1 x ( ) = ( eij − eij − eij + eij ) 2 ( 1121 ) | j∈j moreover column j compute mean-squared residue 1 x ( eij − eij − eij + eij ) 2 ( j ) = | ( 1122 ) i∈i remove row column largest mean-squared residue end phase obtain submatrix × j δ-bicluster however submatrix may maximal addition phase iteratively expand δ-bicluster × j obtain deletion phase long δ-bicluster requirement maintain iteration consider row column involved current bicluster × j calculate mean-squared residue row column smallest mean-squared residue add current δ-bicluster greedy algorithm find one δ-bicluster find multiple bicluster heavy overlap run algorithm multiple time execution δ-bicluster output replace element output bicluster random number although greedy algorithm may find neither optimal bicluster bicluster fast even large matrix enumerate bicluster used maple mentioned submatrix × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 2 × 2 submatrix × j define p-score ei1 j1 ei1 j2 p-score = | ( ei1 j1 − ei2 j1 ) − ( ei1 j2 − ei2 j2 ) | ( 1123 ) ei2 j1 ei2 j2 submatrix × j δ-pcluster ( pattern-based cluster ) p-score every 2 × 2 submatrix × j δ δ ≥ 0 threshold specify user ’ tolerance noise perfect bicluster p-score control noise every element bicluster mean-squared residue capture average noise interesting property δ-pcluster × j δ-pcluster every x × ( x ≥ 2 ) submatrix × j also δ-pcluster monotonicity enable 
112 cluster high-dimensional datum 519 us obtain succinct representation nonredundant δ-pcluster δ-pcluster maximal row column add cluster maintain δ-pcluster property avoid redundancy instead find δ-pcluster need compute maximal δ-pcluster maple algorithm enumerate maximal δ-pcluster systematically enumerate every combination condition used set enumeration tree depthfirst search enumeration framework pattern-growth method frequent pattern mining ( chapter 6 ) consider gene expression datum condition combination j maple find maximal subset gene × j δ-pcluster × j submatrix another δ-pcluster × j maximal δ-pcluster may huge number condition combination maple prune many unfruitful combination used monotonicity δ-pcluster condition combination j exist set gene × j δ-pcluster need consider superset j moreover consider × j candidate δ-pcluster every ( | − 1 ) subset j 0 j × j 0 δ-pcluster maple also employ several prune technique speed search retain completeness return maximal δ-pcluster example examine current δ-pcluster × j maple collect gene condition may add expand cluster candidate gene condition together j form submatrix δ-pcluster already find search × j superset j prune interested reader may refer bibliographic note additional information maple algorithm ( section 117 ) interesting observation search maximal δ-pcluster maple somewhat similar mining frequent close itemset consequently maple borrow depth-first search framework idea prune technique pattern-growth method frequent pattern mining example frequent pattern mining cluster analysis may share similar technique idea advantage maple algorithms enumerate bicluster guarantee completeness result miss overlapping bicluster however challenge enumeration algorithms may become time consume matrix become large customer-purchase matrix hundred thousand customer million product 1124 dimensionality reduction method spectral cluster subspace cluster method try find cluster subspace original datum space situation effective construct new space instead used subspace original datum motivation behind dimensionality reduction method cluster high-dimensional datum example 1114 cluster derive space consider three cluster point figure possible cluster point subspace original space x × 
520 chapter 11 advanced cluster analysis − 0707x + 0707y x figure 119 cluster derive space may effective three cluster would end project onto overlapping area x √ √ 2 2 axe instead construct new dimension − 2 x + 2 ( show dash line figure ) project point onto new dimension three cluster become apparent although example 1114 involve two dimension idea construct new space ( cluster structure hide datum become well manifest ) extend high-dimensional datum preferably newly construct space low dimensionality many dimensionality reduction method straightforward approach apply feature selection extraction method datum set discuss chapter however method may able detect cluster structure therefore method combine feature extraction cluster prefer section introduce spectral cluster group method effective highdimensional datum application figure 1110 show general framework spectral cluster approach ng-jordan-weiss algorithm spectral cluster method let ’ look step framework also note special condition apply ng-jordan-weiss algorithm example give set object o1 distance pair object dist ( oi oj ) ( 1 ≤ j ≤ n ) desire number k cluster spectral cluster approach work follow used distance measure calculate affinity matrix w wij = e − dist ( oi oj ) σ2 σ scaling parameter control fast affinity wij decrease dist ( oi oj ) increase ng-jordan-weiss algorithm wii set 0 
112 cluster high-dimensional datum datum affinity matrix [ wij ] compute lead k eigenvector cluster new space 521 project back cluster original datum av = λv = f ( w ) figure 1110 framework spectral cluster approach source adapt slide 8 http micued08 azran used affinity matrix w derive matrix = f ( w ) way do vary ng-jordan-weiss algorithm define matrix diagonal matrix dii sum ith row w dii = n x wij ( 1124 ) j=1 set 1 1 = d− 2 wd− 2 ( 1125 ) find k lead eigenvector a recall eigenvector square matrix nonzero vector remain proportional original vector multiply matrix mathematically vector v eigenvector matrix av = λv λ call corresponding eigenvalue step derive k new dimension base affinity matrix w typically k much smaller dimensionality original datum ng-jordan-weiss algorithm compute k eigenvector largest eigenvalue x1 xk used k lead eigenvector project original datum new space defined k lead eigenvector run cluster algorithm k-mean find k cluster ng-jordan-weiss algorithm stack k largest eigenvector column form matrix x = [ x1 x2 · · · xk ] ∈ rn×k algorithm form matrix renormalize row x unit length xij yij = qp k 2 j=1 xij ( 1126 ) algorithm treat row point k-dimensional space rk run k-mean ( algorithm serve partition purpose ) cluster point k cluster 
522 chapter 11 advanced cluster analysis v = [ v1 v2 v3 ] w 05 u = [ u1 u2 u3 ] 0 −05 0 10 20 30 40 50 60 1 05 0 −05 −1 0 10 20 30 40 50 60 1 05 05 0 0 0 04 02 0 −02 0 10 10 20 20 30 30 40 40 50 50 60 60 0 1 05 0 −05 0 10 10 20 20 30 30 40 40 50 50 60 60 figure 1111 new dimension cluster result ng-jordan-weiss algorithm source adapt slide 9 http micued08 azran assign original datum point cluster accord transform point assign cluster obtain step 4 ng-jordan-weiss algorithm original object oi assign jth cluster matrix ’ row assign jth cluster result step 4 spectral cluster method dimensionality new space set desire number cluster set expect new dimension able manifest cluster example 1115 ng-jordan-weiss algorithm consider set point figure datum set affinity matrix three largest eigenvector normalize vector show note three new dimension ( form three largest eigenvector ) cluster easily detected spectral cluster effective high-dimensional application image process theoretically work well certain condition apply scalability however challenge compute eigenvector large matrix costly spectral cluster combine cluster method bicluster additional information dimensionality reduction cluster method kernel pca find bibliographic note ( section 117 ) 113 cluster graph network datum cluster analysis graph network datum extract valuable knowledge information datum increasingly popular many application discuss application challenge cluster graph network datum section similarity measure form cluster give section learn graph cluster method section 1133 general term graph network used interchangeably rest section mainly use term graph 
113 cluster graph network datum 523 1131 application challenge customer relationship manager allelectronic notice lot datum relate customer purchase behavior preferably modeled used graph example 1116 bipartite graph customer purchase behavior allelectronic represent bipartite graph bipartite graph vertex divide two disjoint set edge connect vertex one set vertex set allelectronic customer purchase datum one set vertex represent customer one customer per vertex set represent product one product per vertex edge connect customer product represent purchase product customer figure 1112 show illustration “ kind knowledge obtain cluster analysis customer-product bipartite graph ” cluster customer customer buy similar set product place one group customer relationship manager make product recommendation example suppose ada belong customer cluster customer purchase digital camera last 12 month ada yet purchase one manager decide recommend digital camera alternatively cluster product product purchase similar set customer group together cluster information also used product recommendation example digital camera high-speed flash memory card belong product cluster customer purchase digital camera recommend high-speed flash memory card bipartite graph widely used many application consider another example example 1117 web search engine web search engine search log archive record user query corresponding click-through information ( click-through information tell us page give result search user click ) query click-through information represent used bipartite graph two set customer product figure 1112 bipartite graph represent customer-purchase datum 
524 chapter 11 advanced cluster analysis vertex correspond query web page respectively edge link query web page user click web page ask query valuable information obtain cluster analysis query–web page bipartite graph instance may identify query pose different language mean thing click-through information query similar another example web page web form direct graph also know web graph web page vertex hyperlink edge point source page destination page cluster analysis web graph disclose community find hub authoritative web page detect web spam addition bipartite graph cluster analysis also apply type graph include general graph elaborate example 1118 example 1118 social network social network social structure represent graph vertex individual organization link interdependency vertex represent friendship common interest collaborative activity allelectronic ’ customer form social network customer vertex edge link two customer know customer relationship manager interested find useful information derive allelectronic ’ social network cluster analysis obtain cluster network customer cluster know friend common customer within cluster may influence one another regard purchase decision make moreover communication channel design inform “ head ” cluster ( ie “ best ” connect person cluster ) promotional information spread quickly thus may use customer cluster promote sale allelectronic another example author scientific publication form social network author vertex two author connect edge coauthor publication network general weight graph edge two author carry weight represent strength collaboration many publication two author ( end vertex ) coauthor cluster coauthor network provide insight community author pattern collaboration “ challenge specific cluster analysis graph network datum ” cluster method discuss far object represent used set attribute unique feature graph network datum object ( vertex ) relationship ( edge ) give dimension attribute explicitly defined conduct cluster analysis graph network datum two major new challenge “ measure similarity two object graph accordingly ” typically use conventional distance measure euclidean distance instead need develop new measure quantify similarity 
113 cluster graph network datum 525 measure often metric thus raise new challenge regard development efficient cluster method similarity measure graph discuss section 1132 “ design cluster model method effective graph network datum ” graph network datum often complicate carry topological structure sophisticated traditional cluster analysis application many graph datum set large web graph contain least ten billion web page publicly indexable web graph also sparse average vertex connect small number vertex graph discover accurate useful knowledge hide deep datum good cluster method accommodate factor cluster method graph network datum introduce section 1133 1132 similarity measure “ measure similarity distance two vertex graph ” discussion examine two type measure geodesic distance distance base random walk geodesic distance simple measure distance two vertex graph shortest path vertex formally geodesic distance two vertex length term number edge shortest path vertex two vertex connect graph geodesic distance defined infinite used geodesic distance define several useful measurement graph analysis cluster give graph g = ( v e ) v set vertex e set edge define follow vertext v ∈ v eccentricity v denote eccen ( v ) largest geodesic distance v vertex u ∈ v − { v } eccentricity v capture far away v remotest vertex graph radius graph g minimum eccentricity vertex r = min eccen ( v ) v∈v ( 1127 ) radius capture distance “ central point ” “ farthest border ” graph diameter graph g maximum eccentricity vertex = max eccen ( v ) v∈v diameter represent largest distance pair vertex peripheral vertex vertex achieve diameter ( 1128 ) 
526 chapter 11 advanced cluster analysis b c e figure 1113 graph g vertex c e peripheral example 1119 measurement base geodesic distance consider graph g figure eccentricity 2 eccen ( ) = 2 eccen ( b ) = 2 eccen ( c ) = eccen ( ) = eccen ( e ) = thus radius g 2 diameter note necessary = 2 × r vertex c e peripheral vertex simrank similarity base random walk structural context application geodesic distance may inappropriate measure similarity vertex graph introduce simrank similarity measure base random walk structural context graph mathematics random walk trajectory consist take successive random step example 1120 similarity person social network let ’ consider measure similarity two vertex allelectronic customer social network example 1118 similarity explain closeness two participant network close two person term relationship represent social network “ well geodesic distance measure similarity closeness network ” suppose ada bob two customer network network undirected geodesic distance ( ie length shortest path ada bob ) shortest path message pass ada bob vice versa however information useful allelectronic ’ customer relationship management company typically want send specific message one customer another therefore geodesic distance suit application “ similarity mean social network ” consider two way define similarity two customer consider similar one another similar neighbor social network heuristic intuitive practice two person receive recommendation good number common friend often make similar decision kind similarity base local structure ( ie neighborhood ) vertex thus call structural context–based similarity 
113 cluster graph network datum 527 suppose allelectronic send promotional information ada bob social network ada bob may randomly forward information friend ( neighbor ) network closeness ada bob measure likelihood customer simultaneously receive promotional information originally send ada bob kind similarity base random walk reachability network thus refer similarity base random walk let ’ closer look meant similarity base structural context similarity base random walk intuition behind similarity base structural context two vertex graph similar connect similar vertex measure similarity need define notion individual neighborhood direct graph g = ( v e ) v set vertex e ⊆ v × v set edge vertex v ∈ v individual in-neighborhood v defined ( 1129 ) ( v ) = { | ( u v ) ∈ e } symmetrically define individual out-neighborhood v ( 1130 ) ( v ) = { | ( v w ) ∈ e } follow intuition illustrated example 1120 define simrank structural-context similarity value 0 1 pair vertex vertex v ∈ v similarity vertex ( v v ) = 1 neighborhood identical vertex u v ∈ v u = v define x x c ( u v ) = ( x ) ( 1131 ) i ( u ) i ( v ) | x∈i ( u ) y∈i ( v ) c constant 0 vertex may in-neighbor thus define eq ( 1131 ) 0 either ( u ) ( v ) ∅ parameter c specify rate decay similarity propagate across edge “ compute simrank ” straightforward method iteratively evaluate eq ( 1131 ) fix point reach let si ( u v ) simrank score calculate ith round begin set ( 0 u = v s0 ( u v ) = ( 1132 ) 1 u = v use eq ( 1131 ) compute si+1 si si+1 ( u v ) = x c i ( u ) i ( v ) | x x∈i ( u ) y∈i ( v ) si ( x ) ( 1133 ) 
528 chapter 11 advanced cluster analysis show lim si ( u v ) = ( u v ) additional method approximate i→∞ simrank give bibliographic note ( section 117 ) let ’ consider similarity base random walk direct graph strongly connect two node u v path u v another path v u strongly connect graph g = ( v e ) two vertex u v ∈ v define expect distance u v ( u v ) = x u p [ ] l ( ) ( 1134 ) v u v path start u end v may contain cycle reach v end travele tour = w1 → w2 → · · · → wk length l ( ) = k − probability tour defined ( q k−1 1 i=1 o ( wi ) | l ( ) > 0 ( 1135 ) p [ ] = 0 l ( ) = 0 measure probability vertex w receive message originated simultaneously u v extend expect distance notion expect meeting distance x ( u v ) = p [ ] l ( ) ( 1136 ) ( x x ) ( u v ) ( u v ) ( x x ) pair tour u x v x length used constant c 0 1 define expect meeting probability p ( u v ) = x ( u v ) p [ ] c l ( ) ( 1137 ) ( x x ) similarity measure base random walk parameter c specify probability continue walk step trajectory show ( u v ) = p ( u v ) two vertex u v simrank base structural context random walk 1133 graph cluster method let ’ consider conduct cluster graph first describe intuition behind graph cluster discuss two general category graph cluster method find cluster graph imagine cut graph piece piece cluster vertex within cluster well connect vertex different cluster connect much weaker way formally graph g = ( v e ) 
113 cluster graph network datum 529 cut c = ( ) partition set vertex v g v = ∪ ∩ = ∅ cut set cut set edge { ( u v ) ∈ e|u ∈ v ∈ } size cut number edge cut set weight graph size cut sum weight edge cut set “ kind cut good derive cluster graph ” graph theory network application minimum cut importance cut minimum cut ’ size greater cut ’ size polynomial time algorithms compute minimum cut graph use algorithms graph cluster example 1121 cut cluster consider graph g figure graph two cluster { b c e f } { g h j k } one outlier vertex l consider cut c1 = ( { b c e f g h j k } { l } ) one edge namely ( e l ) cross two partition create c1 therefore cut set c1 { ( e l ) } size c1 1 ( note size cut connect graph smaller 1 ) minimum cut c1 lead good cluster separate outlier vertex l rest graph cut c2 = ( { b c e f l } { g h j k } ) lead much better cluster c1 edge cut set c2 connect two “ natural cluster ” graph specifically edge ( h ) ( e k ) cut set edge connect h e k belong one cluster example 1121 indicate used minimum cut unlikely lead good cluster better choose cut vertex u involved edge cut set edge connect u belong one cluster formally let deg ( u ) degree u number edge connect u sparsity cut c = ( ) defined = cut size min { | | } ( 1138 ) sparsest cut c2 b c g f h e k minimum cut c1 l figure 1114 graph g two cut j 
530 chapter 11 advanced cluster analysis cut sparsest sparsity greater sparsity cut may one sparsest cut example 1121 figure 1114 c2 sparsest cut used sparsity objective function sparsest cut try minimize number edge cross partition balance partition size consider cluster graph g = ( v e ) partition graph k cluster modularity cluster assess quality cluster defined = k x i=1   di 2 li − | | ( 1139 ) li number edge vertex ith cluster di sum degree vertex ith cluster modularity cluster graph difference fraction edge fall individual cluster fraction would graph vertex randomly connect optimal cluster graph maximize modularity theoretically many graph cluster problem regard find good cut sparsest cut graph practice however number challenge exist high computational cost many graph cut problem computationally expensive sparsest cut problem example np-hard therefore find optimal solution large graph often impossible good trade-off scalability quality achieve sophisticated graph graph sophisticated one describe involve weight or cycle high dimensionality graph many vertex similarity matrix vertex represent vector ( row matrix ) dimensionality number vertex graph therefore graph cluster method must handle high dimensionality sparsity large graph often sparse meaning vertex average connect small number vertex similarity matrix large sparse graph also sparse two kind method cluster graph datum address challenge one used cluster method high-dimensional datum design specifically cluster graph first group method base generic cluster method highdimensional datum extract similarity matrix graph used similarity measure discuss section generic cluster method apply similarity matrix discover cluster cluster method 
113 cluster graph network datum 531 high-dimensional datum typically employ example many scenario similarity matrix obtain spectral cluster method ( section 1124 ) apply spectral cluster approximate optimal graph cut solution additional information please refer bibliographic note ( section 117 ) second group method specific graph search graph find well-connected component cluster let ’ look method call scan ( structural cluster algorithm network ) example give undirected graph g = ( v e ) vertex u ∈ v neighborhood u 0 ( u ) = { | ( u v ) ∈ e } ∪ { u } used idea structural-context similarity scan measure similarity two vertex u v ∈ v normalize common neighborhood size 0 ( u ) ∩ 0 ( v ) | σ ( u v ) = √ 0 ( u ) 0 ( v ) | ( 1140 ) larger value compute similar two vertex scan used similarity threshold ε define cluster membership vertex u ∈ v ε-neighborhood u defined nε ( u ) = { v ∈ 0 ( u ) σ ( u v ) ≥ ε } ε-neighborhood u contain neighbor u structural-context similarity u least ε scan core vertex vertex inside cluster u ∈ v core vertex nε ( u ) | ≥ µ µ popularity threshold scan grow cluster core vertex vertex v ε-neighborhood core u v assign cluster u process grow cluster continue cluster grow process similar density-based cluster method dbscan ( chapter 10 ) formally vertex v directly reach core u v ∈ nε ( u ) transitively vertex v reach core u exist vertex w1 wn w1 reach u wi reach wi−1 1 < ≤ n v reach wn moreover two vertex u v ∈ v may may core say connect exist core w u v reach w vertex cluster connect cluster maximum set vertex every pair set connect vertex may belong cluster vertex u hub neighborhood 0 ( u ) u contain vertex one cluster vertex belong cluster hub outlier scan algorithm show figure search framework closely resemble cluster-find process dbscan scan find cut graph cluster set vertex connect base transitive similarity structural context advantage scan time complexity linear respect number edge large sparse graph number edge scale number vertex therefore scan expect good scalability cluster large graph 
532 chapter 11 advanced cluster analysis algorithm scan cluster graph datum input graph g = ( v e ) similarity threshold ε population threshold µ output set cluster method set vertex v unlabeled unlabeled vertex u u core generate new cluster-id c insert v ∈ nε ( u ) queue q q = w ← first vertex q r ← set vertex directly reach w ∈ r unlabeled labele nonmember assign current cluster-id c endif unlabeled insert queue q endif endfor remove w q end else label u nonmember endif endfor vertex u labele nonmember ∃x ∈ 0 ( u ) x different cluster-id label u hub else label u outlier endif endfor figure 1115 scan algorithm cluster analysis graph datum 114 cluster constraint user often background knowledge want integrate cluster analysis may also application-specific requirement information modeled cluster constraint approach topic cluster constraint two step section 1141 categorize type constraint cluster graph datum method cluster constraint introduce section 1142 
114 cluster constraint 533 1141 categorization constraint section study categorize constraint used cluster analysis specifically categorize constraint accord subject set strongly constraint enforce discuss chapter 10 cluster analysis involve three essential aspect object instance cluster cluster group object similarity among object therefore first method discuss categorize constraint accord apply thus three type constraint instance constraint cluster constraint similarity measurement constraint instance constraint instance specify pair set instance group cluster analysis two common type constraint category include must-link constraint must-link constraint specify two object x x group one cluster output cluster analysis must-link constraint transitive must-link ( x ) must-link ( z ) must-link ( x z ) link constraint link constraint opposite must-link constraint link constraint specify two object x output cluster analysis x belong different cluster link constraint entail link ( x ) must-link ( x x 0 ) must-link ( 0 ) link ( x 0 0 ) constraint instance defined used specific instance alternatively also defined used instance variable attribute instance example constraint constraint ( x ) must-link ( x ) dist ( x ) ≤  used distance object specify must-link constraint constraint cluster constraint cluster specify requirement cluster possibly used attribute cluster example constraint may specify minimum number object cluster maximum diameter cluster shape cluster ( eg convex ) number cluster specify partition cluster method regard constraint cluster constraint similarity measurement often similarity measure euclidean distance used measure similarity object cluster analysis application exception apply constraint similarity measurement specify requirement similarity calculation must respect example cluster person move object plaza euclidean distance used give 
534 chapter 11 advanced cluster analysis walking distance two point constraint similarity measurement trajectory implement shortest distance cross wall one way express constraint depend category example specify constraint cluster constraint1 diameter cluster larger requirement also expressed used constraint instance constraint10 link ( x ) dist ( x ) > ( 1141 ) example 1122 constraint instance cluster similarity measurement allelectronic cluster customer group customer assign customer relationship manager suppose want specify customer address place group would allow comprehensive service family expressed used must-link constraint instance constraintfamily ( x ) must-link ( x ) xaddress = yaddress allelectronic eight customer relationship manager ensure similar workload place constraint cluster eight cluster cluster least 10 % customer 15 % customer calculate spatial distance two customer used drive distance two however two customer live different country use flight distance instead constraint similarity measurement another way categorize cluster constraint consider firmly constraint respect constraint hard cluster violate constraint unacceptable constraint soft cluster violate constraint preferable acceptable better solution find soft constraint also call preference example 1123 hard soft constraint allelectronic constraintfamily example 1122 hard constraint splitting family different cluster can prevent company provide comprehensive service family lead poor customer satisfaction constraint number cluster ( correspond number customer relationship manager company ) also hard example 1122 also constraint balance size cluster satisfying constraint strongly prefer company flexible willing assign senior capable customer relationship manager oversee larger cluster therefore constraint soft ideally specific datum set set constraint clustering satisfy constraint however possible may cluster datum set 
114 cluster constraint 535 satisfy constraint trivially two constraint set conflict cluster satisfy time example 1124 conflict constraint consider constraint must-link ( x ) dist ( x ) < 5 link ( x ) dist ( x ) > 3 datum set two object x dist ( x ) = 4 cluster satisfy constraint simultaneously consider two constraint must-link ( x ) dist ( x ) < 5 must-link ( x ) dist ( x ) < 3 second constraint redundant give first moreover datum set distance two object least 5 every possible cluster object satisfy constraint “ measure quality usefulness set constraint ” general consider either informativeness coherence informativeness amount information carry constraint beyond cluster model give datum set cluster method set constraint c informativeness c respect measure fraction constraint c unsatisfied cluster compute d higher informativeness specific requirement background knowledge constraint carry coherence set constraint degree agreement among constraint measure redundancy among constraint 1142 method cluster constraint although categorize cluster constraint application may different constraint specific form consequently various technique need handle specific constraint section discuss general principle handle hard soft constraint handle hard constraint general strategy handle hard constraint strictly respect constraint cluster assignment process illustrate idea use partition cluster example 
536 chapter 11 advanced cluster analysis give datum set set constraint instance ( ie must-link link constraint ) extend k-mean method satisfy constraint cop-k-mean algorithm work follow generate superinstance must-link constraint compute transitive closure must-link constraint must-link constraint treat equivalence relation closure give one multiple subset object object subset must assign one cluster represent subset replace object subset mean superinstance also carry weight number object represent step must-link constraint always satisfied conduct modify k-mean cluster recall k-mean object assign closest center nearest-center assignment violate link constraint respect link constraint modify center assignment process k-mean nearest feasible center assignment object assign center sequence step make sure assignment far violate link constraint object assign nearest center assignment respect link constraint cop-k-mean ensure constraint violate every step require backtracking greedy algorithm generate cluster satisfy constraint provide conflict exist among constraint handle soft constraint cluster soft constraint optimization problem cluster violate soft constraint penalty impose cluster therefore optimization goal cluster contain two part optimize cluster quality minimize constraint violation penalty overall objective function combination cluster quality score penalty score illustrate use partition cluster example give datum set set soft constraint instance cvqe ( constrain vector quantization error ) algorithm conduct k-mean cluster enforce constraint violation penalty objective function used cvqe sum distance used k-mean adjust constraint violation penalty calculate follow penalty must-link violation must-link constraint object x assign two different center c1 c2 respectively constraint violate result dist ( c1 c2 ) distance c1 c2 add objective function penalty penalty link violation link constraint object x assign common center c constraint violate 
114 cluster constraint 537 distance dist ( c c 0 ) c c 0 add objective function penalty speeding constrain cluster constraint similarity measurement lead heavy cost cluster consider follow cluster obstacle problem cluster person move object plaza euclidean distance used measure walking distance two point however constraint similarity measurement trajectory implement shortest distance cross wall ( section 1141 ) obstacle may occur object distance two object may derive geometric computation ( eg involve triangulation ) computational cost high large number object obstacle involved cluster obstacle problem represent used graphical notation first point p visible another point q region r straight line join p q intersect obstacle visibility graph graph vg = ( v e ) vertex obstacle corresponding node v two node v1 v2 v joined edge e corresponding vertex represent visible let vg 0 = ( v 0 e 0 ) visibility graph create vg add two additional point p q v 0 e 0 contain edge join two point v 0 two point mutually visible shortest path two point p q subpath vg 0 show figure 1116 ( ) see begin edge p either v1 v2 v3 go path vg end edge either v4 v5 q reduce cost distance computation two pair object point several preprocess optimization technique used one method group point close together microcluster do first triangulating region r triangle grouping nearby point triangle microcluster used method similar birch dbscan show figure 1116 ( b ) process microcluster rather individual point overall computation reduce precomputation perform build two v4 v1 p v2 o1 v3 o2 vg q v5 vg ( ) ( b ) figure 1116 cluster obstacle object ( o1 o2 ) ( ) visibility graph ( b ) triangulation region microcluster source adapt tung hou han [ thh01 ] 
538 chapter 11 advanced cluster analysis kind join index base computation shortest path ( 1 ) vv index pair obstacle vertex ( 2 ) mv index pair microcluster obstacle vertex use index help optimize overall performance used precomputation optimization strategy distance two point ( granularity level microcluster ) compute efficiently thus cluster process perform manner similar typical efficient k-medoid algorithm claran achieve good cluster quality large datum set 115 summary conventional cluster analysis object assign one cluster exclusively however application need assign object one cluster fuzzy probabilistic way fuzzy cluster probabilistic model-based cluster allow object belong one cluster partition matrix record membership degree object belong cluster probabilistic model-based cluster assume cluster parameterized distribution used datum cluster observed sample estimate parameter cluster mixture model assume set observed object mixture instance multiple probabilistic cluster conceptually observed object generate independently first choose probabilistic cluster accord probability cluster choose sample accord probability density function choose cluster expectation-maximization algorithm framework approach maximum likelihood maximum posteriori estimate parameter statistical model expectation-maximization algorithms used compute fuzzy cluster probabilistic model-based cluster high-dimensional datum pose several challenge cluster analysis include model high-dimensional cluster search cluster two major category cluster method high-dimensional datum subspace cluster method dimensionality reduction method subspace cluster method search cluster subspace original space example include subspace search method correlation-based cluster method bicluster method dimensionality reduction method create new space lower dimensionality search cluster bicluster method cluster object attribute simultaneously type bicluster include bicluster constant value constant value column coherent value coherent evolution column two major type bicluster method optimization-based method enumeration method 
116 exercise 539 spectral cluster dimensionality reduction method general idea construct new dimension used affinity matrix cluster graph network datum many application social network analysis challenge include measure similarity object graph design cluster model method graph network datum geodesic distance number edge two vertex graph used measure similarity alternatively similarity graph social network measure used structural context random walk simrank similarity measure base structural context random walk graph cluster modeled compute graph cut sparsest cut may lead good cluster modularity used measure cluster quality scan graph cluster algorithm search graph identify well-connected component cluster constraint used express application-specific requirement background knowledge cluster analysis constraint cluster categorize constraint instance cluster similarity measurement constraint instance include must-link link constraint constraint hard soft hard constraint cluster enforce strictly respect constraint cluster assignment process cluster soft constraint consider optimization problem heuristic used speed constrain cluster 116 exercise 111 traditional cluster method rigid require object belong exclusively one cluster explain special case fuzzy cluster may use k-mean example 112 allelectronic carry 1000 product p1 p1000 consider customer ada bob cathy ada bob purchase three product common p1 p2 p3 997 product ada bob independently purchase seven randomly cathy purchase 10 product randomly select 1000 product euclidean distance probability dist ( ada bob ) > dist ( ada cathy ) jaccard similarity ( chapter 2 ) used learn example 113 show × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 114 compare maple algorithm ( section 1123 ) frequent close itemset mining algorithm closet ( pei han mao [ phm00 ] ) major similarity difference 
540 chapter 11 advanced cluster analysis 115 simrank similarity measure cluster graph network datum ( ) prove lim si ( u v ) = ( u v ) simrank computation i→∞ ( b ) show ( u v ) = p ( u v ) simrank 116 large sparse graph average node low degree similarity matrix used simrank still sparse sense deliberate answer 117 compare scan algorithm ( section 1133 ) dbscan ( section 1041 ) similarity difference 118 consider partition cluster follow constraint cluster number object cluster must nk ( 1 − δ ) nk ( 1 + δ ) n total number object datum set k number cluster desire δ [ 0 1 ) parameter extend k-mean method handle constraint discuss situation constraint hard soft 117 bibliographic note höppner klawonn kruse runkler [ hkkr99 ] provide thorough discussion fuzzy cluster fuzzy c-mean algorithm ( example 117 base ) propose bezdek [ bez81 ] fraley raftery [ fr02 ] give comprehensive overview model-based cluster analysis probabilistic model mclachlan basford [ mb88 ] present systematic introduction mixture model application cluster analysis dempster laird rubin [ dlr77 ] recognize first introduce em algorithm give name however idea em algorithm “ propose many time special circumstance ” admit dempster laird rubin [ dlr77 ] wu [ wu83 ] give correct analysis em algorithm mixture model em algorithms used extensively many datum mining application introduction model-based cluster mixture model em algorithms find recent textbook machine learn statistical learning—for example bishop [ bis06 ] marsland [ mar09 ] alpaydin [ alp11 ] increase dimensionality severe effect distance function indicated beyer et al [ bgrs99 ] also dramatic impact various technique classification cluster semisupervised learn ( radovanović nanopoulos ivanović [ rni09 ] ) kriegel kröger zimek [ kkz09 ] present comprehensive survey method cluster high-dimensional datum clique algorithm develop agrawal gehrke gunopulos raghavan [ aggr98 ] proclus algorithm propose aggawal procopiuc wolf et al [ + 99 ] technique bicluster initially propose hartigan [ har72 ] term bicluster coin mirkin [ mir98 ] cheng church [ cc00 ] introduce 
117 bibliographic note 541 bicluster gene expression datum analysis many study bicluster model method notion δ-pcluster introduce wang wang yang yu [ wwyy02 ] informative survey see madeira oliveira [ mo04 ] tanay sharan shamir [ tss04 ] chapter introduce δ-cluster algorithm cheng church [ cc00 ] maple pei zhang cho et al [ + 03 ] example optimization-based method enumeration method bicluster respectively donath hoffman [ dh73 ] fiedler [ fie73 ] pioneer spectral cluster chapter use algorithm propose ng jordan weis [ njw01 ] example thorough tutorial spectral cluster see luxburg [ lux07 ] cluster graph network datum important fast-growing topic schaeffer [ sch07 ] provide survey simrank measure similarity develop jeh widom [ jw02a ] xu et al [ xyfs07 ] propose scan algorithm arora rao vazirani [ arv09 ] discuss sparsest cut approximation algorithms cluster constraint extensively study davidson wagstaff basu [ dwb06 ] propose measure informativeness coherence copk-mean algorithm give wagstaff et al [ wcrs01 ] cvqe algorithm propose davidson ravi [ dr05 ] tung han lakshmanan ng [ thln01 ] present framework constraint-based cluster base user-specified constraint efficient method constraint-based spatial cluster existence physical obstacle constraint propose tung hou han [ thh01 ] 
13 datum mining trend research frontier young research field datum mining make significant progress cover broad spectrum application since 1980s today datum mining used vast array area numerous commercial datum mining system service available many challenge however still remain final chapter introduce mining complex datum type prelude in-depth study reader may choose addition focus trend research frontier datum mining section 131 present overview methodology mining complex datum type extend concept task introduce book mining include mining time-series sequential pattern biological sequence graph network spatiotemporal datum include geospatial datum moving-object datum cyber-physical system datum multimedium datum text datum web datum datum stream section 132 briefly introduce approach datum mining include statistical method theoretical foundation visual audio datum mining section 133 learn datum mining application business science include financial retail telecommunication industry science engineering recommender system social impact datum mining discuss section 134 include ubiquitous invisible datum mining privacy-preserve datum mining finally section 135 speculate current expect datum mining trend arise response new challenge field 131 mining complex datum type section outline major development research effort mining complex datum type complex datum type summarize figure section 1311 cover mining sequence datum time-series symbolic sequence biological sequence section 1312 discuss mining graph social information network section 1313 address mining kind datum include spatial datum spatiotemporal datum moving-object datum cyber-physical system datum multimedium datum text datum datum mining concept technique doi b978-0-12-381479-100013-7 c 2012 elsevier right re-serve 585 
586 chapter 13 datum mining trend research frontier c p l e x p e f sequence datum graph network mining kind datum time-series datum ( eg stock market datum ) symbolic sequence ( eg customer shopping sequence web click stream ) biological sequence ( eg dna protein sequence ) homogeneous ( link type ) heterogeneous ( link different type ) example graph social information network etc spatial datum spatiotemporal datum cyber-physical system datum multimedium datum text datum web datum datum stream figure 131 complex datum type mining web datum datum stream due broad scope theme section present high-level overview topic discuss in-depth book 1311 mining sequence datum time-series symbolic sequence biological sequence sequence order list event sequence may categorize three group base characteristic event describe ( 1 ) time-series datum ( 2 ) symbolic sequence datum ( 3 ) biological sequence let ’ consider type time-series datum sequence datum consist long sequence numeric datum record equal time interval ( eg per minute per hour per day ) time-series datum generate many natural economic process stock market scientific medical natural observation symbolic sequence datum consist long sequence event nominal datum typically observed equal time interval many sequence gap ( ie lapse record event ) matter much example include customer shopping sequence web click stream well sequence event science engineering natural social development biological sequence include dna protein sequence sequence typically long carry important complicate hide semantic meaning gap usually important let ’ look datum mining sequence datum type 
131 mining complex datum type 587 similarity search time-series datum time-series datum set consist sequence numeric value obtain repeat measurement time value typically measure equal time interval ( eg every minute hour day ) time-series databasis popular many application stock market analysis economic sale forecasting budgetary analysis utility study inventory study yield projection workload projection process quality control also useful study natural phenomena ( eg atmosphere temperature wind earthquake ) scientific engineering experiment medical treatment unlike normal database query find datum match give query exactly similarity search find datum sequence differ slightly give query sequence many time-series similarity query require subsequence match find set sequence contain subsequence similar give query sequence similarity search often necessary first perform datum dimensionality reduction transformation time-series datum typical dimensionality reduction technique include ( 1 ) discrete fourier transform ( dft ) ( 2 ) discrete wavelet transform ( dwt ) ( 3 ) singular value decomposition ( svd ) base principle component analysis ( pca ) touch concept chapter 3 thorough explanation beyond scope book go great detail technique datum signal map signal transform space small subset “ strongest ” transform coefficient save feature feature form feature space projection transform space index construct original transform time-series datum speed search query-based similarity search technique include normalization transformation atomic match ( ie find pair gap-free window small length similar ) window stitching ( ie stitching similar window form pair large similar subsequence allow gap atomic match ) subsequence order ( ie linearly order subsequence match determine whether enough similar piece exist ) numerous software package exist similarity search time-series datum recently researcher propose transform time-series datum piecewise aggregate approximation datum view sequence symbolic representation problem similarity search transform one match subsequence symbolic sequence datum identify motif ( ie frequently occur sequential pattern ) build index hashing mechanism efficient search base motif experiment show approach fast simple comparable search quality dft dwt dimensionality reduction method regression trend analysis time-series datum regression analysis time-series datum study substantially field statistic signal analysis however one may often need go beyond pure regression 
chapter 13 datum mining trend research frontier price 588 allelectronic stock 10-day move average time figure 132 time-series datum stock price allelectronic time trend show dash curve calculate move average analysis perform trend analysis many practical application trend analysis build integrate model used follow four major component movement characterize time-series datum trend long-term movement indicate general direction time-series graph move time example used weight move average least square method find trend curf dash curve indicated figure 132 cyclic movement long-term oscillation trend line curve seasonal variation nearly identical pattern time series appear follow corresponding season successive year holiday shopping season effective trend analysis datum often need “ deseasonalize ” base seasonal index compute autocorrelation random movement characterize sporadic change due chance event labor dispute announce personnel change within company trend analysis also used time-series forecasting find mathematical function approximately generate historic pattern time series used make long-term short-term prediction future value arima ( auto-regressive integrate move average ) long-memory time-series modele autoregression popular method analysis sequential pattern mining symbolic sequence symbolic sequence consist order set element event record without concrete notion time many application involve datum 
131 mining complex datum type 589 symbolic sequence customer shopping sequence web click stream program execution sequence biological sequence sequence event science engineering natural social development biological sequence carry complicate semantic meaning pose many challenge research issue investigation conduct field bioinformatic sequential pattern mining focuse extensively mining symbolic sequence sequential pattern frequent subsequence exist single sequence set sequence sequence α = ha1 a2 · · · subsequence another sequence β = hb1 b2 · · · bm exist integer 1 ≤ j1 < j2 < · · · < jn ≤ a1 ⊆ bj1 a2 ⊆ bj2 ⊆ bjn example α = h { ab } di β = h { abc } { } { de } ai b c e item α subsequence mining sequential pattern consist mining set subsequence frequent one sequence set sequence many scalable algorithms develop result extensive study area alternatively mine set close sequential pattern sequential pattern close exist sequential pattern 0 proper subsequence 0 0 ( frequency ) support s similar frequent pattern mining counterpart also study efficient mining multidimensional multilevel sequential pattern constraint-based frequent pattern mining user-specified constraint used reduce search space sequential pattern mining derive pattern interest user refer constraint-based sequential pattern mining moreover may relax constraint enforce additional constraint problem sequential pattern mining derive different kind pattern sequence datum example enforce gap constraint pattern derive contain consecutive subsequence subsequence small gap alternatively may derive periodic sequential pattern fold event proper-size window find recur subsequence window another approach derive partial order pattern relax requirement strict sequential order mining subsequence pattern besides mining partial order pattern sequential pattern mining methodology also extend mining tree lattice episode order pattern sequence classification classification method perform model construction base feature vector however sequence explicit feature even sophisticated feature selection technique dimensionality potential feature still high sequential nature feature difficult capture make sequence classification challenge task sequence classification method organized three category ( 1 ) featurebased classification transform sequence feature vector apply conventional classification method ( 2 ) sequence distance–based classification distance function measure similarity sequence determine 
590 chapter 13 datum mining trend research frontier quality classification significantly ( 3 ) model-based classification used hide markov model ( hmm ) statistical model classify sequence time-series numeric-valu datum feature selection technique symbolic sequence easily apply time-series datum without discretization however discretization cause information loss recently propose time-series shapelet method used time-series subsequence maximally represent class feature achieve quality classification result alignment biological sequence biological sequence generally refer sequence nucleotide amino acid biological sequence analysis compare align index analyze biological sequence thus play crucial role bioinformatic modern biology sequence alignment base fact live organism related evolution imply nucleotide ( dna rna ) protein sequence species closer evolution exhibit similarity alignment process line sequence achieve maximal identity level also express degree similarity sequence two sequence homologous share common ancestor degree similarity obtain sequence alignment useful determine possibility homology two sequence alignment also help determine relative position multiple species evolution tree call phylogenetic tree problem alignment biological sequence describe follow give two input biological sequence identify similar sequence long conserve subsequence number sequence align exactly two problem know pairwise sequence alignment otherwise multiple sequence alignment sequence compare align either nucleotide ( rna ) amino acid ( protein ) nucleotide two symbol align identical however amino acid two symbol align identical one derive substitution likely occur nature two kind alignment local alignment global alignment former mean portion sequence align whereas latter require alignment entire length sequence either nucleotide amino acid insertion deletion substitution occur nature different probability substitution matrix used represent probability substitution nucleotide amino acid probability insertion deletion usually use gap character − indicate position preferable align two symbol evaluate quality alignment score mechanism typically defined usually count identical similar symbol positive score gap negative one algebraic sum score take alignment measure goal alignment achieve maximal score among possible alignment however expensive ( exactly np-hard problem ) find optimal alignment therefore various heuristic method develop find suboptimal alignment 
131 mining complex datum type 591 dynamic programming approach commonly used sequence alignment among many available analysis package blast ( basic local alignment search tool ) one popular tool biosequence analysis hide markov model biological sequence analysis give biological sequence biologist would like analyze sequence represent represent structure statistical regularity sequence class biologist construct various probabilistic model markov chain hide markov model model probability state depend previous state therefore particularly useful analysis biological sequence datum common method construct hide markov model forward algorithm viterbi algorithm baum-welch algorithm give sequence symbol x forward algorithm find probability obtain x model viterbi algorithm find probable path ( corresponding x ) model whereas baum-welch algorithm learn adjust model parameter best explain set training sequence 1312 mining graph network graph represent general class structure set sequence lattice tree broad range graph application web social network information network biological network bioinformatic chemical informatic computer vision multimedium text retrieval hence graph network mining become increasingly important heavily research overview follow major theme ( 1 ) graph pattern mining ( 2 ) statistical modele network ( 3 ) datum clean integration validation network analysis ( 4 ) cluster classification graph homogeneous network ( 5 ) cluster ranking classification heterogeneous network ( 6 ) role discovery link prediction information network ( 7 ) similarity search olap information network ( 8 ) evolution information network graph pattern mining graph pattern mining mining frequent subgraph ( also call ( sub ) graph pattern ) one set graph method mining graph pattern categorize apriori-based pattern growth–base approach alternatively mine set close graph graph g close exist proper supergraph g 0 carry support count g moreover many variant graph pattern include approximate frequent graph coherent graph dense graph user-specified constraint push deep graph pattern mining process improve mining efficiency graph pattern mining many interesting application example used generate compact effective graph index structure base concept 
592 chapter 13 datum mining trend research frontier frequent discriminative graph pattern approximate structure similarity search achieve explore graph index structure multiple graph feature moreover classification graph also perform effectively used frequent discriminative subgraph feature statistical modele network network consist set node corresponding object associate set property set edge ( link ) connect node represent relationship object network homogeneous node link type friend network coauthor network web page network network heterogeneous node link different type publication network ( link together author conference paper content ) health-care network ( link together doctor nurse patient disease treatment ) researcher propose multiple statistical model modele homogeneous network well-known generative model random graph model ( ie erdös-rényi model ) watts-strogatz model scale-free model scalefree model assume network follow power law distribution ( also know pareto distribution heavy-tailed distribution ) large-scale social network small-world phenomenon observed network characterize high degree local cluster small fraction node ( ie node interconnect one another ) degree separation remain node social network exhibit certain evolutionary characteristic tend follow densification power law state network become increasingly dense time shrink diameter another characteristic effective diameter often decrease network grow node out-degree in-degree typically follow heavytailed distribution datum clean integration validation information network analysis real-world datum often incomplete noisy uncertain unreliable information redundancy may exist among multiple piece datum interconnect large network information redundancy explore network perform quality datum clean datum integration information validation trustability analysis network analysis example distinguish author share name examine networked connection heterogeneous object coauthor publication venue term addition identify inaccurate author information present bookseller explore network build base author information provide multiple bookseller sophisticated information network analysis method develop direction many case portion datum serve “ training ” relatively clean reliable datum consensus datum multiple information 
131 mining complex datum type 593 provider used help consolidate remain unreliable portion datum reduce costly effort labele datum hand training massive dynamic real-world datum set cluster classification graph homogeneous network large graph network cohesive structure often hide among massive interconnect node link cluster analysis method develop large network uncover network structure discover hide community hub outlier base network topological structure associate property various kind network cluster method develop categorize either partition hierarchical density-based algorithms moreover give human-labele training datum discovery network structure guide human-specify heuristic constraint supervised classification semi-supervised classification network recent hot topic datum mining research community cluster ranking classification heterogeneous network heterogeneous network contain interconnect node link different type interconnect structure contain rich information used mutually enhance node link propagate knowledge one type another cluster ranking heterogeneous network perform hand-inhand context highly rank link cluster may contribute lower-rank counterpart evaluation cohesiveness cluster cluster may help consolidate high ranking link dedicate cluster mutual enhancement ranking cluster prompt development algorithm call rankclus moreover user may specify different ranking rule present labele link certain datum type knowledge one type propagate type propagation reach link type via heterogeneous-type connection algorithms develop supervised learn semi-supervised learn heterogeneous network role discovery link prediction information network exist many hide role relationship among different link heterogeneous network example include advisor–advisee leader–follower relationship research publication network discover hide role relationship expert specify constraint base background knowledge enforce constraint may help crosscheck validation large interconnect network information redundancy network often used help weed link follow constraint 
594 chapter 13 datum mining trend research frontier similarly link prediction perform base assessment ranking expect relationship among candidate link example may predict paper author may write read cite base author ’ recent publication history trend research similar topic study often require analyze proximity network link trend connection similar neighbor roughly speaking person refer link prediction link mining however link mining cover additional task include link-based object classification object type prediction link type prediction link existence prediction link cardinality estimation object reconciliation ( predict whether two object fact ) also include group detection ( cluster object ) well subgraph identification ( find characteristic subgraph within network ) metadata mining ( uncover schema-type information regard unstructured datum ) similarity search olap information network similarity search primitive operation database web search engine heterogeneous information network consist multityped interconnect object example include bibliographic network social medium network two object consider similar link similar way multityped object general object similarity within network determine base network structure object property similarity measure moreover network cluster hierarchical network structure help organize object network identify subcommunity well facilitate similarity search furthermore similarity defined differently per user consider different linkage path derive various similarity semantic network know path-based similarity organize network base notion similarity cluster generate multiple hierarchy within network online analytical process ( olap ) perform example drill dice information network base different level abstraction different angle view olap operation may generate multiple interrelate network relationship among network may disclose interesting hide semantic evolution social information network network dynamic constantly evolve detect evolve community evolve regularity anomaly homogeneous heterogeneous network help person better understand structural evolution network predict trend irregularity evolve network homogeneous network evolve community discover subnetwork consist object type set friend coauthor however heterogeneous network community discover subnetwork consist object different type connect set paper author venue term also derive set evolve object type like evolve author theme 
131 mining complex datum type 595 1313 mining kind datum addition sequence graph many kind semi-structure unstructured datum spatiotemporal multimedium hypertext datum interesting application datum carry various kind semantic either store dynamically stream system call specialize datum mining methodology thus mining multiple kind datum include spatial datum spatiotemporal datum cyber-physical system datum multimedium datum text datum web datum datum stream increasingly important task datum mining subsection overview methodology mining kind datum mining spatial datum spatial datum mining discover pattern knowledge spatial datum spatial datum many case refer geospace-related datum store geospatial datum repository datum “ vector ” “ raster ” format form imagery geo-reference multimedium recently large geographic datum warehouse construct integrate thematic geographically reference datum multiple source construct spatial datum cube contain spatial dimension measure support spatial olap multidimensional spatial datum analysis spatial datum mining perform spatial datum warehouse spatial databasis geospatial datum repository popular topic geographic knowledge discovery spatial datum mining include mining spatial association co-location pattern spatial cluster spatial classification spatial modele spatial trend outlier analysis mining spatiotemporal datum move object spatiotemporal datum datum relate space time spatiotemporal datum mining refer process discover pattern knowledge spatiotemporal datum typical example spatiotemporal datum mining include discover evolutionary history city land uncover weather pattern predict earthquake hurricane determine global warm trend spatiotemporal datum mining become increasingly important far-reaching implication give popularity mobile phone gps device internet-based map service weather service digital earth well satellite rfid sensor wireless video technology among many kind spatiotemporal datum moving-object datum ( ie datum move object ) especially important example animal scientist attach telemetry equipment wildlife analyze ecological behavior mobility manager emb gps car better monitor guide vehicle meteorologist use weather satellite radar observe hurricane massive-scale moving-object datum become rich complex ubiquitous example moving-object datum mining include mining movement pattern multiple move object ( ie discovery relationship among multiple move object move cluster leader follower merge convoy swarm pincer well collective movement pattern ) example 
596 chapter 13 datum mining trend research frontier moving-object datum mining include mining periodic pattern one set move object mining trajectory pattern cluster model outlier mining cyber-physical system datum cyber-physical system ( cp ) typically consist large number interact physical information component cp system may interconnect form large heterogeneous cyber-physical network example cyber-physical network include patient care system link patient monitoring system network medical information emergency handle system transportation system link transportation monitoring network consist many sensor video camera traffic information control system battlefield commander system link reconnaissance network battlefield information analysis system clearly cyber-physical system network ubiquitous form critical component modern information infrastructure datum generate cyber-physical system dynamic volatile noisy inconsistent interdependent contain rich spatiotemporal information critically important real-time decision make comparison typical spatiotemporal datum mining mining cyber-physical datum require link current situation large information base perform real-time calculation return prompt response research area include rare-event detection anomaly analysis cyber-physical datum stream reliability trustworthiness cyber-physical datum analysis effective spatiotemporal datum analysis cyber-physical network integration stream datum mining real-time automate control process mining multimedium datum multimedium datum mining discovery interesting pattern multimedium databasis store manage large collection multimedium object include image datum video datum audio datum well sequence datum hypertext datum contain text text markup linkage multimedium datum mining interdisciplinary field integrate image process understand computer vision datum mining pattern recognition issue multimedium datum mining include content-based retrieval similarity search generalization multidimensional analysis multimedium datum cube contain additional dimension measure multimedium information topic multimedium mining include classification prediction analysis mining association video audio datum mining ( section 1323 ) mining text datum text mining interdisciplinary field draw information retrieval datum mining machine learn statistic computational linguistic substantial portion information store text news article technical paper book digital library email message blog web page hence research text mining active important goal derive high-quality information text 
131 mining complex datum type 597 typically do discovery pattern trend mean statistical pattern learn topic modele statistical language modele text mining usually require structuring input text ( eg parse along addition derive linguistic feature removal other subsequent insertion database ) follow derive pattern within structure datum evaluation interpretation output “ high quality ” text mining usually refer combination relevance novelty interestingness typical text mining task include text categorization text cluster entity extraction production granular taxonomy sentiment analysis document summarization entity-relation modele ( ie learn relation name entity ) example include multilingual datum mining multidimensional text analysis contextual text mining trust evolution analysis text datum well text mining application security biomedical literature analysis online medium analysis analytical customer relationship management various kind text mining analysis software tool available academic institution open-source forum industry text mining often also used wordnet sematic web wikipedia information source enhance understand mining text datum mining web datum world wide web serve huge widely distribute global information center news advertisement consumer information financial management education government e-commerce contain rich dynamic collection information web page content hypertext structure multimedium hyperlink information access usage information provide fertile source datum mining web mining application datum mining technique discover pattern structure knowledge web accord analysis target web mining organized three main area web content mining web structure mining web usage mining web content mining analyze web content text multimedium datum structure datum ( within web page link across web page ) do understand content web page provide scalable informative keyword-based page indexing concept resolution web page relevance ranking web page content summary valuable information related web search analysis web page reside either surface web deep web surface web portion web index typical search engine deep web ( hide web ) refer web content part surface web content provide underlie database engine web content mining study extensively researcher search engine web service company web content mining build link across multiple web page individual therefore potential inappropriately disclose personal information study privacy-preserve datum mining address concern development technique protect personal privacy web web structure mining process used graph network mining theory method analyze node connection structure web extract pattern hyperlink hyperlink structural component connect 
598 chapter 13 datum mining trend research frontier web page another location also mine document structure within page ( eg analyze treelike structure page structure describe html xml tag usage ) kind web structure mining help us understand web content may also help transform web content relatively structure datum set web usage mining process extract useful information ( eg user click stream ) server log find pattern related general particular group user understand user ’ search pattern trend association predict user look internet help improve search efficiency effectiveness well promote product related information different group user right time web search company routinely conduct web usage mining improve quality service mining datum stream stream datum refer datum flow system vast volume change dynamically possibly infinite contain multidimensional feature datum store traditional database system moreover system may able read stream sequential order pose great challenge effective mining stream datum substantial research lead progress development efficient method mining datum stream area mining frequent sequential pattern multidimensional analysis ( eg construction stream cube ) classification cluster outlier analysis online detection rare event datum stream general philosophy develop single-scan a-few-scan algorithms used limit compute storage capability include collect information stream datum slide window tilt time window ( recent datum register finest granularity distant datum register coarser granularity ) explore technique like microcluster limit aggregation approximation many application stream datum mining explored—for example real-time detection anomaly computer network traffic botnet text stream video stream power-grid flow web search sensor network cyber-physical system 132 methodology datum mining due broad scope datum mining large variety datum mining methodology methodology datum mining thoroughly cover book section briefly discuss several interesting methodology fully address previous chapter methodology list figure 133 1321 statistical datum mining datum mining technique describe book primarily draw computer science discipline include datum mining machine learn datum warehousing algorithms design efficient handle huge amount datum 
132 methodology datum mining h e r n n g e h l g e statistical datum mining foundation datum mining visual audio datum mining 599 regression generalized linear model analysis variance mixed-effect model factor analysis discriminant analysis survival analysis datum reduction datum compression probability statistical theory microeconomic view pattern discovery inductive database datum visualization datum mining result visualization datum mining process visualization interactive visual datum mining audio datum mining figure 133 datum mining methodology typically multidimensional possibly various complex type however many well-established statistical technique datum analysis particularly numeric datum technique apply extensively scientific datum ( eg datum experiment physics engineering manufacturing psychology medicine ) well datum economic social science technique principal component analysis ( chapter 3 ) cluster ( chapter 10 11 ) already address book thorough discussion major statistical method datum analysis beyond scope book however several method mentioned sake completeness pointer technique provide bibliographic note ( section 138 ) regression general method used predict value response ( dependent ) variable one predictor ( independent ) variable variable numeric various form regression linear multiple weight polynomial nonparametric robust ( robust method useful error fail satisfy normalcy condition datum contain significant outlier ) generalized linear model model generalization ( generalized additive model ) allow categorical ( nominal ) response variable ( transformation 
600 chapter 13 datum mining trend research frontier ) related set predictor variable manner similar modele numeric response variable used linear regression generalized linear model include logistic regression poisson regression analysis variance technique analyze experimental datum two population describe numeric response variable one categorical variable ( factor ) general anova ( single-factor analysis variance ) problem involve comparison k population treatment mean determine least two mean different complex anova problem also exist mixed-effect model model analyze group data—data classify accord one grouping variable typically describe relationship response variable covariate datum group accord one factor common area application include multilevel datum repeat measure datum block design longitudinal datum factor analysis method used determine variable combine generate give factor example many psychiatric datum possible measure certain factor interest directly ( eg intelligence ) however often possible measure quantity ( eg student test score ) reflect factor interest none variable designate dependent discriminant analysis technique used predict categorical response variable unlike generalized linear model assume independent variable follow multivariate normal distribution procedure attempt determine several discriminant function ( linear combination independent variable ) discriminate among group defined response variable discriminant analysis commonly used social science survival analysis several well-established statistical technique exist survival analysis technique originally design predict probability patient undergo medical treatment would survive least time t method survival analysis however also commonly apply manufacturing setting estimate life span industrial equipment popular method include kaplanmeier estimate survival cox proportional hazard regression model extension quality control various statistic used prepare chart quality control shewhart chart cusum chart ( display group summary statistic ) statistic include mean standard deviation range count move average move standard deviation move range 1322 view datum mining foundation research theoretical foundation datum mining yet mature solid systematic theoretical foundation important help provide coherent 
132 methodology datum mining 601 framework development evaluation practice datum mining technology several theory basis datum mining include follow datum reduction theory basis datum mining reduce datum representation datum reduction trade accuracy speed response need obtain quick approximate answer query large databasis datum reduction technique include singular value decomposition ( drive element behind principal component analysis ) wavelet regression log-linear model histogram cluster sampling construction index tree datum compression accord theory basis datum mining compress give datum encode term bit association rule decision tree cluster encode base minimum description length principle state “ best ” theory infer datum set one minimize length theory datum encode used theory predictor datum encode typically bit probability statistical theory accord theory basis datum mining discover joint probability distribution random variable example bayesian belief network hierarchical bayesian model microeconomic view microeconomic view consider datum mining task find pattern interesting extent used decision-make process enterprise ( eg regard marketing strategy production plan ) view one utility pattern consider interesting act enterprise regard face optimization problem object maximize utility value decision theory datum mining become nonlinear optimization problem pattern discovery inductive databasis theory basis datum mining discover pattern occur datum association classification model sequential pattern area machine learn neural network association mining sequential pattern mining cluster several subfield contribute theory knowledge base view database consist datum pattern user interact system query datum theory ( ie pattern ) knowledge base knowledge base actually inductive database theory mutually exclusive example pattern discovery also see form datum reduction datum compression ideally theoretical framework able model typical datum mining task ( eg association classification cluster ) probabilistic nature able handle different form datum consider iterative interactive essence datum mining effort require establish well-defined framework datum mining satisfy requirement 
602 chapter 13 datum mining trend research frontier 1323 visual audio datum mining visual datum mining discover implicit useful knowledge large datum set used datum or knowledge visualization technique human visual system controlled eye brain latter thought powerful highly parallel process reasoning engine contain large knowledge base visual datum mining essentially combine power component make highly attractive effective tool comprehension datum distribution pattern cluster outlier datum visual datum mining view integration two discipline datum visualization datum mining also closely related computer graphic multimedium system human–computer interaction pattern recognition high-performance compute general datum visualization datum mining integrate follow way datum visualization datum database datum warehouse view different granularity abstraction level different combination attribute dimension datum present various visual form boxplot 3-d cube datum distribution chart curf surface link graph show datum visualization section chapter figure 134 135 statsoft show figure 134 boxplot show multiple variable combination statsoft source wwwstatsoftcom 
132 methodology datum mining 603 figure 135 multidimensional datum distribution analysis statsoft source wwwstatsoftcom datum distribution multidimensional space visual display help give user clear impression overview datum characteristic large datum set datum mining result visualization visualization datum mining result presentation result knowledge obtain datum mining visual form form may include scatter plot boxplot ( chapter 2 ) well decision tree association rule cluster outlier generalized rule example scatter plot show figure 136 sas enterprise miner figure 137 mineset used plane associate set pillar describe set association rule mine database figure 138 also mineset present decision tree figure 139 ibm intelligent miner present set cluster property associate datum mining process visualization type visualization present various process datum mining visual form user see datum extract database datum warehouse extract well select datum clean integrate preprocessed mine moreover may also show method select datum mining result store may view figure 1310 show visual presentation datum mining process clementine datum mining system 
604 chapter 13 datum mining trend research frontier figure 136 visualization datum mining result sas enterprise miner interactive visual datum mining ( interactive ) visual datum mining visualization tool used datum mining process help user make smart datum mining decision example datum distribution set attribute display used colored sector ( whole space represent circle ) display help user determine sector first select classification good split point sector may example show figure 1311 output perception-based classification ( pbc ) system develop university munich audio datum mining used audio signal indicate pattern datum feature datum mining result although visual datum mining may disclose interesting pattern used graphical display require user concentrate watch pattern identify interesting novel feature within sometimes quite tiresome pattern transform sound music instead watch picture listen pitch rhythm tune melody identify anything interesting unusual may relieve burden visual concentration 
132 methodology datum mining figure 137 visualization association rule mineset figure 138 visualization decision tree mineset 605 
606 chapter 13 datum mining trend research frontier figure 139 visualization cluster grouping ibm intelligent miner figure 1310 visualization datum mining process clementine 
133 datum mining application 607 figure 1311 perception-based classification interactive visual mining approach relax visual mining therefore audio datum mining interesting complement visual mining 133 datum mining application book study principle method mining relational datum datum warehouse complex datum type datum mining relatively young discipline wide diverse application still nontrivial gap general principle datum mining application-specific effective datum mining tool section examine several application domain list figure discuss customize datum mining method tool develop application 1331 datum mining financial datum analysis bank financial institution offer wide variety banking investment credit service ( latter include business mortgage automobile loan credit card ) also offer insurance stock investment service 
608 chapter 13 datum mining trend research frontier financial datum analysis retail telecommunication industry science engineering datum mining application intrusion detection prevention recommender system figure 1312 common datum mining application domain financial datum collect banking financial industry often relatively complete reliable high quality facilitate systematic datum analysis datum mining present typical case design construction datum warehouse multidimensional datum analysis datum mining like many application datum warehouse need construct banking financial datum multidimensional datum analysis method used analyze general property datum example company ’ financial officer may want view debt revenue change month region sector factor along maximum minimum total average trend deviation statistical information datum warehouse datum cube ( include advanced datum cube concept multifeature discovery-driven regression prediction datum cube ) characterization class comparison cluster outlier analysis play important role financial datum analysis mining loan payment prediction customer credit policy analysis loan payment prediction customer credit analysis critical business bank many factor strongly weakly influence loan payment performance customer credit rating datum mining method attribute selection attribute relevance ranking may help identify important factor eliminate irrelevant one example factor related risk loan payment include loan-to-value ratio term loan debt ratio ( total amount monthly debt versus total monthly income ) payment-to-income ratio customer income level education level residence region credit history analysis customer payment history may find say payment-to-income ratio dominant factor education level debt ratio bank may decide adjust loan-grant policy 
133 datum mining application 609 grant loan customer whose application previously deny whose profile show relatively low risk accord critical factor analysis classification cluster customer target marketing classification cluster method used customer group identification target marketing example use classification identify crucial factor may influence customer ’ decision regard banking customer similar behavior regard loan payment may identify multidimensional cluster technique help identify customer group associate new customer appropriate customer group facilitate target marketing detection money launder financial crime detect money launder financial crime important integrate information multiple heterogeneous databasis ( eg bank transaction databasis federal state crime history databasis ) long potentially related study multiple datum analysis tool used detect unusual pattern large amount cash flow certain period certain group customer useful tool include datum visualization tool ( display transaction activity used graph time group customer ) linkage information network analysis tool ( identify link among different customer activity ) classification tool ( filter unrelated attribute rank highly related one ) cluster tool ( group different case ) outlier analysis tool ( detect unusual amount fund transfer activity ) sequential pattern analysis tool ( characterize unusual access sequence ) tool may identify important relationship pattern activity help investigator focus suspicious case detailed examination 1332 datum mining retail telecommunication industry retail industry well-fit application area datum mining since collect huge amount datum sale customer shopping history good transportation consumption service quantity datum collect continue expand rapidly especially due increase availability ease popularity business conduct web e-commerce today major chain store also web site customer make purchase online business amazoncom ( wwwamazoncom ) exist solely online without brick-and-mortar ( ie physical ) store location retail datum provide rich source datum mining retail datum mining help identify customer buy behavior discover customer shopping pattern trend improve quality customer service achieve better customer retention satisfaction enhance good consumption ratio design effective good transportation distribution policy reduce cost business example datum mining retail industry outlined follow design construction datum warehouse retail datum cover wide spectrum ( include sale customer employee good transportation consumption 
610 chapter 13 datum mining trend research frontier service ) many way design datum warehouse industry level detail include vary substantially outcome preliminary datum mining exercise used help guide design development datum warehouse structure involve decide dimension level include preprocess perform facilitate effective datum mining multidimensional analysis sale customer product time region retail industry require timely information regard customer need product sale trend fashion well quality cost profit service commodity therefore important provide powerful multidimensional analysis visualization tool include construction sophisticated datum cube accord need datum analysis advanced datum cube structure introduce chapter 5 useful retail datum analysis facilitate analysis multidimensional aggregate complex condition analysis effectiveness sale campaign retail industry conduct sale campaign used advertisement coupon various kind discount bonuse promote product attract customer careful analysis effectiveness sale campaign help improve company profit multidimensional analysis used purpose compare amount sale number transaction contain sale item sale period versus contain item sale campaign moreover association analysis may disclose item likely purchase together item sale especially comparison sale campaign customer retention—analysis customer loyalty use customer loyalty card information register sequence purchase particular customer customer loyalty purchase trend analyze systematically good purchase different period customer group sequence sequential pattern mining used investigate change customer consumption loyalty suggest adjustment pricing variety good help retain customer attract new one product recommendation cross-referencing item mining association sale record may discover customer buy digital camera likely buy another set item information used form product recommendation collaborative recommender system ( section 1335 ) use datum mining technique make personalize product recommendation live customer transaction base opinion customer product recommendation also advertised sale receipt weekly flyer web help improve customer service aid customer select item increase sale similarly information “ hot item week ” attractive deal display together associative information promote sale fraudulent analysis identification unusual pattern fraudulent activity cost retail industry million dollar per year important ( 1 ) identify potentially fraudulent user atypical usage pattern ( 2 ) detect attempt gain fraudulent entry unauthorized access individual organizational 
133 datum mining application 611 account ( 3 ) discover unusual pattern may need special attention many pattern discover multidimensional analysis cluster analysis outlier analysis another industry handle huge amount datum telecommunication industry quickly evolved offer local long-distance telephone service provide many comprehensive communication service include cellular phone smart phone internet access email text message image computer web datum transmission datum traffic integration telecommunication computer network internet numerous mean communication compute way change face telecommunication compute create great demand datum mining help understand business dynamic identify telecommunication pattern catch fraudulent activity make better use resource improve service quality datum mining task telecommunication share many similarity retail industry common task include construct large-scale datum warehouse perform multidimensional visualization olap in-depth analysis trend customer pattern sequential pattern task contribute business improvement cost reduction customer retention fraud analysis sharpen edge competition many datum mining task customize datum mining tool telecommunication flourishing expect play increasingly important role business datum mining popularly used many industry insurance manufacturing health care well analysis governmental institutional administration datum although industry characteristic datum set application demand share many common principle methodology therefore effective mining one industry may gain experience methodology transfer industrial application 1333 datum mining science engineering past many scientific datum analysis task tend handle relatively small homogeneous datum set datum typically analyze used “ formulate hypothesis build model evaluate result ” paradigm case statistical technique typically employ analysis ( see section 1321 ) massive datum collection storage technology recently change landscape scientific datum analysis today scientific datum amassed much higher speed lower cost result accumulation huge volume high-dimensional datum stream datum heterogenous datum contain rich spatial temporal information consequently scientific application shift “ hypothesize-and-test ” paradigm toward “ collect store datum mine new hypothesis confirm datum experimentation ” process shift bring new challenge datum mining vast amount datum collect scientific domain ( include geoscience astronomy meteorology geology biological science ) used sophisticated 
612 chapter 13 datum mining trend research frontier telescope multispectral high-resolution remote satellite sensor global position system new generation biological datum collection analysis technology large datum set also generate due fast numeric simulation various field climate ecosystem modele chemical engineering fluid dynamic structural mechanic look challenge bring emerge scientific application datum mining datum warehouse datum preprocess datum preprocess datum warehouse critical information exchange datum mining create warehouse often require find mean resolve inconsistent incompatible datum collect multiple environment different time period require reconcile semantic reference system geometry measurement accuracy precision method need integrate datum heterogeneous source identify event instance consider climate ecosystem datum spatial temporal require cross-referencing geospatial datum major problem analyze datum many event spatial domain temporal domain example el nino event occur every four seven year previous datum might collect systematically today method also need efficient computation sophisticated spatial aggregate handle spatial-related datum stream mining complex datum type scientific datum set heterogeneous nature typically involve semi-structure unstructured datum multimedium datum georeference stream datum well datum sophisticated deeply hide semantic ( eg genomic proteomic datum ) robust dedicate analysis method need handle spatiotemporal datum biological datum related concept hierarchy complex semantic relationship example bioinformatic research problem identify regulatory influence gene gene regulation refer gene cell switch ( ) determine cell ’ function different biological process involve different set gene act together precisely regulate pattern thus understand biological process need identify participate gene regulator require development sophisticated datum mining method analyze large biological datum set clue regulatory influence specific gene find dna segment ( “ regulatory sequence ” ) mediate influence graph-based network-based mining often difficult impossible model several physical phenomena process due limitation exist modele approach alternatively labele graph network may used capture many spatial topological geometric biological relational characteristic present scientific datum set graph network modele object mine represent vertex graph edge vertex represent relationship object example graph used model chemical structure biological pathway datum generate numeric 
133 datum mining application 613 simulation fluid-flow simulation success graph network modele however depend improvement scalability efficiency many graph-based datum mining task classification frequent pattern mining cluster visualization tool domain-specific knowledge high-level graphical user interface visualization tool require scientific datum mining system integrate exist domain-specific datum information system guide researcher general user search pattern interpret visualize discover pattern used discover knowledge decision make datum mining engineering share many similarity datum mining science practice often collect massive amount datum require datum preprocess datum warehousing scalable mining complex type datum typically use visualization make good use graph network moreover many engineering process need real-time response mining datum stream real time often become critical component massive amount human communication datum pour daily life communication exist many form include news blog article web page online discussion product reviews twitter message advertisement communication web various kind social network hence datum mining social science social study become increasingly popular moreover user reader feedback regard product speech article analyze deduce general opinion sentiment view society analysis result used predict trend improve work help decision make computer science generate unique kind datum example computer program long execution often generate huge-size trace computer network complex structure network flow dynamic massive sensor network may generate large amount datum varied reliability computer system databasis suffer various kind attack data access may raise security privacy concern unique kind datum provide fertile land datum mining datum mining computer science used help monitor system status improve system performance isolate software bug detect software plagiarism analyze computer system fault uncover network intrusion recognize system malfunction datum mining software system engineering operate static dynamic ( ie stream-based ) datum depend whether system dump trace beforehand postanalysis must react real time handle online datum various method develop domain integrate extend method machine learn datum mining system engineering pattern recognition statistic datum mining computer science active rich domain datum miner unique challenge require development sophisticated scalable real-time datum mining system engineering method 
614 chapter 13 datum mining trend research frontier 1334 datum mining intrusion detection prevention security computer system datum continual risk extensive growth internet increase availability tool trick intrude attack network prompt intrusion detection prevention become critical component networked system intrusion defined set action threaten integrity confidentiality availability network resource ( eg user account file system system kernel ) intrusion detection system intrusion prevention system monitor network traffic or system execution malicious activity however former produce report whereas latter place in-line able actively block intrusion detected main function intrusion prevention system identify malicious activity log information say activity attempt stop activity report activity majority intrusion detection prevention system use either signaturebased detection anomaly-based detection signature-based detection method detection utilize signature attack pattern preconfigured predetermine domain expert signature-based intrusion prevention system monitor network traffic match signature match find intrusion detection system report anomaly intrusion prevention system take additional appropriate action note since system usually quite dynamic signature need update laboriously whenever new software version arrive change network configuration situation occur another drawback detection mechanism identify case match signature unable detect new previously unknown intrusion trick anomaly-based detection method build model normal network behavior ( call profile ) used detect new pattern significantly deviate profile deviation may represent actual intrusion simply new behavior need add profile main advantage anomaly detection may detect novel intrusion yet observed typically human analyst must sort deviation ascertain represent real intrusion limit factor anomaly detection high percentage false positive new pattern intrusion add set signature enhance signature-based detection datum mining method help intrusion detection prevention system enhance performance various way follow new datum mining algorithms intrusion detection datum mining algorithms used signature-based anomaly-based detection signature-based detection training datum labele either “ normal ” “ ” classifier derive detect know intrusion research area 
133 datum mining application 615 include application classification algorithms association rule mining cost-sensitive modele anomaly-based detection build model normal behavior automatically detect significant deviation method include application cluster outlier analysis classification algorithms statistical approach technique used must efficient scalable capable handle network datum high volume dimensionality heterogeneity association correlation discriminative pattern analysis help select build discriminative classifier association correlation discriminative pattern mining apply find relationship system attribute describe network datum information provide insight regard selection useful attribute intrusion detection new attribute derive aggregate datum may also helpful summary count traffic match particular pattern analysis stream datum due transient dynamic nature intrusion malicious attack crucial perform intrusion detection datum stream environment moreover event may normal consider malicious view part sequence event thus necessary study sequence event frequently encounter together find sequential pattern identify outlier datum mining method find evolve cluster build dynamic classification model datum stream also necessary real-time intrusion detection distribute datum mining intrusion launch several different location target many different destination distribute datum mining method may used analyze network datum several network location detect distribute attack visualization query tool visualization tool available view anomalous pattern detected tool may include feature view association discriminative pattern cluster outlier intrusion detection system also graphical user interface allow security analyst pose query regard network datum intrusion detection result summary computer system continual risk break security datum mining technology used develop strong intrusion detection prevention system may employ signature-based anomaly-based detection 1335 datum mining recommender system today ’ consumer face million good service shopping online recommender system help consumer make product recommendation likely interest user book cds movie restaurant online news article service recommender system may use either contentbased approach collaborative approach hybrid approach combine content-based collaborative method 
616 chapter 13 datum mining trend research frontier content-based approach recommend item similar item user prefer query past rely product feature textual item description collaborative approach ( collaborative filter approach ) may consider user ’ social environment recommend item base opinion customer similar taste preference user recommender system use broad range technique information retrieval statistic machine learn datum mining search similarity among item customer preference consider example 131 example 131 scenario used recommender system suppose visit web site online bookstore ( eg amazon ) intention purchasing book want read type name book first time visit web site browse even make purchase last christmas web store remember previous visit store click stream information information regard past purchase system display description price book specify compare interest customer similar interest recommend additional book title say “ customer buy book specify also buy title ” survey list see another title spark interest decide purchase one well suppose go another online store intention purchasing digital camera system suggest additional item consider base previously mine sequential pattern “ customer buy kind digital camera likely buy particular brand printer memory card photo editing software within three ” decide buy camera without additional item week later receive coupon store regard additional item advantage recommender system provide personalization customer e-commerce promote one-to-one marketing amazon pioneer use collaborative recommender system offer “ personalize store every customer ” part marketing strategy personalization benefit consumer company involved accurate model customer company gain better understand customer need serve need result greater success regard cross-selling related product upsel product affinity one-to-one promotion larger basket customer retention recommendation problem consider set c user set item let u utility function measure usefulness item user c utility commonly represent rating initially defined item previously rate user example join movie recommendation system user typically ask rate several movie space c × possible user item huge recommendation system able extrapolate know unknown rating predict item–user combination item highest predict utility user recommend user 
133 datum mining application 617 “ utility item estimate user ” content-based method estimate base utility assign user item similar many system focus recommend item contain textual information web site article news message look commonality among item movie may look similar genre director actor article may look similar term content-based method root information theory make use keyword ( describe item ) user profile contain information user ’ taste need profile may obtain explicitly ( eg questionnaire ) learn user ’ transactional behavior time collaborative recommender system try predict utility item user u base item previously rate user similar u example recommend book collaborative recommender system try find user history agree u ( eg tend buy similar book give similar rating book ) collaborative recommender system either memory ( heuristic ) base model base memory-based method essentially use heuristic make rating prediction base entire collection item previously rate user unknown rating item–user combination estimate aggregate rating similar user item typically k-nearest-neighbor approach used find k user ( neighbor ) similar target user u various approach used compute similarity user popular approach use either pearson ’ correlation coefficient ( section 332 ) cosine similarity ( section 247 ) weight aggregate used adjust fact different user may use rating scale differently model-based collaborative recommender system use collection rating learn model used make rating prediction example probabilistic model cluster ( find cluster like-minded customer ) bayesian network machine learn technique used recommender system face major challenge scalability ensure quality recommendation consumer example regard scalability collaborative recommender system must able search million potential neighbor real time site used browse pattern indication product preference may thousand datum point customer ensure quality recommendation essential gain consumer ’ trust consumer follow system recommendation end liking product less likely use recommender system classification system recommender system make two type error false negative false positive false negative product system fail recommend although consumer would like false positive product recommend consumer like false positive less desirable annoy anger consumer content-based recommender system limit feature used describe item recommend 
618 chapter 13 datum mining trend research frontier another challenge content-based collaborative recommender system deal new user buy history yet available hybrid approach integrate content-based collaborative method achieve improve recommendation netflix prize open competition hold online dvd-rental service payout $ 1000000 best recommender algorithm predict user rating film base previous rating competition study show predictive accuracy recommender system substantially improve blending multiple predictor especially used ensemble many substantially different method rather refine single technique collaborative recommender system form intelligent query answer consist analyze intent query provide generalized neighborhood associate information relevant query example rather simply return book description price response customer ’ query return additional information related query explicitly ask ( eg book evaluation comment recommendation book sale statistic ) provide intelligent answer query 134 datum mining society us datum mining part daily life although may often unaware presence section 1341 look several example “ ubiquitous invisible ” datum mining affect everyday thing product stock local supermarket ad see surfing internet crime prevention datum mining offer individual many benefit improve customer service satisfaction well lifestyle general however also serious implication regard one ’ right privacy datum security issue topic section 1342 1341 ubiquitous invisible datum mining datum mining present many aspect daily life whether realize affect shop work search information even influence leisure time health well-being section look example ubiquitous ( ever-present ) datum mining several example also represent invisible datum mining “ smart ” software search engine customer-adaptive web service ( eg used recommender algorithms ) “ intelligent ” database system email manager ticket master incorporate datum mining functional component often unbeknownst user grocery store print personalize coupon customer receipt online store recommend additional item base customer interest datum mining innovatively influenced buy way shop experience shopping one example wal-mart hundred million customer visit ten thousand store every week wal-mart allow supplier access datum 
134 datum mining society 619 product perform analysis used datum mining software allow supplier identify customer buy pattern different store control inventory product placement identify new merchandize opportunity affect item ( many ) end store ’ shelves—something think next time wander aisle wal-mart datum mining shape online shopping experience many shopper routinely turn online store purchase book music movie toy recommender system discuss section 1335 offer personalize product recommendation base opinion customer amazoncom forefront used personalize datum mining–based approach marketing strategy observed traditional brick-and-mortar store hardest part get customer store customer likely buy something since cost go another store high therefore marketing brick-and-mortar store tend emphasize draw customer rather actual in-store customer experience contrast online store customer “ walk ” enter another online store click mouse amazoncom capitalize difference offer “ personalize store every ” use several datum mining technique identify customer ’ like make reliable recommendation topic shopping suppose lot buy credit card nowadays unusual receive phone call one ’ credit card company regard suspicious unusual pattern spending credit card company use datum mining detect fraudulent usage save billion dollar year many company increasingly use datum mining customer relationship management ( crm ) help provide customize personal service address individual customer ’ need lieu mass marketing study browse purchasing pattern web store company tailor advertisement promotion customer profile customer less likely annoyed unwanted mass mailing junk mail action result substantial cost saving company customer benefit likely notified offer actually interest result less waste personal time greater satisfaction datum mining greatly influenced way person use computer search information work get internet example decide check email unbeknownst several annoying email already delete thank spam filter used classification algorithms recognize spam process email go google ( wwwgooglecom ) provide access information billion web page index server google one popular widely used internet search engine used google search information become way life many person google popular even become new verb english language meaning “ search ( something ) internet used google search engine extension comprehensive search ” 1 decide type keyword 1 http open-dictionarycom 
620 chapter 13 datum mining trend research frontier topic interest google return list web site topic mine index organized set datum mining algorithms include pagerank moreover type “ boston new york ” google show bus train schedule boston new york however minor change “ boston paris ” lead flight schedule boston paris smart offering information service likely base frequent pattern mine click stream many previous query view result google query various ad pop relate query google ’ strategy tailor advertising match user ’ interest one typical service explore every internet search provider also make happier less likely pester irrelevant ad datum mining omnipresent see daily-encounter example can go scenario many case datum mining invisible user may unaware examine result return datum mining click actually fed new datum datum mining function datum mining become improve accept technology continue research development need many area mentioned challenge throughout book include efficiency scalability increase user interaction incorporation background knowledge visualization technique effective method find interesting pattern improve handle complex datum type stream datum realtime datum mining web mining addition integration datum mining exist business scientific technology provide domain-specific datum mining tool contribute advancement technology success datum mining solution tailor e-commerce application opposed generic datum mining system example 1342 privacy security social impact datum mining information accessible electronic form available web increasingly powerful datum mining tool develop put use increase concern datum mining may pose threat privacy datum security however important note many datum mining application even touch personal datum prominent example include application involve natural resource prediction flood drought meteorology astronomy geography geology biology scientific engineering datum furthermore study datum mining research focus development scalable algorithms involve personal datum focus datum mining technology discovery general statistically significant pattern specific information regard individual sense believe real privacy concern unconstrained access individual record especially access privacy-sensitive information credit card transaction record health-care record personal financial record biological trait justice investigation ethnicity datum mining application involve personal datum many case simple method remove sensitive id datum may protect privacy individual nevertheless privacy concern exist wherever 
134 datum mining society 621 personally identifiable information collect store digital form datum mining program able access datum even datum preparation improper nonexistent disclosure control root cause privacy issue handle concern numerous datum security-enhancing technique develop addition great deal recent effort develop privacypreserve datum mining method section look advance protect privacy datum security datum mining “ secure privacy individual collect mining datum ” many datum security–enhancing technique develop help protect datum databasis employ multilevel security model classify restrict datum accord various security level user permit access authorize level show however user execute specific query authorize security level still infer sensitive information similar possibility occur datum mining encryption another technique individual datum item may encode may involve blind signature ( build public key encryption ) biometric encryption ( eg image person ’ iris fingerprint used encode personal information ) anonymous databasis ( permit consolidation various databasis limit access personal information need know personal information encrypt store different location ) intrusion detection another active area research help protect privacy personal datum privacy-preserve datum mining area datum mining research response privacy protection datum mining also know privacy-enhance privacysensitive datum mining deal obtain valid datum mining result without disclose underlie sensitive datum value privacy-preserve datum mining method use form transformation datum perform privacy preservation typically method reduce granularity representation preserve privacy example may generalize datum individual customer customer group reduction granularity cause loss information possibly usefulness datum mining result natural trade-off information loss privacy privacy-preserve datum mining method classify follow category randomization method method add noise datum mask attribute value record noise add sufficiently large individual record value especially sensitive one re-cover however add skillfully final result datum mining basically preserve technique design derive aggregate distribution perturbed datum subsequently datum mining technique develop work aggregate distribution k-anonymity l-diversity method method alter individual record uniquely identify k-anonymity method granularity datum representation reduce sufficiently give record map onto least k record datum used technique like generalization suppression k-anonymity method weak homogeneity 
622 chapter 13 datum mining trend research frontier sensitive value within group value may infer alter record l-diversity model design handle weakness enforce intragroup diversity sensitive value ensure anonymization goal make sufficiently difficult adversary use combination record attribute exactly identify individual record distribute privacy preservation large datum set can partition distribute either horizontally ( ie datum set partition different subset record distribute across multiple site ) vertically ( ie datum set partition distribute attribute ) even combination individual site may want share entire datum set may consent limit information sharing use variety protocol overall effect method maintain privacy individual object derive aggregate result datum downgrading effectiveness datum mining result many case even though datum may available output datum mining ( eg association rule classification model ) may result violation privacy solution can downgrade effectiveness datum mining either modify datum mining result hiding association rule slightly distort classification model recently researcher propose new idea privacy-preserve datum mining notion differential privacy general idea two datum set close one another ( ie differ tiny datum set single element ) give differentially private algorithm behave approximately datum set definition give strong guarantee presence absence tiny datum set ( eg represent individual ) affect final output query significantly base notion set differential privacy-preserve datum mining algorithms develop research direction ongoing expect powerful privacy-preserve datum publish datum mining algorithms near future like technology datum mining misuse however must lose sight benefit datum mining research bring range insight gain medical scientific application increase customer satisfaction help company better suit client ’ need expect computer scientist policy expert counterterrorism expert continue work social scientist lawyer company consumer take responsibility build solution ensure datum privacy protection security way may continue reap benefit datum mining term time money saving discovery new knowledge 135 datum mining trend diversity datum datum mining task datum mining approach pose many challenge research issue datum mining development efficient effective datum 
135 datum mining trend 623 mining method system service interactive integrate datum mining environment key area study use datum mining technique solve large sophisticated application problem important task datum mining researcher datum mining system application developer section describe trend datum mining reflect pursuit challenge application exploration early datum mining application put lot effort help business gain competitive edge exploration datum mining business continue expand e-commerce e-marketing become mainstream retail industry datum mining increasingly used exploration application area web text analysis financial analysis industry government biomedicine science emerge application area include datum mining counterterrorism mobile ( wireless ) datum mining generic datum mining system may limitation deal application-specific problem may see trend toward development application-specific datum mining system tool well invisible datum mining function embed various kind service scalable interactive datum mining method contrast traditional datum analysis method datum mining must able handle huge amount datum efficiently possible interactively amount datum collect continue increase rapidly scalable algorithms individual integrate datum mining function become essential one important direction toward improve overall efficiency mining process increase user interaction constraint-based mining provide user add control allow specification use constraint guide datum mining system search interesting pattern knowledge integration datum mining search engine database system datum warehouse system cloud compute system search engine database system datum warehouse system cloud compute system mainstream information process compute system important ensure datum mining serve essential datum analysis component smoothly integrate information process environment datum mining service tightly couple system seamless unify framework invisible function ensure datum availability datum mining portability scalability high performance integrate information process environment multidimensional datum analysis exploration mining social information network mining social information network link analysis critical task network ubiquitous complex development scalable effective knowledge discovery method application large number network datum essential outlined section 1312 mining spatiotemporal moving-object cyber-physical system cyberphysical system well spatiotemporal datum mount rapidly due 
624 chapter 13 datum mining trend research frontier popular use cellular phone gps sensor wireless equipment outlined section 1313 many challenge research issue realize real-time effective knowledge discovery datum mining multimedium text web datum outlined section 1313 mining kind datum recent focus datum mining research great progress make yet still many open issue solve mining biological biomedical datum unique combination complexity richness size importance biological biomedical datum warrant special attention datum mining mining dna protein sequence mining highdimensional microarray datum biological pathway network analysis topic field area biological datum mining research include mining biomedical literature link analysis across heterogeneous biological datum information integration biological datum datum mining datum mining software engineering system engineering software program large computer system become increasingly bulky size sophisticated complexity tend originate integration multiple component develop different implementation team trend make increasingly challenge task ensure software robustness reliability analysis execution buggy software program essentially datum mining process—trace datum generate program execution may disclose important pattern outlier can lead eventual automate discovery software bug expect development datum mining methodology system debug enhance software robustness bring new vigor system engineering visual audio datum mining visual audio datum mining effective way integrate human ’ visual audio system discover knowledge huge amount datum systematic development technique facilitate promotion human participation effective efficient datum analysis distribute datum mining real-time datum stream mining traditional datum mining method design work centralize location work well many distribute compute environment present today ( eg internet intranet local area network high-speed wireless network sensor network cloud compute ) advance distribute datum mining method expect moreover many application involve stream datum ( eg e-commerce web mining stock analysis intrusion detection mobile datum mining datum mining counterterrorism ) require dynamic datum mining model build real time additional research need direction privacy protection information security datum mining abundance personal confidential information available electronic form couple increasingly powerful datum mining tool pose threat datum privacy security grow interest datum mining counterterrorism also add concern 
136 summary 625 development privacy-preserve datum mining method foresee collaboration technologist social scientist law expert government company need produce rigorous privacy security protection mechanism datum publish datum mining confidence look forward next generation datum mining technology benefit bring 136 summary mining complex datum type pose challenge issue many dedicate line research development chapter present high-level overview mining complex datum type include mining sequence datum time series symbolic sequence biological sequence mining graph network mining kind datum include spatiotemporal cyber-physical system datum multimedium text web datum datum stream several well-established statistical method propose datum analysis regression generalized linear model analysis variance mixed-effect model factor analysis discriminant analysis survival analysis quality control full coverage statistical datum analysis method beyond scope book interested reader refer statistical literature cite bibliographic note ( section 138 ) researcher strive build theoretical foundation datum mining several interesting proposal appear base datum reduction datum compression probability statistic theory microeconomic theory pattern discovery–based inductive databasis visual datum mining integrate datum mining datum visualization discover implicit useful knowledge large datum set visual datum mining include datum visualization datum mining result visualization datum mining process visualization interactive visual datum mining audio datum mining used audio signal indicate datum pattern feature datum mining result many customize datum mining tool develop domain-specific application include finance retail telecommunication industry science engineering intrusion detection prevention recommender system application domain-based study integrate domain-specific knowledge datum analysis technique provide mission-specific datum mining solution ubiquitous datum mining constant presence datum mining many aspect daily life influence shop work search information use computer well leisure time health well-being invisible datum mining “ smart ” software search engine customer-adaptive web service 
626 chapter 13 datum mining trend research frontier ( eg used recommender algorithms ) email manager incorporate datum mining functional component often unbeknownst user major social concern datum mining issue privacy datum security privacy-preserve datum mining deal obtain valid datum mining result without disclose underlie sensitive value goal ensure privacy protection security preserve overall quality datum mining result datum mining trend include effort toward exploration new application area improve scalable interactive constraint-based mining method integration datum mining web service database warehousing cloud compute system mining social information network trend include mining spatiotemporal cyber-physical system datum biological datum system engineering datum multimedium text datum addition web mining distribute real-time datum stream mining visual audio mining privacy security datum mining 137 exercise 131 sequence datum ubiquitous diverse application chapter present general overview sequential pattern mining sequence classification sequence similarity search trend analysis biological sequence alignment modele however cover sequence cluster present overview method sequence cluster 132 chapter present overview sequence pattern mining graph pattern mining method mining tree pattern partial order pattern also study research summarize method mining structure pattern include sequence tree graph partial order relationship examine kind structural pattern mining cover research propose application create new mining problem 133 many study analyze homogeneous information network ( eg social network consist friend link friend ) however many application involve heterogeneous information network ( ie network link multiple type object research paper conference author topic ) major difference methodology mining heterogeneous information network method homogeneous counterpart 134 research describe datum mining application present chapter discuss different form datum mining used application 135 establishment theoretical foundation important datum mining name describe main theoretical foundation propose datum mining comment satisfy ( fail satisfy ) requirement ideal theoretical framework datum mining 
137 exercise 627 136 ( research project ) build theory datum mining require set theoretical framework major datum mining function explain framework take one theory example ( eg datum compression theory ) examine major datum mining function fit framework function fit well current theoretical framework propose way extend framework explain function 137 strong linkage statistical datum analysis datum mining person think datum mining automate scalable method statistical datum analysis agree disagree perception present one statistical analysis method automate or scale nicely integration current datum mining methodology 138 difference visual datum mining datum visualization datum visualization may suffer datum abundance problem example easy visually discover interesting property network connection social network huge complex dense connection propose visualization method may help person see network topology interesting feature social network 139 propose implementation method audio datum mining integrate audio visual datum mining bring fun power datum mining possible develop video datum mining method state scenario solution make integrate audiovisual mining effective 1310 general-purpose computer domain-independent relational database system become large market last several decade however many person feel generic datum mining system prevail datum mining market think datum mining focus effort develop domain-independent datum mining tool develop domain-specific datum mining solution present reasoning 1311 recommender system way differ customer productbased cluster system differ typical classification predictive modele system outline one method collaborative filter discuss work limitation practice 1312 suppose local bank datum mining system bank study debit card usage pattern notice make many transaction home renovation store bank decide contact offer information regard special loan home improvement ( ) discuss may conflict right privacy ( b ) describe another situation feel datum mining infringe privacy ( c ) describe privacy-preserve datum mining method may allow bank perform customer pattern analysis without infringe customer ’ right privacy ( ) example datum mining can used help society think way can used may detrimental society 
628 chapter 13 datum mining trend research frontier 1313 major challenge face bring datum mining research market illustrate one datum mining research issue view may strong impact market society discuss approach research issue 1314 base view challenge research problem datum mining give number year good number researcher implementor would plan make good progress toward effective solution problem 1315 base experience knowledge suggest new frontier datum mining mentioned chapter 138 bibliographic note mining complex datum type many research paper book cover various theme list recent book well-cite survey research article reference time-series analysis study statistic computer science community decade many textbook box jenkin reinsel [ bjr08 ] brockwell davis [ bd02 ] chatfield [ cha03b ] hamilton [ ham94 ] shumway stoffer [ ss05 ] fast subsequence match method time-series databasis present faloutsos ranganathan manolopoulos [ frm94 ] agrawal lin sawhney shim [ alss95 ] develop method fast similarity search presence noise scaling translation time-series databasis shasha zhu present overview method high-performance discovery time series [ sz04 ] sequential pattern mining method study many researcher include agrawal srikant [ as95 ] zaki [ zak01 ] pei han mortazavi-asl et al [ + 04 ] yan han afshar [ yha03 ] study sequence classification include ji bailey dong [ jbd05 ] ye keogh [ yk09 ] survey xing pei keogh [ xpk10 ] dong pei [ dp07 ] provide overview sequence datum mining method method analysis biological sequence include markov chain hide markov model introduce many book tutorial waterman [ wat95 ] setubal meidanis [ sm97 ] durbin eddy krogh mitchison [ dekm98 ] baldi brunak [ bb01 ] krane raymer [ kr03 ] rabiner [ rab89 ] jone pevzner [ jp04 ] baxevanis ouellette [ bo04 ] information blast ( see also korf yandell bedell [ kyb03 ] ) find ncbi web site graph pattern mining study extensively include holder cook djoko [ hcd94 ] inokuchi washio motoda [ iwm98 ] kuramochi karypis [ kk01 ] yan han [ yh02 yh03a ] borgelt berthold [ bb02 ] huan wang bandyopadhyay et al [ + 04 ] gaston tool nijssen kok [ nk04 ] 
138 bibliographic note 629 great deal research social information network analysis include newman [ new10 ] easley kleinberg [ ek10 ] yu han faloutsos [ yhf10 ] wasserman faust [ wf94 ] watt [ wat03 ] newman barabasi watt [ nbw06 ] statistical modele network study popularly albert barbasi [ ab99 ] watt [ wat03 ] faloutsos faloutsos faloutsos [ fff99 ] kumar raghavan rajagopalan et al [ + 00 ] leskovec kleinberg faloutsos [ lkf05 ] datum clean integration validation information network analysis study many include bhattacharya getoor [ bg04 ] yin han yu [ yhy07 yhy08 ] cluster ranking classification network study extensively include brin page [ bp98 ] chakrabarti dom indyk [ cdi98 ] kleinberg [ kle99 ] getoor friedman koller taskar [ gfkt01 ] newman m girvan [ ng04 ] yin han yang yu [ yhyy04 ] yin han yu [ yhy05 ] xu yuruk feng schweiger [ xyfs07 ] kuli basu dhillon mooney [ kbdm09 ] sun han zhao et al [ + 09 ] neville gallaher eliassi-rad [ nge-r09 ] ji sun danilevsky et al [ + 10 ] role discovery link prediction information network study extensively well krebs [ kre02 ] kubica moore schneider [ kms03 ] liben-nowell kleinberg [ l-nk03 ] wang han jia et al [ + 10 ] similarity search olap information network study many include tian hankin patel [ thp08 ] chen yan zhu et al [ + 08 ] evolution social information network study many researcher chakrabarti kumar tomkin [ ckt06 ] chi song zhou et al [ + 07 ] tang liu zhang nazeri [ tlzn08 ] xu zhang yu long [ xzyl08 ] kim han [ kh09 ] sun tang han [ + 10 ] spatial spatiotemporal datum mining study extensively collection paper miller han [ mh09 ] introduce textbook shekhar chawla [ sc03 ] hsu lee wang [ hlw07 ] spatial cluster algorithms study extensively chapter 10 11 book research conduct spatial warehouse olap stefanovic han koperski [ shk00 ] spatial spatiotemporal datum mining koperski han [ kh95 ] mamouli cao kollio hadjieleftheriou et al [ + 04 ] tsoukatos gunopulos [ tg01 ] hadjieleftheriou kollio gunopulos tsotra [ hkgt03 ] mining moving-object datum study many vlachos gunopulos kollio [ vgk02 ] tao faloutsos papadia liu [ tfpl04 ] li han kim gonzalez [ lhkg07 ] lee han whang [ lhw07 ] li ding han et al [ + 10 ] bibliography temporal spatial spatiotemporal datum mining research see collection roddick hornsby spiliopoulou [ rhs01 ] multimedium datum mining deep root image process pattern recognition study extensively many textbook include gonzalez wood [ gw07 ] russ [ rus06 ] duda hart stork [ dhs01 ] z zhang r zhang [ zz09 ] search mining multimedium datum study many ( see eg fayyad smyth [ fs93 ] faloutsos lin [ fl95 ] natsev rastogi 
630 chapter 13 datum mining trend research frontier shim [ nrs99 ] zaı̈ane han zhu [ zhz00 ] ) overview image mining method give hsu lee zhang [ hlz02 ] text datum analysis study extensively information retrieval many textbook survey article croft metzler strohman [ cms09 ] s buttcher c clarke g cormack [ bcc10 ] man raghavan schutze [ mrs08 ] grossman frieder [ gr04 ] baeza-yate riberio-neto [ byrn11 ] zhai [ zha08 ] feldman sanger [ fs06 ] berry [ ber03 ] weis indurkhya zhang damerau [ wizd04 ] text mining fast-developing field numerous paper publish recent year cover many topic topic model ( eg blei lafferty [ bl09 ] ) sentiment analysis ( eg pang lee [ pl07 ] ) contextual text mining ( eg mei zhai [ mz06 ] ) web mining another focuse theme book like chakrabarti [ cha03a ] liu [ liu06 ] berry [ ber03 ] web mining substantially improve search engine influential milestone work brin page [ bp98 ] kleinberg [ kle99 ] chakrabarti dom kumar et al [ + 99 ] kleinberg tomkin [ kt99 ] numerous result generate since search log mining ( eg silvestri [ sil10 ] ) blog mining ( eg mei liu su zhai [ mlsz06 ] ) mining online forum ( eg cong wang lin et al [ + 08 ] ) book survey stream datum system stream datum process include babu widom [ bw01 ] babcock babu datar et al [ + 02 ] muthukrishnan [ mut05 ] aggarwal [ agg06 ] stream datum mining research cover stream cube model ( eg chen dong han et al [ + 02 ] ) stream frequent pattern mining ( eg manku motwani [ mm02 ] karp papadimitriou shenker [ kps03 ] ) stream classification ( eg domingo hulten [ dh00 ] wang fan yu han [ wfyh03 ] aggarwal han wang yu [ ahwy04b ] ) stream cluster ( eg guha mishra motwani ’ callaghan [ gmmo00 ] aggarwal han wang yu [ ahwy03 ] ) many book discuss datum mining application financial datum analysis financial modele see example benninga [ ben08 ] higgin [ hig08 ] retail datum mining customer relationship management see example book berry linoff [ bl04 ] berson smith thearle [ bst99 ] telecommunication-related datum mining see example horak [ hor08 ] also book scientific datum analysis grossman kamath kegelmeyer et al [ + 01 ] kamath [ kam09 ] issue theoretical foundation datum mining address many researcher example mannila present summary study foundation datum mining [ man00 ] datum reduction view datum mining summarize new jersey datum reduction report barbará dumouchel faloutos et al [ + 97 ] datum compression view find study minimum description length principle grunwald rissanen [ gr07 ] pattern discovery point view datum mining address numerous machine learn datum mining study range association mining decision tree induction sequential pattern mining cluster probability theory point view popular statistic machine learn literature 
138 bibliographic note 631 bayesian network hierarchical bayesian model chapter 9 probabilistic graph model ( eg koller friedman [ kf09 ] ) kleinberg papadimitriou raghavan [ kpr98 ] present microeconomic view treat datum mining optimization problem study inductive database view include imielinski mannila [ im96 ] de raedt gun nijssen [ rgn10 ] statistical method datum analysis describe many book hastie tibshirani friedman [ htf09 ] freedman pisani purf [ fpp07 ] devore [ dev03 ] kutner nachtsheim neter li [ knnl04 ] dobson [ dob01 ] breiman friedman olshen stone [ bfos84 ] pinheiro bate [ pb00 ] johnson wichern [ jw02b ] huberty [ hub94 ] shumway stoffer [ ss05 ] miller [ mil98 ] visual datum mining popular book visual display datum information include tufte [ tuf90 tuf97 tuf01 ] summary technique visualize datum present cleveland [ cle93 ] dedicate visual datum mining book visual datum mining technique tool datum visualization mining soukup davidson [ sd02 ] book information visualization datum mining knowledge discovery edit fayyad grinstein wierse [ fgw01 ] contain collection article visual datum mining method ubiquitous invisible datum mining discuss many text include john [ joh99 ] article book edit kargupta joshi sivakumar yesha [ kjsy04 ] book business @ speed thought succeed digital economy gate [ gat00 ] discuss e-commerce customer relationship management provide interesting perspective datum mining future mena [ men03 ] informative book use datum mining detect prevent crime cover many form criminal activity range fraud detection money launder insurance crime identity crime intrusion detection datum mining issue regard privacy datum security address popularly literature book privacy security datum mining include thuraisingham [ thu04 ] aggarwal yu [ ay08 ] vaidya clifton zhu [ vcz10 ] fung wang fu yu [ fwfy10 ] research article include agrawal srikant [ as00 ] evfimievski srikant agrawal gehrke [ esag02 ] vaidya clifton [ vc03 ] differential privacy introduce dwork [ dwo06 ] study many hay rastogi miklau suciu [ hrms10 ] many discussion trend research direction datum mining various forum several book collection article issue kargupta han yu et al [ + 08 ] 

partition algorithms [ sound ] first introduce basic concept partition algorithms 
partition algorithms basic concept  partition method discover grouping datum optimize specific objective function iteratively improve quality partition  k-partition method partition dataset n object set k cluster objective function optimized ( eg sum square distance minimize ck centroid medoid cluster ck )  typical objective function sum square error ( sse ) = sse ( c )  2 k 2 | x − c | ∑∑ k k 1 xi∈ck problem definition give k find partition k cluster optimize choose partition criterion  global optimal need exhaustively enumerate partition  heuristic method ( ie greedy algorithms ) k-mean k-median k-medoid etc partition method essentially discover grouping datum mean get k group want partition mean 2k group optimize specific object function example sum square distance iteratively improve quality partition k-partition method partition dataset n object set k cluster definitely iteratively improve object function optimized example object function can sum square distance minimize c sub k centroid medoid cluster capital c sub k typical objective function sum square error often written sse cluster okay sse sum k cluster k 1 k okay cluster object s cluster square error simply say distance sum square distance whole function k cluster cluster get point center sum square distance square error want minimize objective function general think conceptual think get nice k cluster object center somehow shorter okay shorter sum square distance square error actually pretty small s ca nt get within cluster center sum square error pretty small get k thing add together whole thing smaller k-partition method essentially problem give k number cluster want find partition k cluster optimize choose partition criterion example sum square distance however find global optimal actually need exhaustively enumerate partition potential number partition actually exponential realistically get heuristic method mean greedy algorithms example typically used like k-mean k-median k-medoid method greedy algorithms heuristic method find nice partition [ music ] 

method 
k-mean cluster method  k-mean ( macqueen ’ 67 lloyd ’ ’ 82 )  cluster represent center cluster  give k number cluster k-mean cluster algorithm outlined follow  select k point initial centroid  repeat   form k cluster assign point closest centroid  re-compute centroid ( ie mean point ) cluster convergence criterion satisfied  different kind measure used  2 manhattan distance ( l1 norm ) euclidean distance ( l2 norm ) cosine similarity 
example k-mean cluster assign point cluster original datum point & randomly select k = 2 centroid execution k-mean cluster algorithm select k point initial centroid repeat • form k cluster assign point closest centroid • re-compute centroid ( ie mean point ) cluster convergence criterion satisfied 3 recompute cluster center redo point assignment 
discussion k-mean method efficiency ( tkn ) n # object k # cluster # iteration  normally k < < n thus efficient method  k-mean cluster often terminate local optimal  initialization important find high-quality cluster  need specify k number cluster advance  way automatically determine “ best ” k  practice one often run range value select “ best ” k value  sensitive noisy datum outlier  variation used k-median k-medoid etc  k-mean applicable object continuous n-dimensional space  used k-mode categorical datum  suitable discover cluster non-convex shape  used density-based cluster kernel k-mean etc  4 
variation k-mean  many variant k-mean method vary different aspect  choose better initial centroid estimate   k-medoid k-median k-mode discuss lecture apply feature transformation technique  5 discuss lecture choose different representative prototype cluster   + intelligent k-mean genetic k-mean weight k-mean kernel k-mean discuss lecture 

cluster [ sound ] discuss quality k-mean cluster quite sensitive initial relation status 
initialization k-mean  different initialization may generate rather different cluster result ( can far optimal )  original proposal ( macqueen ’ 67 ) select k seed randomly  need run algorithm multiple time used different seed  many method propose better initialization k seed  2 + ( arthur & vassilvitskii ’ 07 )  first centroid select random  next centroid select one farthest currently select ( selection base weight probability score )  selection continue k centroid obtain need see good initialization find good quality cluster used k-mean method give simple example everybody see different initialization may generate rather different cluster result may optimal give example suppose four point find two cluster initialize solid circle mean two point get one cluster like give center like two attract two two one cluster actually become stabilize calculate center center still move find pretty ugly cluster perceive vertically group thing two cluster sum square distance sse actually become rather small point view know quality cluster can sensitive initialization recently macqueen 1967 say select k seed randomly need run algorithm multiple time used different seed software package may even say run algorithms 200 time finally find one derive best k-mean cluster simply say want find sse sum square arrow minimize many method also propose better initialization example s one call + propose essentially + initialization follow first centroid select randomly next centroid select try find farthest point currently select point mean selection base weight probability score base different probability select best next one selection continue k centroid obtain initialization do run k-mean algorithm will give simple example protein c poor initialization may lead poor cluster example take datum set black point have show give two choice initialize red line blue line initialization get cluster assign object two different cluster different color one upper part part assign red cluster lower part assign blue cluster 
example poor initialization may lead poor cluster assign point cluster recompute cluster center another random selection k centroid datum point  rerun k-mean used another random k seed  run k-mean generate poor quality cluster 3 recalculate center probably see center actually moved recalculation re-assign point run want probably see finally be stable like actually good cluster mean run k-mean generate poor quality cluster simply give us simple show need smart interrelation multiple random initialization best one initialization k-mean cluster [ music ] 

method [ music ] be go introduce another interesting k partition cluster method call k-medoid cluster method need study k-medoid cluster method k-mean algorithm sensitive outlier mean sensitive outlier give simple example look company s salary add another high salary average salary whole company shift quite lot let s look k-medoid k-medoid mean instead take mean value object cluster centroid actually use centrally locate object cluster call medoid mean k-medoid cluster algorithm go similar way first select k point initial representative object mean initial k-medoid difference k-mean k-mean select k virtual centroid one k representative real object put one repeat loop assign similarly assign point cluster closest medoid 
handle outlier k-mean k-medoid  k-mean algorithm sensitive outlier —since object extremely large value may substantially distort distribution datum  k-medoid instead take mean value object cluster reference point medoid used centrally locate object cluster  k-medoid cluster algorithm  select k point initial representative object ( ie initial k medoid )  repeat  2  assign point cluster closest medoid  randomly select non-representative object oi  compute total cost swap medoid oi  < 0 swap oi form new set medoid convergence criterion satisfied randomly select non-representative object suppose s sub see whether use sub replace one medoid m whether improve quality class ring mean total cost swap negative simply say sloppy reduce square arrow go swap object oi form new set medoid need redo assignment process go convergence criterium satisfied will see small example typical k-medoid algorithm exact look pam example suppose give ten small number point small graph 2d space want find two cluster 
pam typical k-medoid algorithm 10 10 10 9 9 9 8 8 8 arbitrary choose k object initial medoid 7 6 5 4 3 2 1 assign remain object nearest medoid 7 6 5 4 3 2 1 0 0 0 1 2 3 4 5 6 7 8 9 10 7 6 5 4 3 2 1 0 0 1 2 3 4 5 6 7 8 9 10 0 k=2 3 10 3 4 5 6 7 8 9 10 10 compute total cost swap 9 object re-assignment swap oramdom swap medoid oi improve cluster quality quality improve convergence criterion satisfied 2 randomly select nonmedoid object oramdom select initial k medoid randomly repeat 1 8 7 6 9 8 7 6 5 5 4 4 3 3 2 2 1 1 0 0 0 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9 10 begin arbitrarily choose k object choose two object initial medoid find cluster medoid follow okay see whether randomly choose another object like random say non-medoic object want see whether can become medoid would reduce total cost always say get better sse case suppose choose one find really reduce total sse actually get another one like get orange one look cluster form know one reduce total sse simply say quality cluster improve swap essentially list initially select initial k medoid randomly object reassignment try swap medoid random non-medoid object sub improve cluster quality will convergence criterion satisfied simple execution illustrate idea k-medoid execute 
discussion k-medoid cluster  k-medoid cluster find representative object ( medoid ) cluster  pam ( partition around medoid kaufmann & rousseeuw 1987 )  start initial set medoid  iteratively replace one medoid one non-medoid improve total sum square error ( sse ) result cluster  pam work effectively small datum set scale well large datum set ( due computational complexity )  computational complexity pam ( k ( n − k ) 2 ) ( quite expensive )  efficiency improvement pam  clara ( kaufmann & rousseeuw 1990 )  4  pam sample ( ks2 + k ( n − k ) ) sample size claran ( ng & han 1994 ) randomize re-sample ensure efficiency + quality see k-medoid cluster essentially try find k representative object medoid cluster typical arrow pam call partition around medoid develop 1987 kaufmann & rousseeuw start initial set medoid iteratively replace one medoid one non-medoid swap improve total sum square error total quality cluster method work effectively small datum set keep try different swap scale well computational complexity quite high look detail actually computational complexity every swap actually s square number point s quite expensive improve efficiency s one proposal author 1990 call clara essentially s pam sample mean instead used whole point choose sample s sample size computational complexity square actually come size sample however sample initial sample selection good final classroom quality can poor 1994 s another algorithm call claran propose every iteration randomize re-sample mean keep exact sample randomize re-sample ensure efficiency quality cluster [ music ] 

cluster method [ music ] session be go introduce k-median k-mode cluster method two interesting alternative k-mean cluster method want k-median median better mean encounter outlier 
k-median handle outlier compute median median less sensitive outlier mean  think median salary vs mean salary large firm add top executive  k-median instead take mean value object cluster reference point median used ( l1-norm distance measure ) k  criterion function k-median algorithm   k-median cluster algorithm ∑ | x k 1 xi∈ck ij − med kj |  select k point initial representative object ( ie initial k median )  repeat  2 =  assign every point nearest median  re-compute median used median individual feature convergence criterion satisfied median usually less sensitive outlier compare mean example large firm okay can many employee want find median salary even be add add top executive mean salary may change quite lot median can still stable s reason instead compute mean value object cluster take median centroid use l1 norm mean distance distance measure criterion function k-median algorithm written center sum total sum k one number cluster k cluster object cluster look difference difference take absolute value distance median k-median cluster algorithm essentially written follow first begin select k point initial representative object mean initial k median get loop assign every point nearest median re-compute median used median individual feature process repeat convergence criterion satisfied look k-mode another interesting alternative k-mean k-mode essentially handle categorical datum k-mean handle non-numerical categorical datum course map categorical value 1 however mapping generate quality cluster high-dimensional datum person propose k-mode method extension k-mean replace mean cluster mode similarity measure object x center cluster z written follow okay similarity see distance measure distance function okay jth field object x mean x sub j look jth field cluster center z s distance essentially object equal center attribute essentially say different say distance one s dissimilar distance largest however value equal attribute center use formula formula actually mean many many object occur cluster okay value actual small example attribute cluster s one value frequent guy equal value division lead one distance smallest case frequent j value one would closer one distance smaller one rare go rare value number distinct value quite big distance closer l quite reasonable definition 
k-mode cluster categorical datum k-mean handle non-numerical ( categorical ) datum  mapping categorical value 0 generate quality cluster highdimensional datum  k-mode extension k-mean replace mean cluster mode  dissimilarity measure object x center cluster z  φ ( xj zj ) = 1 – nl xj = zj 1 xj ǂ zj  zj categorical value attribute j zl nl number object cluster l njr number object whose attribute value r  dissimilarity measure ( distance function ) frequency-based  algorithm still base iterative object cluster assignment centroid update  fuzzy k-mode method propose calculate fuzzy cluster membership value object cluster  mixture categorical numerical datum used k-prototype method  3 mean dc measure distance function essentially frequency-based frequent closer last frequent far away evan may equal value whole algorithm list still base iterative one first object cluster assignment centroid update take one loop tear criterion reach another iterative method call fuzzy k-mode method fuzzy k-mode method essentially calculate fuzzy cluster membership value object s cluster simply say give fuzzy cluster value s close cluster fuzzy value closer s far away cluster fuzzy value somewhat closer zero okay datum attribute categorical attribute attribute numerical attribute use k-prototype method essentially numerical one use method function similar k-mean categorical one use similar k-mode finally integrate get k-prototype method [ music ] 

kernel k-mean cluster [ sound ] hi last session lecture be go introduce kernel k-mean cluster method 
kernel k-mean cluster kernel k-mean used detect non-convex cluster  k-mean detect cluster linearly separable  idea project datum onto high-dimensional kernel space perform k-mean cluster  map datum point input space onto high-dimensional feature space used kernel function  perform k-mean map feature space  computational complexity higher k-mean  need compute store n x n kernel matrix generate kernel function original datum  widely study spectral cluster consider variant kernel k-mean cluster  2 kernel k-mean essentially know k-mean detect cluster linearly separable difficulty handle non-convex cluster example look set datum point say k equal 2 want find two cluster different color example red one core part right center blue one big ring surround circle however k-mean find something linearly separable likely chop every cluster two half find something quite ugly idea project datum onto high-dimensional kernel space perform k-mean cluster space may able solve problem cluster problem mean will remark datum plan input space high damage feature space used corner function perform k-mean map feature space course computation complexity can higher need compute store n n kernel matrix generate kernel function original datum original datum contain n object n large n n kernel matrix can large actually widely study spectral cluster consider variant kernel k-mean cluster s kernel k-mean s pretty interesting let s look kernel function kernel k-mean cluster typical kernel function example may polynomial kernel degree h use formula gaussian radial basis function rbf rbf kernel typical gaussian function sigmoid kernel defined way 
kernel function kernel k-mean cluster  typical kernel function  polynomial kernel degree h k ( xi xj ) = ( xi∙xj + 1 ) h  gaussian radial basis function ( rbf ) kernel k ( xi xj ) = e  sigmoid kernel k ( xi xj ) = tanh ( κxi∙xj −δ ) | x − x j | 2 2σ 2 φ ( xi ) • φ ( x j ) =  formula kernel matrix k two point xi xj є ck k xi x j  sse criterion kernel k-mean = sse ( c )  k ∑ ∑ | φ ( x ) − c k 1 xi∈ck formula cluster centroid k 2 ∑ φ ( x ) ck = xi∈ck | ck |  cluster perform without actual individual projection φ ( xi ) φ ( xj ) 3 datum point xi xj є ck formula kernel matrix x mean two point xi sub x sub j within cluster c sub k map kernel function way want compute sum square error okay kernel k-mean formula follow whether see want find number cluster one k k number cluster cluster point cluster c sub k part need use square distance phi xi cluster center c sub k formula cluster centroid similar k-mean use c sub k center essentially defined sum function divide size cluster cluster perform even without actual individual projection one datum point need compute formula give kernel function see mapping suppose want check map real point rbf corner okay give five original point like x sub 1 x sub 2 x sub 3 x sub five point original space set sigma equal 4 actually set sigma 
example kernel function kernel k-mean cluster | x − x j | 2 2σ 2 k ( xi xj ) = e  suppose 5 original 2-dimensional point  x1 ( 0 0 ) x2 ( 4 4 ) x3 ( 4 4 ) x4 ( 4 4 ) x5 ( 4 4 )  set 𝜎𝜎 4 follow point kernel space  gaussian radial basis function ( rbf ) kernel  eg 𝑥𝑥1 − 𝑥𝑥2 2 = 0−4 original space x1 x2 x3 x4 x5 4 𝑥𝑥 0 4 −4 𝑦𝑦 0 4 4 −4 −4 4 −4 2 + 0−4 𝑲𝑲 ( 𝒙𝒙𝒊𝒊 𝒙𝒙𝟏𝟏 ) 0 𝑒𝑒 −1 𝑒𝑒 −1 𝑒𝑒 −1 𝑒𝑒 −1 2 = 32 therefore 𝐾𝐾 𝑥𝑥1 𝑥𝑥2 = 𝑒𝑒 rbf kernel space ( 𝜎𝜎 = 4 ) 𝑒𝑒 − 𝑲𝑲 ( 𝒙𝒙𝒊𝒊 𝒙𝒙𝟐𝟐 ) 42 42 2⋅4 2 0 = 𝑒𝑒 𝑒𝑒 −2 𝑒𝑒 −4 𝑒𝑒 −2 −1 𝑲𝑲 ( 𝒙𝒙𝒊𝒊 𝒙𝒙𝟑𝟑 ) 𝑲𝑲 ( 𝒙𝒙𝒊𝒊 𝒙𝒙𝟒𝟒 ) 𝑒𝑒 −1 𝑒𝑒 −1 0 𝑒𝑒 −2 𝑒𝑒 −2 𝑒𝑒 −2 𝑒𝑒 −4 𝑒𝑒 −4 0 𝑒𝑒 −2 𝑲𝑲 ( 𝒙𝒙𝒊𝒊 𝒙𝒙𝟓𝟓 ) 𝑒𝑒 −1 𝑒𝑒 −2 𝑒𝑒 −4 𝑒𝑒 −2 0 32 2⋅42 − = 𝑒𝑒 −1 value well okay equal 4 calculation little nicer simpler probably see want calculate distance okay formula formula top part see x1 zero zero two zero x two x four four two four compute distance basically compute sum square distance equal 32 base formula get e power minus 32 divide time 4 square get without e power minus see mapping suppose sigma equal 4 get k equal one will get x1 x1 look x1 x1 be part essentially derive zero okay mapping get x1 x2 see get value e power similarly derive value see originally five point map rpf kernel space get 5 5 matrix s computation can expensive map n n matrix 
example kernel k-mean cluster original datum set result k-mean cluster result gaussian kernel k-mean cluster  datum set generate quality cluster k-mean since contain - covex cluster  gaussian rbf kernel transformation map datum kernel matrix k two point xi xj = φ ( xi ) • φ ( x j ) gaussian kernel k ( xi xj ) = e xx | x − x j | 2 2σ 2 j  k-mean cluster conduct map datum generate quality cluster 5 want calculation kernel k-mean cluster used example example see original datum point 
recommend reading           6 j macqueen method classification analysis multivariate observation proc 5th berkeley symp mathematical statistic probability 1967 s lloyd least square quantization pcm ieee tran information theory 28 ( 2 ) 1982 a k jain r c dube algorithms cluster datum prentice hall 1988 l kaufman p j rousseeuw find group datum introduction cluster analysis john wiley & son 1990 r ng j han efficient effective cluster method spatial datum mining vldb94 b schölkopf a smola k r müller nonlinear component analysis kernel eigenvalue problem neural computation 10 ( 5 ) 1299–1319 1998 i s dhillon y guan b kuli kernel k-mean spectral cluster normalize cut kdd ’ 04 d arthur s vassilvitskii + advantage careful seeding soda ’ 07 c k reddy b vinzamuri survey partitional hierarchical cluster algorithms ( chap 4 ) aggarwal reddy ( ed ) datum cluster algorithms application crc press 2014 m j zaki w meira datum mining analysis fundamental concept algorithms cambridge univ press 2014 datum set want use k-mean generate two cluster able generate like pretty ugly see dense kernel split ring split well generate something really ugly like quality cluster however gaussian rbf kernel transformation map kernel matrix k two point used function gaussian kernel will able generate pretty nice cluster mean k-mean cluster actually conduct map datum generate quality cluster s gaussian k-mean cluster can rather powerful set interesting reference want look first macqueen s paper lloyd paper see publish actually 1957 bell lab paper collection essentially material first appear 1957 bell lab internal report okay interesting paper book may like read well thank [ music ] 

10 9 8 7 6 5 4 3 2 1 
author jiawei han bliss professor engineering department computer science university illinois urbana-champaign receive numerous award contribution research knowledge discovery datum mining include acm sigkdd innovation award ( 2004 ) ieee computer society technical achievement award ( 2005 ) ieee w wallace mcdowell award ( 2009 ) fellow acm ieee serve founding editor-in-chief acm transaction knowledge discovery datum ( 2006–2011 ) editorial board member several journal include ieee transaction knowledge datum engineering datum mining knowledge discovery micheline kamber master ’ degree computer science ( specialize artificial intelligence ) concordium university montreal quebec nserc scholar work researcher mcgill university simon fraser university switzerland background datum mining passion writing easyto-understand term help make text favorite professional instructor student jian pei currently associate professor school compute science simon fraser university british columbia receive degree compute science simon fraser university 2002 dr jiawei han ’ supervision publish prolifically premier academic forum datum mining databasis web search information retrieval actively serve academic community publication receive thousand citation several prestigious award associate editor several datum mining datum analytic journal xxxv 
2 get know datum ’ tempting jump straight mining first need get datum ready involve closer look attribute datum value real-world datum typically noisy enormous volume ( often several gigabyte ) may originate hodgepodge heterogenous source chapter get familiar datum knowledge datum useful datum preprocess ( see chapter 3 ) first major task datum mining process want know follow type attribute field make datum kind value attribute attribute discrete continuous-valu datum look like value distribute way visualize datum get better sense spot outlier measure similarity datum object respect other gain insight datum help subsequent analysis “ learn datum ’ helpful datum preprocess ” begin section 21 study various attribute type include nominal attribute binary attribute ordinal attribute numeric attribute basic statistical description used learn attribute ’ value describe section 22 give temperature attribute example determine mean ( average value ) median ( middle value ) mode ( common value ) measure central tendency give us idea “ middle ” center distribution know basic statistic regard attribute make easier fill miss value smooth noisy value spot outlier datum preprocess knowledge attribute attribute value also help fix inconsistency incur datum integration plot measure central tendency show us datum symmetric skewer quantile plot histogram scatter plot graphic display basic statistical description useful datum preprocess provide insight area mining field datum visualization provide many additional technique view datum graphical mean help identify relation trend biase “ hide ” unstructured datum set technique may simple scatter-plot matrix ( datum mining concept technique doi b978-0-12-381479-100002-2 c 2012 elsevier right re-serve 39 
40 chapter 2 get know datum two attribute map onto 2-d grid ) sophisticated method treemaps ( hierarchical partition screen display base attribute value ) datum visualization technique describe section 23 finally may want examine similar ( dissimilar ) datum object example suppose database datum object patient describe symptom may want find similarity dissimilarity individual patient information allow us find cluster like patient within datum set dissimilarity object may also used detect outlier datum perform nearest-neighbor classification ( cluster topic chapter 10 11 nearest-neighbor classification discuss chapter 9 ) many measure assess similarity dissimilarity general measure refer proximity measure think proximity two object function distance attribute value although proximity also calculate base probability rather actual distance measure datum proximity describe section 24 summary end chapter know different attribute type basic statistical measure describe central tendency dispersion ( spread ) attribute datum also know technique visualize attribute distribution compute similarity dissimilarity object 21 datum object attribute type datum set make datum object datum object represent entity—in sale database object may customer store item sale medical database object may patient university database object may student professor course datum object typically describe attribute datum object also refer sample example instance datum point object datum object store database datum tuple row database correspond datum object column correspond attribute section define attribute look various attribute type 211 attribute attribute datum field represent characteristic feature datum object noun attribute dimension feature variable often used interchangeably literature term dimension commonly used datum warehousing machine learn literature tend use term feature statistician prefer term variable datum mining database professional commonly use term attribute well attribute describe customer object include example customer id name address observed value give attribute know observation set attribute used describe give object call attribute vector ( feature vector ) distribution datum involve one attribute ( variable ) call univariate bivariate distribution involve two attribute 
21 datum object attribute type 41 type attribute determine set possible values—nominal binary ordinal numeric—the attribute follow subsection introduce type 212 nominal attribute nominal mean “ relate ” value nominal attribute symbol name thing value represent kind category code state nominal attribute also refer categorical value meaningful order computer science value also know enumeration example 21 nominal attribute suppose hair color marital status two attribute describe person object application possible value hair color black brown blond red auburn gray white attribute marital status take value single married divorce widow hair color marital status nominal attribute another example nominal attribute occupation value teacher dentist programmer farmer although say value nominal attribute symbol “ name thing ” possible represent symbol “ name ” number hair color instance assign code 0 black 1 brown another example customor id possible value numeric however case number intend used quantitatively mathematical operation value nominal attribute meaningful make sense subtract one customer id number another unlike say subtract age value another ( age numeric attribute ) even though nominal attribute may integer value consider numeric attribute integer meant used quantitatively say numeric attribute section 215 nominal attribute value meaningful order quantitative make sense find mean ( average ) value median ( middle ) value attribute give set object one thing interest however attribute ’ commonly occur value value know mode one measure central tendency learn measure central tendency section 22 213 binary attribute binary attribute nominal attribute two category state 0 1 0 typically mean attribute absent 1 mean present binary attribute refer boolean two state correspond true false example 22 binary attribute give attribute smoker describe patient object 1 indicate patient smoke 0 indicate patient similarly suppose 
42 chapter 2 get know datum patient undergo medical test two possible outcome attribute medical test binary value 1 mean result test patient positive 0 mean result negative binary attribute symmetric state equally valuable carry weight preference outcome code 0 one example can attribute gender state male female binary attribute asymmetric outcome state equally important positive negative outcome medical test hiv convention code important outcome usually rarest one 1 ( eg hiv positive ) 0 ( eg hiv negative ) 214 ordinal attribute ordinal attribute attribute possible value meaningful order ranking among magnitude successive value know example 23 ordinal attribute suppose drink size correspond size drink available fast-food restaurant nominal attribute three possible value small medium large value meaningful sequence ( correspond increase drink size ) however tell value much bigger say medium large example ordinal attribute include grade ( eg + a− + ) professional rank professional rank enumerate sequential order example assistant associate full professor private private first class specialist corporal sergeant army rank ordinal attribute useful register subjective assessment quality measure objectively thus ordinal attribute often used survey rating one survey participant ask rate satisfied customer customer satisfaction follow ordinal category 0 dissatisfied 1 somewhat dissatisfied 2 neutral 3 satisfied 4 satisfied ordinal attribute may also obtain discretization numeric quantity splitting value range finite number order category describe chapter 3 datum reduction central tendency ordinal attribute represent mode median ( middle value order sequence ) mean defined note nominal binary ordinal attribute qualitative describe feature object without give actual size quantity value qualitative attribute typically word represent category integer used represent computer code category opposed measurable quantity ( eg 0 small drink size 1 medium 2 large ) follow subsection look numeric attribute provide quantitative measurement object 
21 datum object attribute type 215 43 numeric attribute numeric attribute quantitative measurable quantity represent integer real value numeric attribute interval-scaled ratio-scale interval-scaled attribute interval-scaled attribute measure scale equal-size unit value interval-scaled attribute order positive 0 negative thus addition provide ranking value attribute allow us compare quantify difference value example 24 interval-scaled attribute temperature attribute interval-scaled suppose outdoor temperature value number different day day object order value obtain ranking object respect temperature addition quantify difference value example temperature 20◦ c five degree higher temperature 15◦ c calendar date another example instance year 2002 2010 eight year apart temperature celsius fahrenheit true zero-point neither 0◦ c 0◦ f indicate “ ” ( celsius scale example unit measurement 100 difference melt temperature boil temperature water atmospheric pressure ) although compute difference temperature value talk one temperature value multiple another without true zero say instance 10◦ c twice warm 5◦ c speak value term ratio similarly true zero-point calendar date ( year 0 correspond begin time ) bring us ratio-scale attribute true zero-point exit interval-scaled attribute numeric compute mean value addition median mode measure central tendency ratio-scale attribute ratio-scale attribute numeric attribute inherent zero-point measurement ratio-scale speak value multiple ( ratio ) another value addition value order also compute difference value well mean median mode example 25 ratio-scale attribute unlike temperature celsius fahrenheit kelvin ( k ) temperature scale consider true zero-point ( 0◦ k = −27315◦ c ) point particle comprise matter zero kinetic energy example ratio-scale attribute include count attribute year experience ( eg object employee ) number word ( eg object document ) additional example include attribute measure weight height latitude longitude 
44 chapter 2 get know datum coordinate ( eg cluster house ) monetary quantity ( eg 100 time richer $ 100 $ 1 ) 216 discrete versus continuous attribute presentation organized attribute nominal binary ordinal numeric type many way organize attribute type type mutually exclusive classification algorithms develop field machine learn often talk attribute either discrete continuous type may processed differently discrete attribute finite countably infinite set value may may represent integer attribute hair color smoker medical test drink size finite number value discrete note discrete attribute may numeric value 0 1 binary attribute value 0 110 attribute age attribute countably infinite set possible value infinite value put one-to-one correspondence natural number example attribute customer id countably infinite number customer grow infinity reality actual set value countable ( value put one-to-one correspondence set integer ) zip code another example attribute discrete continuous term numeric attribute continuous attribute often used interchangeably literature ( confuse classic sense continuous value real number whereas numeric value either integer real number ) practice real value represent used finite number digit continuous attribute typically represent floating-point variable 22 basic statistical description datum datum preprocess successful essential overall picture datum basic statistical description used identify property datum highlight datum value treat noise outlier section discuss three area basic statistical description start measure central tendency ( section 221 ) measure location middle center datum distribution intuitively speaking give attribute value fall particular discuss mean median mode midrange addition assess central tendency datum set also would like idea dispersion datum datum spread common datum dispersion measure range quartile interquartile range five-number summary boxplot variance standard deviation datum measure useful identify outlier describe section 222 finally use many graphic display basic statistical description visually inspect datum ( section 223 ) statistical graphical datum presentation software 
22 basic statistical description datum 45 package include bar chart pie chart line graph popular display datum summary distribution include quantile plot quantile–quantile plot histogram scatter plot 221 measure central tendency mean median mode section look various way measure central tendency datum suppose attribute x like salary record set object let x1 x2 xn set n observed value observation x value may also refer datum set ( x ) plot observation salary would value fall give us idea central tendency datum measure central tendency include mean median mode midrange common effective numeric measure “ center ” set datum ( arithmetic ) mean let x1 x2 xn set n value observation numeric attribute x like salary mean set value n x x̄ = xi i=1 n = x1 + x2 + · · · + xn n ( 21 ) correspond built-in aggregate function average ( avg ( ) sql ) provide relational database system example 26 mean suppose follow value salary ( thousand dollar ) show increase order 30 36 47 50 52 52 56 60 63 70 70 used eq ( 21 ) 30 + 36 + 47 + 50 + 52 + 52 + 56 + 60 + 63 + 70 + 70 + 110 12 696 = = 58 12 x̄ = thus mean salary $ 58000 sometimes value xi set may associate weight wi = 1 n weight reflect significance importance occurrence frequency attach respective value case compute n x x̄ = wi xi i=1 n x = w1 x1 + w2 x2 + · · · + wn xn w1 + w2 + · · · + wn wi i=1 call weight arithmetic mean weight average ( 22 ) 
46 chapter 2 get know datum although mean singlemost useful quantity describe datum set always best way measure center datum major problem mean sensitivity extreme ( eg outlier ) value even small number extreme value corrupt mean example mean salary company may substantially push highly paid manager similarly mean score class exam can pull quite bit low score offset effect cause small number extreme value instead use trim mean mean obtain chop value high low extreme example sort value observed salary remove top bottom 2 % compute mean avoid trimming large portion ( 20 % ) end result loss valuable information skewer ( asymmetric ) datum better measure center datum median middle value set order datum value value separate higher half datum set lower half probability statistic median generally apply numeric datum however may extend concept ordinal datum suppose give datum set n value attribute x sort increase order n odd median middle value order set n even median unique two middlemost value value x numeric attribute case convention median take average two middlemost value example 27 median let ’ find median datum example datum already sort increase order even number observation ( ie 12 ) therefore median unique value within two middlemost value 52 56 ( within sixth seventh value list ) convention assign = 108 average two middlemost value median 52+56 2 2 = thus median $ 54000 suppose first 11 value list give odd number value median middlemost value sixth value list value $ 52000 median expensive compute large number observation numeric attribute however easily approximate value assume datum group interval accord xi datum value frequency ( ie number datum value ) interval know example employee may group accord annual salary interval $ 10–20000 $ 20–30000 let interval contain median frequency median interval approximate median entire datum set ( eg median salary ) interpolation used formula  p n 2 − freq l median = l1 + width ( 23 ) freqmedian l1 lower median interval n number value  pboundary entire datum set freq l sum frequency interval 
22 basic statistical description datum 47 lower median interval freqmedian frequency median interval width width median interval mode another measure central tendency mode set datum value occur frequently set therefore determine qualitative quantitative attribute possible greatest frequency correspond several different value result one mode datum set one two three mode respectively call unimodal bimodal trimodal general datum set two mode multimodal extreme datum value occur mode example 28 mode datum example 26 bimodal two mode $ 52000 $ 70000 unimodal numeric datum moderately skewer ( asymmetrical ) follow empirical relation mean − mode ≈ 3 × ( mean − median ) ( 24 ) imply mode unimodal frequency curf moderately skewer easily approximate mean median value know midrange also used assess central tendency numeric datum set average largest smallest value set measure easy compute used sql aggregate function max ( ) min ( ) example 29 midrange midrange datum example 26 30000+110000 2 = $ 70000 unimodal frequency curve perfect symmetric datum distribution mean median mode center value show figure 21 ( ) datum real application symmetric may instead either positively skewer mode occur value smaller median ( figure 21b ) negatively skewer mode occur value greater median ( figure 21c ) mean median mode mode mean median ( ) symmetric datum ( b ) positively skewer datum mean mode median ( c ) negatively skewer datum figure 21 mean median mode symmetric versus positively negatively skewer datum 
48 chapter 2 get know datum 222 measure dispersion datum range quartile variance standard deviation interquartile range look measure assess dispersion spread numeric datum measure include range quantile quartile percentile interquartile range five-number summary display boxplot useful identify outlier variance standard deviation also indicate spread datum distribution range quartile interquartile range start let ’ study range quantile quartile percentile interquartile range measure datum dispersion let x1 x2 xn set observation numeric attribute x range set difference largest ( max ( ) ) smallest ( min ( ) ) value suppose datum attribute x sort increase numeric order imagine pick certain datum point split datum distribution equal-size consecutive set figure datum point call quantile quantile point take regular interval datum distribution divide essentially equalsize consecutive set ( say “ essentially ” may datum value x divide datum exactly equal-sized subset readability refer equal ) kth q-quantile give datum distribution value x q datum value less x ( q − k ) q datum value x k integer 0 < k < q q − 1 q-quantile 2-quantile datum point divide lower upper half datum distribution correspond median 4-quantiles three datum point split datum distribution four equal part part represent one-fourth datum distribution commonly refer quartile 100-quantile commonly refer percentile divide datum distribution 100 equal-sized consecutive set median quartile percentile widely used form quantile 25 % q1 q2 q3 median 75th 25th percentile percentile figure 22 plot datum distribution attribute x quantile plot quartile three quartile divide distribution four equal-size consecutive subset second quartile correspond median 
22 basic statistical description datum 49 quartile give indication distribution ’ center spread shape first quartile denote q1 25th percentile cut lowest 25 % datum third quartile denote q3 75th percentile—it cut lowest 75 % ( highest 25 % ) datum second quartile 50th percentile median give center datum distribution distance first third quartile simple measure spread give range cover middle half datum distance call interquartile range ( iqr ) defined iqr = q3 − q1 ( 25 ) example 210 interquartile range quartile three value split sort datum set four equal part datum example 26 contain 12 observation already sort increase order thus quartile datum third sixth ninth value respectively sort list therefore q1 = $ 47000 q3 $ 63000 thus interquartile range iqr = 63 − 47 = $ 16000 ( note sixth value median $ 52000 although datum set two median since number datum value even ) five-number summary boxplot outlier single numeric measure spread ( eg iqr ) useful describe skewer distribution look symmetric skewer datum distribution figure 21 symmetric distribution median ( measure central tendency ) split datum equal-size half occur skewer distribution therefore informative also provide two quartile q1 q3 along median common rule thumb identify suspect outlier single value fall least 15 × iqr third quartile first quartile q1 median q3 together contain information endpoint ( eg tail ) datum fuller summary shape distribution obtain provide lowest highest datum value well know five-number summary five-number summary distribution consist median ( q2 ) quartile q1 q3 smallest largest individual observation written order minimum q1 median q3 maximum boxplot popular way visualize distribution boxplot incorporate five-number summary follow typically end box quartile box length interquartile range median marked line within box two line ( call whisker ) outside box extend smallest ( minimum ) largest ( maximum ) observation 
50 chapter 2 get know datum 220 200 180 160 unit price ( $ ) 140 120 100 80 60 40 20 branch 1 branch 2 branch 3 branch 4 figure 23 boxplot unit price datum item sell four branch allelectronic give time period deal moderate number observation worthwhile plot potential outlier individually boxplot whisker extend extreme low high observation value less 15 × iqr beyond quartile otherwise whisker terminate extreme observation occur within 15 × iqr quartile remain case plot individually boxplot used comparison several set compatible datum example 211 boxplot figure 23 show boxplot unit price datum item sell four branch allelectronic give time period branch 1 see median price item sell $ 80 q1 $ 60 q3 $ 100 notice two outlying observation branch plot individually value 175 202 15 time iqr 40 boxplot compute ( n log n ) time approximate boxplot compute linear sublinear time depend quality guarantee require variance standard deviation variance standard deviation measure datum dispersion indicate spread datum distribution low standard deviation mean datum observation tend close mean high standard deviation indicate datum spread large range value 
22 basic statistical description datum variance n observation x1 x2 xn numeric attribute x n n x x 1 1 σ2 = ( xi − x̄ ) 2 = xi2 − x̄ 2 n n i=1 51 ( 26 ) i=1 x̄ mean value observation defined eq ( 21 ) standard deviation σ observation square root variance σ 2 example 212 variance standard deviation example 26 find x̄ = $ 58000 used eq ( 21 ) mean determine variance standard deviation datum example set n = 12 use eq ( 26 ) obtain 1 ( 302 + 362 + 472 + 1102 ) − 582 12 ≈ 37917 √ σ ≈ 37917 ≈ 1947 σ2 = basic property standard deviation σ measure spread follow σ measure spread mean consider mean choose measure center σ = 0 spread observation value otherwise σ > 0 importantly observation unlikely several standard deviation away mathematically used chebyshev ’ inequality show  mean  least 1 − k12 × 100 % observation k standard deviation mean therefore standard deviation good indicator spread datum set computation variance standard deviation scalable large databasis 223 graphic display basic statistical description datum section study graphic display basic statistical description include quantile plot quantile–quantile plot histogram scatter plot graph helpful visual inspection datum useful datum preprocess first three show univariate distribution ( ie datum one attribute ) scatter plot show bivariate distribution ( ie involve two attribute ) quantile plot follow subsection cover common graphic display datum distribution quantile plot simple effective way first look univariate datum distribution first display datum give attribute ( allow user 
52 chapter 2 get know datum assess overall behavior unusual occurrence ) second plot quantile information ( see section 222 ) let xi = 1 n datum sort increase order x1 smallest observation xn largest ordinal numeric attribute x observation xi pair percentage fi indicate approximately fi × 100 % datum value xi say “ approximately ” may value exactly fraction fi datum xi note 025 percentile correspond quartile q1 050 percentile median 075 percentile q3 let fi = − 05 n ( 27 ) 1 number increase equal step n range 2n ( slightly 1 0 ) 1 − 2n ( slightly 1 ) quantile plot xi graph fi allow us compare different distribution base quantile example give quantile plot sale datum two different time period compare q1 median q3 fi value glance example 213 quantile plot figure 24 show quantile plot unit price datum table 21 quantile–quantile plot unit price ( $ ) quantile–quantile plot q-q plot graph quantile one univariate distribution corresponding quantile another powerful visualization tool allow user view whether shift go one distribution another suppose two set observation attribute variable unit price take two different branch location let x1 xn datum first branch y1 ym datum second datum set sort increase order = n ( ie number point set ) simply plot yi xi yi xi ( − 05 ) n quantile respective datum set < n ( ie second branch fewer observation first ) point q-q plot yi ( − 05 ) m quantile 140 120 100 80 60 40 20 0 000 q3 median q1 025 050 f-value 075 figure 24 quantile plot unit price datum table 21 100 
22 basic statistical description datum 53 table 21 set unit price datum item sell branch allelectronic unit price ( $ ) count item sell 40 43 47 − 74 75 78 − 115 117 120 275 300 250 − 360 515 540 − 320 270 350 branch 2 ( unit price $ ) 120 110 q3 100 median 90 80 70 q1 60 50 40 40 50 60 70 80 90 branch 1 ( unit price $ ) 100 110 120 figure 25 q-q plot unit price datum two allelectronic branch datum plot ( − 05 ) m quantile x datum computation typically involve interpolation example 214 quantile–quantile plot figure 25 show quantile–quantile plot unit price datum item sell two branch allelectronic give time period point correspond quantile datum set show unit price item sell branch 1 versus branch 2 quantile ( aid comparison straight line represent case give quantile unit price branch darker point correspond datum q1 median q3 respectively ) see example q1 unit price item sell branch 1 slightly less branch word 25 % item sell branch 1 less 
54 chapter 2 get know datum equal $ 60 25 % item sell branch 2 less equal $ 64 50th percentile ( marked median also q2 ) see 50 % item sell branch 1 less $ 78 50 % item branch 2 less $ 85 general note shift distribution branch 1 respect branch 2 unit price item sell branch 1 tend lower branch 2 histogram histogram ( frequency histogram ) least century old widely used “ histos ” mean pole mast “ gram ” mean chart histogram chart pole plot histogram graphical method summarize distribution give attribute x x nominal automobile model item type pole vertical bar draw know value x height bar indicate frequency ( ie count ) x value result graph commonly know bar chart x numeric term histogram prefer range value x partition disjoint consecutive subrange subrange refer bucket bin disjoint subset datum distribution x range bucket know width typically bucket equal width example price attribute value range $ 1 $ 200 ( round nearest dollar ) partition subrange 1 20 21 40 41 60 subrange bar draw height represent total count item observed within subrange histogram partition rule discuss chapter 3 datum reduction example 215 histogram figure 26 show histogram datum set table 21 bucket ( bin ) defined equal-width range represent $ 20 increment frequency count item sell although histogram widely used may effective quantile plot q-q plot boxplot method compare group univariate observation scatter plot datum correlation scatter plot one effective graphical method determine appear relationship pattern trend two numeric attribute construct scatter plot pair value treat pair coordinate algebraic sense plot point plane figure 27 show scatter plot set datum table 21 scatter plot useful method provide first look bivariate datum see cluster point outlier explore possibility correlation relationship two attribute x correlated one attribute imply correlation positive negative null ( uncorrelated ) figure 28 show example positive negative correlation two attribute plot point pattern slope 
22 basic statistical description datum 55 6000 count item sell 5000 4000 3000 2000 1000 0 40–59 60–79 80–99 unit price ( $ ) 100–119 120–139 figure 26 histogram table 21 datum set 700 item sell 600 500 400 300 200 100 0 0 20 40 60 80 unit price ( $ ) 100 120 140 figure 27 scatter plot table 21 datum set ( ) ( b ) figure 28 scatter plot used find ( ) positive ( b ) negative correlation attribute 
56 chapter 2 get know datum figure 29 three case observed correlation two plot attribute datum set lower left upper right mean value x increase value increase suggest positive correlation ( figure 28a ) pattern plot point slope upper left lower right value x increase value decrease suggest negative correlation ( figure 28b ) line best fit draw study correlation variable statistical test correlation give chapter 3 datum integration ( eq ( 33 ) ) figure 29 show three case correlation relationship two attribute give datum set section 232 show scatter plot extend n attribute result scatter-plot matrix conclusion basic datum description ( eg measure central tendency measure dispersion ) graphic statistical display ( eg quantile plot histogram scatter plot ) provide valuable insight overall behavior datum help identify noise outlier especially useful datum clean 23 datum visualization convey datum user effectively datum visualization aim communicate datum clearly effectively graphical representation datum visualization used extensively many applications—for example work report manage business operation tracking progress task popularly take advantage visualization technique discover datum relationship otherwise easily observable look raw datum nowadays person also use datum visualization create fun interesting graphic section briefly introduce basic concept datum visualization start multidimensional datum store relational databasis discuss several representative approach include pixel-oriented technique geometric projection technique icon-based technique hierarchical graph-based technique discuss visualization complex datum relation 
23 datum visualization 231 57 pixel-oriented visualization technique simple way visualize value dimension use pixel color pixel reflect dimension ’ value datum set dimension pixel-oriented technique create window screen one dimension dimension value record map pixel corresponding position window color pixel reflect corresponding value inside window datum value arrange global order share window global order may obtain sort datum record way ’ meaningful task hand example 216 pixel-oriented visualization allelectronic maintain customer information table consist four dimension income credit limit transaction volume age analyze correlation income attribute visualization sort customer income-ascending order use order lay customer datum four visualization window show figure pixel color choose smaller value lighter shading used pixelbased visualization easily observe follow credit limit increase income increase customer whose income middle range likely purchase allelectronic clear correlation income age pixel-oriented technique datum record also order query-dependent way example give point query sort record descend order similarity point query fill window layer datum record linear way may work well wide window first pixel row far away last pixel previous row though next global order moreover pixel next one window even though two next global order solve problem lay datum record space-filling curve ( ) income ( b ) credit_limit ( c ) transaction_volume ( ) age figure 210 pixel-oriented visualization four attribute sort customer income ascend order 
58 chapter 2 get know datum ( ) hilbert curve ( b ) gray code ( c ) z-curve figure 211 frequently used 2-d space-filling curf one datum record dim 6 dim 6 dim 5 dim 1 dim 4 dim 2 dim 3 ( ) dim 5 dim 1 dim 4 dim 2 dim 3 ( b ) figure 212 circle segment technique ( ) represent datum record circle segment ( b ) layer pixel circle segment fill window space-filling curve curve range cover entire n-dimensional unit hypercube since visualization window 2-d use 2-d space-filling curve figure 211 show frequently used 2-d space-filling curf note window rectangular example circle segment technique used window shape segment circle illustrated figure 212 technique ease comparison dimension dimension window locate side side form circle 232 geometric projection visualization technique drawback pixel-oriented visualization technique help us much understand distribution datum multidimensional space example show whether dense area multidimensional subspace geometric 
23 datum visualization 59 80 70 60 50 40 30 20 10 0 0 10 20 30 40 x 50 60 70 80 figure 213 visualization 2-d datum set used scatter plot source rareevent-geoinformatica06pdf projection technique help user find interesting projection multidimensional datum set central challenge geometric projection technique try address visualize high-dimensional space 2-d display scatter plot display 2-d datum point used cartesian coordinate third dimension add used different color shape represent different datum point figure 213 show example x two spatial attribute third dimension represent different shape visualization see point type “ + ” “ × ” tend colocate 3-d scatter plot used three axe cartesian coordinate system also used color display 4-d datum point ( figure 214 ) datum set four dimension scatter plot usually ineffective scatter-plot matrix technique useful extension scatter plot ndimensional datum set scatter-plot matrix n × n grid 2-d scatter plot provide visualization dimension every dimension figure 215 show example visualize iris datum set datum set consist 450 sample three species iris flower five dimension datum set length width sepal petal species scatter-plot matrix become less effective dimensionality increase another popular technique call parallel coordinate handle higher dimensionality visualize n-dimensional datum point parallel coordinate technique draw n equally space axe one dimension parallel one display axe 
60 chapter 2 get know datum figure 214 visualization 3-d datum set used scatter plot source http scatter plotjpg datum record represent polygonal line intersect axis point corresponding associate dimension value ( figure 216 ) major limitation parallel coordinate technique effectively show datum set many record even datum set several thousand record visual clutter overlap often reduce readability visualization make pattern hard find 233 icon-based visualization technique icon-based visualization technique use small icon represent multidimensional datum value look two popular icon-based technique chernoff face stick figure chernoff face introduce 1973 statistician herman chernoff display multidimensional datum 18 variable ( dimension ) cartoon human face ( figure 217 ) chernoff face help reveal trend datum component face eye ears mouth nose represent value dimension shape size placement orientation example dimension map follow facial characteristic eye size eye spacing nose length nose width mouth curvature mouth width mouth openness pupil size eyebrow slant eye eccentricity head eccentricity chernoff face make use ability human mind recognize small difference facial characteristic assimilate many facial characteristic 
23 datum visualization 10 30 50 70 0 10 61 20 80 70 sepal length ( mm ) 60 50 40 70 50 petal length ( mm ) 30 10 45 40 35 30 25 20 sepal width ( mm ) 25 20 15 10 5 0 petal width ( mm ) 40 50 60 70 80 iris species 20 setosa 30 versicolor 40 virginica figure 215 visualization iris datum set used scatter-plot matrix source http gsgscmatgif view large table datum tedious condense datum chernoff face make datum easier user digest way facilitate visualization regularity irregularity present datum although power relate multiple relationship limit another limitation specific datum value show furthermore facial feature vary perceive importance mean similarity two face ( represent two multidimensional datum point ) vary depend order dimension assign facial characteristic therefore mapping carefully choose eye size eyebrow slant find important asymmetrical chernoff face propose extension original technique since face vertical symmetry ( along y-axis ) left right side face identical waste space asymmetrical chernoff face double number facial characteristic thus allow 36 dimension display stick figure visualization technique map multidimensional datum five-piece stick figure figure four limb body two dimension map display ( x ) axe remain dimension map angle 
62 chapter 2 get know datum 10 5 x 0 –5 –10 ⫻1 ⫻2 ⫻3 ⫻4 ⫻5 ⫻6 ⫻7 ⫻8 ⫻9 ⫻10 figure 216 visualization used parallel coordinate source parallel coordithml figure 217 chernoff face face represent n-dimensional datum point ( n ≤ 18 ) or length limb figure 218 show census datum age income map display axe remain dimension ( gender education ) map stick figure datum item relatively dense respect two display dimension result visualization show texture pattern reflect datum trend 
23 datum visualization 63 figure 218 census datum represent used stick figure source professor g grinstein department computer science university massachusett lowell 234 hierarchical visualization technique visualization technique discuss far focus visualize multiple dimension simultaneously however large datum set high dimensionality would difficult visualize dimension time hierarchical visualization technique partition dimension subset ( ie subspace ) subspace visualize hierarchical manner “ worlds-within-world ” also know n-vision representative hierarchical visualization method suppose want visualize 6-d datum set dimension f x1 x5 want observe dimension f change respect dimension first fix value dimension x3 x4 x5 select value say c3 c4 c5 visualize f x1 x2 used 3-d plot call world show figure position origin inner world locate point ( c3 c4 c5 ) outer world another 3-d plot used dimension x3 x4 x5 user interactively change outer world location origin inner world user view result change inner world moreover user vary dimension used inner world outer world give dimension level world used method call “ worlds-withinworld ” another example hierarchical visualization method tree-maps display hierarchical datum set nest rectangle example figure 220 show tree-map visualize google news story news story organized seven category show large rectangle unique color within category ( ie rectangle top level ) news story partition smaller subcategory 
64 chapter 2 get know datum figure 219 “ worlds-within-world ” ( also know n-vision ) source http 1dipstick5gif 235 visualize complex datum relation early day visualization technique mainly numeric datum recently non-numeric datum text social network become available visualize analyze datum attract lot interest many new visualization technique dedicate kind datum example many person web tag various object picture blog entry product reviews tag cloud visualization statistic user-generated tag often tag cloud tag list alphabetically user-preferred order importance tag indicated font size color figure 221 show tag cloud visualize popular tag used web site tag cloud often used two way first tag cloud single item use size tag represent number time tag apply item different user second visualize tag statistic multiple item use size tag represent number item tag apply popularity tag addition complex datum complex relation among datum entry also raise challenge visualization example figure 222 used disease influence graph visualize correlation disease node graph disease size node proportional prevalence corresponding disease two node link edge corresponding disease strong correlation width edge proportional strength correlation pattern two corresponding disease 
24 measure datum similarity dissimilarity 65 figure 220 newsmap use tree-maps visualize google news headline story source wwwcsumd newsmappng summary visualization provide effective tool explore datum introduce several popular method essential idea behind many exist tool method moreover visualization used datum mining various aspect addition visualize datum visualization used represent datum mining process pattern obtain mining method user interaction datum visual datum mining important research development direction 24 measure datum similarity dissimilarity datum mining application cluster outlier analysis nearest-neighbor classification need way assess alike unalike object comparison one another example store may want search cluster customer object result group customer similar characteristic ( eg similar income area residence age ) information used marketing cluster 
66 chapter 2 get know datum figure 221 used tag cloud visualize popular web site tag source snapshot january 23 2010 high blood pressure ( hb ) allergy ( al ) st li overweight ( ov ) en high cholesterol level ( hc ) ki arthritis ( ar ) trouble see ( tr ) li risk diabetes ( ri ) asthma ( ) ca th diabetes ( di ) hayfever ( ha ) hc thyroid problem ( th ) di heart disease ( ) em tr ar hb cancer ( cn ) os sleep disorder ( sl ) ov eczema ( ec ) chronic bronchitis ( ch ) cn osteoporosis ( os ) prostate ( pr ) cardiovascular ( ca ) ps glaucoma ( gl ) ec pr stroke ( st ) liver condition ( li ) ch psa test abnormal ( ps ) kidney ( ki ) endometriosis ( en ) emphysema ( em ) ha al ri sl gl figure 222 disease influence graph person least 20 year old nhane datum set collection datum object object within cluster similar one another dissimilar object cluster outlier analysis also employ clustering-based technique identify potential outlier object highly dissimilar other knowledge object similarity also used nearest-neighbor classification scheme give object ( eg patient ) assign class label ( relate say diagnosis ) base similarity toward object model 
24 measure datum similarity dissimilarity 67 section present similarity dissimilarity measure refer measure proximity similarity dissimilarity related similarity measure two object j typically return value 0 object unalike higher similarity value greater similarity object ( typically value 1 indicate complete similarity object identical ) dissimilarity measure work opposite way return value 0 object ( therefore far dissimilar ) higher dissimilarity value dissimilar two object section 241 present two datum structure commonly used type application datum matrix ( used store datum object ) dissimilarity matrix ( used store dissimilarity value pair object ) also switch different notation datum object previously used chapter since deal object describe one attribute discuss object dissimilarity compute object describe nominal attribute ( section 242 ) binary attribute ( section 243 ) numeric attribute ( section 244 ) ordinal attribute ( section 245 ) combination attribute type ( section 246 ) section 247 provide similarity measure long sparse datum vector term-frequency vector represent document information retrieval know compute dissimilarity useful study attribute also reference later topic cluster ( chapter 10 11 ) outlier analysis ( chapter 12 ) nearest-neighbor classification ( chapter 9 ) 241 datum matrix versus dissimilarity matrix section 22 look way study central tendency dispersion spread observed value attribute x object one-dimensional describe single attribute section talk object describe multiple attribute therefore need change notation suppose n object ( eg person item course ) describe p attribute ( also call measurement feature age height weight gender ) object x1 = ( x11 x12 x1p ) x2 = ( x21 x22 x2p ) xij value object xi jth attribute brevity hereafter refer object xi object i object may tuple relational database also refer datum sample feature vector main memory-based cluster nearest-neighbor algorithms typically operate either follow two datum structure datum matrix ( object-by-attribute structure ) structure store n datum object form relational table n-by-p matrix ( n object ×p attribute )   x11 · · · x1f · · · x1p ··· ··· ··· ··· ···     ( 28 )  xi1 · · · xif · · · xip    ··· ··· ··· ··· ··· xn1 · · · xnf · · · xnp 
68 chapter 2 get know datum row correspond object part notation may use f index p attribute dissimilarity matrix ( object-by-object structure ) structure store collection proximity available pair n object often represent n-by-n table   0  d ( 2 1 ) 0      d ( 3 1 ) ( 3 2 ) 0 ( 29 )       ( n 1 ) ( n 2 ) · · · · · · 0 ( j ) measure dissimilarity “ difference ” object j general ( j ) non-negative number close 0 object j highly similar “ near ” become larger differ note ( ) = 0 difference object furthermore ( j ) = ( j ) ( readability show ( j ) entry matrix symmetric ) measure dissimilarity discuss throughout remainder chapter measure similarity often expressed function measure dissimilarity example nominal datum sim ( j ) = 1 − ( j ) ( 210 ) sim ( j ) similarity object j throughout rest chapter also comment measure similarity datum matrix make two entity “ thing ” namely row ( object ) column ( attribute ) therefore datum matrix often call two-mode matrix dissimilarity matrix contain one kind entity ( dissimilarity ) call one-mode matrix many cluster nearest-neighbor algorithms operate dissimilarity matrix datum form datum matrix transform dissimilarity matrix apply algorithms 242 proximity measure nominal attribute nominal attribute take two state ( section 212 ) example map color nominal attribute may say five state red yellow green pink blue let number state nominal attribute m state denote letter symbol set integer 1 2 m notice integer used datum handle represent specific order 
24 measure datum similarity dissimilarity 69 “ dissimilarity compute object describe nominal attribute ” dissimilarity two object j compute base ratio mismatch ( j ) = p−m p ( 211 ) number match ( ie number attribute j state ) p total number attribute describe object weight assign increase effect assign greater weight match attribute larger number state example 217 dissimilarity nominal attribute suppose sample datum table 22 except object-identifier attribute test-1 available test-1 nominal ( use test-2 test-3 later example ) let ’ compute dissimilarity matrix ( eq 29 )   0  d ( 2 1 ) 0      d ( 3 1 ) ( 3 2 ) 0 ( 4 1 ) ( 4 2 ) ( 4 3 ) 0 since one nominal attribute test-1 set p = 1 eq ( 211 ) ( j ) evaluate 0 object j match 1 object differ thus get  0 1   1 0  0 1 1 0 1     0 see object dissimilar except object 1 4 ( ie ( 4 1 ) = 0 ) table 22 sample datum table contain attribute mixed type object identifier test-1 ( nominal ) test-2 ( ordinal ) test-3 ( numeric ) 1 2 3 4 code code b code c code excellent fair good excellent 45 22 64 28 
70 chapter 2 get know datum alternatively similarity compute sim ( j ) = 1 − ( j ) = p ( 212 ) proximity object describe nominal attribute compute used alternative encode scheme nominal attribute encode used asymmetric binary attribute create new binary attribute state object give state value binary attribute represent state set 1 remain binary attribute set example encode nominal attribute map color binary attribute create five color previously list object color yellow yellow attribute set 1 remain four attribute set proximity measure form encode calculate used method discuss next subsection 243 proximity measure binary attribute let ’ look dissimilarity similarity measure object describe either symmetric asymmetric binary attribute recall binary attribute one two state 0 1 0 mean attribute absent 1 mean present ( section 213 ) give attribute smoker describe patient instance 1 indicate patient smoke 0 indicate patient treat binary attribute numeric mislead therefore method specific binary datum necessary compute dissimilarity “ compute dissimilarity two binary attribute ” one approach involve compute dissimilarity matrix give binary datum binary attribute thought weight 2 × 2 contingency table table 23 q number attribute equal 1 object j r number attribute equal 1 object equal 0 object j number attribute equal 0 object equal 1 object j number attribute equal 0 object j total number attribute p p = q + r + + recall symmetric binary attribute state equally valuable dissimilarity base symmetric binary attribute call symmetric binary dissimilarity object j describe symmetric binary attribute table 23 contingency table binary attribute object j object 1 0 sum 1 q q+s 0 r r t sum q+r s+t p 
24 measure datum similarity dissimilarity 71 dissimilarity j ( j ) = r s q+r s+t ( 213 ) asymmetric binary attribute two state equally important positive ( 1 ) negative ( 0 ) outcome disease test give two asymmetric binary attribute agreement two 1s ( positive match ) consider significant two 0s ( negative match ) therefore binary attribute often consider “ monary ” ( one state ) dissimilarity base attribute call asymmetric binary dissimilarity number negative match consider unimportant thus ignore follow computation ( j ) = r s q+r s ( 214 ) complementarily measure difference two binary attribute base notion similarity instead dissimilarity example asymmetric binary similarity object j compute sim ( j ) = q = 1 − ( j ) q+r s ( 215 ) coefficient sim ( j ) eq ( 215 ) call jaccard coefficient popularly reference literature symmetric asymmetric binary attribute occur datum set mixed attribute approach describe section 246 apply example 218 dissimilarity binary attribute suppose patient record table ( table 24 ) contain attribute name gender fever cough test-1 test-2 test-3 test-4 name object identifier gender symmetric attribute remain attribute asymmetric binary asymmetric attribute value let value ( yes ) p ( positive ) set 1 value n ( negative ) set suppose distance object table 24 relational table patient describe binary attribute name gender fever cough test-1 test-2 test-3 test-4 jack jim mary f n n p n p n n n n n p n n n 
72 chapter 2 get know datum ( patient ) compute base asymmetric attribute accord eq ( 214 ) distance pair three patients—jack mary jim—is ( jack jim ) = 1+1 = 067 1+1+1 ( jack mary ) = 0+1 = 033 2+0+1 ( jim mary ) = 1+2 = 075 1+1+2 measurement suggest jim mary unlikely similar disease highest dissimilarity value among three pair three patient jack mary likely similar disease 244 dissimilarity numeric datum minkowski distance section describe distance measure commonly used compute dissimilarity object describe numeric attribute measure include euclidean manhattan minkowski distance case datum normalize apply distance calculation involve transform datum fall within smaller common range [ −1 1 ] [ 00 10 ] consider height attribute example can measure either meter inch general express attribute smaller unit lead larger range attribute thus tend give attribute greater effect “ weight ” normalize datum attempt give attribute equal weight may may useful particular application method normalize datum discuss detail chapter 3 datum preprocess popular distance measure euclidean distance ( ie straight line “ crow fly ” ) let = ( xi1 xi2 xip ) j = ( xj1 xj2 xjp ) two object describe p numeric attribute euclidean distance object j defined q ( j ) = ( xi1 − xj1 ) 2 + ( xi2 − xj2 ) 2 + · · · + ( xip − xjp ) 2 ( 216 ) another well-known measure manhattan ( city block ) distance name distance block two point city ( 2 block 3 block total 5 block ) defined ( j ) = xi1 − xj1 | + xi2 − xj2 | + · · · + xip − xjp | ( 217 ) euclidean manhattan distance satisfy follow mathematical property non-negativity ( j ) ≥ 0 distance non-negative number identity indiscernible ( ) = 0 distance object 0 
24 measure datum similarity dissimilarity 73 symmetry ( j ) = ( j ) distance symmetric function triangle inequality ( j ) ≤ ( k ) + ( k j ) go directly object object j space make detour object k measure satisfy condition know metric please note non-negativity property imply three property example 219 euclidean distance manhattan distance let x1 = ( 1 2 ) x2 = ( 3 5 ) represent √ two object show figure euclidean distance two 22 + 32 = manhattan distance two 2 + 3 = 5 minkowski distance generalization euclidean manhattan distance defined q ( j ) = h xi1 − xj1 h + xi2 − xj2 h + · · · + xip − xjp h ( 218 ) h real number h ≥ 1 ( distance also call lp norm literature symbol p refer notation h keep p number attribute consistent rest chapter ) represent manhattan distance h = 1 ( ie l1 norm ) euclidean distance h = 2 ( ie l2 norm ) supremum distance ( also refer lmax l∞ norm chebyshev distance ) generalization minkowski distance h → ∞ compute find attribute f give maximum difference value two object difference supremum distance defined formally  1 h p x p h  ( j ) = lim xif − xjf | = max xif − xjf | ( 219 ) h→∞ f 1 f l∞ norm also know uniform norm x2 = ( 3 5 ) 5 4 euclidean distance = ( 22 + 32 ) 2 = 361 3 3 2 x1 = ( 1 2 ) manhattan distance 2+3=5 supremum distance 5–2=3 2 1 1 2 3 figure 223 euclidean manhattan supremum distance two object 
74 chapter 2 get know datum example 220 supremum distance let ’ use two object x1 = ( 1 2 ) x2 = ( 3 5 ) figure second attribute give greatest difference value object 5 − 2 = supremum distance object attribute assign weight accord perceive importance weight euclidean distance compute q ( j ) = w1 xi1 − xj1 2 + w2 xi2 − xj2 2 + · · · + wm xip − xjp 2 ( 220 ) weighting also apply distance measure well 245 proximity measure ordinal attribute value ordinal attribute meaningful order ranking yet magnitude successive value unknown ( section 214 ) example include sequence small medium large size attribute ordinal attribute may also obtain discretization numeric attribute splitting value range finite number category category organized rank range numeric attribute map ordinal attribute f mf state example range interval-scaled attribute temperature ( celsius ) organized follow state −30 −10 −10 10 10 30 represent category cold temperature moderate temperature warm temperature respectively let represent number possible state ordinal attribute order state define ranking 1 mf “ ordinal attribute handled ” treatment ordinal attribute quite similar numeric attribute compute dissimilarity object suppose f attribute set ordinal attribute describe n object dissimilarity computation respect f involve follow step value f ith object xif f mf order state represent ranking 1 mf replace xif corresponding rank rif ∈ { 1 mf } since ordinal attribute different number state often necessary map range attribute onto [ 00 10 ] attribute equal weight perform datum normalization replace rank rif ith object f th attribute zif = rif − 1 mf − 1 ( 221 ) dissimilarity compute used distance measure describe section 244 numeric attribute used zif represent f value ith object 
24 measure datum similarity dissimilarity 75 example 221 dissimilarity ordinal attribute suppose sample datum show earlier table 22 except time object-identifier continuous ordinal attribute test-2 available three state test-2 fair good excellent mf = step 1 replace value test-2 rank four object assign rank 3 1 2 3 respectively step 2 normalizes ranking mapping rank 1 00 rank 2 05 rank 3 step 3 use say euclidean distance ( eq 216 ) result follow dissimilarity matrix  0 10 0   05 05 0 0 10 05      0 therefore object 1 2 dissimilar object 2 4 ( ie ( 2 1 ) = 10 ( 4 2 ) = 10 ) make intuitive sense since object 1 4 excellent object 2 fair opposite end range value test-2 similarity value ordinal attribute interpreted dissimilarity sim ( j ) = 1 − ( j ) 246 dissimilarity attribute mixed type section 242 245 discuss compute dissimilarity object describe attribute type type may either nominal symmetric binary asymmetric binary numeric ordinal however many real databasis object describe mixture attribute type general database contain attribute type “ compute dissimilarity object mixed attribute type ” one approach group type attribute together perform separate datum mining ( eg cluster ) analysis type feasible analysis derive compatible result however real application unlikely separate analysis per attribute type generate compatible result preferable approach process attribute type together perform single analysis one technique combine different attribute single dissimilarity matrix bring meaningful attribute onto common scale interval [ 00 10 ] suppose datum set contain p attribute mixed type dissimilarity ( j ) object j defined ( f ) ( f ) f 1 δij dij pp ( f ) f 1 δij pp ( j ) = ( 222 ) 
76 chapter 2 get know datum ( f ) indicator δij = 0 either ( 1 ) xif xjf miss ( ie measurement attribute f object object j ) ( 2 ) xif = xjf = 0 attribute ( f ) f asymmetric binary otherwise δij = contribution attribute f ( f ) dissimilarity j ( ie dij ) compute dependent type ( f ) f numeric dij = attribute f xif −xjf | maxh xhf −minh xhf h run nonmissing object ( f ) ( f ) f nominal binary dij = 0 xif = xjf otherwise dij = 1 f ordinal compute rank rif zif = rif −1 mf −1 treat zif numeric step identical already see individual attribute type difference numeric attribute normalize value map interval [ 00 10 ] thus dissimilarity object compute even attribute describe object different type example 222 dissimilarity attribute mixed type let ’ compute dissimilarity matrix object table consider attribute different type example 217 221 work dissimilarity matrix individual attribute procedure follow test-1 ( nominal ) test-2 ( ordinal ) outlined earlier process attribute mixed type therefore use dissimilarity matrix obtain test-1 test-2 later compute eq ( 222 ) first however need compute dissimilarity matrix third attribute test-3 ( numeric ) ( 3 ) must compute dij follow case numeric attribute let maxh xh = 64 minh xh = difference two used eq ( 222 ) normalize value dissimilarity matrix result dissimilarity matrix test-3  0 055   045 040  0 100 014 0 086     0 use dissimilarity matrix three attribute computation ( f ) eq ( 222 ) indicator δij = 1 three attribute f get example ( 3 1 ) = 1 ( 1 ) 1 ( 050 ) 1 ( 045 ) 3 = result dissimilarity matrix obtain 
24 measure datum similarity dissimilarity 77 datum describe three attribute mixed type  0 085   065 013  0 083 071 0 079     0 table 22 intuitively guess object 1 4 similar base value test-1 test-2 confirm dissimilarity matrix ( 4 1 ) lowest value pair different object similarly matrix indicate object 1 2 least similar 247 cosine similarity document represent thousand attribute record frequency particular word ( keyword ) phrase document thus document object represent call term-frequency vector example table 25 see document1 contain five instance word team hockey occur three time word coach absent entire document indicated count value datum highly asymmetric term-frequency vector typically long sparse ( ie many 0 value ) application used structure include information retrieval text document cluster biological taxonomy gene feature mapping traditional distance measure study chapter work well sparse numeric datum example two term-frequency vector may many 0 value common meaning corresponding document share many word make similar need measure focus word two document common occurrence frequency word word need measure numeric datum ignore zero-match cosine similarity measure similarity used compare document say give ranking document respect give vector query word let x two vector comparison used cosine measure table 25 document vector term-frequency vector document team coach hockey baseball soccer penalty score win loss season document1 document2 document3 document4 5 3 0 0 0 0 7 1 3 2 0 0 0 0 2 0 2 1 1 1 0 1 0 2 0 0 0 2 2 1 3 0 0 0 0 3 0 1 0 0 
78 chapter 2 get know datum similarity function sim ( x ) = x·y | ( 223 ) | euclidean norm vector x = ( x1 x2 xp ) defined q x12 + x22 + · · · + xp2 conceptually length vector similarly | euclidean norm vector y measure compute cosine angle vector x y cosine value 0 mean two vector 90 degree ( orthogonal ) match closer cosine value 1 smaller angle greater match vector note cosine similarity measure obey property section 244 define metric measure refer nonmetric measure example 223 cosine similarity two term-frequency vector suppose x first two term-frequency vector table x = ( 5 0 3 0 2 0 0 2 0 0 ) = ( 3 0 2 0 1 1 0 1 0 1 ) similar x used eq ( 223 ) compute cosine similarity two vector get xt · = 5 × 3 + 0 × 0 + 3 × 2 + 0 × 0 + 2 × 1 + 0 × 1 + 0 × 0 + 2 × 1 + 0 × 0 + 0 × 1 = 25 p | = 52 + 02 + 32 + 02 + 22 + 02 + 02 + 22 + 02 + 02 = 648 p | = 32 + 02 + 22 + 02 + 12 + 12 + 02 + 12 + 02 + 12 = 412 sim ( x ) = 094 therefore used cosine similarity measure compare document would consider quite similar attribute binary-valu cosine similarity function interpreted term share feature attribute suppose object x possess ith attribute xi = xt · number attribute possessed ( ie share ) x | geometric mean number attribute possessed x number possessed y thus sim ( x ) measure relative possession common attribute simple variation cosine similarity precede scenario sim ( x ) = x·y x·x+y·y−x·y ( 224 ) ratio number attribute share x number attribute possessed x y function know tanimoto coefficient tanimoto distance frequently used information retrieval biology taxonomy 
26 exercise 25 79 summary datum set make datum object datum object represent entity datum object describe attribute attribute nominal binary ordinal numeric value nominal ( categorical ) attribute symbol name thing value represent kind category code state binary attribute nominal attribute two possible state ( 1 0 true false ) two state equally important attribute symmetric otherwise asymmetric ordinal attribute attribute possible value meaningful order ranking among magnitude successive value know numeric attribute quantitative ( ie measurable quantity ) represent integer real value numeric attribute type interval-scaled ratioscale value interval-scaled attribute measure fix equal unit ratio-scale attribute numeric attribute inherent zero-point measurement ratio-scale speak value order magnitude larger unit measurement basic statistical description provide analytical foundation datum preprocess basic statistical measure datum summarization include mean weight mean median mode measure central tendency datum range quantile quartile interquartile range variance standard deviation measure dispersion datum graphical representation ( eg boxplot quantile plot quantile– quantile plot histogram scatter plot ) facilitate visual inspection datum thus useful datum preprocess mining datum visualization technique may pixel-oriented geometric-based icon-based hierarchical method apply multidimensional relational datum additional technique propose visualization complex datum text social network measure object similarity dissimilarity used datum mining application cluster outlier analysis nearest-neighbor classification measure proximity compute attribute type study chapter combination attribute example include jaccard coefficient asymmetric binary attribute euclidean manhattan minkowski supremum distance numeric attribute application involve sparse numeric datum vector term-frequency vector cosine measure tanimoto coefficient often used assessment similarity 26 exercise 21 give three additional commonly used statistical measure already illustrated chapter characterization datum dispersion discuss compute efficiently large databasis 
80 chapter 2 get know datum 22 suppose datum analysis include attribute age age value datum tuple ( increase order ) 13 15 16 16 19 20 20 21 22 22 25 25 25 25 30 33 33 35 35 35 35 36 40 45 46 52 70 ( ) mean datum median ( b ) mode datum comment datum ’ modality ( ie bimodal trimodal etc ) ( c ) midrange datum ( ) find ( roughly ) first quartile ( q1 ) third quartile ( q3 ) datum ( e ) give five-number summary datum ( f ) show boxplot datum ( g ) quantile–quantile plot different quantile plot 23 suppose value give set datum group interval interval corresponding frequency follow age 1–5 6–15 16–20 21–50 51–80 81–110 frequency 200 450 300 1500 700 44 compute approximate median value datum 24 suppose hospital test age body fat datum 18 randomly select adult follow result age % fat 23 95 23 265 27 78 27 178 39 314 41 259 47 274 49 272 50 312 age % fat 52 346 54 425 54 288 56 334 57 302 58 341 58 329 60 412 61 357 ( ) calculate mean median standard deviation age % fat ( b ) draw boxplot age % fat ( c ) draw scatter plot q-q plot base two variable 25 briefly outline compute dissimilarity object describe follow ( ) nominal attribute ( b ) asymmetric binary attribute 
27 bibliographic note 81 ( c ) numeric attribute ( ) term-frequency vector 26 give two object represent tuple ( 22 1 42 10 ) ( 20 0 36 8 ) ( ) ( b ) ( c ) ( ) compute euclidean distance two object compute manhattan distance two object compute minkowski distance two object used q = 3 compute supremum distance two object 27 median one important holistic measure datum analysis propose several method median approximation analyze respective complexity different parameter setting decide extent real value approximate moreover suggest heuristic strategy balance accuracy complexity apply method give 28 important define select similarity measure datum analysis however commonly accept subjective similarity measure result vary depend similarity measure used nonetheless seemingly different similarity measure may equivalent transformation suppose follow 2-d datum set x1 x2 x3 x4 x5 a1 15 2 16 12 15 a2 17 19 18 15 10 ( ) consider datum 2-d datum point give new datum point x = ( 14 16 ) query rank database point base similarity query used euclidean distance manhattan distance supremum distance cosine similarity ( b ) normalize datum set make norm datum point equal use euclidean distance transform datum rank datum point 27 bibliographic note method descriptive datum summarization study statistic literature long onset computer good summary statistical descriptive datum mining method include freedman pisani purf [ fpp07 ] devore [ dev95 ] 
82 chapter 2 get know datum statistics-based visualization datum used boxplot quantile plot quantile–quantile plot scatter plot loess curf see cleveland [ cle93 ] pioneer work datum visualization technique describe visual display quantitative information [ tuf83 ] envision information [ tuf90 ] visual explanation image quantity evidence narrative [ tuf97 ] tufte addition graphic graphic information process bertin [ ber81 ] visualize datum cleveland [ cle93 ] information visualization datum mining knowledge discovery edit fayyad grinstein wierse [ fgw01 ] major conference symposium visualization include acm human factor compute system ( chi ) visualization international symposium information visualization research visualization also publish transaction visualization computer graphic journal computational graphical statistic ieee computer graphic application many graphical user interface visualization tool develop find various datum mining product several book datum mining ( eg datum mining solution westphal blaxton [ wb98 ] ) present many good example visual snapshot survey visualization technique see “ visual technique explore databasis ” keim [ kei97 ] similarity distance measure among various variable introduce many textbook study cluster analysis include hartigan [ har75 ] jain dube [ jd88 ] kaufman rousseeuw [ kr90 ] arabie hubert de soete [ ahs96 ] method combine attribute different type single dissimilarity matrix introduce kaufman rousseeuw [ kr90 ] 
10 cluster analysis basic concept method imagine director customer relationship allelectronic five manager work would like organize company ’ customer five group group assign different manager strategically would like customer group similar possible moreover two give customer different business pattern place group intention behind business strategy develop customer relationship campaign specifically target group base common feature share customer per group kind datum mining technique help accomplish task unlike classification class label ( group id ) customer unknown need discover grouping give large number customer many attribute describe customer profile costly even infeasible human study datum manually come way partition customer strategic group need cluster tool help cluster process grouping set datum object multiple group cluster object within cluster high similarity dissimilar object cluster dissimilarity similarity assessed base attribute value describe object often involve distance measures1 cluster datum mining tool root many application area biology security business intelligence web search chapter present basic concept method cluster analysis section 101 introduce topic study requirement cluster method massive amount datum various application learn several basic cluster technique organized follow category partition method ( section 102 ) hierarchical method ( section 103 ) density-based method ( section 104 ) grid-based method ( section 105 ) section 106 briefly discuss evaluate 1 datum similarity dissimilarity discuss detail section may want refer section quick review datum mining concept technique doi b978-0-12-381479-100010-1 c 2012 elsevier right re-serve 443 
444 chapter 10 cluster analysis basic concept method cluster method discussion advanced method cluster re-serve chapter 11 101 cluster analysis section set groundwork study cluster analysis section 1011 define cluster analysis present example useful section 1012 learn aspect compare cluster method well requirement cluster overview basic cluster technique present section 1013 1011 cluster analysis cluster analysis simply cluster process partition set datum object ( observation ) subset subset cluster object cluster similar one another yet dissimilar object cluster set cluster result cluster analysis refer cluster context different cluster method may generate different clustering datum set partition perform human cluster algorithm hence cluster useful lead discovery previously unknown group within datum cluster analysis widely used many application business intelligence image pattern recognition web search biology security business intelligence cluster used organize large number customer group customer within group share strong similar characteristic facilitate development business strategy enhance customer relationship management moreover consider consultant company large number project improve project management cluster apply partition project category base similarity project audit diagnosis ( improve project delivery outcome ) conduct effectively image recognition cluster used discover cluster “ subclass ” handwritten character recognition system suppose datum set handwritten digit digit labele either 1 2 3 note large variance way person write digit take number 2 example person may write small circle left bottom part other may use cluster determine subclass “ 2 ” represent variation way 2 written used multiple model base subclass improve overall recognition accuracy cluster also find many application web search example keyword search may often return large number hit ( ie page relevant search ) due extremely large number web page cluster used organize search result group present result concise easily accessible way moreover cluster technique develop cluster document topic commonly used information retrieval practice 
101 cluster analysis 445 datum mining function cluster analysis used standalone tool gain insight distribution datum observe characteristic cluster focus particular set cluster analysis alternatively may serve preprocess step algorithms characterization attribute subset selection classification would operate detected cluster select attribute feature cluster collection datum object similar one another within cluster dissimilar object cluster cluster datum object treat implicit class sense cluster sometimes call automatic classification critical difference cluster automatically find grouping distinct advantage cluster analysis cluster also call datum segmentation application cluster partition large datum set group accord similarity cluster also used outlier detection outlier ( value “ far away ” cluster ) may interesting common case application outlier detection include detection credit card fraud monitoring criminal activity electronic commerce example exceptional case credit card transaction expensive infrequent purchase may interest possible fraudulent activity outlier detection subject chapter 12 datum cluster vigorous development contribute area research include datum mining statistic machine learn spatial database technology information retrieval web search biology marketing many application area owing huge amount datum collect databasis cluster analysis recently become highly active topic datum mining research branch statistic cluster analysis extensively study main focus distance-based cluster analysis cluster analysis tool base k-mean k-medoid several method also build many statistical analysis software package system s-plus spss sas machine learn recall classification know supervised learn class label information give learn algorithm supervised tell class membership training tuple cluster know unsupervised learn class label information present reason cluster form learn observation rather learn example datum mining effort focuse find method efficient effective cluster analysis large databasis active theme research focus scalability cluster method effectiveness method cluster complex shape ( eg nonconvex ) type datum ( eg text graph image ) high-dimensional cluster technique ( eg cluster object thousand feature ) method cluster mixed numerical nominal datum large databasis 1012 requirement cluster analysis cluster challenge research field section learn requirement cluster datum mining tool well aspect used compare cluster method 
446 chapter 10 cluster analysis basic concept method follow typical requirement cluster datum mining scalability many cluster algorithms work well small datum set contain fewer several hundred datum object however large database may contain million even billion object particularly web search scenario cluster sample give large datum set may lead bias result therefore highly scalable cluster algorithms need ability deal different type attribute many algorithms design cluster numeric ( interval-based ) datum however application may require cluster datum type binary nominal ( categorical ) ordinal datum mixture datum type recently application need cluster technique complex datum type graph sequence image document discovery cluster arbitrary shape many cluster algorithms determine cluster base euclidean manhattan distance measure ( chapter 2 ) algorithms base distance measure tend find spherical cluster similar size density however cluster can shape consider sensor example often deploy environment surveillance cluster analysis sensor reading detect interesting phenomena may want use cluster find frontier run forest fire often spherical important develop algorithms detect cluster arbitrary shape requirement domain knowledge determine input parameter many cluster algorithms require user provide domain knowledge form input parameter desire number cluster consequently cluster result may sensitive parameter parameter often hard determine especially high-dimensionality datum set user yet grasp deep understand datum require specification domain knowledge burden user also make quality cluster difficult control ability deal noisy datum real-world datum set contain outlier or miss unknown erroneous datum sensor reading example often noisy—some reading may inaccurate due sense mechanism reading may erroneous due interference surround transient object cluster algorithms sensitive noise may produce poor-quality cluster therefore need cluster method robust noise incremental cluster insensitivity input order many application incremental update ( represent newer datum ) may arrive time cluster algorithms incorporate incremental update exist cluster structure instead recompute new cluster scratch cluster algorithms may also sensitive input datum order give set datum object cluster algorithms may return dramatically different clustering depend order object present incremental cluster algorithms algorithms insensitive input order need 
101 cluster analysis 447 capability cluster high-dimensionality datum datum set contain numerous dimension attribute cluster document example keyword regard dimension often thousand keyword cluster algorithms good handle low-dimensional datum datum set involve two three dimension find cluster datum object highdimensional space challenge especially consider datum sparse highly skewer constraint-based cluster real-world application may need perform cluster various kind constraint suppose job choose location give number new automatic teller machine ( atms ) city decide upon may cluster household consider constraint city ’ river highway network type number customer per cluster challenge task find datum group good cluster behavior satisfy specify constraint interpretability usability user want cluster result interpretable comprehensible usable cluster may need tie specific semantic interpretation application important study application goal may influence selection cluster feature cluster method follow orthogonal aspect cluster method compare partition criterium method object partition hierarchy exist among cluster cluster level conceptually method useful example partition customer group group manager alternatively method partition datum object hierarchically cluster form different semantic level example text mining may want organize corpus document multiple general topic “ politic ” “ sport ” may subtopic instance “ football ” “ basketball ” “ baseball ” “ hockey ” exist subtopic “ ” latter four subtopic lower level hierarchy “ sport ” separation cluster method partition datum object mutually exclusive cluster cluster customer group group take care one manager customer may belong one group situation cluster may exclusive datum object may belong one cluster example cluster document topic document may related multiple topic thus topic cluster may exclusive similarity measure method determine similarity two object distance distance defined euclidean space 
448 chapter 10 cluster analysis basic concept method road network vector space space method similarity may defined connectivity base density contiguity may rely absolute distance two object similarity measure play fundamental role design cluster method distance-based method often take advantage optimization technique - continuity-based method often find cluster arbitrary shape cluster space many cluster method search cluster within entire give datum space method useful low-dimensionality datum set highdimensional datum however many irrelevant attribute make similarity measurement unreliable consequently cluster find full space often meaningless ’ often better instead search cluster within different subspace datum set subspace cluster discover cluster subspace ( often low dimensionality ) manifest object similarity conclude cluster algorithms several requirement factor include scalability ability deal different type attribute noisy datum incremental update cluster arbitrary shape constraint interpretability usability also important addition cluster method differ respect partition level whether cluster mutually exclusive similarity measure used whether subspace cluster perform 1013 overview basic cluster method many cluster algorithms literature difficult provide crisp categorization cluster method category may overlap method may feature several category nevertheless useful present relatively organized picture cluster method general major fundamental cluster method classify follow category discuss rest chapter partition method give set n object partition method construct k partition datum partition represent cluster k ≤ n divide datum k group group must contain least one object word partition method conduct one-level partition datum set basic partition method typically adopt exclusive cluster separation object must belong exactly one group requirement may relax example fuzzy partition technique reference technique give bibliographic note ( section 109 ) partition method distance-based give k number partition construct partition method create initial partition used iterative relocation technique attempt improve partition move object one group another general criterion good partition object cluster “ close ” related whereas object different cluster “ far apart ” different various kind 
101 cluster analysis 449 criterium judge quality partition traditional partition method extend subspace cluster rather search full datum space useful many attribute datum sparse achieve global optimality partitioning-based cluster often computationally prohibitive potentially require exhaustive enumeration possible partition instead application adopt popular heuristic method greedy approach like k-mean k-medoid algorithms progressively improve cluster quality approach local optimum heuristic cluster method work well find spherical-shap cluster - medium-size databasis find cluster complex shape large datum set partitioning-based method need extend partitioning-based cluster method study depth section 102 hierarchical method hierarchical method create hierarchical decomposition give set datum object hierarchical method classify either agglomerative divisive base hierarchical decomposition form agglomerative approach also call bottom-up approach start object form separate group successively merge object group close one another group merged one ( topmost level hierarchy ) termination condition hold divisive approach also call top-down approach start object cluster successive iteration cluster split smaller cluster eventually object one cluster termination condition hold hierarchical cluster method distance-based - continuitybased various extension hierarchical method consider cluster subspace well hierarchical method suffer fact step ( merge split ) do never undo rigidity useful lead smaller computation cost worry combinatorial number different choice technique correct erroneous decision however method improve quality hierarchical cluster propose hierarchical cluster method study section 103 density-based method partition method cluster object base distance object method find spherical-shap cluster encounter difficulty discover cluster arbitrary shape cluster method develop base notion density general idea continue grow give cluster long density ( number object datum point ) “ neighborhood ” exceed threshold example datum point within give cluster neighborhood give radius contain least minimum number point method used filter noise outlier discover cluster arbitrary shape density-based method divide set object multiple exclusive cluster hierarchy cluster typically density-based method consider exclusive cluster consider fuzzy cluster moreover density-based method extend full space subspace cluster density-based cluster method study section 104 
450 chapter 10 cluster analysis basic concept method grid-based method grid-based method quantize object space finite number cell form grid structure cluster operation perform grid structure ( ie quantized space ) main advantage approach fast process time typically independent number datum object dependent number cell dimension quantized space used grid often efficient approach many spatial datum mining problem include cluster therefore grid-based method integrate cluster method density-based method hierarchical method gridbase cluster study section 105 method briefly summarize figure cluster algorithms integrate idea several cluster method sometimes difficult classify give algorithm uniquely belong one cluster method category furthermore application may cluster criterium require integration several cluster technique follow section examine cluster method detail advanced cluster method related issue discuss chapter general notation used follow let datum set n object cluster object describe variable variable also call attribute dimension method partition method general characteristic – find mutually exclusive cluster spherical shape – distance-based – may use mean medoid ( etc ) represent cluster center – effective - medium-size datum set hierarchical method – cluster hierarchical decomposition ( ie multiple level ) – correct erroneous merge split – may incorporate technique like microcluster consider object “ linkage ” density-based method – find arbitrarily shape cluster – cluster dense region object space separated low-density region – cluster density point must minimum number point within “ neighborhood ” – may filter outlier grid-based method – use multiresolution grid datum structure – fast process time ( typically independent number datum object yet dependent grid size ) figure 101 overview cluster method discuss chapter note algorithms may combine various method 
102 partition method 451 therefore may also refer point d-dimensional object space object represent bold italic font ( eg p ) 102 partition method simplest fundamental version cluster analysis partition organize object set several exclusive group cluster keep problem specification concise assume number cluster give background knowledge parameter start point partition method formally give datum set n object k number cluster form partition algorithm organize object k partition ( k ≤ n ) partition represent cluster cluster form optimize objective partition criterion dissimilarity function base distance object within cluster “ similar ” one another “ dissimilar ” object cluster term datum set attribute section learn well-known commonly used partition methods—k-mean ( section 1021 ) k-medoid ( section 1022 ) also learn several variation classic partition method scale handle large datum set 1021 k-mean centroid-based technique suppose datum set contain n object euclidean space partition method distribute object k cluster c1 ck ci ⊂ ci ∩ cj = ∅ ( 1 ≤ j ≤ k ) objective function used assess partition quality object within cluster similar one another dissimilar object cluster objective function aim high intracluster similarity low intercluster similarity centroid-based partition technique used centroid cluster ci represent cluster conceptually centroid cluster center point centroid defined various way mean medoid object ( point ) assign cluster difference object p ∈ ci ci representative cluster measure dist ( p ci ) dist ( x ) euclidean distance two point x y quality cluster ci measure withincluster variation sum square error object ci centroid ci defined = k x x dist ( p ci ) 2 ( 101 ) i=1 p∈ci e sum square error object datum set p point space represent give object ci centroid cluster ci ( p ci multidimensional ) word object cluster distance 
452 chapter 10 cluster analysis basic concept method object cluster center square distance sum objective function try make result k cluster compact separate possible optimize within-cluster variation computationally challenge worst case would enumerate number possible partitioning exponential number cluster check within-cluster variation value show problem np-hard general euclidean space even two cluster ( ie k = 2 ) moreover problem np-hard general number cluster k even 2-d euclidean space number cluster k dimensionality space fix problem solve time ( ndk+1 log n ) n number object overcome prohibitive computational cost exact solution greedy approach often used practice prime example k-mean algorithm simple commonly used “ k-mean algorithm work ” k-mean algorithm define centroid cluster mean value point within cluster proceed follow first randomly select k object initially represent cluster mean center remain object object assign cluster similar base euclidean distance object cluster mean k-mean algorithm iteratively improve within-cluster variation cluster compute new mean used object assign cluster previous iteration object reassign used update mean new cluster center iteration continue assignment stable cluster form current round form previous round k-mean procedure summarize figure 102 algorithm k-mean k-mean algorithm partition cluster ’ center represent mean value object cluster input k number cluster datum set contain n object output set k cluster method ( 1 ) arbitrarily choose k object initial cluster center ( 2 ) repeat ( 3 ) ( ) assign object cluster object similar base mean value object cluster ( 4 ) update cluster mean calculate mean value object cluster ( 5 ) change figure 102 k-mean partition algorithm 
102 partition method 453 + + + + + ( ) initial cluster + ( b ) iterate + + + ( c ) final cluster figure 103 cluster set object used k-mean method ( b ) update cluster center reassign object accordingly ( mean cluster marked + ) example 101 cluster k-mean partition consider set object locate 2-d space depict figure 103 ( ) let k = 3 user would like object partition three cluster accord algorithm figure 102 arbitrarily choose three object three initial cluster center cluster center marked + object assign cluster base cluster center nearest distribution form silhouette encircle dot curf show figure 103 ( ) next cluster center update mean value cluster recalculate base current object cluster used new cluster center object redistribute cluster base cluster center nearest redistribution form new silhouette encircle dash curf show figure 103 ( b ) process iterate lead figure 103 ( c ) process iteratively reassigning object cluster improve partition refer iterative relocation eventually reassignment object cluster occur process terminate result cluster return cluster process k-mean method guarantee converge global optimum often terminate local optimum result may depend initial random selection cluster center ( ask give example show exercise ) obtain good result practice common run k-mean algorithm multiple time different initial cluster center time complexity k-mean algorithm ( nkt ) n total number object k number cluster number iteration normally k n n therefore method relatively scalable efficient process large datum set several variant k-mean method differ selection initial k-mean calculation dissimilarity strategy calculate cluster mean 
454 chapter 10 cluster analysis basic concept method k-mean method apply mean set object defined may case application datum nominal attribute involved k-mode method variant k-mean extend k-mean paradigm cluster nominal datum replace mean cluster mode used new dissimilarity measure deal nominal object frequency-based method update mode cluster k-mean k-mode method integrate cluster datum mixed numeric nominal value necessity user specify k number cluster advance see disadvantage study overcome difficulty however provide approximate range k value used analytical technique determine best k compare cluster result obtain different k value k-mean method suitable discover cluster nonconvex shape cluster different size moreover sensitive noise outlier datum point small number datum substantially influence mean value “ make k-mean algorithm scalable ” one approach make k-mean method efficient large datum set use good-sized set sample cluster another employ filter approach used spatial hierarchical datum index save cost compute mean third approach explore microcluster idea first group nearby object “ microcluster ” perform k-mean cluster microcluster microcluster discuss section 103 1022 k-medoid representative object-based technique k-mean algorithm sensitive outlier object far away majority datum thus assign cluster dramatically distort mean value cluster inadvertently affect assignment object cluster effect particularly exacerbate due use squared-error function eq ( 101 ) observed example 102 example 102 drawback k-mean consider six point 1-d space value 1 2 3 8 9 10 25 respectively intuitively visual inspection may imagine point partition cluster { 1 2 3 } { 8 9 10 } point 25 exclude appear outlier would k-mean partition value apply k-mean used k = 2 eq ( 101 ) partition { { 1 2 3 } { 8 9 10 25 } } within-cluster variation ( 1 − 2 ) 2 + ( 2 − 2 ) 2 + ( 3 − 2 ) 2 + ( 8 − 13 ) 2 + ( 9 − 13 ) 2 + ( 10 − 13 ) 2 + ( 25 − 13 ) 2 = 196 give mean cluster { 1 2 3 } 2 mean { 8 9 10 25 } compare partition { { 1 2 3 8 } { 9 10 25 } } k-mean compute withincluster variation ( 1 − 35 ) 2 + ( 2 − 35 ) 2 + ( 3 − 35 ) 2 + ( 8 − 35 ) 2 + ( 9 − 1467 ) 2 + ( 10 − 1467 ) 2 + ( 25 − 1467 ) 2 = 18967 
102 partition method 455 give 35 mean cluster { 1 2 3 8 } 1467 mean cluster { 9 10 25 } latter partition lowest within-cluster variation therefore k-mean method assign value 8 cluster different contain 9 10 due outlier point moreover center second cluster 1467 substantially far member cluster “ modify k-mean algorithm diminish sensitivity outlier ” instead take mean value object cluster reference point pick actual object represent cluster used one representative object per cluster remain object assign cluster representative object similar partition method perform base principle minimize sum dissimilarity object p corresponding representative object absolute-error criterion used defined = k x x dist ( p oi ) ( 102 ) i=1 p∈ci e sum absolute error object p datum set oi representative object ci basis k-medoid method group n object k cluster minimize absolute error ( eq 102 ) k = 1 find exact median ( n2 ) time however k general positive number k-medoid problem np-hard partition around medoid ( pam ) algorithm ( see figure 105 later ) popular realization k-medoid cluster tackle problem iterative greedy way like k-mean algorithm initial representative object ( call seed ) choose arbitrarily consider whether replace representative object nonrepresentative object would improve cluster quality possible replacement try iterative process replace representative object object continue quality result cluster improve replacement quality measure cost function average dissimilarity object representative object cluster specifically let o1 ok current set representative object ( ie medoid ) determine whether nonrepresentative object denote orandom good replacement current medoid oj ( 1 ≤ j ≤ k ) calculate distance every object p closest object set { o1 oj−1 orandom oj+1 ok } use distance update cost function reassignment object { o1 oj−1 orandom oj+1 ok } simple suppose object p currently assign cluster represent medoid oj ( figure 104a b ) need reassign p different cluster oj replace orandom object p need reassign either orandom cluster represent oi ( = j ) whichever closest example figure 104 ( ) p closest oi therefore reassign oi figure 104 ( b ) however p closest orandom reassign orandom instead p currently assign cluster represent object oi = j 
456 chapter 10 cluster analysis basic concept method oi p oj orandom ( ) reassign oi oi oj p oi oj oi oj p orandom ( b ) reassign orandom ( c ) change orandom p orandom datum object cluster center swap swap ( ) reassign orandom figure 104 four case cost function k-medoid cluster object remain assign cluster represent oi long still closer oi orandom ( figure 104c ) otherwise reassign orandom ( figure 104d ) time reassignment occur difference absolute error e contribute cost function therefore cost function calculate difference absolute-error value current representative object replace nonrepresentative object total cost swap sum cost incur nonrepresentative object total cost negative oj replace swap orandom actual absolute-error e reduce total cost positive current representative object oj consider acceptable nothing change iteration “ method robust—k-mean k-medoid ” k-medoid method robust k-mean presence noise outlier medoid less influenced outlier extreme value mean however complexity iteration k-medoid algorithm ( k ( n − k ) 2 ) large value n k computation become costly much costly k-mean method method require user specify k number cluster “ scale k-medoid method ” typical k-medoid partition algorithm like pam ( figure 105 ) work effectively small datum set scale well large datum set deal larger datum set sampling-based method call clara ( cluster large application ) used instead take whole datum set consideration clara used random sample datum set pam algorithm apply compute best medoid sample ideally sample closely represent original datum set many case large sample work well create object equal probability select sample representative object ( medoid ) choose likely similar would choose whole datum set clara build clustering multiple random sample return best cluster output complexity compute medoid random sample ( ks 2 + k ( n − k ) ) size sample k number cluster n total number object clara deal larger datum set pam effectiveness clara depend sample size notice pam search best k-medoid among give datum set whereas clara search best k-medoid among select sample datum set clara find good cluster best sample medoid far best k-medoid object 
103 hierarchical method 457 algorithm k-medoid pam k-medoid algorithm partition base medoid central object input k number cluster datum set contain n object output set k cluster method ( 1 ) arbitrarily choose k object initial representative object seed ( 2 ) repeat ( 3 ) assign remain object cluster nearest representative object ( 4 ) randomly select nonrepresentative object orandom ( 5 ) compute total cost swap representative object oj orandom ( 6 ) < 0 swap oj orandom form new set k representative object ( 7 ) change figure 105 pam k-medoid partition algorithm one best k-medoid select sampling clara never find best cluster ( ask provide example demonstrate exercise ) “ might improve quality scalability clara ” recall search better medoid pam examine every object datum set every current medoid whereas clara confine candidate medoid random sample datum set randomize algorithm call claran ( cluster large application base upon randomize search ) present trade-off cost effectiveness used sample obtain cluster first randomly select k object datum set current medoid randomly select current medoid x object one current medoid replace x improve absolute-error criterion yes replacement make claran conduct randomize search l time set current medoid l step consider local optimum claran repeat randomize process time return best local optimal final result 103 hierarchical method partition method meet basic cluster requirement organize set object number exclusive group situation may want partition datum group different level hierarchy hierarchical cluster method work grouping datum object hierarchy “ tree ” cluster represent datum object form hierarchy useful datum summarization visualization example manager human resource allelectronic 
458 chapter 10 cluster analysis basic concept method may organize employee major group executive manager staff partition group smaller subgroup instance general group staff divide subgroup senior officer officer trainee group form hierarchy easily summarize characterize datum organized hierarchy used find say average salary manager officer consider handwritten character recognition another example set handwriting sample may first partition general group group correspond unique character group partition subgroup since character may written multiple substantially different way necessary hierarchical partition continue recursively desire granularity reach previous example although partition datum hierarchically assume datum hierarchical structure ( eg manager level allelectronic hierarchy staff ) use hierarchy summarize represent underlie datum compress way hierarchy particularly useful datum visualization alternatively application may believe datum bear underlie hierarchical structure want discover example hierarchical cluster may uncover hierarchy allelectronic employee structure say salary study evolution hierarchical cluster may group animal accord biological feature uncover evolutionary path hierarchy species another example grouping configuration strategic game ( eg chess checker ) hierarchical way may help develop game strategy used train player section study hierarchical cluster method section 1031 begin discussion agglomerative versus divisive hierarchical cluster organize object hierarchy used bottom-up top-down strategy respectively agglomerative method start individual object cluster iteratively merged form larger cluster conversely divisive method initially let give object form one cluster iteratively split smaller cluster hierarchical cluster method encounter difficulty regard selection merge split point decision critical group object merged split process next step operate newly generate cluster neither undo do previously perform object swap cluster thus merge split decision well choose may lead low-quality cluster moreover method scale well decision merge split need examine evaluate many object cluster promising direction improve cluster quality hierarchical method integrate hierarchical cluster cluster technique result multiple-phase ( multiphase ) cluster introduce two method namely birch chameleon birch ( section 1033 ) begin partition object hierarchically used tree structure leaf low-level nonleaf node view “ microcluster ” depend resolution scale apply 
103 hierarchical method 459 cluster algorithms perform macrocluster microcluster chameleon ( section 1034 ) explore dynamic modele hierarchical cluster several orthogonal way categorize hierarchical cluster method instance may categorize algorithmic method probabilistic method bayesian method agglomerative divisive multiphase method algorithmic meaning consider datum object deterministic compute cluster accord deterministic distance object probabilistic method use probabilistic model capture cluster measure quality cluster fitness model discuss probabilistic hierarchical cluster section bayesian method compute distribution possible clustering instead output single deterministic cluster datum set return group cluster structure probability conditional give datum bayesian method consider advanced topic discuss book 1031 agglomerative versus divisive hierarchical cluster hierarchical cluster method either agglomerative divisive depend whether hierarchical decomposition form bottom-up ( merge ) topdown ( splitting ) fashion let ’ closer look strategy agglomerative hierarchical cluster method used bottom-up strategy typically start let object form cluster iteratively merge cluster larger larger cluster object single cluster certain termination condition satisfied single cluster become hierarchy ’ root merge step find two cluster closest ( accord similarity measure ) combine two form one cluster two cluster merged per iteration cluster contain least one object agglomerative method require n iteration divisive hierarchical cluster method employ top-down strategy start place object one cluster hierarchy ’ root divide root cluster several smaller subcluster recursively partition cluster smaller one partition process continue cluster lowest level coherent enough—either contain one object object within cluster sufficiently similar either agglomerative divisive hierarchical cluster user specify desire number cluster termination condition example 103 agglomerative versus divisive hierarchical cluster figure 106 show application agne ( agglomerative nest ) agglomerative hierarchical cluster method diana ( divisive analysis ) divisive hierarchical cluster method datum set five object { b c e } initially agne agglomerative method place object cluster cluster merged step-by-step accord criterion example cluster c1 c2 may merged object c1 object c2 form minimum euclidean distance two object 
chapter 10 cluster analysis basic concept method agglomerative ( agne ) step 0 step 1 step 2 step 3 step 4 ab b abcde c cde de e step 4 step 3 step 2 step 1 divisive ( diana ) step 0 figure 106 agglomerative divisive hierarchical cluster datum object { b c e } level l=0 b c e 10 l=1 l=2 06 l=3 04 l=4 02 08 similarity scale 460 00 figure 107 dendrogram representation hierarchical cluster datum object { b c e } different cluster single-linkage approach cluster represent object cluster similarity two cluster measure similarity closest pair datum point belong different cluster cluster-merge process repeat object eventually merged form one cluster diana divisive method proceed contrast way object used form one initial cluster cluster split accord principle maximum euclidean distance closest neighboring object cluster cluster-split process repeat eventually new cluster contain single object tree structure call dendrogram commonly used represent process hierarchical cluster show object group together ( agglomerative method ) partition ( divisive method ) step-by-step figure 107 show dendrogram five object present figure 106 l = 0 show five object singleton cluster level l = 1 object b group together form 
103 hierarchical method 461 first cluster stay together subsequent level also use vertical axis show similarity scale cluster example similarity two group object { b } { c e } roughly 016 merged together form single cluster challenge divisive method partition large cluster several smaller one example 2n−1 − 1 possible way partition set n object two exclusive subset n number object n large computationally prohibitive examine possibility consequently divisive method typically used heuristic partition lead inaccurate result sake efficiency divisive method typically backtrack partition decision make cluster partition alternative partition cluster consider due challenge divisive method many agglomerative method divisive method 1032 distance measure algorithmic method whether used agglomerative method divisive method core need measure distance two cluster cluster generally set object four widely used measure distance cluster follow p − p0 | distance two object point p p0 mi mean cluster ci ni number object ci also know linkage measure minimum distance distmin ( ci cj ) = maximum distance distmax ( ci cj ) = mean distance average distance min { p − p0 | } ( 103 ) max { p − p0 | } ( 104 ) p∈ci p0 ∈cj p∈ci p0 ∈cj distmean ( ci cj ) = mi − mj | distavg ( ci cj ) = 1 ni nj x ( 105 ) p − p0 | ( 106 ) p∈ci p0 ∈cj algorithm used minimum distance dmin ( ci cj ) measure distance cluster sometimes call nearest-neighbor cluster algorithm moreover cluster process terminate distance nearest cluster exceed user-defined threshold call single-linkage algorithm view datum point node graph edge form path node cluster merge two cluster ci cj correspond add edge nearest pair node ci cj edge link cluster always go distinct cluster result graph generate tree thus agglomerative hierarchical cluster algorithm used minimum distance measure also call 
462 chapter 10 cluster analysis basic concept method minimal span tree algorithm span tree graph tree connect vertex minimal span tree one least sum edge weight algorithm used maximum distance dmax ( ci cj ) measure distance cluster sometimes call farthest-neighbor cluster algorithm cluster process terminate maximum distance nearest cluster exceed user-defined threshold call complete-linkage algorithm view datum point node graph edge link node think cluster complete subgraph edge connect node cluster distance two cluster determine distant node two cluster farthest-neighbor algorithms tend minimize increase diameter cluster iteration true cluster rather compact approximately equal size method produce high-quality cluster otherwise cluster produce meaningless previous minimum maximum measure represent two extreme measure distance cluster tend overly sensitive outlier noisy datum use mean average distance compromise minimum maximum distance overcome outlier sensitivity problem whereas mean distance simplest compute average distance advantageous handle categoric well numeric datum computation mean vector categoric datum difficult impossible define example 104 single versus complete linkage let us apply hierarchical cluster datum set figure 108 ( ) figure 108 ( b ) show dendrogram used single linkage figure 108 ( c ) show case used complete linkage edge cluster { b j h } { c g f e } omitted ease presentation example show used single linkage find hierarchical cluster defined local proximity whereas complete linkage tend find cluster opt global closeness variation four essential linkage measure discuss example measure distance two cluster distance centroid ( ie central object ) cluster 1033 birch multiphase hierarchical cluster used cluster feature tree balanced iterative reduce cluster used hierarchy ( birch ) design cluster large amount numeric datum integrate hierarchical cluster ( initial microcluster stage ) cluster method iterative partition ( later macrocluster stage ) overcome two difficulty agglomerative cluster method ( 1 ) scalability ( 2 ) inability undo do previous step birch used notion cluster feature summarize cluster cluster feature tree ( cf-tree ) represent cluster hierarchy structure help 
103 hierarchical method b c 463 e j h g f ( ) datum set b c e j h g f b c e f g h j c ( b ) cluster used single linkage b c e j h g f b h j e f g ( c ) cluster used complete linkage figure 108 hierarchical cluster used single complete linkage cluster method achieve good speed scalability large even stream databasis also make effective incremental dynamic cluster incoming object consider cluster n d-dimensional datum object point cluster feature ( cf ) cluster 3-d vector summarize information cluster object defined cf = hn ls ssi ( 107 ) p ls linear n point ( ie ni=1 xi ) ss square sum pn sum datum point ( ie i=1 xi 2 ) cluster feature essentially summary statistic give cluster used cluster feature easily derive many useful statistic cluster example cluster ’ centroid x0 radius r diameter n p x0 = i=1 n xi = ls n ( 108 ) 
464 chapter 10 cluster analysis basic concept method = = v u n ux u ( xi − x0 ) 2 u i=1 n = v ux n x n u u ( xi − xj ) 2 u i=1 j=1 n ( n − 1 ) nss − 2ls2 + nls n2 = 2nss − 2ls2 n ( n − 1 ) ( 109 ) ( 1010 ) r average distance member object centroid average pairwise distance within cluster r reflect tightness cluster around centroid summarize cluster used cluster feature avoid store detailed information individual object point instead need constant size space store cluster feature key birch efficiency space moreover cluster feature additive two disjoint cluster c1 c2 cluster feature cf1 = hn1 ls1 ss1 cf2 = hn2 ls2 ss2 respectively cluster feature cluster form merge c1 c2 simply cf1 + cf2 = hn1 + n2 ls1 + ls2 ss1 + ss2 ( 1011 ) example 105 cluster feature suppose three point ( 2 5 ) ( 3 2 ) ( 4 3 ) cluster c1 cluster feature c1 cf1 = h3 ( 2 + 3 + 4 5 + 2 + 3 ) ( 22 + 32 + 42 52 + 22 + 32 ) = h3 ( 9 10 ) ( 29 38 ) suppose c1 disjoint second cluster c2 cf2 = h3 ( 35 36 ) ( 417 440 ) cluster feature new cluster c3 form merge c1 c2 derive add cf1 cf2 cf3 = h3 + 3 ( 9 + 35 10 + 36 ) ( 29 + 417 38 + 440 ) = h6 ( 44 46 ) ( 446 478 ) cf-tree height-balanced tree store cluster feature hierarchical cluster example show figure definition nonleaf node tree descendant “ ” nonleaf node store sum cfs child thus summarize cluster information child cf-tree two parameter branch factor b threshold t branch factor specify maximum number child per nonleaf node threshold parameter specify maximum diameter subcluster store leaf node tree two parameter implicitly control result tree ’ size give limit amount main memory important consideration birch minimize time require output ( o ) birch apply multiphase cluster technique single scan datum set yield basic good cluster 
103 hierarchical method cf1 cf11 cf12 cf2 cf1k cfk 465 root level first level figure 109 cf-tree structure one additional scan optionally used improve quality primary phase phase 1 birch scan database build initial in-memory cf-tree view multilevel compression datum try preserve datum ’ inherent cluster structure phase 2 birch apply ( select ) cluster algorithm cluster leaf node cf-tree remove sparse cluster outlier group dense cluster larger one phase 1 cf-tree build dynamically object insert thus method incremental object insert closest leaf entry ( subcluster ) diameter subcluster store leaf node insertion larger threshold value leaf node possibly node split insertion new object information object pass toward root tree size cf-tree change modify threshold size memory need store cf-tree larger size main memory larger threshold value specify cf-tree rebuild rebuild process perform build new tree leaf node old tree thus process rebuild tree do without necessity reread object point similar insertion node split construction b+-tree therefore build tree datum read heuristic method introduce deal outlier improve quality cf-tree additional scan datum cf-tree build cluster algorithm typical partition algorithm used cf-tree phase 2 “ effective birch ” time complexity algorithm ( n ) n number object cluster experiment show linear scalability algorithm respect number object good quality cluster datum however since node cf-tree hold limit number entry due size cf-tree node always correspond user may consider natural cluster moreover cluster spherical shape birch perform well used notion radius diameter control boundary cluster 
466 chapter 10 cluster analysis basic concept method idea cluster feature cf-tree apply beyond birch idea borrow many other tackle problem cluster stream dynamic datum 1034 chameleon multiphase hierarchical cluster used dynamic modele chameleon hierarchical cluster algorithm used dynamic modele determine similarity pair cluster chameleon cluster similarity assessed base ( 1 ) well connect object within cluster ( 2 ) proximity cluster two cluster merged interconnectivity high close together thus chameleon depend static user-supplied model automatically adapt internal characteristic cluster merged merge process facilitate discovery natural homogeneous cluster apply datum type long similarity function specify figure 1010 illustrate chameleon work chameleon used k-nearest-neighbor graph approach construct sparse graph vertex graph represent datum object exist edge two vertex ( object ) one object among k-most similar object edge weight reflect similarity object chameleon used graph partition algorithm partition k-nearest-neighbor graph large number relatively small subcluster minimize edge cut cluster c partition subcluster ci cj minimize weight edge would cut c bisect ci cj assess absolute interconnectivity cluster ci cj chameleon used agglomerative hierarchical cluster algorithm iteratively merge subcluster base similarity determine pair similar subcluster take account interconnectivity closeness cluster specifically chameleon determine similarity pair cluster ci cj accord relative interconnectivity ri ( ci cj ) relative closeness rc ( ci cj ) relative interconnectivity ri ( ci cj ) two cluster ci cj defined absolute interconnectivity ci cj normalize respect k-nearest-neighbor graph datum set construct sparse graph partition graph final cluster merge partition figure 1010 chameleon hierarchical cluster base k-nearest neighbor dynamic modele source base karypis han kumar [ khk99 ] 
103 hierarchical method 467 internal interconnectivity two cluster ci cj ri ( ci cj ) = ec { ci cj } | 1 2 ( ecci | + eccj | ) ( 1012 ) ec { ci cj } edge cut previously defined cluster contain ci cj similarly ecci ( eccj ) minimum sum cut edge partition ci ( cj ) two roughly equal part relative closeness rc ( ci cj ) pair cluster ci cj absolute closeness ci cj normalize respect internal closeness two cluster ci cj defined rc ( ci cj ) = sec { ci cj } ci | ci cj | ec ci c | j + ci c sec cj | ( 1013 ) sec { ci cj } average weight edge connect vertex ci vertex cj sec ci ( sec cj ) average weight edge belong mincut bisector cluster ci ( cj ) chameleon show greater power discover arbitrarily shape cluster high quality several well-known algorithms birch densitybased dbscan ( section 1041 ) however process cost high-dimensional datum may require ( n2 ) time n object worst case 1035 probabilistic hierarchical cluster algorithmic hierarchical cluster method used linkage measure tend easy understand often efficient cluster commonly used many cluster analysis application however algorithmic hierarchical cluster method suffer several drawback first choose good distance measure hierarchical cluster often far trivial second apply algorithmic method datum object miss attribute value case datum partially observed ( ie attribute value object miss ) easy apply algorithmic hierarchical cluster method distance computation conduct third algorithmic hierarchical cluster method heuristic step locally search good splitting decision consequently optimization goal result cluster hierarchy unclear probabilistic hierarchical cluster aim overcome disadvantage used probabilistic model measure distance cluster one way look cluster problem regard set datum object cluster sample underlie datum generation mechanism analyze formally generative model example conduct cluster analysis set marketing survey assume survey collect sample opinion possible customer datum generation mechanism probability 
468 chapter 10 cluster analysis basic concept method distribution opinion respect different customer obtain directly completely task cluster estimate generative model accurately possible used observed datum object cluster practice assume datum generative model adopt common distribution function gaussian distribution bernoulli distribution govern parameter task learn generative model reduce find parameter value model best fit observed datum set example 106 generative model suppose give set 1-d point x = { x1 xn } cluster analysis let us assume datum point generate gaussian distribution n ( µ σ 2 ) = √ 2 1 2π σ 2 e − ( x−µ ) 2 2σ ( 1014 ) parameter µ ( mean ) σ 2 ( variance ) probability point xi ∈ x generate model ( x −µ ) 2 1 − e 2σ 2 p ( xi µ σ 2 ) = √ 2π σ 2 ( 1015 ) consequently likelihood x generate model l ( n ( µ σ 2 ) x ) = p ( x|µ σ 2 ) = n i=1 √ 1 2π σ 2 e − ( xi −µ ) 2 2σ 2 ( 1016 ) task learn generative model find parameter µ σ 2 likelihood l ( n ( µ σ 2 ) x ) maximize find n ( µ0 σ02 ) = arg max { l ( n ( µ σ 2 ) x ) } ( 1017 ) max { l ( n ( µ σ 2 ) x ) } call maximum likelihood give set object quality cluster form object measure maximum likelihood set object partition cluster c1 cm quality measure q ( { c1 cm } ) = i=1 p ( ci ) ( 1018 ) 
103 hierarchical method 469 p ( ) maximum likelihood merge two cluster cj1 cj2 cluster cj1 ∪ cj2 change quality overall cluster q ( ( { c1 cm } − { cj1 cj2 } ) ∪ { cj1 ∪ cj2 } ) − q ( { c1 cm } ) qm p ( ci ) · p ( cj1 ∪ cj2 ) = i=1 − p ( ci ) p ( cj1 ) p ( cj2 ) i=1 = i=1  p ( cj1 ∪ cj2 ) −1 p ( ci ) p ( cj1 ) p ( cj2 )  ( 1019 ) q choose merge two cluster hierarchical cluster i=1 p ( ci ) constant pair cluster therefore give cluster c1 c2 distance measure dist ( ci cj ) = − log p ( c1 ∪ c2 ) p ( c1 ) p ( c2 ) ( 1020 ) probabilistic hierarchical cluster method adopt agglomerative cluster framework use probabilistic model ( eq 1020 ) measure distance cluster upon close observation eq ( 1019 ) see merge two cluster may p ( c ∪c ) always lead improvement cluster quality p ( cj j1 ) p ( cj2j ) may less 1 2 example assume gaussian distribution function used model figure although merge cluster c1 c2 result cluster better fit gaussian distribution merge cluster c3 c4 lower cluster quality gaussian function fit merged cluster well base observation probabilistic hierarchical cluster scheme start one cluster per object merge two cluster ci cj distance negative iteration try find ci cj maximize p ( c ∪c ) p ( c ∪c ) j j log p ( ci ) p ( c iteration continue long log p ( ci ) p ( c > 0 long j ) j ) improvement cluster quality pseudocode give figure 1012 probabilistic hierarchical cluster method easy understand generally efficiency algorithmic agglomerative hierarchical cluster method fact share framework probabilistic model interpretable sometimes less flexible distance metric probabilistic model handle partially observed datum example give multidimensional datum set object miss value dimension learn gaussian model dimension independently used observed value dimension result cluster hierarchy accomplish optimization goal fitting datum select probabilistic model drawback used probabilistic hierarchical cluster output one hierarchy respect choose probabilistic model handle uncertainty cluster hierarchy give datum set may exist multiple hierarchy 
470 chapter 10 cluster analysis basic concept method c1 c2 ( ) c3 c4 ( b ) ( c ) figure 1011 merge cluster probabilistic hierarchical cluster ( ) merge cluster c1 c2 lead increase overall cluster quality merge cluster ( b ) c3 ( c ) c4 algorithm probabilistic hierarchical cluster algorithm input = { o1 } datum set contain n object output hierarchy cluster method ( 1 ) create cluster object ci = { oi } 1 ≤ ≤ n ( 2 ) = 1 n p ( c ∪c ) ( 3 ) j find pair cluster ci cj ci cj = arg maxi6=j log p ( c ) p ( c ) ( 4 ) j log p ( c ) p ( c ) > 0 merge ci cj ( 5 ) else stop p ( c ∪c ) j j figure 1012 probabilistic hierarchical cluster algorithm fit observed datum neither algorithmic approach probabilistic approach find distribution hierarchy recently bayesian tree-structure model develop handle problem bayesian sophisticated probabilistic cluster method consider advanced topic cover book 
104 density-based method 104 471 density-based method partition hierarchical method design find spherical-shap cluster difficulty find cluster arbitrary shape “ ” shape oval cluster figure give datum would likely inaccurately identify convex region noise outlier include cluster find cluster arbitrary shape alternatively model cluster dense region datum space separated sparse region main strategy behind density-based cluster method discover cluster nonspherical shape section learn basic technique density-based cluster study three representative method namely dbscan ( section 1041 ) optic ( section 1042 ) denclue ( section 1043 ) 1041 dbscan density-based cluster base connect region high density “ find dense region density-based cluster ” density object measure number object close o dbscan ( density-based spatial cluster application noise ) find core object object dense neighborhood connect core object neighborhood form dense region cluster “ dbscan quantify neighborhood object ” user-specified parameter  > 0 used specify radius neighborhood consider every object -neighborhood object space within radius  center due fix neighborhood size parameterized  density neighborhood measure simply number object neighborhood determine whether neighborhood dense dbscan used another user-specified figure 1013 cluster arbitrary shape 
472 chapter 10 cluster analysis basic concept method parameter minpt specify density threshold dense region object core object -neighborhood object contain least minpt object core object pillar dense region give set object identify core object respect give parameter  minpt cluster task therein reduce used core object neighborhood form dense region dense region cluster core object q object p say p directly density-reachable q ( respect  minpt ) p within -neighborhood q clearly object p directly density-reachable another object q q core object p -neighborhood q used directly density-reachable relation core object “ bring ” object -neighborhood dense region “ assemble large dense region used small dense region center core object ” dbscan p density-reachable q ( respect  minpt ) chain object p1 pn p1 = q pn = p pi+1 directly density-reachable pi respect  minpt 1 ≤ ≤ n pi ∈ d note density-reachability equivalence relation symmetric o1 o2 core object o1 density-reachable o2 o2 density-reachable o1 however o2 core object o1 o1 may density-reachable o2 vice versa connect core object well neighbor dense region dbscan used notion density-connectedness two object p1 p2 ∈ density-connect respect  minpt object q ∈ p1 p2 densityreachable q respect  minpt unlike density-reachability densityconnectedness equivalence relation easy show object o1 o2 o3 o1 o2 density-connect o2 o3 density-connect o1 o3 example 107 density-reachability density-connectivity consider figure 1014 give  represent radius circle say let minpt = 3 labele point p r core object -neighborhood contain least three point object q directly density-reachable m object directly density-reachable p vice versa object q ( indirectly ) density-reachable p q directly densityreachable directly density-reachable p however p densityreachable q q core object similarly r density-reachable density-reachable r thus r density-connect use closure density-connectedness find connect dense region cluster close set density-based cluster subset c ⊆ cluster ( 1 ) two object o1 o2 ∈ c o1 o2 density-connect ( 2 ) exist object ∈ c another object o0 ∈ ( − c ) o0 densityconnect 
104 density-based method 473 q p r figure 1014 density-reachability density-connectivity density-based cluster source base ester kriegel sander xu [ eksx96 ] “ dbscan find cluster ” initially object give datum set marked “ ” dbscan randomly select unvisite object p mark p “ visit ” check whether -neighborhood p contain least minpt object p marked noise point otherwise new cluster c create p object -neighborhood p add candidate set n dbscan iteratively add c object n belong cluster process object p0 n carry label “ unvisite ” dbscan mark “ visit ” check -neighborhood -neighborhood p0 least minpt object object -neighborhood p0 add n dbscan continue add object c c longer expand n empty time cluster c complete thus output find next cluster dbscan randomly select unvisite object remain one cluster process continue object visit pseudocode dbscan algorithm give figure 1015 spatial index used computational complexity dbscan ( n log n ) n number database object otherwise complexity ( n2 ) appropriate setting user-defined parameter  minpt algorithm effective find arbitrary-shap cluster 1042 optic order point identify cluster structure although dbscan cluster object give input parameter  ( maximum radius neighborhood ) minpt ( minimum number point require neighborhood core object ) encumber user responsibility select parameter value lead discovery acceptable cluster problem associate many cluster algorithms parameter setting 
474 chapter 10 cluster analysis basic concept method algorithm dbscan density-based cluster algorithm input datum set contain n object  radius parameter minpt neighborhood density threshold output set density-based cluster method ( 1 ) mark object unvisite ( 2 ) ( 3 ) randomly select unvisite object p ( 4 ) mark p visit ( 5 ) -neighborhood p least minpt object ( 6 ) create new cluster c add p c ( 7 ) let n set object -neighborhood p ( 8 ) point p0 n ( 9 ) p0 unvisite ( 10 ) mark p0 visit ( 11 ) -neighborhood p0 least minpt point add point n ( 12 ) p0 yet member cluster add p0 c ( 13 ) end ( 14 ) output c ( 15 ) else mark p noise ( 16 ) object unvisite figure 1015 dbscan algorithm usually empirically set difficult determine especially real-world highdimensional datum set algorithms sensitive parameter value slightly different setting may lead different clustering datum moreover real-world high-dimensional datum set often skewer distribution intrinsic cluster structure may well characterize single set global density parameter note density-based cluster monotonic respect neighborhood threshold dbscan fix minpt value two neighborhood threshold 1 < 2 cluster c respect 1 minpt must subset cluster c 0 respect 2 minpt mean two object density-based cluster must also cluster lower density requirement overcome difficulty used one set global parameter cluster analysis cluster analysis method call optic propose optic explicitly produce datum set cluster instead output cluster order linear list 
104 density-based method 475 object analysis represent density-based cluster structure datum object denser cluster list closer cluster order order equivalent density-based cluster obtain wide range parameter setting thus optic require user provide specific density threshold cluster order used extract basic cluster information ( eg cluster center arbitrary-shap cluster ) derive intrinsic cluster structure well provide visualization cluster construct different clustering simultaneously object processed specific order order select object density-reachable respect lowest  value cluster higher density ( lower  ) finished first base idea optic need two important piece information per object core-distance object p smallest value  0  0 neighborhood p least minpt object  0 minimum distance threshold make p core object p core object respect  minpt core-distance p undefined reachability-distance object p q minimum radius value make p density-reachable q accord definition density-reachability q core object p must neighborhood q therefore reachability-distance q p max { core-distance ( q ) dist ( p q ) } q core object respect  minpt reachability-distance p q undefined object p may directly reachable multiple core object therefore p may multiple reachability-distance respect different core object smallest reachability-distance p particular interest give shortest path p connect dense cluster example 108 core-distance reachability-distance figure 1016 illustrate concept coredistance reachability-distance suppose  = 6 mm minpt = coredistance p distance  0 p fourth closest datum object p reachability-distance q1 p core-distance p ( ie  0 = 3 mm ) greater euclidean distance p q1 reachability-distance q2 respect p euclidean distance p q2 greater core-distance p optic compute order object give database object database store core-distance suitable reachability-distance optic maintain list call orderseed generate output order object orderseed sort reachability-distance respective closest core object smallest reachability-distance object optic begin arbitrary object input database current object p retrieve -neighborhood p determine core-distance set reachability-distance undefined current object p written output 
476 chapter 10 cluster analysis basic concept method = 6 mm p = 3 mm = 6 mm  p q1 q2 core-distance p reachability-distance ( p q1 ) = = 3 mm reachability-distance ( p q2 ) = dist ( p q2 ) figure 1016 optic terminology source base ankerst breunig kriegel sander [ abks99 ] p core object optic simply move next object orderseed list ( input database orderseed empty ) p core object object q -neighborhood p optic update reachability-distance p insert q orderseed q yet processed iteration continue input fully consume orderseed empty datum set ’ cluster order represent graphically help visualize understand cluster structure datum set example figure 1017 reachability plot simple 2-d datum set present general overview datum structure cluster datum object plot cluster order ( horizontal axis ) together respective reachability-distance ( vertical axis ) three gaussian “ bump ” plot reflect three cluster datum set method also develop view cluster structure high-dimensional datum various level detail structure optic algorithm similar dbscan consequently two algorithms time complexity complexity ( n log n ) spatial index used ( n2 ) otherwise n number object 1043 denclue cluster base density distribution function density estimation core issue density-based cluster method denclue ( density-based cluster ) cluster method base set density distribution function first give background density estimation describe denclue algorithm probability statistic density estimation estimation unobservable underlie probability density function base set observed datum context density-based cluster unobservable underlie probability density function true distribution population possible object analyze observed datum set regard random sample population 
104 density-based method 477 reachability-distance undefined cluster order object figure 1017 cluster order optic source adapt ankerst breunig kriegel sander [ abks99 ] 1 2 figure 1018 subtlety density estimation dbscan optic increase neighborhood radius slightly 1 2 result much higher density dbscan optic density calculate count number object neighborhood defined radius parameter  density estimate highly sensitive radius value used example figure 1018 density change significantly radius increase small amount overcome problem kernel density estimation used nonparametric density estimation approach statistic general idea behind kernel density estimation simple treat observed object indicator 
478 chapter 10 cluster analysis basic concept method high-probability density surround region probability density point depend distance point observed object formally let x1 xn independent identically distribute sample random variable f kernel density approximation probability density function   n x − xi 1 x ( 1021 ) k fˆh ( x ) = nh h i=1 k ( ) kernel h bandwidth serve smooth parameter kernel regard function modele influence sample point within neighborhood technically kernel k ( ) isra non-negative real-valu integrable func+∞ tion satisfy two requirement −∞ k ( u ) du = 1 k ( −u ) = k ( u ) value u frequently used kernel standard gaussian function mean 0 variance 1   x − xi 1 − ( x − 2xi ) 2 2h k ( 1022 ) √ e h 2π denclue used gaussian kernel estimate density base give set object cluster point x∗ call density attractor local maximum estimate density function avoid trivial local maximum point denclue used noise threshold ξ consider density attractor x∗ fˆ ( x∗ ) ≥ ξ nontrivial density attractor center cluster object analysis assign cluster density attractor used stepwise hill-climb procedure object x hill-climb procedure start x guide gradient estimate density function density attractor x compute x0 = x xj+1 = xj + δ ∇ fˆ ( xj ) ∇ fˆ ( xj ) | ( 1023 ) δ parameter control speed convergence ∇ fˆ ( x ) = hd+2 n 1   x − xi ( x − x ) k i=1 h pn ( 1024 ) hill-climb procedure stop step k > 0 fˆ ( xk+1 ) < fˆ ( xk ) assign x density attractor x∗ = xk object x outlier noise converge hillclimb procedure local maximum x∗ fˆ ( x∗ ) < ξ cluster denclue set density attractor x set input object c object c assign density attractor x exist path every pair density attractor density ξ used multiple density attractor connect path denclue find cluster arbitrary shape 
105 grid-based method 479 denclue several advantage regard generalization several well-known cluster method single-linkage approach dbscan moreover denclue invariant noise kernel density estimation effectively reduce influence noise uniformly distribute noise input datum 105 grid-based method cluster method discuss far data-driven—they partition set object adapt distribution object embedding space alternatively grid-based cluster method take space-driven approach partition embedding space cell independent distribution input object grid-based cluster approach used multiresolution grid datum structure quantize object space finite number cell form grid structure operation cluster perform main advantage approach fast process time typically independent number datum object yet dependent number cell dimension quantized space section illustrate grid-based cluster used two typical example sting ( section 1051 ) explore statistical information store grid cell clique ( section 1052 ) represent - density-based approach subspace cluster high-dimensional datum space 1051 sting statistical information grid sting grid-based multiresolution cluster technique embedding spatial area input object divide rectangular cell space divide hierarchical recursive way several level rectangular cell correspond different level resolution form hierarchical structure cell high level partition form number cell next lower level statistical information regard attribute grid cell mean maximum minimum value precompute store statistical parameter statistical parameter useful query process datum analysis task figure 1019 show hierarchical structure sting cluster statistical parameter higher-level cell easily compute parameter lower-level cell parameter include follow attribute-independent parameter count attribute-dependent parameter mean stdev ( standard deviation ) min ( minimum ) max ( maximum ) type distribution attribute value cell follow normal uniform exponential none ( distribution unknown ) attribute select measure analysis price house object datum load database parameter count mean stdev min max bottom-level cell calculate directly datum value distribution may either assign user distribution type know 
480 chapter 10 cluster analysis basic concept method first layer ( – 1 ) st layer ith layer figure 1019 hierarchical structure sting cluster beforehand obtain hypothesis test χ 2 test type distribution higher-level cell compute base majority distribution type corresponding lower-level cell conjunction threshold filter process distribution lower-level cell disagree fail threshold test distribution type high-level cell set none “ statistical information useful query answer ” statistical parameter used top-down grid-based manner follow first layer within hierarchical structure determine query-answer process start layer typically contain small number cell cell current layer compute confidence interval ( estimate probability range ) reflect cell ’ relevancy give query irrelevant cell remove consideration process next lower level examine remain relevant cell process repeat bottom layer reach time query specification meet region relevant cell satisfy query return otherwise datum fall relevant cell retrieve processed meet query ’ requirement interesting property sting approach cluster result dbscan granularity approach 0 ( ie toward low-level datum ) word used count cell size information dense cluster identify approximately used sting therefore sting also regard density-based cluster method “ advantage sting offer cluster method ” sting offer several advantage ( 1 ) grid-based computation query-independent statistical information store cell represent summary information datum grid cell independent query ( 2 ) grid structure facilitate parallel process incremental update ( 3 ) method ’ efficiency major advantage sting go database compute statistical parameter cell hence time complexity generate cluster ( n ) n total number object generate hierarchical structure query process time 
105 grid-based method 481 ( g ) g total number grid cell lowest level usually much smaller n sting used multiresolution approach cluster analysis quality sting cluster depend granularity lowest level grid structure granularity fine cost process increase substantially however bottom level grid structure coarse may reduce quality cluster analysis moreover sting consider spatial relationship child neighboring cell construction parent cell result shape result cluster isothetic cluster boundary either horizontal vertical diagonal boundary detected may lower quality accuracy cluster despite fast process time technique 1052 clique apriori-like subspace cluster method datum object often ten attribute many may irrelevant value attribute may vary considerably factor make difficult locate cluster span entire datum space may meaningful instead search cluster within different subspace datum example consider healthinformatic application patient record contain extensive attribute describe personal information numerous symptom condition family history find nontrivial group patient even attribute strongly agree unlikely bird flu patient instance age gender job attribute may vary dramatically within wide range value thus difficult find cluster within entire datum space instead search subspace may find cluster similar patient lower-dimensional space ( eg patient similar one respect symptom like high fever cough runny nose age 3 16 ) clique ( cluster quest ) simple grid-based method find densitybased cluster subspace clique partition dimension nonoverlapping interval thereby partition entire embedding space datum object cell used density threshold identify dense cell sparse one cell dense number object map exceed density threshold main strategy behind clique identify candidate search space used monotonicity dense cell respect dimensionality base apriori property used frequent pattern association rule mining ( chapter 6 ) context cluster subspace monotonicity say follow k-dimensional cell c ( k > 1 ) least l point every ( k − 1 ) dimensional projection c cell ( k − 1 ) dimensional subspace least l point consider figure 1020 embedding datum space contain three dimension age salary vacation 2-d cell say subspace form age salary contain l point projection cell every dimension age salary respectively contain least l point clique perform cluster two step first step clique partition d-dimensional datum space nonoverlapping rectangular unit identify dense unit among clique find dense cell subspace 
482 chapter 10 cluster analysis basic concept method 7 salary ( $ 10000 ) 6 5 4 3 2 1 0 20 30 40 50 60 age 30 40 50 60 age 7 vacation ( week ) 6 5 4 3 2 1 vacation 0 20 50 age sa la ry 30 figure 1020 dense unit find respect age dimension salary vacation intersected provide candidate search space dense unit higher dimensionality 
106 evaluation cluster 483 clique partition every dimension interval identify interval contain least l point l density threshold clique iteratively join two k-dimensional dense cell c1 c2 subspace ( di1 dik ) ( dj1 djk ) respectively di1 = dj1 dik−1 = djk−1 c1 c2 share interval dimension join operation generate new ( k + 1 ) dimensional candidate cell c space ( di1 dik−1 dik djk ) clique check whether number point c pass density threshold iteration terminate candidate generate candidate cell dense second step clique used dense cell subspace assemble cluster arbitrary shape idea apply minimum description length ( mdl ) principle ( chapter 8 ) use maximal region cover connect dense cell maximal region hyperrectangle every cell fall region dense region extend dimension subspace find best description cluster general np-hard thus clique adopt simple greedy approach start arbitrary dense cell find maximal region cover cell work remain dense cell yet cover greedy method terminate dense cell cover “ effective clique ” clique automatically find subspace highest dimensionality high-density cluster exist subspace insensitive order input object presume canonical datum distribution scale linearly size input good scalability number dimension datum increase however obtain meaningful cluster dependent proper tune grid size ( stable structure ) density threshold difficult practice grid size density threshold used across combination dimension datum set thus accuracy cluster result may degraded expense method ’ simplicity moreover give dense region projection region onto lower-dimensionality subspace also dense result large overlap among report dense region furthermore difficult find cluster rather different density within different dimensional subspace several extension approach follow similar philosophy example think grid set fix bin instead used fix bin dimension use adaptive data-driven strategy dynamically determine bin dimension base datum distribution statistic alternatively instead used density threshold may use entropy ( chapter 8 ) measure quality subspace cluster 106 evaluation cluster learn cluster know several popular cluster method may ask “ try cluster method datum set evaluate whether cluster result good ” general cluster evaluation assess 
484 chapter 10 cluster analysis basic concept method feasibility cluster analysis datum set quality result generate cluster method major task cluster evaluation include follow assess cluster tendency task give datum set assess whether nonrandom structure exist datum blindly apply cluster method datum set return cluster however cluster mine may mislead cluster analysis datum set meaningful nonrandom structure datum determine number cluster datum set algorithms k-mean require number cluster datum set parameter moreover number cluster regard interesting important summary statistic datum set therefore desirable estimate number even cluster algorithm used derive detailed cluster measure cluster quality apply cluster method datum set want assess good result cluster number measure used method measure well cluster fit datum set other measure well cluster match ground truth truth available also measure score clustering thus compare two set cluster result datum set rest section discuss three topic 1061 assess cluster tendency cluster tendency assessment determine whether give datum set non-random structure may lead meaningful cluster consider datum set non-random structure set uniformly distribute point datum space even though cluster algorithm may return cluster datum cluster random meaningful example 109 cluster require nonuniform distribution datum figure 1021 show datum set uniformly distribute 2-d datum space although cluster algorithm may still artificially partition point group group unlikely mean anything significant application due uniform distribution datum “ assess cluster tendency datum set ” intuitively try measure probability datum set generate uniform datum distribution achieve used statistical test spatial randomness illustrate idea let ’ look simple yet effective statistic call hopkin statistic hopkin statistic spatial statistic test spatial randomness variable distribute space give datum set regard sample 
106 evaluation cluster 485 figure 1021 datum set uniformly distribute datum space random variable want determine far away uniformly distribute datum space calculate hopkin statistic follow sample n point p1 pn uniformly d point probability include sample point pi find nearest neighbor pi ( 1 ≤ ≤ n ) let xi distance pi nearest neighbor d xi = min { dist ( pi v ) } v∈d ( 1025 ) sample n point q1 qn uniformly d qi ( 1 ≤ ≤ n ) find nearest neighbor qi − { qi } let yi distance qi nearest neighbor − { qi } yi = min { dist ( qi v ) } v∈d v6=qi ( 1026 ) calculate hopkin statistic h pn h = pn i=1 xi i=1 yi + pn i=1 yi ( 1027 ) “ hopkin statistic tell us likely datum set follow pn uniform distribution datum space ” uniformly distribute i=1 yi pn x would close thus h would 05 however i=1 p p highly skewer ni=1 yi would substantially smaller ni=1 xi expectation thus h would close 0 
486 chapter 10 cluster analysis basic concept method null hypothesis homogeneous hypothesis—that uniformly distribute thus contain meaningful cluster nonhomogeneous hypothesis ( ie uniformly distribute thus contain cluster ) alternative hypothesis conduct hopkin statistic test iteratively used 05 threshold reject alternative hypothesis h > 05 unlikely statistically significant cluster 1062 determine number cluster determine “ right ” number cluster datum set important cluster algorithms like k-mean require parameter also appropriate number cluster control proper granularity cluster analysis regard find good balance compressibility accuracy cluster analysis consider two extreme case treat entire datum set cluster would maximize compression datum cluster analysis value hand treat object datum set cluster give finest cluster resolution ( ie accurate due zero distance object corresponding cluster center ) method like k-mean even achieve best cost however one object per cluster enable datum summarization determine number cluster far easy often “ right ” number ambiguous figure right number cluster often depend distribution ’ shape scale datum set well cluster resolution require user many possible way estimate number cluster briefly introduce simple yet popular effective method q simple method set number cluster n2 datum set n √ point expectation cluster 2n point elbow method base observation increase number cluster help reduce sum within-cluster variance cluster cluster allow one capture finer group datum object similar however marginal effect reduce sum within-cluster variance may drop many cluster form splitting cohesive cluster two give small reduction consequently heuristic select right number cluster use turn point curve sum within-cluster variance respect number cluster technically give number k > 0 form k cluster datum set question used cluster algorithm like k-mean calculate sum within-cluster variance var ( k ) plot curve var respect k first ( significant ) turn point curve suggest “ right ” number advanced method determine number cluster used information criterium information theoretic approach please refer bibliographic note information ( section 109 ) 
106 evaluation cluster 487 “ right ” number cluster datum set also determine crossvalidation technique often used classification ( chapter 8 ) first divide give datum set part next use − 1 part build cluster model use remain part test quality cluster example point test set find closest centroid consequently use sum square distance point test set closest centroid measure well cluster model fit test set integer k > 0 repeat process time derive clustering k cluster used part turn test set average quality measure take overall quality measure compare overall quality measure respect different value k find number cluster best fit datum 1063 measure cluster quality suppose assessed cluster tendency give datum set may also try predetermine number cluster set apply one multiple cluster method obtain clustering datum set “ good cluster generate method compare clustering generate different method ” method choose measure quality cluster general method categorize two group accord whether ground truth available ground truth ideal cluster often build used human expert ground truth available used extrinsic method compare cluster group truth measure ground truth unavailable use intrinsic method evaluate goodness cluster consider well cluster separated ground truth consider supervision form “ cluster ” hence extrinsic method also know supervised method intrinsic method unsupervised method let ’ look simple method category extrinsic method ground truth available compare cluster assess cluster thus core task extrinsic method assign score q ( c cg ) cluster c give ground truth cg whether extrinsic method effective largely depend measure q used general measure q cluster quality effective satisfy follow four essential criterium cluster homogeneity require pure cluster cluster better cluster suppose ground truth say object datum set belong category l1 ln consider cluster c1 wherein cluster c ∈ c1 contain object two category li lj ( 1 ≤ < j ≤ n ) also 
488 chapter 10 cluster analysis basic concept method consider cluster c2 identical c1 except c2 split two cluster contain object li lj respectively cluster quality measure q respect cluster homogeneity give higher score c2 c1 q ( c2 cg ) > q ( c1 cg ) cluster completeness counterpart cluster homogeneity cluster completeness require cluster two object belong category accord ground truth assign cluster cluster completeness require cluster assign object belong category ( accord ground truth ) cluster consider cluster c1 contain cluster c1 c2 member belong category accord ground truth let cluster c2 identical c1 except c1 c2 merged one cluster c2 cluster quality measure q respect cluster completeness give higher score c2 q ( c2 cg ) > q ( c1 cg ) rag bag many practical scenario often “ rag bag ” category contain object merged object category often call “ miscellaneous ” “ ” rag bag criterion state putt heterogeneous object pure cluster penalize putt rag bag consider cluster c1 cluster c ∈ c1 object c except one denote belong category accord ground truth consider cluster c2 identical c1 except assign cluster c 0 = c c2 c 0 contain object various category accord ground truth thus noisy word c 0 c2 rag bag cluster quality measure q respect rag bag criterion give higher score c2 q ( c2 cg ) > q ( c1 cg ) small cluster preservation small category split small piece cluster small piece may likely become noise thus small category discover cluster small cluster preservation criterion state splitting small category piece harmful splitting large category piece consider extreme case let datum set n + 2 object accord ground truth n object denote o1 belong one category two object denote on+1 on+2 belong another category suppose cluster c1 three cluster c1 = { o1 } c2 = { on+1 } c3 = { on+2 } let cluster c2 three cluster namely c1 = { o1 on−1 } c2 = { } c3 = { on+1 on+2 } word c1 split small category c2 split big category cluster quality measure q preserve small cluster give higher score c2 q ( c2 cg ) > q ( c1 cg ) many cluster quality measure satisfy four criterium introduce bcube precision recall metric satisfy four criterium bcube evaluate precision recall every object cluster give datum set accord ground truth precision object indicate many object cluster belong category object recall 
106 evaluation cluster 489 object reflect many object category assign cluster formally let = { o1 } set object c cluster d let l ( oi ) ( 1 ≤ ≤ n ) category oi give ground truth c ( oi ) cluster id oi c two object oi oj ( 1 ≤ j ≤ n = j ) correctness relation oi oj cluster c give ( 1 l ( oi ) = l ( oj ) ⇔ c ( oi ) = c ( oj ) correctness ( oi oj ) = 0 otherwise ( 1028 ) bcube precision defined x n x oj i6=j c ( oi ) c ( oj ) precision bcube = correctness ( oi oj ) k { oj i = j c ( oi ) = c ( oj ) } k i=1 n ( 1029 ) bcube recall defined x n x oj i6=j l ( oi ) l ( oj ) recall bcube = i=1 correctness ( oi oj ) k { oj i = j l ( oi ) = l ( oj ) } k n ( 1030 ) intrinsic method ground truth datum set available use intrinsic method assess cluster quality general intrinsic method evaluate cluster examine well cluster separated compact cluster many intrinsic method advantage similarity metric object datum set silhouette coefficient measure datum set n object suppose partition k cluster c1 ck object ∈ calculate ( ) average distance object cluster belong similarly b ( ) minimum average distance cluster belong formally suppose ∈ ci ( 1 ≤ ≤ k ) p ( ) = o0 ∈ci o6=o0 dist ( ) ci | − 1 0 ( 1031 ) 
490 chapter 10 cluster analysis basic concept method ( p b ( ) = min cj 1≤j≤k j6=i 0 ) o0 ∈cj dist ( ) cj | ( 1032 ) silhouette coefficient defined ( ) = b ( ) − ( ) max { ( ) b ( ) } ( 1033 ) value silhouette coefficient −1 value ( ) reflect compactness cluster belong smaller value compact cluster value b ( ) capture degree separated cluster larger b ( ) separated cluster therefore silhouette coefficient value approach 1 cluster contain compact far away cluster preferable case however silhouette coefficient value negative ( ie b ( ) < ( ) ) mean expectation closer object another cluster object cluster many case bad situation avoid measure cluster ’ fitness within cluster compute average silhouette coefficient value object cluster measure quality cluster use average silhouette coefficient value object datum set silhouette coefficient intrinsic measure also used elbow method heuristically derive number cluster datum set replace sum within-cluster variance 107 summary cluster collection datum object similar one another within cluster dissimilar object cluster process grouping set physical abstract object class similar object call cluster cluster analysis extensive application include business intelligence image pattern recognition web search biology security cluster analysis used standalone datum mining tool gain insight datum distribution preprocess step datum mining algorithms operate detected cluster cluster dynamic field research datum mining related unsupervised learn machine learn cluster challenge field typical requirement include scalability ability deal different type datum attribute discovery cluster arbitrary shape minimal requirement domain knowledge determine input parameter ability deal noisy datum incremental cluster 
108 exercise 491 insensitivity input order capability cluster high-dimensionality datum constraint-based cluster well interpretability usability many cluster algorithms develop categorize several orthogonal aspect regard partition criterium separation cluster similarity measure used cluster space chapter discuss major fundamental cluster method follow category partition method hierarchical method density-based method grid-based method algorithms may belong one category partition method first create initial set k partition parameter k number partition construct used iterative relocation technique attempt improve partition move object one group another typical partition method include k-mean k-medoid claran hierarchical method create hierarchical decomposition give set datum object method classify either agglomerative ( bottom-up ) divisive ( top-down ) base hierarchical decomposition form compensate rigidity merge split quality hierarchical agglomeration improve analyze object linkage hierarchical partition ( eg chameleon ) first perform microcluster ( grouping object “ microcluster ” ) operate microcluster cluster technique iterative relocation ( birch ) density-based method cluster object base notion density grow cluster either accord density neighborhood object ( eg dbscan ) accord density function ( eg denclue ) optic density-based method generate augment order datum ’ cluster structure grid-based method first quantize object space finite number cell form grid structure perform cluster grid structure sting typical example grid-based method base statistical information store grid cell clique grid-based subspace cluster algorithm cluster evaluation assess feasibility cluster analysis datum set quality result generate cluster method task include assess cluster tendency determine number cluster measure cluster quality 108 exercise 101 briefly describe give example follow approach cluster partition method hierarchical method density-based method grid-based method 
492 chapter 10 cluster analysis basic concept method 102 suppose datum mining task cluster point ( ( x ) represent location ) three cluster point a1 ( 2 10 ) a2 ( 2 5 ) a3 ( 8 4 ) b1 ( 5 8 ) b2 ( 7 5 ) b3 ( 6 4 ) c1 ( 1 2 ) c2 ( 4 9 ) distance function euclidean distance suppose initially assign a1 b1 c1 center cluster respectively use k-mean algorithm show ( ) three cluster center first round execution ( b ) final three cluster 103 use example show k-mean algorithm may find global optimum optimize within-cluster variation 104 k-mean algorithm interesting note choose initial cluster center carefully may able speed algorithm ’ convergence also guarantee quality final cluster + algorithm variant k-mean choose initial center follow first select one center uniformly random object datum set iteratively object p choose center choose object new center object choose random probability proportional dist ( p ) 2 dist ( p ) distance p closest center already choose iteration continue k center select explain method speed convergence k-mean algorithm also guarantee quality final cluster result 105 provide pseudocode object reassignment step pam algorithm 106 k-mean k-medoid algorithms perform effective cluster ( ) illustrate strength weakness k-mean comparison k-medoid ( b ) illustrate strength weakness scheme comparison hierarchical cluster scheme ( eg agne ) 107 prove dbscan density-connectedness equivalence relation 108 prove dbscan fix minpt value two neighborhood threshold 1 < 2 cluster c respect 1 minpt must subset cluster c 0 respect 2 minpt 109 provide pseudocode optic algorithm 1010 birch encounter difficulty find cluster arbitrary shape optic propose modification birch help find cluster arbitrary shape 1011 provide pseudocode step clique find dense cell subspace 
108 exercise 493 1012 present condition density-based cluster suitable partitioning-based cluster hierarchical cluster give application example support argument 1013 give example specific cluster method integrate example one cluster algorithm used preprocess step another addition provide reasoning integration two method may sometimes lead improve cluster quality efficiency 1014 cluster recognize important datum mining task broad application give one application example follow case ( ) application used cluster major datum mining function ( b ) application used cluster preprocess tool datum preparation datum mining task 1015 datum cube multidimensional databasis contain nominal ordinal numeric datum hierarchical aggregate form base learn cluster method design cluster method find cluster large datum cube effectively efficiently 1016 describe follow cluster algorithms term follow criterium ( 1 ) shape cluster determine ( 2 ) input parameter must specify ( 3 ) limitation ( ) ( b ) ( c ) ( ) ( e ) ( f ) k-mean k-medoid clara birch chameleon dbscan 1017 human eye fast effective judge quality cluster method 2-d datum design datum visualization method may help human visualize datum cluster judge cluster quality 3-d datum even higher-dimensional datum 1018 suppose allocate number automatic teller machine ( atms ) give region satisfy number constraint household workplace may cluster typically one atm assign per cluster cluster however may constrain two factor ( 1 ) obstacle object ( ie bridge river highway affect atm accessibility ) ( 2 ) additional user-specified constraint atm serve least 10000 household cluster algorithm k-mean modify quality cluster constraint 1019 constraint-based cluster aside minimum number customer cluster ( atm allocation ) constraint many kind 
494 chapter 10 cluster analysis basic concept method constraint example constraint can form maximum number customer per cluster average income customer per cluster maximum distance every two cluster categorize kind constraint impose cluster produce discuss perform cluster efficiently kind constraint 1020 design privacy-preserve cluster method datum owner would able ask third party mine datum quality cluster without worry potential inappropriate disclosure certain private sensitive information store datum 1021 show bcube metric satisfy four essential requirement extrinsic cluster evaluation method 109 bibliographic note cluster extensively study 40 year across many discipline due broad application book pattern classification machine learn contain chapter cluster analysis unsupervised learn several textbook dedicate method cluster analysis include hartigan [ har75 ] jain dube [ jd88 ] kaufman rousseeuw [ kr90 ] arabie hubert de sorte [ ahs96 ] also many survey article different aspect cluster method recent one include jain murty flynn [ jmf99 ] parson haque liu [ phl04 ] jain [ jai10 ] partition method k-mean algorithm first introduce lloyd [ llo57 ] macqueen [ mac67 ] arthur vassilvitskii [ av07 ] present + algorithm filter algorithm used spatial hierarchical datum index speed computation cluster mean give kanungo mount netanyahu et al [ + 02 ] k-medoid algorithms pam clara propose kaufman rousseeuw [ kr90 ] k-mode ( cluster nominal datum ) k-prototype ( cluster hybrid datum ) algorithms propose huang [ hua98 ] k-mode cluster algorithm also propose independently chaturvedi green carroll [ cgc94 cgc01 ] claran algorithm propose ng han [ nh94 ] ester kriegel xu [ ekx95 ] propose technique improvement performance claran used efficient spatial access method r∗-tree focuse technique k-means-based scalable cluster algorithm propose bradley fayyad reina [ bfr98 ] early survey agglomerative hierarchical cluster algorithms conduct day edelsbrunner [ de84 ] agglomerative hierarchical cluster agne divisive hierarchical cluster diana introduce kaufman rousseeuw [ kr90 ] interesting direction improve cluster quality hierarchical cluster method integrate hierarchical cluster distance-based iterative relocation nonhierarchical cluster method example birch zhang ramakrishnan livny [ zrl96 ] first perform hierarchical cluster 
109 bibliographic note 495 cf-tree apply technique hierarchical cluster also perform sophisticated linkage analysis transformation nearest-neighbor analysis cure guha rastogi shim [ grs98 ] rock ( cluster nominal attribute ) guha rastogi shim [ grs99 ] chameleon karypis han kumar [ khk99 ] probabilistic hierarchical cluster framework follow normal linkage algorithms used probabilistic model define cluster similarity develop friedman [ fri03 ] heller ghahramani [ hg05 ] density-based cluster method dbscan propose ester kriegel sander xu [ eksx96 ] ankerst breunig kriegel sander [ abks99 ] develop optic cluster-order method facilitate density-based cluster without worry parameter specification denclue algorithm base set density distribution function propose hinneburg keim [ hk98 ] hinneburg gabriel [ hg07 ] develop denclue 20 include new hill-climb procedure gaussian kernel adjust step size automatically sting grid-based multiresolution approach collect statistical information grid cell propose wang yang muntz [ wym97 ] wavecluster develop sheikholeslami chatterjee zhang [ scz98 ] multiresolution cluster approach transform original feature space wavelet transform scalable method cluster nominal datum study gibson kleinberg raghavan [ gkr98 ] guha rastogi shim [ grs99 ] ganti gehrke ramakrishnan [ ggr99 ] also many cluster paradigm example fuzzy cluster method discuss kaufman rousseeuw [ kr90 ] bezdek [ bez81 ] bezdek pal [ bp92 ] high-dimensional cluster apriori-based dimension-growth subspace cluster algorithm call clique propose agrawal gehrke gunopulos raghavan [ aggr98 ] integrate density-based grid-based cluster method recent study proceed cluster stream datum babcock badu datar et al [ + 02 ] k-median-based datum stream cluster algorithm propose guha mishra motwani ’ callaghan [ gmmo00 ] ’ callaghan et al [ + 02 ] method cluster evolve datum stream propose aggarwal han wang yu [ ahwy03 ] framework project cluster high-dimensional datum stream propose aggarwal han wang yu [ ahwy04a ] cluster evaluation discuss monograph survey article jain dube [ jd88 ] halkidi batistakis vazirgiannis [ hbv01 ] extrinsic method cluster quality evaluation extensively explore recent study include meilǎ [ mei03 mei05 ] amigó gonzalo artile verdejo [ agav09 ] four essential criterium introduce chapter formulate amigó gonzalo artile verdejo [ agav09 ] individual criterium also mentioned earlier example meilǎ [ mei03 ] rosenberg hirschberg [ rh07 ] bagga baldwin [ bb98 ] introduce bcube metric silhouette coefficient describe kaufman rousseeuw [ kr90 ] 
11 advanced cluster analysis learn fundamental cluster analysis chapter chapter discuss advanced topic cluster analysis specifically investigate four major perspective probabilistic model-based cluster section 111 introduce general framework method derive cluster object assign probability belong cluster probabilistic model-based cluster widely used many datum mining application text mining cluster high-dimensional datum dimensionality high conventional distance measure dominate noise section 112 introduce fundamental method cluster analysis high-dimensional datum cluster graph network datum graph network datum increasingly popular application online social network world wide web digital library section 113 study key issue cluster graph network datum include similarity measurement cluster method cluster constraint discussion far assume constraint cluster application however various constraint may exist constraint may rise background knowledge spatial distribution object learn conduct cluster analysis different kind constraint section 114 end chapter good grasp issue technique regard advanced cluster analysis 111 probabilistic model-based cluster cluster analysis method discuss far datum object assign one number cluster cluster assignment rule require application assign customer marketing manager however datum mining concept technique doi b978-0-12-381479-100011-3 c 2012 elsevier right re-serve 497 
498 chapter 11 advanced cluster analysis application rigid requirement may desirable section demonstrate need fuzzy flexible cluster assignment application introduce general method compute probabilistic cluster assignment “ situation may datum object belong one cluster ” consider example 111 example 111 cluster product reviews allelectronic online store customer purchase online also create reviews product every product receive reviews instead product may many reviews many other none moreover review may involve multiple product thus review editor allelectronic task cluster reviews ideally cluster topic example group product service issue highly related assign review one cluster exclusively would work well task suppose cluster “ camera camcorder ” another “ ” review talk compatibility camcorder computer review relate cluster however exclusively belong either cluster would like use cluster method allow review belong one cluster review indeed involve one topic reflect strength review belong cluster want assignment review cluster carry weight represent partial membership scenario object may belong multiple cluster occur often many application illustrated example 112 example 112 cluster study user search intent allelectronic online store record customer browse purchasing behavior log important datum mining task use log datum categorize understand user search intent example consider user session ( short period user interact online store ) user search product make comparison among different product look customer support information cluster analysis help difficult predefine user behavior pattern thoroughly cluster contain similar user browse trajectory may represent similar user behavior however every session belong one cluster example suppose user session involve purchase digital camera form one cluster user session compare laptop computer form another cluster user one session make order digital camera time compare several laptop computer session belong cluster extent section systematically study theme cluster allow object belong one cluster start notion fuzzy cluster section generalize concept probabilistic model-based cluster section section 1113 introduce expectation-maximization algorithm general framework mining cluster 
111 probabilistic model-based cluster 499 1111 fuzzy cluster give set object x = { x1 xn } fuzzy set subset x allow object x membership degree 0 formally fuzzy set modeled function fs x → [ 0 1 ] example 113 fuzzy set digital camera unit sell popular camera allelectronic use follow formula compute degree popularity digital camera give sale pop ( ) = ( 1 1000 1000 unit sell ( < 1000 ) unit sell ( 111 ) function pop ( ) define fuzzy set popular digital camera example suppose sale digital camera allelectronic show table fuzzy set popular digital camera { ( 005 ) b ( 1 ) c ( 086 ) ( 027 ) } degree membership written parenthesis apply fuzzy set idea cluster give set object cluster fuzzy set object cluster call fuzzy cluster consequently cluster contain multiple fuzzy cluster formally give set object o1 fuzzy cluster k fuzzy cluster c1 ck represent used partition matrix = [ wij ] ( 1 ≤ ≤ n 1 ≤ j ≤ k ) wij membership degree oi fuzzy cluster cj partition matrix satisfy follow three requirement object oi cluster cj 0 ≤ wij ≤ requirement enforce fuzzy cluster fuzzy set object oi k x wij = requirement ensure every object - j=1 pate cluster equivalently table 111 set digital camera sale allelectronic camera sale ( unit ) b c 50 1320 860 270 
500 chapter 11 advanced cluster analysis cluster cj 0 < n x wij < n requirement ensure every cluster i=1 least one object membership value nonzero example 114 fuzzy cluster suppose allelectronic online store six reviews keyword contain reviews list table 112 group reviews two fuzzy cluster c1 c2 c1 “ digital camera ” “ lens ” c2 “ ” partition matrix  1 1  1  = 2 3  0 0  0 0  0  1  3 1 1 use keyword “ digital camera ” “ lens ” feature cluster c1 “ computer ” feature cluster c2 review ri cluster cj ( 1 ≤ ≤ 6 1 ≤ j ≤ 2 ) wij defined wij = ri ∩ cj | ri ∩ cj | = ri ∩ ( c1 ∪ c2 ) | ri ∩ { digital camera lens computer } | fuzzy cluster review r4 belong cluster c1 c2 membership degree 23 31 respectively “ evaluate well fuzzy cluster describe datum set ” consider set object o1 fuzzy cluster c k cluster c1 ck let = [ wij ] ( 1 ≤ ≤ n 1 ≤ j ≤ k ) partition matrix let c1 ck center cluster c1 ck respectively center defined either mean medoid way specific application discuss chapter 10 distance similarity object center cluster object assign used measure well table 112 set reviews keyword used review id keyword r1 r2 r3 r4 r5 r6 digital camera lens digital camera lens digital camera lens computer computer cpu computer computer game 
111 probabilistic model-based cluster 501 object belong cluster idea extend fuzzy cluster object oi cluster cj wij > 0 dist ( oi cj ) measure well oi represent cj thus belong cluster cj object participate one cluster sum distance corresponding cluster center weight degree membership capture well object fit cluster formally object oi sum square error ( sse ) give sse ( oi ) = k x p wij dist ( oi cj ) 2 ( 112 ) j=1 parameter p ( p ≥ 1 ) control influence degree membership larger value p larger influence degree membership orthogonally sse cluster cj sse ( cj ) = n x p wij dist ( oi cj ) 2 ( 113 ) i=1 finally sse cluster defined sse ( c ) = n x k x p wij dist ( oi cj ) 2 ( 114 ) i=1 j=1 sse used measure well fuzzy cluster fit datum set fuzzy cluster also call soft cluster allow object belong one cluster easy see traditional ( rigid ) cluster enforce object belong one cluster exclusively special case fuzzy cluster defer discussion compute fuzzy cluster section 1113 1112 probabilistic model-based cluster “ fuzzy cluster ( section 1111 ) provide flexibility allow object participate multiple cluster general framework specify clustering object may participate multiple cluster probabilistic way ” section introduce general notion probabilistic model-based cluster answer question discuss chapter 10 conduct cluster analysis datum set assume object datum set fact belong different inherent category recall cluster tendency analysis ( section 1061 ) used examine whether datum set contain object may lead meaningful cluster inherent category hide datum latent mean directly observed instead infer used datum observed example topic hide set reviews allelectronic online store latent one read topic directly however topic infer reviews review one multiple topic 
502 chapter 11 advanced cluster analysis therefore goal cluster analysis find hide category datum set subject cluster analysis regard sample possible instance hide category without category label cluster derive cluster analysis infer used datum set design approach hide category statistically assume hide category distribution datum space mathematically represent used probability density function ( distribution function ) call hide category probabilistic cluster probabilistic cluster c probability density function f point datum space f ( ) relative likelihood instance c appear example 115 probabilistic cluster suppose digital camera sell allelectronic divide two category c1 consumer line ( eg point-and-shoot camera ) c2 professional line ( eg single-len reflex camera ) respective probability density function f1 f2 show figure 111 respect attribute price price value say $ 1000 f1 ( 1000 ) relative likelihood price consumer-line camera $ 1000 similarly f2 ( 1000 ) relative likelihood price professional-line camera $ 1000 probability density function f1 f2 observed directly instead allelectronic infer distribution analyze price digital camera sell moreover camera often come well-determine category ( eg “ consumer line ” “ professional line ” ) instead category typically base user background knowledge vary example camera prosumer segment may regard high end consumer line customer low end professional line other analyst allelectronic consider category probabilistic cluster conduct cluster analysis price camera approach category probability consumer line professional line price 1000 figure 111 probability density function two probabilistic cluster 
111 probabilistic model-based cluster 503 suppose want find k probabilistic cluster c1 ck cluster analysis datum set n object regard finite sample possible instance cluster conceptually assume form follow cluster cj ( 1 ≤ j ≤ k ) associate probability ωj instance sample cluster often assume ω1 ωk give part problem set p kj=1 ωj = 1 ensure object generate k cluster parameter ωj capture background knowledge relative population cluster cj run follow two step generate object d step execute n time total generate n object o1 choose cluster cj accord probability ω1 ωk choose instance cj accord probability density function fj datum generation process basic assumption mixture model formally mixture model assume set observed object mixture instance multiple probabilistic cluster conceptually observed object generate independently two step first choose probabilistic cluster accord probability cluster choose sample accord probability density function choose cluster give datum set k number cluster require task probabilistic model-based cluster analysis infer set k probabilistic cluster likely generate used datum generation process important question remain measure likelihood set k probabilistic cluster probability generate observed datum set consider set c k probabilistic cluster c1 ck probability density function f1 fk respectively probability ω1 ωk object probability generate cluster cj ( 1 ≤ j ≤ k ) give p ( o|cj ) = ωj fj ( ) therefore probability generate set c cluster p ( o|c ) = k x ωj fj ( ) ( 115 ) j=1 since object assume generate independently datum set = { o1 } n object p ( d|c ) = n i=1 p ( oi c ) = k n x ωj fj ( oi ) ( 116 ) i=1 j=1 clear task probabilistic model-based cluster analysis datum set find set c k probabilistic cluster p ( d|c ) maximize maximize p ( d|c ) often intractable general probability density function 
504 chapter 11 advanced cluster analysis cluster take arbitrarily complicate form make probabilistic model-based cluster computationally feasible often compromise assume probability density function parameterized distribution formally let o1 n observed object 21 2k parameter k distribution denote = { o1 } 2 = { 21 2k } respectively object oi ∈ ( 1 ≤ ≤ n ) eq ( 115 ) rewrite p ( oi 2 ) = k x ωj pj ( oi 2j ) ( 117 ) j=1 pj ( oi 2j ) probability oi generate jth distribution used parameter 2j consequently eq ( 116 ) rewrite p ( o|2 ) = n x k ωj pj ( oi 2j ) ( 118 ) i=1 j=1 used parameterized probability distribution model task probabilistic model-based cluster analysis infer set parameter 2 maximize eq ( 118 ) example 116 univariate gaussian mixture model let ’ use univariate gaussian distribution example assume probability density function cluster follow 1-d gaussian distribution suppose k cluster two parameter probability density function cluster center µj standard deviation σj ( 1 ≤ j ≤ k ) denote parameter 2j = ( µj σj ) 2 = { 21 2k } let datum set = { o1 } oi ( 1 ≤ ≤ n ) real number point oi ∈ 1 e p ( oi 2j ) = √ 2π σj − ( oi −µj ) 2 2σ 2 ( 119 ) assume cluster probability ω1 = ω2 = · · · = ωk = k1 plug eq ( 119 ) eq ( 117 ) k 2 ( oi −µj ) 1x 1 − p ( oi 2 ) = e 2σ 2 √ k 2π σj ( 1110 ) j=1 apply eq ( 118 ) n p ( o|2 ) = k 2 ( oi −µj ) 1 yx 1 − e 2σ 2 √ k 2π σj ( 1111 ) i=1 j=1 task probabilistic model-based cluster analysis used univariate gaussian mixture model infer 2 eq ( 1111 ) maximize 
111 probabilistic model-based cluster 505 1113 expectation-maximization algorithm “ compute fuzzy clustering probabilistic model-based clustering ” section introduce principled approach let ’ start review k-mean cluster problem k-mean algorithm study chapter 10 easily show k-mean cluster special case fuzzy cluster ( exercise 111 ) k-mean algorithm iterate cluster improve iteration consist two step expectation step ( e-step ) give current cluster center object assign cluster center closest object object expect belong closest cluster maximization step ( m-step ) give cluster assignment cluster algorithm adjust center sum distance object assign cluster new center minimize similarity object assign cluster maximize generalize two-step method tackle fuzzy cluster probabilistic model-based cluster general expectation-maximization ( em ) algorithm framework approach maximum likelihood maximum posteriori estimate parameter statistical model context fuzzy probabilistic model-based cluster em algorithm start initial set parameter iterate cluster improve cluster converge change sufficiently small ( less preset threshold ) iteration also consist two step expectation step assign object cluster accord current fuzzy cluster parameter probabilistic cluster maximization step find new cluster parameter maximize sse fuzzy cluster ( eq 114 ) expect likelihood probabilistic model-based cluster example 117 fuzzy cluster used em algorithm consider six point figure 112 coordinate point also show let ’ compute two fuzzy cluster used em algorithm randomly select two point say c1 = c2 = b initial center two cluster first iteration conduct expectation step maximization step follow e-step point calculate membership degree cluster point assign c1 c2 membership weight 1 dist ( c1 ) 2 1 1 + 2 dist ( c1 ) dist ( c2 ) 2 = dist ( c2 ) 2 dist ( c1 ) 2 dist ( c1 ) 2 + dist ( c2 ) 2 dist ( c1 ) 2 + dist ( c2 ) 2 
506 chapter 11 advanced cluster analysis e ( 18 11 ) b ( 4 10 ) ( 14 8 ) c ( 9 6 ) f ( 21 7 ) ( 3 3 ) x figure 112 datum set fuzzy cluster table 113 intermediate result first three iteration example 117 ’ em algorithm iteration 1 2 3 e-step ` 1 0 = 0 1 ` 073 mt = 027 ` 080 mt = 020 048 052 042 058 m-step 041 059 # 047 053 049 051 091 009 026 074 033 067 076 024 099 001 002 098 014 086 c1 = ( 847 512 ) c2 = ( 1042 899 ) # 042 058 # 023 077 c1 = ( 851 611 ) c2 = ( 1442 869 ) c1 = ( 640 624 ) c2 = ( 1655 864 ) respectively dist ( ) euclidean distance rationale close c1 dist ( c1 ) small membership degree respect c1 high also normalize membership degree sum degree object equal 1 point wa c1 = 1 wa c2 = exclusively belong c1 41 = 048 point b wb c1 = 0 wb c2 = point c wc c1 = 45+41 45 wc c2 = 45+41 = degree membership point show partition matrix table 113 m-step recalculate centroid accord partition matrix minimize sse give eq ( 114 ) new centroid adjust x 2 wo c j point cj = ( 1112 ) x 2 wo c j point j = 1 2 
111 probabilistic model-based cluster 507 example 12 × 3 + 02 × 4 + 0482 × 9 + 0422 × 14 + 0412 × 18 + 0472 × 21 12 + 02 + 0482 + 0422 + 0412 + 0472  12 × 3 + 02 × 10 + 0482 × 6 + 0422 × 8 + 0412 × 11 + 0472 × 7 12 + 02 + 0482 + 0422 + 0412 + 0472  c1 = = ( 847 512 )  c2 = 02 × 3 + 12 × 4 + 0522 × 9 + 0582 × 14 + 0592 × 18 + 0532 × 21 02 + 12 + 0522 + 0582 + 0592 + 0532  02 × 3 + 12 × 10 + 0522 × 6 + 0582 × 8 + 0592 × 11 + 0532 × 7 02 + 12 + 0522 + 0582 + 0592 + 0532 = ( 1042 899 ) repeat iteration iteration contain e-step m-step table 113 show result first three iteration algorithm stop cluster center converge change small enough “ apply em algorithm compute probabilistic model-based cluster ” let ’ use univariate gaussian mixture model ( example 116 ) illustrate example 118 used em algorithm mixture model give set object = { o1 } want mine set parameter 2 = { 21 2k } p ( o|2 ) eq ( 1111 ) maximize 2j = ( µj σj ) mean standard deviation respectively jth univariate gaussian distribution ( 1 ≤ j ≤ k ) apply em algorithm assign random value parameter 2 initial value iteratively conduct e-step m-step follow parameter converge change sufficiently small e-step object oi ∈ ( 1 ≤ ≤ n ) calculate probability oi belong distribution p ( oi 2j ) p ( 2j oi 2 ) = pk l=1 p ( oi 2l ) ( 1113 ) m-step adjust parameter 2 expect likelihood p ( o|2 ) eq ( 1111 ) maximize achieve set pn n p ( 2j oi 2 ) 1x 1 i=1 oi p ( 2j oi 2 ) µj = oi pn = pn k k l=1 p ( 2j ol 2 ) i=1 p ( 2j oi 2 ) i=1 ( 1114 ) 
508 chapter 11 advanced cluster analysis σj = sp n 2 i=1 p ( 2j oi 2 ) ( oi − uj ) pn i=1 p ( 2j oi 2 ) ( 1115 ) many application probabilistic model-based cluster show effective general partition method fuzzy cluster method distinct advantage appropriate statistical model used capture latent cluster em algorithm commonly used handle many learn problem datum mining statistic due simplicity note general em algorithm may converge optimal solution may instead converge local maximum many heuristic explore avoid example can run em process multiple time used different random initial value furthermore em algorithm costly number distribution large datum set contain observed datum point 112 cluster high-dimensional datum cluster method study far work well dimensionality high less 10 attribute however important application high dimensionality “ conduct cluster analysis high-dimensional datum ” section study approach cluster high-dimensional datum section 1121 start overview major challenge approach used method high-dimensional datum cluster divide two category subspace cluster method ( section 1122 ) dimensionality reduction method ( section 1123 ) 1121 cluster high-dimensional datum problem challenge major methodology present specific method cluster high-dimensional datum let ’ first demonstrate need cluster analysis high-dimensional datum used example examine challenge call new method categorize major method accord whether search cluster subspace original space whether create new lower-dimensionality space search cluster application datum object may describe 10 attribute object refer high-dimensional datum space example 119 high-dimensional datum cluster allelectronic keep track product purchase every customer customer-relationship manager want cluster customer group accord purchase allelectronic 
112 cluster high-dimensional datum 509 table 114 customer purchase datum customer p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 ada bob cathy 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 customer purchase datum high dimensionality allelectronic carry ten thousand product therefore customer ’ purchase profile vector product carry company ten thousand dimension “ traditional distance measure frequently used low-dimensional cluster analysis also effective high-dimensional datum ” consider customer table 114 10 product p1 p10 used demonstration customer purchase product 1 set corresponding bit otherwise 0 appear let ’ calculate euclidean distance ( eq 216 ) among ada bob cathy easy see dist ( ada bob ) = dist ( bob cathy ) = dist ( ada cathy ) = √ 2 accord euclidean distance three customer equivalently similar ( dissimilar ) however close look tell us ada similar cathy bob ada cathy share one common purchase item p1 show example 119 traditional distance measure ineffective high-dimensional datum distance measure may dominate noise many dimension therefore cluster full high-dimensional space unreliable find cluster may meaningful “ kind cluster meaningful high-dimensional datum ” cluster analysis high-dimensional datum still want group similar object together however datum space often big messy additional challenge need find cluster cluster set attribute manifest cluster word cluster high-dimensional datum often defined used small set attribute instead full datum space essentially cluster high-dimensional datum return group object cluster ( conventional cluster analysis ) addition cluster set attribute characterize cluster example table 114 characterize similarity ada cathy p1 may return attribute ada cathy purchase p1 cluster high-dimensional datum search cluster space exist thus two major kind method subspace cluster approach search cluster exist subspace give high-dimensional datum space subspace defined used subset attribute full space subspace cluster approach discuss section 1122 
510 chapter 11 advanced cluster analysis dimensionality reduction approach try construct much lower-dimensional space search cluster space often method may construct new dimension combine dimension original datum dimensionality reduction method topic section 1124 general cluster high-dimensional datum raise several new challenge addition conventional cluster major issue create appropriate model cluster high-dimensional datum unlike conventional cluster low-dimensional space cluster hide high-dimensional datum often significantly smaller example cluster customer-purchase datum would expect many user similar purchase pattern search small meaningful cluster like find needle haystack show conventional distance measure ineffective instead often consider various sophisticated technique model correlation consistency among object subspace typically exponential number possible subspace dimensionality reduction option thus optimal solution often computationally prohibitive example original datum space 1000 dimension want 1000 find cluster dimensionality 10 = 263 × 1023 possible 10 subspace 1122 subspace cluster method “ find subspace cluster high-dimensional datum ” many method propose generally categorize three major group subspace search method correlation-based cluster method bicluster method subspace search method subspace search method search various subspace cluster cluster subset object similar subspace similarity often capture conventional measure distance density example clique algorithm introduce section 1052 subspace cluster method enumerate subspace cluster subspace dimensionality-increas order apply antimonotonicity prune subspace cluster may exist major challenge subspace search method face search series subspace effectively efficiently generally two kind strategy bottom-up approach start low-dimensional subspace search higherdimensional subspace may cluster higher-dimensional 
112 cluster high-dimensional datum 511 subspace various prune technique explore reduce number higherdimensional subspace need search clique example bottom-up approach top-down approach start full space search smaller smaller subspace recursively top-down approach effective locality assumption hold require subspace cluster determine local neighborhood example 1110 proclus top-down subspace approach proclus k-medoid-like method first generate k potential cluster center high-dimensional datum set used sample datum set refine subspace cluster iteratively iteration current k-medoid proclus consider local neighborhood medoid whole datum set identify subspace cluster minimize standard deviation distance point neighborhood medoid dimension subspace medoid determine point datum set assign closest medoid accord corresponding subspace cluster possible outlier identify next iteration new medoid replace exist one improve cluster quality correlation-based cluster method subspace search method search cluster similarity measure used conventional metric like distance density correlation-based approach discover cluster defined advanced correlation model example 1111 correlation-based approach used pca example pca-based approach first apply pca ( principal component analysis see chapter 3 ) derive set new uncorrelated dimension mine cluster new space subspace addition pca space transformation may used hough transform fractal dimension additional detail subspace search method correlation-based cluster method please refer bibliographic note ( section 117 ) bicluster method application want cluster object attribute simultaneously result cluster know bicluster meet four requirement ( 1 ) small set object participate cluster ( 2 ) cluster involve small number attribute ( 3 ) object may participate multiple cluster participate cluster ( 4 ) attribute may involved multiple cluster involved cluster section 1123 discuss bicluster detail 
512 chapter 11 advanced cluster analysis 1123 bicluster cluster analysis discuss far cluster object accord attribute value object attribute treat way however application object attribute defined symmetric way datum analysis involve search datum matrix submatrix show unique pattern cluster kind cluster technique belong category bicluster section first introduce two motivate application example biclustering— gene expression recommender system learn different type bicluster last present bicluster method application example bicluster technique first propose address need analyze gene expression datum gene unit passing-on trait live organism offspr typically gene reside segment dna gene critical live thing specify protein functional rna chain hold information build maintain live organism ’ cell pass genetic trait offspr synthesis functional gene product either rna protein rely process gene expression genotype genetic makeup cell organism individual phenotype observable characteristic organism gene expression fundamental level genetic genotype cause phenotype used dna chip ( also know dna microarray ) biological engineering technique measure expression level large number ( possibly ) organism ’ gene number different experimental condition condition may correspond different time point experiment sample different organ roughly speaking gene expression datum dna microarray datum conceptually condition matrix row correspond one gene column correspond one sample condition element matrix real number record expression level gene specific condition figure 113 show illustration cluster viewpoint interesting issue gene expression datum matrix analyze two dimensions—the gene dimension condition dimension analyze gene dimension treat gene object treat condition attribute mining gene dimension may find pattern share multiple gene cluster gene group example may find group gene express similarly highly interesting bioinformatic find pathway analyze condition dimension treat condition object treat gene attribute way may find pattern condition cluster condition group example may find difference gene expression compare group tumor sample nontumor sample 
112 cluster high-dimensional datum 513 condition gene w11 w12 w1m w21 w22 w2m w31 w32 w3m wn1 wn2 wnm figure 113 microarrary datum matrix example 1112 gene expression gene expression matrix popular bioinformatic research development example important task classify new gene used expression datum gene gene know class symmetrically may classify new sample ( eg new patient ) used expression datum sample sample know class ( eg tumor nontumor ) task invaluable understand mechanism disease clinical treatment see many gene expression datum mining problem highly related cluster analysis however challenge instead cluster one dimension ( eg gene condition ) many case need cluster two dimension simultaneously ( eg gene condition ) moreover unlike cluster model discuss far cluster gene expression datum matrix submatrix usually follow characteristic small set gene participate cluster cluster involve small subset condition gene may participate multiple cluster may participate cluster condition may involved multiple cluster may involved cluster find cluster condition matrix need new cluster technique meet follow requirement bicluster cluster gene defined used subset condition cluster condition defined used subset gene 
514 chapter 11 advanced cluster analysis cluster neither exclusive ( eg one gene participate multiple cluster ) exhaustive ( eg gene may participate cluster ) bicluster useful bioinformatic also application well consider recommender system example example 1113 used bicluster recommender system allelectronic collect datum customer ’ evaluation product used datum recommend product customer datum modeled customer-product matrix row represent customer column represent product element matrix represent customer ’ evaluation product may score ( eg like like somewhat like ) purchase behavior ( eg buy ) figure 114 illustrate structure customer-product matrix analyze two dimension customer dimension product dimension treat customer object product attribute allelectronic find customer group similar preference purchase pattern used product object customer attribute allelectronic mine product group similar customer interest moreover allelectronic mine cluster customer product simultaneously cluster contain subset customer involve subset product example allelectronic highly interested find group customer like group product cluster submatrix customer-product matrix element high value used cluster allelectronic make recommendation two direction first company recommend product new customer similar customer cluster second company recommend customer new product similar involved cluster bicluster gene expression datum matrix bicluster customerproduct matrix usually follow characteristic small set customer participate cluster cluster involve small subset product customer participate multiple cluster may participate cluster customer w11 w21 ··· wn1 product w12 · · · w22 · · · ··· ··· wn2 · · · figure 114 customer–product matrix w1m w2m ··· wnm 
112 cluster high-dimensional datum 515 product may involved multiple cluster may involved cluster bicluster apply customer-product matrix mine cluster satisfying requirement type bicluster “ model bicluster mine ” let ’ start basic notation sake simplicity use “ gene ” “ condition ” refer two dimension discussion discussion easily extend application example simply replace “ gene ” “ condition ” “ customer ” “ product ” tackle customer-product bicluster problem let = { a1 } set gene b = { b1 bm } set condition let e = [ eij ] gene expression datum matrix gene-condition matrix 1 ≤ ≤ n 1 ≤ j ≤ m submatrix × j defined subset ⊆ gene subset j ⊆ b condition example matrix show figure 115 { a1 a33 a86 } × { b6 b12 b36 b99 } submatrix bicluster submatrix gene condition follow consistent pattern define different type bicluster base pattern simplest case submatrix × j ( ⊆ j ⊆ b ) bicluster constant value ∈ j ∈ j eij = c c constant example submatrix { a1 a33 a86 } × { b6 b12 b36 b99 } figure 115 bicluster constant value bicluster interesting row constant value though different row may different value bicluster constant value row submatrix × j ∈ j ∈ j eij = c + αi αi adjustment row i example figure 116 show bicluster constant value row symmetrically bicluster constant value column submatrix × j ∈ j ∈ j eij = c + βj βj adjustment column j a1 ··· a33 ··· a86 ··· ··· ··· ··· ··· ··· ··· ··· b6 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b12 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b36 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b99 · · · 60 · · · ··· ··· 60 · · · ··· ··· 60 · · · ··· ··· figure 115 gene-condition matrix submatrix bicluster 
516 chapter 11 advanced cluster analysis generally bicluster interesting row change synchronize way respect column vice versa mathematically bicluster coherent value ( also know pattern-based cluster ) submatrix × j ∈ j ∈ j eij = c + αi + βj αi βj adjustment row column j respectively example figure 117 show bicluster coherent value show × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 moreover instead used addition define bicluster coherent value used multiplication eij = c · αi · βj clearly bicluster constant value row column special case bicluster coherent value application may interested - down-regulate change across gene condition without constrain exact value bicluster coherent evolution row submatrix × j i1 i2 ∈ j1 j2 ∈ j ( ei1 j1 − ei1 j2 ) ( ei2 j1 − ei2 j2 ) ≥ example figure 118 show bicluster coherent evolution row symmetrically define bicluster coherent evolution column next study mine bicluster 10 20 50 0 10 20 50 0 10 20 50 0 10 20 50 0 10 20 50 0 figure 116 bicluster constant value row 10 20 50 0 50 60 90 40 30 40 70 20 70 80 110 60 20 30 60 10 figure 117 bicluster coherent value 10 20 50 0 50 100 100 80 30 50 90 20 70 1000 120 100 20 30 80 10 figure 118 bicluster coherent evolution row 
112 cluster high-dimensional datum 517 bicluster method previous specification type bicluster consider ideal case real datum set perfect bicluster rarely exist exist usually small instead random noise affect reading eij thus prevent bicluster nature appear perfect shape two major type method discover bicluster datum may come noise optimization-based method conduct iterative search iteration submatrix highest significance score identify bicluster process terminate user-specified condition meet due cost concern computation greedy search often employ find local optimal bicluster enumeration method use tolerance threshold specify degree noise allow bicluster mine try enumerate submatrix bicluster satisfy requirement use δ-cluster maple algorithms example illustrate idea optimization used δ-cluster algorithm submatrix × j mean ith row 1 x eij = eij | ( 1116 ) j∈j symmetrically mean jth column 1 x eij = eij | ( 1117 ) i∈i mean element submatrix 1 x 1 x 1 x eij = eij = eij eij = | | | i∈i j∈j i∈i ( 1118 ) j∈j quality submatrix bicluster measure mean-squared residue value 1 x h ( × j ) = ( eij − eij − eij + eij ) 2 ( 1119 ) | i∈i j∈j submatrix × j δ-bicluster h ( × j ) ≤ δ δ ≥ 0 threshold δ = 0 × j perfect bicluster coherent value set δ > 0 user specify tolerance average noise per element perfect bicluster eq ( 1119 ) residue element residue ( eij ) = eij − eij − eij + eij ( 1120 ) maximal δ-bicluster δ-bicluster × j exist another δ-bicluster 0 × j 0 ⊆ 0 j ⊆ j 0 least one inequality hold find 
518 chapter 11 advanced cluster analysis maximal δ-bicluster largest size computationally costly therefore use heuristic greedy search method obtain local optimal cluster algorithm work two phase deletion phase start whole matrix mean-squared residue matrix δ iteratively remove row column iteration row compute mean-squared residue 1 x ( ) = ( eij − eij − eij + eij ) 2 ( 1121 ) | j∈j moreover column j compute mean-squared residue 1 x ( eij − eij − eij + eij ) 2 ( j ) = | ( 1122 ) i∈i remove row column largest mean-squared residue end phase obtain submatrix × j δ-bicluster however submatrix may maximal addition phase iteratively expand δ-bicluster × j obtain deletion phase long δ-bicluster requirement maintain iteration consider row column involved current bicluster × j calculate mean-squared residue row column smallest mean-squared residue add current δ-bicluster greedy algorithm find one δ-bicluster find multiple bicluster heavy overlap run algorithm multiple time execution δ-bicluster output replace element output bicluster random number although greedy algorithm may find neither optimal bicluster bicluster fast even large matrix enumerate bicluster used maple mentioned submatrix × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 2 × 2 submatrix × j define p-score ei1 j1 ei1 j2 p-score = | ( ei1 j1 − ei2 j1 ) − ( ei1 j2 − ei2 j2 ) | ( 1123 ) ei2 j1 ei2 j2 submatrix × j δ-pcluster ( pattern-based cluster ) p-score every 2 × 2 submatrix × j δ δ ≥ 0 threshold specify user ’ tolerance noise perfect bicluster p-score control noise every element bicluster mean-squared residue capture average noise interesting property δ-pcluster × j δ-pcluster every x × ( x ≥ 2 ) submatrix × j also δ-pcluster monotonicity enable 
112 cluster high-dimensional datum 519 us obtain succinct representation nonredundant δ-pcluster δ-pcluster maximal row column add cluster maintain δ-pcluster property avoid redundancy instead find δ-pcluster need compute maximal δ-pcluster maple algorithm enumerate maximal δ-pcluster systematically enumerate every combination condition used set enumeration tree depthfirst search enumeration framework pattern-growth method frequent pattern mining ( chapter 6 ) consider gene expression datum condition combination j maple find maximal subset gene × j δ-pcluster × j submatrix another δ-pcluster × j maximal δ-pcluster may huge number condition combination maple prune many unfruitful combination used monotonicity δ-pcluster condition combination j exist set gene × j δ-pcluster need consider superset j moreover consider × j candidate δ-pcluster every ( | − 1 ) subset j 0 j × j 0 δ-pcluster maple also employ several prune technique speed search retain completeness return maximal δ-pcluster example examine current δ-pcluster × j maple collect gene condition may add expand cluster candidate gene condition together j form submatrix δ-pcluster already find search × j superset j prune interested reader may refer bibliographic note additional information maple algorithm ( section 117 ) interesting observation search maximal δ-pcluster maple somewhat similar mining frequent close itemset consequently maple borrow depth-first search framework idea prune technique pattern-growth method frequent pattern mining example frequent pattern mining cluster analysis may share similar technique idea advantage maple algorithms enumerate bicluster guarantee completeness result miss overlapping bicluster however challenge enumeration algorithms may become time consume matrix become large customer-purchase matrix hundred thousand customer million product 1124 dimensionality reduction method spectral cluster subspace cluster method try find cluster subspace original datum space situation effective construct new space instead used subspace original datum motivation behind dimensionality reduction method cluster high-dimensional datum example 1114 cluster derive space consider three cluster point figure possible cluster point subspace original space x × 
520 chapter 11 advanced cluster analysis − 0707x + 0707y x figure 119 cluster derive space may effective three cluster would end project onto overlapping area x √ √ 2 2 axe instead construct new dimension − 2 x + 2 ( show dash line figure ) project point onto new dimension three cluster become apparent although example 1114 involve two dimension idea construct new space ( cluster structure hide datum become well manifest ) extend high-dimensional datum preferably newly construct space low dimensionality many dimensionality reduction method straightforward approach apply feature selection extraction method datum set discuss chapter however method may able detect cluster structure therefore method combine feature extraction cluster prefer section introduce spectral cluster group method effective highdimensional datum application figure 1110 show general framework spectral cluster approach ng-jordan-weiss algorithm spectral cluster method let ’ look step framework also note special condition apply ng-jordan-weiss algorithm example give set object o1 distance pair object dist ( oi oj ) ( 1 ≤ j ≤ n ) desire number k cluster spectral cluster approach work follow used distance measure calculate affinity matrix w wij = e − dist ( oi oj ) σ2 σ scaling parameter control fast affinity wij decrease dist ( oi oj ) increase ng-jordan-weiss algorithm wii set 0 
112 cluster high-dimensional datum datum affinity matrix [ wij ] compute lead k eigenvector cluster new space 521 project back cluster original datum av = λv = f ( w ) figure 1110 framework spectral cluster approach source adapt slide 8 http micued08 azran used affinity matrix w derive matrix = f ( w ) way do vary ng-jordan-weiss algorithm define matrix diagonal matrix dii sum ith row w dii = n x wij ( 1124 ) j=1 set 1 1 = d− 2 wd− 2 ( 1125 ) find k lead eigenvector a recall eigenvector square matrix nonzero vector remain proportional original vector multiply matrix mathematically vector v eigenvector matrix av = λv λ call corresponding eigenvalue step derive k new dimension base affinity matrix w typically k much smaller dimensionality original datum ng-jordan-weiss algorithm compute k eigenvector largest eigenvalue x1 xk used k lead eigenvector project original datum new space defined k lead eigenvector run cluster algorithm k-mean find k cluster ng-jordan-weiss algorithm stack k largest eigenvector column form matrix x = [ x1 x2 · · · xk ] ∈ rn×k algorithm form matrix renormalize row x unit length xij yij = qp k 2 j=1 xij ( 1126 ) algorithm treat row point k-dimensional space rk run k-mean ( algorithm serve partition purpose ) cluster point k cluster 
522 chapter 11 advanced cluster analysis v = [ v1 v2 v3 ] w 05 u = [ u1 u2 u3 ] 0 −05 0 10 20 30 40 50 60 1 05 0 −05 −1 0 10 20 30 40 50 60 1 05 05 0 0 0 04 02 0 −02 0 10 10 20 20 30 30 40 40 50 50 60 60 0 1 05 0 −05 0 10 10 20 20 30 30 40 40 50 50 60 60 figure 1111 new dimension cluster result ng-jordan-weiss algorithm source adapt slide 9 http micued08 azran assign original datum point cluster accord transform point assign cluster obtain step 4 ng-jordan-weiss algorithm original object oi assign jth cluster matrix ’ row assign jth cluster result step 4 spectral cluster method dimensionality new space set desire number cluster set expect new dimension able manifest cluster example 1115 ng-jordan-weiss algorithm consider set point figure datum set affinity matrix three largest eigenvector normalize vector show note three new dimension ( form three largest eigenvector ) cluster easily detected spectral cluster effective high-dimensional application image process theoretically work well certain condition apply scalability however challenge compute eigenvector large matrix costly spectral cluster combine cluster method bicluster additional information dimensionality reduction cluster method kernel pca find bibliographic note ( section 117 ) 113 cluster graph network datum cluster analysis graph network datum extract valuable knowledge information datum increasingly popular many application discuss application challenge cluster graph network datum section similarity measure form cluster give section learn graph cluster method section 1133 general term graph network used interchangeably rest section mainly use term graph 
113 cluster graph network datum 523 1131 application challenge customer relationship manager allelectronic notice lot datum relate customer purchase behavior preferably modeled used graph example 1116 bipartite graph customer purchase behavior allelectronic represent bipartite graph bipartite graph vertex divide two disjoint set edge connect vertex one set vertex set allelectronic customer purchase datum one set vertex represent customer one customer per vertex set represent product one product per vertex edge connect customer product represent purchase product customer figure 1112 show illustration “ kind knowledge obtain cluster analysis customer-product bipartite graph ” cluster customer customer buy similar set product place one group customer relationship manager make product recommendation example suppose ada belong customer cluster customer purchase digital camera last 12 month ada yet purchase one manager decide recommend digital camera alternatively cluster product product purchase similar set customer group together cluster information also used product recommendation example digital camera high-speed flash memory card belong product cluster customer purchase digital camera recommend high-speed flash memory card bipartite graph widely used many application consider another example example 1117 web search engine web search engine search log archive record user query corresponding click-through information ( click-through information tell us page give result search user click ) query click-through information represent used bipartite graph two set customer product figure 1112 bipartite graph represent customer-purchase datum 
524 chapter 11 advanced cluster analysis vertex correspond query web page respectively edge link query web page user click web page ask query valuable information obtain cluster analysis query–web page bipartite graph instance may identify query pose different language mean thing click-through information query similar another example web page web form direct graph also know web graph web page vertex hyperlink edge point source page destination page cluster analysis web graph disclose community find hub authoritative web page detect web spam addition bipartite graph cluster analysis also apply type graph include general graph elaborate example 1118 example 1118 social network social network social structure represent graph vertex individual organization link interdependency vertex represent friendship common interest collaborative activity allelectronic ’ customer form social network customer vertex edge link two customer know customer relationship manager interested find useful information derive allelectronic ’ social network cluster analysis obtain cluster network customer cluster know friend common customer within cluster may influence one another regard purchase decision make moreover communication channel design inform “ head ” cluster ( ie “ best ” connect person cluster ) promotional information spread quickly thus may use customer cluster promote sale allelectronic another example author scientific publication form social network author vertex two author connect edge coauthor publication network general weight graph edge two author carry weight represent strength collaboration many publication two author ( end vertex ) coauthor cluster coauthor network provide insight community author pattern collaboration “ challenge specific cluster analysis graph network datum ” cluster method discuss far object represent used set attribute unique feature graph network datum object ( vertex ) relationship ( edge ) give dimension attribute explicitly defined conduct cluster analysis graph network datum two major new challenge “ measure similarity two object graph accordingly ” typically use conventional distance measure euclidean distance instead need develop new measure quantify similarity 
113 cluster graph network datum 525 measure often metric thus raise new challenge regard development efficient cluster method similarity measure graph discuss section 1132 “ design cluster model method effective graph network datum ” graph network datum often complicate carry topological structure sophisticated traditional cluster analysis application many graph datum set large web graph contain least ten billion web page publicly indexable web graph also sparse average vertex connect small number vertex graph discover accurate useful knowledge hide deep datum good cluster method accommodate factor cluster method graph network datum introduce section 1133 1132 similarity measure “ measure similarity distance two vertex graph ” discussion examine two type measure geodesic distance distance base random walk geodesic distance simple measure distance two vertex graph shortest path vertex formally geodesic distance two vertex length term number edge shortest path vertex two vertex connect graph geodesic distance defined infinite used geodesic distance define several useful measurement graph analysis cluster give graph g = ( v e ) v set vertex e set edge define follow vertext v ∈ v eccentricity v denote eccen ( v ) largest geodesic distance v vertex u ∈ v − { v } eccentricity v capture far away v remotest vertex graph radius graph g minimum eccentricity vertex r = min eccen ( v ) v∈v ( 1127 ) radius capture distance “ central point ” “ farthest border ” graph diameter graph g maximum eccentricity vertex = max eccen ( v ) v∈v diameter represent largest distance pair vertex peripheral vertex vertex achieve diameter ( 1128 ) 
526 chapter 11 advanced cluster analysis b c e figure 1113 graph g vertex c e peripheral example 1119 measurement base geodesic distance consider graph g figure eccentricity 2 eccen ( ) = 2 eccen ( b ) = 2 eccen ( c ) = eccen ( ) = eccen ( e ) = thus radius g 2 diameter note necessary = 2 × r vertex c e peripheral vertex simrank similarity base random walk structural context application geodesic distance may inappropriate measure similarity vertex graph introduce simrank similarity measure base random walk structural context graph mathematics random walk trajectory consist take successive random step example 1120 similarity person social network let ’ consider measure similarity two vertex allelectronic customer social network example 1118 similarity explain closeness two participant network close two person term relationship represent social network “ well geodesic distance measure similarity closeness network ” suppose ada bob two customer network network undirected geodesic distance ( ie length shortest path ada bob ) shortest path message pass ada bob vice versa however information useful allelectronic ’ customer relationship management company typically want send specific message one customer another therefore geodesic distance suit application “ similarity mean social network ” consider two way define similarity two customer consider similar one another similar neighbor social network heuristic intuitive practice two person receive recommendation good number common friend often make similar decision kind similarity base local structure ( ie neighborhood ) vertex thus call structural context–based similarity 
113 cluster graph network datum 527 suppose allelectronic send promotional information ada bob social network ada bob may randomly forward information friend ( neighbor ) network closeness ada bob measure likelihood customer simultaneously receive promotional information originally send ada bob kind similarity base random walk reachability network thus refer similarity base random walk let ’ closer look meant similarity base structural context similarity base random walk intuition behind similarity base structural context two vertex graph similar connect similar vertex measure similarity need define notion individual neighborhood direct graph g = ( v e ) v set vertex e ⊆ v × v set edge vertex v ∈ v individual in-neighborhood v defined ( 1129 ) ( v ) = { | ( u v ) ∈ e } symmetrically define individual out-neighborhood v ( 1130 ) ( v ) = { | ( v w ) ∈ e } follow intuition illustrated example 1120 define simrank structural-context similarity value 0 1 pair vertex vertex v ∈ v similarity vertex ( v v ) = 1 neighborhood identical vertex u v ∈ v u = v define x x c ( u v ) = ( x ) ( 1131 ) i ( u ) i ( v ) | x∈i ( u ) y∈i ( v ) c constant 0 vertex may in-neighbor thus define eq ( 1131 ) 0 either ( u ) ( v ) ∅ parameter c specify rate decay similarity propagate across edge “ compute simrank ” straightforward method iteratively evaluate eq ( 1131 ) fix point reach let si ( u v ) simrank score calculate ith round begin set ( 0 u = v s0 ( u v ) = ( 1132 ) 1 u = v use eq ( 1131 ) compute si+1 si si+1 ( u v ) = x c i ( u ) i ( v ) | x x∈i ( u ) y∈i ( v ) si ( x ) ( 1133 ) 
528 chapter 11 advanced cluster analysis show lim si ( u v ) = ( u v ) additional method approximate i→∞ simrank give bibliographic note ( section 117 ) let ’ consider similarity base random walk direct graph strongly connect two node u v path u v another path v u strongly connect graph g = ( v e ) two vertex u v ∈ v define expect distance u v ( u v ) = x u p [ ] l ( ) ( 1134 ) v u v path start u end v may contain cycle reach v end travele tour = w1 → w2 → · · · → wk length l ( ) = k − probability tour defined ( q k−1 1 i=1 o ( wi ) | l ( ) > 0 ( 1135 ) p [ ] = 0 l ( ) = 0 measure probability vertex w receive message originated simultaneously u v extend expect distance notion expect meeting distance x ( u v ) = p [ ] l ( ) ( 1136 ) ( x x ) ( u v ) ( u v ) ( x x ) pair tour u x v x length used constant c 0 1 define expect meeting probability p ( u v ) = x ( u v ) p [ ] c l ( ) ( 1137 ) ( x x ) similarity measure base random walk parameter c specify probability continue walk step trajectory show ( u v ) = p ( u v ) two vertex u v simrank base structural context random walk 1133 graph cluster method let ’ consider conduct cluster graph first describe intuition behind graph cluster discuss two general category graph cluster method find cluster graph imagine cut graph piece piece cluster vertex within cluster well connect vertex different cluster connect much weaker way formally graph g = ( v e ) 
113 cluster graph network datum 529 cut c = ( ) partition set vertex v g v = ∪ ∩ = ∅ cut set cut set edge { ( u v ) ∈ e|u ∈ v ∈ } size cut number edge cut set weight graph size cut sum weight edge cut set “ kind cut good derive cluster graph ” graph theory network application minimum cut importance cut minimum cut ’ size greater cut ’ size polynomial time algorithms compute minimum cut graph use algorithms graph cluster example 1121 cut cluster consider graph g figure graph two cluster { b c e f } { g h j k } one outlier vertex l consider cut c1 = ( { b c e f g h j k } { l } ) one edge namely ( e l ) cross two partition create c1 therefore cut set c1 { ( e l ) } size c1 1 ( note size cut connect graph smaller 1 ) minimum cut c1 lead good cluster separate outlier vertex l rest graph cut c2 = ( { b c e f l } { g h j k } ) lead much better cluster c1 edge cut set c2 connect two “ natural cluster ” graph specifically edge ( h ) ( e k ) cut set edge connect h e k belong one cluster example 1121 indicate used minimum cut unlikely lead good cluster better choose cut vertex u involved edge cut set edge connect u belong one cluster formally let deg ( u ) degree u number edge connect u sparsity cut c = ( ) defined = cut size min { | | } ( 1138 ) sparsest cut c2 b c g f h e k minimum cut c1 l figure 1114 graph g two cut j 
530 chapter 11 advanced cluster analysis cut sparsest sparsity greater sparsity cut may one sparsest cut example 1121 figure 1114 c2 sparsest cut used sparsity objective function sparsest cut try minimize number edge cross partition balance partition size consider cluster graph g = ( v e ) partition graph k cluster modularity cluster assess quality cluster defined = k x i=1   di 2 li − | | ( 1139 ) li number edge vertex ith cluster di sum degree vertex ith cluster modularity cluster graph difference fraction edge fall individual cluster fraction would graph vertex randomly connect optimal cluster graph maximize modularity theoretically many graph cluster problem regard find good cut sparsest cut graph practice however number challenge exist high computational cost many graph cut problem computationally expensive sparsest cut problem example np-hard therefore find optimal solution large graph often impossible good trade-off scalability quality achieve sophisticated graph graph sophisticated one describe involve weight or cycle high dimensionality graph many vertex similarity matrix vertex represent vector ( row matrix ) dimensionality number vertex graph therefore graph cluster method must handle high dimensionality sparsity large graph often sparse meaning vertex average connect small number vertex similarity matrix large sparse graph also sparse two kind method cluster graph datum address challenge one used cluster method high-dimensional datum design specifically cluster graph first group method base generic cluster method highdimensional datum extract similarity matrix graph used similarity measure discuss section generic cluster method apply similarity matrix discover cluster cluster method 
113 cluster graph network datum 531 high-dimensional datum typically employ example many scenario similarity matrix obtain spectral cluster method ( section 1124 ) apply spectral cluster approximate optimal graph cut solution additional information please refer bibliographic note ( section 117 ) second group method specific graph search graph find well-connected component cluster let ’ look method call scan ( structural cluster algorithm network ) example give undirected graph g = ( v e ) vertex u ∈ v neighborhood u 0 ( u ) = { | ( u v ) ∈ e } ∪ { u } used idea structural-context similarity scan measure similarity two vertex u v ∈ v normalize common neighborhood size 0 ( u ) ∩ 0 ( v ) | σ ( u v ) = √ 0 ( u ) 0 ( v ) | ( 1140 ) larger value compute similar two vertex scan used similarity threshold ε define cluster membership vertex u ∈ v ε-neighborhood u defined nε ( u ) = { v ∈ 0 ( u ) σ ( u v ) ≥ ε } ε-neighborhood u contain neighbor u structural-context similarity u least ε scan core vertex vertex inside cluster u ∈ v core vertex nε ( u ) | ≥ µ µ popularity threshold scan grow cluster core vertex vertex v ε-neighborhood core u v assign cluster u process grow cluster continue cluster grow process similar density-based cluster method dbscan ( chapter 10 ) formally vertex v directly reach core u v ∈ nε ( u ) transitively vertex v reach core u exist vertex w1 wn w1 reach u wi reach wi−1 1 < ≤ n v reach wn moreover two vertex u v ∈ v may may core say connect exist core w u v reach w vertex cluster connect cluster maximum set vertex every pair set connect vertex may belong cluster vertex u hub neighborhood 0 ( u ) u contain vertex one cluster vertex belong cluster hub outlier scan algorithm show figure search framework closely resemble cluster-find process dbscan scan find cut graph cluster set vertex connect base transitive similarity structural context advantage scan time complexity linear respect number edge large sparse graph number edge scale number vertex therefore scan expect good scalability cluster large graph 
532 chapter 11 advanced cluster analysis algorithm scan cluster graph datum input graph g = ( v e ) similarity threshold ε population threshold µ output set cluster method set vertex v unlabeled unlabeled vertex u u core generate new cluster-id c insert v ∈ nε ( u ) queue q q = w ← first vertex q r ← set vertex directly reach w ∈ r unlabeled labele nonmember assign current cluster-id c endif unlabeled insert queue q endif endfor remove w q end else label u nonmember endif endfor vertex u labele nonmember ∃x ∈ 0 ( u ) x different cluster-id label u hub else label u outlier endif endfor figure 1115 scan algorithm cluster analysis graph datum 114 cluster constraint user often background knowledge want integrate cluster analysis may also application-specific requirement information modeled cluster constraint approach topic cluster constraint two step section 1141 categorize type constraint cluster graph datum method cluster constraint introduce section 1142 
114 cluster constraint 533 1141 categorization constraint section study categorize constraint used cluster analysis specifically categorize constraint accord subject set strongly constraint enforce discuss chapter 10 cluster analysis involve three essential aspect object instance cluster cluster group object similarity among object therefore first method discuss categorize constraint accord apply thus three type constraint instance constraint cluster constraint similarity measurement constraint instance constraint instance specify pair set instance group cluster analysis two common type constraint category include must-link constraint must-link constraint specify two object x x group one cluster output cluster analysis must-link constraint transitive must-link ( x ) must-link ( z ) must-link ( x z ) link constraint link constraint opposite must-link constraint link constraint specify two object x output cluster analysis x belong different cluster link constraint entail link ( x ) must-link ( x x 0 ) must-link ( 0 ) link ( x 0 0 ) constraint instance defined used specific instance alternatively also defined used instance variable attribute instance example constraint constraint ( x ) must-link ( x ) dist ( x ) ≤  used distance object specify must-link constraint constraint cluster constraint cluster specify requirement cluster possibly used attribute cluster example constraint may specify minimum number object cluster maximum diameter cluster shape cluster ( eg convex ) number cluster specify partition cluster method regard constraint cluster constraint similarity measurement often similarity measure euclidean distance used measure similarity object cluster analysis application exception apply constraint similarity measurement specify requirement similarity calculation must respect example cluster person move object plaza euclidean distance used give 
534 chapter 11 advanced cluster analysis walking distance two point constraint similarity measurement trajectory implement shortest distance cross wall one way express constraint depend category example specify constraint cluster constraint1 diameter cluster larger requirement also expressed used constraint instance constraint10 link ( x ) dist ( x ) > ( 1141 ) example 1122 constraint instance cluster similarity measurement allelectronic cluster customer group customer assign customer relationship manager suppose want specify customer address place group would allow comprehensive service family expressed used must-link constraint instance constraintfamily ( x ) must-link ( x ) xaddress = yaddress allelectronic eight customer relationship manager ensure similar workload place constraint cluster eight cluster cluster least 10 % customer 15 % customer calculate spatial distance two customer used drive distance two however two customer live different country use flight distance instead constraint similarity measurement another way categorize cluster constraint consider firmly constraint respect constraint hard cluster violate constraint unacceptable constraint soft cluster violate constraint preferable acceptable better solution find soft constraint also call preference example 1123 hard soft constraint allelectronic constraintfamily example 1122 hard constraint splitting family different cluster can prevent company provide comprehensive service family lead poor customer satisfaction constraint number cluster ( correspond number customer relationship manager company ) also hard example 1122 also constraint balance size cluster satisfying constraint strongly prefer company flexible willing assign senior capable customer relationship manager oversee larger cluster therefore constraint soft ideally specific datum set set constraint clustering satisfy constraint however possible may cluster datum set 
114 cluster constraint 535 satisfy constraint trivially two constraint set conflict cluster satisfy time example 1124 conflict constraint consider constraint must-link ( x ) dist ( x ) < 5 link ( x ) dist ( x ) > 3 datum set two object x dist ( x ) = 4 cluster satisfy constraint simultaneously consider two constraint must-link ( x ) dist ( x ) < 5 must-link ( x ) dist ( x ) < 3 second constraint redundant give first moreover datum set distance two object least 5 every possible cluster object satisfy constraint “ measure quality usefulness set constraint ” general consider either informativeness coherence informativeness amount information carry constraint beyond cluster model give datum set cluster method set constraint c informativeness c respect measure fraction constraint c unsatisfied cluster compute d higher informativeness specific requirement background knowledge constraint carry coherence set constraint degree agreement among constraint measure redundancy among constraint 1142 method cluster constraint although categorize cluster constraint application may different constraint specific form consequently various technique need handle specific constraint section discuss general principle handle hard soft constraint handle hard constraint general strategy handle hard constraint strictly respect constraint cluster assignment process illustrate idea use partition cluster example 
536 chapter 11 advanced cluster analysis give datum set set constraint instance ( ie must-link link constraint ) extend k-mean method satisfy constraint cop-k-mean algorithm work follow generate superinstance must-link constraint compute transitive closure must-link constraint must-link constraint treat equivalence relation closure give one multiple subset object object subset must assign one cluster represent subset replace object subset mean superinstance also carry weight number object represent step must-link constraint always satisfied conduct modify k-mean cluster recall k-mean object assign closest center nearest-center assignment violate link constraint respect link constraint modify center assignment process k-mean nearest feasible center assignment object assign center sequence step make sure assignment far violate link constraint object assign nearest center assignment respect link constraint cop-k-mean ensure constraint violate every step require backtracking greedy algorithm generate cluster satisfy constraint provide conflict exist among constraint handle soft constraint cluster soft constraint optimization problem cluster violate soft constraint penalty impose cluster therefore optimization goal cluster contain two part optimize cluster quality minimize constraint violation penalty overall objective function combination cluster quality score penalty score illustrate use partition cluster example give datum set set soft constraint instance cvqe ( constrain vector quantization error ) algorithm conduct k-mean cluster enforce constraint violation penalty objective function used cvqe sum distance used k-mean adjust constraint violation penalty calculate follow penalty must-link violation must-link constraint object x assign two different center c1 c2 respectively constraint violate result dist ( c1 c2 ) distance c1 c2 add objective function penalty penalty link violation link constraint object x assign common center c constraint violate 
114 cluster constraint 537 distance dist ( c c 0 ) c c 0 add objective function penalty speeding constrain cluster constraint similarity measurement lead heavy cost cluster consider follow cluster obstacle problem cluster person move object plaza euclidean distance used measure walking distance two point however constraint similarity measurement trajectory implement shortest distance cross wall ( section 1141 ) obstacle may occur object distance two object may derive geometric computation ( eg involve triangulation ) computational cost high large number object obstacle involved cluster obstacle problem represent used graphical notation first point p visible another point q region r straight line join p q intersect obstacle visibility graph graph vg = ( v e ) vertex obstacle corresponding node v two node v1 v2 v joined edge e corresponding vertex represent visible let vg 0 = ( v 0 e 0 ) visibility graph create vg add two additional point p q v 0 e 0 contain edge join two point v 0 two point mutually visible shortest path two point p q subpath vg 0 show figure 1116 ( ) see begin edge p either v1 v2 v3 go path vg end edge either v4 v5 q reduce cost distance computation two pair object point several preprocess optimization technique used one method group point close together microcluster do first triangulating region r triangle grouping nearby point triangle microcluster used method similar birch dbscan show figure 1116 ( b ) process microcluster rather individual point overall computation reduce precomputation perform build two v4 v1 p v2 o1 v3 o2 vg q v5 vg ( ) ( b ) figure 1116 cluster obstacle object ( o1 o2 ) ( ) visibility graph ( b ) triangulation region microcluster source adapt tung hou han [ thh01 ] 
538 chapter 11 advanced cluster analysis kind join index base computation shortest path ( 1 ) vv index pair obstacle vertex ( 2 ) mv index pair microcluster obstacle vertex use index help optimize overall performance used precomputation optimization strategy distance two point ( granularity level microcluster ) compute efficiently thus cluster process perform manner similar typical efficient k-medoid algorithm claran achieve good cluster quality large datum set 115 summary conventional cluster analysis object assign one cluster exclusively however application need assign object one cluster fuzzy probabilistic way fuzzy cluster probabilistic model-based cluster allow object belong one cluster partition matrix record membership degree object belong cluster probabilistic model-based cluster assume cluster parameterized distribution used datum cluster observed sample estimate parameter cluster mixture model assume set observed object mixture instance multiple probabilistic cluster conceptually observed object generate independently first choose probabilistic cluster accord probability cluster choose sample accord probability density function choose cluster expectation-maximization algorithm framework approach maximum likelihood maximum posteriori estimate parameter statistical model expectation-maximization algorithms used compute fuzzy cluster probabilistic model-based cluster high-dimensional datum pose several challenge cluster analysis include model high-dimensional cluster search cluster two major category cluster method high-dimensional datum subspace cluster method dimensionality reduction method subspace cluster method search cluster subspace original space example include subspace search method correlation-based cluster method bicluster method dimensionality reduction method create new space lower dimensionality search cluster bicluster method cluster object attribute simultaneously type bicluster include bicluster constant value constant value column coherent value coherent evolution column two major type bicluster method optimization-based method enumeration method 
116 exercise 539 spectral cluster dimensionality reduction method general idea construct new dimension used affinity matrix cluster graph network datum many application social network analysis challenge include measure similarity object graph design cluster model method graph network datum geodesic distance number edge two vertex graph used measure similarity alternatively similarity graph social network measure used structural context random walk simrank similarity measure base structural context random walk graph cluster modeled compute graph cut sparsest cut may lead good cluster modularity used measure cluster quality scan graph cluster algorithm search graph identify well-connected component cluster constraint used express application-specific requirement background knowledge cluster analysis constraint cluster categorize constraint instance cluster similarity measurement constraint instance include must-link link constraint constraint hard soft hard constraint cluster enforce strictly respect constraint cluster assignment process cluster soft constraint consider optimization problem heuristic used speed constrain cluster 116 exercise 111 traditional cluster method rigid require object belong exclusively one cluster explain special case fuzzy cluster may use k-mean example 112 allelectronic carry 1000 product p1 p1000 consider customer ada bob cathy ada bob purchase three product common p1 p2 p3 997 product ada bob independently purchase seven randomly cathy purchase 10 product randomly select 1000 product euclidean distance probability dist ( ada bob ) > dist ( ada cathy ) jaccard similarity ( chapter 2 ) used learn example 113 show × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 114 compare maple algorithm ( section 1123 ) frequent close itemset mining algorithm closet ( pei han mao [ phm00 ] ) major similarity difference 
540 chapter 11 advanced cluster analysis 115 simrank similarity measure cluster graph network datum ( ) prove lim si ( u v ) = ( u v ) simrank computation i→∞ ( b ) show ( u v ) = p ( u v ) simrank 116 large sparse graph average node low degree similarity matrix used simrank still sparse sense deliberate answer 117 compare scan algorithm ( section 1133 ) dbscan ( section 1041 ) similarity difference 118 consider partition cluster follow constraint cluster number object cluster must nk ( 1 − δ ) nk ( 1 + δ ) n total number object datum set k number cluster desire δ [ 0 1 ) parameter extend k-mean method handle constraint discuss situation constraint hard soft 117 bibliographic note höppner klawonn kruse runkler [ hkkr99 ] provide thorough discussion fuzzy cluster fuzzy c-mean algorithm ( example 117 base ) propose bezdek [ bez81 ] fraley raftery [ fr02 ] give comprehensive overview model-based cluster analysis probabilistic model mclachlan basford [ mb88 ] present systematic introduction mixture model application cluster analysis dempster laird rubin [ dlr77 ] recognize first introduce em algorithm give name however idea em algorithm “ propose many time special circumstance ” admit dempster laird rubin [ dlr77 ] wu [ wu83 ] give correct analysis em algorithm mixture model em algorithms used extensively many datum mining application introduction model-based cluster mixture model em algorithms find recent textbook machine learn statistical learning—for example bishop [ bis06 ] marsland [ mar09 ] alpaydin [ alp11 ] increase dimensionality severe effect distance function indicated beyer et al [ bgrs99 ] also dramatic impact various technique classification cluster semisupervised learn ( radovanović nanopoulos ivanović [ rni09 ] ) kriegel kröger zimek [ kkz09 ] present comprehensive survey method cluster high-dimensional datum clique algorithm develop agrawal gehrke gunopulos raghavan [ aggr98 ] proclus algorithm propose aggawal procopiuc wolf et al [ + 99 ] technique bicluster initially propose hartigan [ har72 ] term bicluster coin mirkin [ mir98 ] cheng church [ cc00 ] introduce 
117 bibliographic note 541 bicluster gene expression datum analysis many study bicluster model method notion δ-pcluster introduce wang wang yang yu [ wwyy02 ] informative survey see madeira oliveira [ mo04 ] tanay sharan shamir [ tss04 ] chapter introduce δ-cluster algorithm cheng church [ cc00 ] maple pei zhang cho et al [ + 03 ] example optimization-based method enumeration method bicluster respectively donath hoffman [ dh73 ] fiedler [ fie73 ] pioneer spectral cluster chapter use algorithm propose ng jordan weis [ njw01 ] example thorough tutorial spectral cluster see luxburg [ lux07 ] cluster graph network datum important fast-growing topic schaeffer [ sch07 ] provide survey simrank measure similarity develop jeh widom [ jw02a ] xu et al [ xyfs07 ] propose scan algorithm arora rao vazirani [ arv09 ] discuss sparsest cut approximation algorithms cluster constraint extensively study davidson wagstaff basu [ dwb06 ] propose measure informativeness coherence copk-mean algorithm give wagstaff et al [ wcrs01 ] cvqe algorithm propose davidson ravi [ dr05 ] tung han lakshmanan ng [ thln01 ] present framework constraint-based cluster base user-specified constraint efficient method constraint-based spatial cluster existence physical obstacle constraint propose tung hou han [ thh01 ] 
13 datum mining trend research frontier young research field datum mining make significant progress cover broad spectrum application since 1980s today datum mining used vast array area numerous commercial datum mining system service available many challenge however still remain final chapter introduce mining complex datum type prelude in-depth study reader may choose addition focus trend research frontier datum mining section 131 present overview methodology mining complex datum type extend concept task introduce book mining include mining time-series sequential pattern biological sequence graph network spatiotemporal datum include geospatial datum moving-object datum cyber-physical system datum multimedium datum text datum web datum datum stream section 132 briefly introduce approach datum mining include statistical method theoretical foundation visual audio datum mining section 133 learn datum mining application business science include financial retail telecommunication industry science engineering recommender system social impact datum mining discuss section 134 include ubiquitous invisible datum mining privacy-preserve datum mining finally section 135 speculate current expect datum mining trend arise response new challenge field 131 mining complex datum type section outline major development research effort mining complex datum type complex datum type summarize figure section 1311 cover mining sequence datum time-series symbolic sequence biological sequence section 1312 discuss mining graph social information network section 1313 address mining kind datum include spatial datum spatiotemporal datum moving-object datum cyber-physical system datum multimedium datum text datum datum mining concept technique doi b978-0-12-381479-100013-7 c 2012 elsevier right re-serve 585 
586 chapter 13 datum mining trend research frontier c p l e x p e f sequence datum graph network mining kind datum time-series datum ( eg stock market datum ) symbolic sequence ( eg customer shopping sequence web click stream ) biological sequence ( eg dna protein sequence ) homogeneous ( link type ) heterogeneous ( link different type ) example graph social information network etc spatial datum spatiotemporal datum cyber-physical system datum multimedium datum text datum web datum datum stream figure 131 complex datum type mining web datum datum stream due broad scope theme section present high-level overview topic discuss in-depth book 1311 mining sequence datum time-series symbolic sequence biological sequence sequence order list event sequence may categorize three group base characteristic event describe ( 1 ) time-series datum ( 2 ) symbolic sequence datum ( 3 ) biological sequence let ’ consider type time-series datum sequence datum consist long sequence numeric datum record equal time interval ( eg per minute per hour per day ) time-series datum generate many natural economic process stock market scientific medical natural observation symbolic sequence datum consist long sequence event nominal datum typically observed equal time interval many sequence gap ( ie lapse record event ) matter much example include customer shopping sequence web click stream well sequence event science engineering natural social development biological sequence include dna protein sequence sequence typically long carry important complicate hide semantic meaning gap usually important let ’ look datum mining sequence datum type 
131 mining complex datum type 587 similarity search time-series datum time-series datum set consist sequence numeric value obtain repeat measurement time value typically measure equal time interval ( eg every minute hour day ) time-series databasis popular many application stock market analysis economic sale forecasting budgetary analysis utility study inventory study yield projection workload projection process quality control also useful study natural phenomena ( eg atmosphere temperature wind earthquake ) scientific engineering experiment medical treatment unlike normal database query find datum match give query exactly similarity search find datum sequence differ slightly give query sequence many time-series similarity query require subsequence match find set sequence contain subsequence similar give query sequence similarity search often necessary first perform datum dimensionality reduction transformation time-series datum typical dimensionality reduction technique include ( 1 ) discrete fourier transform ( dft ) ( 2 ) discrete wavelet transform ( dwt ) ( 3 ) singular value decomposition ( svd ) base principle component analysis ( pca ) touch concept chapter 3 thorough explanation beyond scope book go great detail technique datum signal map signal transform space small subset “ strongest ” transform coefficient save feature feature form feature space projection transform space index construct original transform time-series datum speed search query-based similarity search technique include normalization transformation atomic match ( ie find pair gap-free window small length similar ) window stitching ( ie stitching similar window form pair large similar subsequence allow gap atomic match ) subsequence order ( ie linearly order subsequence match determine whether enough similar piece exist ) numerous software package exist similarity search time-series datum recently researcher propose transform time-series datum piecewise aggregate approximation datum view sequence symbolic representation problem similarity search transform one match subsequence symbolic sequence datum identify motif ( ie frequently occur sequential pattern ) build index hashing mechanism efficient search base motif experiment show approach fast simple comparable search quality dft dwt dimensionality reduction method regression trend analysis time-series datum regression analysis time-series datum study substantially field statistic signal analysis however one may often need go beyond pure regression 
chapter 13 datum mining trend research frontier price 588 allelectronic stock 10-day move average time figure 132 time-series datum stock price allelectronic time trend show dash curve calculate move average analysis perform trend analysis many practical application trend analysis build integrate model used follow four major component movement characterize time-series datum trend long-term movement indicate general direction time-series graph move time example used weight move average least square method find trend curf dash curve indicated figure 132 cyclic movement long-term oscillation trend line curve seasonal variation nearly identical pattern time series appear follow corresponding season successive year holiday shopping season effective trend analysis datum often need “ deseasonalize ” base seasonal index compute autocorrelation random movement characterize sporadic change due chance event labor dispute announce personnel change within company trend analysis also used time-series forecasting find mathematical function approximately generate historic pattern time series used make long-term short-term prediction future value arima ( auto-regressive integrate move average ) long-memory time-series modele autoregression popular method analysis sequential pattern mining symbolic sequence symbolic sequence consist order set element event record without concrete notion time many application involve datum 
131 mining complex datum type 589 symbolic sequence customer shopping sequence web click stream program execution sequence biological sequence sequence event science engineering natural social development biological sequence carry complicate semantic meaning pose many challenge research issue investigation conduct field bioinformatic sequential pattern mining focuse extensively mining symbolic sequence sequential pattern frequent subsequence exist single sequence set sequence sequence α = ha1 a2 · · · subsequence another sequence β = hb1 b2 · · · bm exist integer 1 ≤ j1 < j2 < · · · < jn ≤ a1 ⊆ bj1 a2 ⊆ bj2 ⊆ bjn example α = h { ab } di β = h { abc } { } { de } ai b c e item α subsequence mining sequential pattern consist mining set subsequence frequent one sequence set sequence many scalable algorithms develop result extensive study area alternatively mine set close sequential pattern sequential pattern close exist sequential pattern 0 proper subsequence 0 0 ( frequency ) support s similar frequent pattern mining counterpart also study efficient mining multidimensional multilevel sequential pattern constraint-based frequent pattern mining user-specified constraint used reduce search space sequential pattern mining derive pattern interest user refer constraint-based sequential pattern mining moreover may relax constraint enforce additional constraint problem sequential pattern mining derive different kind pattern sequence datum example enforce gap constraint pattern derive contain consecutive subsequence subsequence small gap alternatively may derive periodic sequential pattern fold event proper-size window find recur subsequence window another approach derive partial order pattern relax requirement strict sequential order mining subsequence pattern besides mining partial order pattern sequential pattern mining methodology also extend mining tree lattice episode order pattern sequence classification classification method perform model construction base feature vector however sequence explicit feature even sophisticated feature selection technique dimensionality potential feature still high sequential nature feature difficult capture make sequence classification challenge task sequence classification method organized three category ( 1 ) featurebased classification transform sequence feature vector apply conventional classification method ( 2 ) sequence distance–based classification distance function measure similarity sequence determine 
590 chapter 13 datum mining trend research frontier quality classification significantly ( 3 ) model-based classification used hide markov model ( hmm ) statistical model classify sequence time-series numeric-valu datum feature selection technique symbolic sequence easily apply time-series datum without discretization however discretization cause information loss recently propose time-series shapelet method used time-series subsequence maximally represent class feature achieve quality classification result alignment biological sequence biological sequence generally refer sequence nucleotide amino acid biological sequence analysis compare align index analyze biological sequence thus play crucial role bioinformatic modern biology sequence alignment base fact live organism related evolution imply nucleotide ( dna rna ) protein sequence species closer evolution exhibit similarity alignment process line sequence achieve maximal identity level also express degree similarity sequence two sequence homologous share common ancestor degree similarity obtain sequence alignment useful determine possibility homology two sequence alignment also help determine relative position multiple species evolution tree call phylogenetic tree problem alignment biological sequence describe follow give two input biological sequence identify similar sequence long conserve subsequence number sequence align exactly two problem know pairwise sequence alignment otherwise multiple sequence alignment sequence compare align either nucleotide ( rna ) amino acid ( protein ) nucleotide two symbol align identical however amino acid two symbol align identical one derive substitution likely occur nature two kind alignment local alignment global alignment former mean portion sequence align whereas latter require alignment entire length sequence either nucleotide amino acid insertion deletion substitution occur nature different probability substitution matrix used represent probability substitution nucleotide amino acid probability insertion deletion usually use gap character − indicate position preferable align two symbol evaluate quality alignment score mechanism typically defined usually count identical similar symbol positive score gap negative one algebraic sum score take alignment measure goal alignment achieve maximal score among possible alignment however expensive ( exactly np-hard problem ) find optimal alignment therefore various heuristic method develop find suboptimal alignment 
131 mining complex datum type 591 dynamic programming approach commonly used sequence alignment among many available analysis package blast ( basic local alignment search tool ) one popular tool biosequence analysis hide markov model biological sequence analysis give biological sequence biologist would like analyze sequence represent represent structure statistical regularity sequence class biologist construct various probabilistic model markov chain hide markov model model probability state depend previous state therefore particularly useful analysis biological sequence datum common method construct hide markov model forward algorithm viterbi algorithm baum-welch algorithm give sequence symbol x forward algorithm find probability obtain x model viterbi algorithm find probable path ( corresponding x ) model whereas baum-welch algorithm learn adjust model parameter best explain set training sequence 1312 mining graph network graph represent general class structure set sequence lattice tree broad range graph application web social network information network biological network bioinformatic chemical informatic computer vision multimedium text retrieval hence graph network mining become increasingly important heavily research overview follow major theme ( 1 ) graph pattern mining ( 2 ) statistical modele network ( 3 ) datum clean integration validation network analysis ( 4 ) cluster classification graph homogeneous network ( 5 ) cluster ranking classification heterogeneous network ( 6 ) role discovery link prediction information network ( 7 ) similarity search olap information network ( 8 ) evolution information network graph pattern mining graph pattern mining mining frequent subgraph ( also call ( sub ) graph pattern ) one set graph method mining graph pattern categorize apriori-based pattern growth–base approach alternatively mine set close graph graph g close exist proper supergraph g 0 carry support count g moreover many variant graph pattern include approximate frequent graph coherent graph dense graph user-specified constraint push deep graph pattern mining process improve mining efficiency graph pattern mining many interesting application example used generate compact effective graph index structure base concept 
592 chapter 13 datum mining trend research frontier frequent discriminative graph pattern approximate structure similarity search achieve explore graph index structure multiple graph feature moreover classification graph also perform effectively used frequent discriminative subgraph feature statistical modele network network consist set node corresponding object associate set property set edge ( link ) connect node represent relationship object network homogeneous node link type friend network coauthor network web page network network heterogeneous node link different type publication network ( link together author conference paper content ) health-care network ( link together doctor nurse patient disease treatment ) researcher propose multiple statistical model modele homogeneous network well-known generative model random graph model ( ie erdös-rényi model ) watts-strogatz model scale-free model scalefree model assume network follow power law distribution ( also know pareto distribution heavy-tailed distribution ) large-scale social network small-world phenomenon observed network characterize high degree local cluster small fraction node ( ie node interconnect one another ) degree separation remain node social network exhibit certain evolutionary characteristic tend follow densification power law state network become increasingly dense time shrink diameter another characteristic effective diameter often decrease network grow node out-degree in-degree typically follow heavytailed distribution datum clean integration validation information network analysis real-world datum often incomplete noisy uncertain unreliable information redundancy may exist among multiple piece datum interconnect large network information redundancy explore network perform quality datum clean datum integration information validation trustability analysis network analysis example distinguish author share name examine networked connection heterogeneous object coauthor publication venue term addition identify inaccurate author information present bookseller explore network build base author information provide multiple bookseller sophisticated information network analysis method develop direction many case portion datum serve “ training ” relatively clean reliable datum consensus datum multiple information 
131 mining complex datum type 593 provider used help consolidate remain unreliable portion datum reduce costly effort labele datum hand training massive dynamic real-world datum set cluster classification graph homogeneous network large graph network cohesive structure often hide among massive interconnect node link cluster analysis method develop large network uncover network structure discover hide community hub outlier base network topological structure associate property various kind network cluster method develop categorize either partition hierarchical density-based algorithms moreover give human-labele training datum discovery network structure guide human-specify heuristic constraint supervised classification semi-supervised classification network recent hot topic datum mining research community cluster ranking classification heterogeneous network heterogeneous network contain interconnect node link different type interconnect structure contain rich information used mutually enhance node link propagate knowledge one type another cluster ranking heterogeneous network perform hand-inhand context highly rank link cluster may contribute lower-rank counterpart evaluation cohesiveness cluster cluster may help consolidate high ranking link dedicate cluster mutual enhancement ranking cluster prompt development algorithm call rankclus moreover user may specify different ranking rule present labele link certain datum type knowledge one type propagate type propagation reach link type via heterogeneous-type connection algorithms develop supervised learn semi-supervised learn heterogeneous network role discovery link prediction information network exist many hide role relationship among different link heterogeneous network example include advisor–advisee leader–follower relationship research publication network discover hide role relationship expert specify constraint base background knowledge enforce constraint may help crosscheck validation large interconnect network information redundancy network often used help weed link follow constraint 
594 chapter 13 datum mining trend research frontier similarly link prediction perform base assessment ranking expect relationship among candidate link example may predict paper author may write read cite base author ’ recent publication history trend research similar topic study often require analyze proximity network link trend connection similar neighbor roughly speaking person refer link prediction link mining however link mining cover additional task include link-based object classification object type prediction link type prediction link existence prediction link cardinality estimation object reconciliation ( predict whether two object fact ) also include group detection ( cluster object ) well subgraph identification ( find characteristic subgraph within network ) metadata mining ( uncover schema-type information regard unstructured datum ) similarity search olap information network similarity search primitive operation database web search engine heterogeneous information network consist multityped interconnect object example include bibliographic network social medium network two object consider similar link similar way multityped object general object similarity within network determine base network structure object property similarity measure moreover network cluster hierarchical network structure help organize object network identify subcommunity well facilitate similarity search furthermore similarity defined differently per user consider different linkage path derive various similarity semantic network know path-based similarity organize network base notion similarity cluster generate multiple hierarchy within network online analytical process ( olap ) perform example drill dice information network base different level abstraction different angle view olap operation may generate multiple interrelate network relationship among network may disclose interesting hide semantic evolution social information network network dynamic constantly evolve detect evolve community evolve regularity anomaly homogeneous heterogeneous network help person better understand structural evolution network predict trend irregularity evolve network homogeneous network evolve community discover subnetwork consist object type set friend coauthor however heterogeneous network community discover subnetwork consist object different type connect set paper author venue term also derive set evolve object type like evolve author theme 
131 mining complex datum type 595 1313 mining kind datum addition sequence graph many kind semi-structure unstructured datum spatiotemporal multimedium hypertext datum interesting application datum carry various kind semantic either store dynamically stream system call specialize datum mining methodology thus mining multiple kind datum include spatial datum spatiotemporal datum cyber-physical system datum multimedium datum text datum web datum datum stream increasingly important task datum mining subsection overview methodology mining kind datum mining spatial datum spatial datum mining discover pattern knowledge spatial datum spatial datum many case refer geospace-related datum store geospatial datum repository datum “ vector ” “ raster ” format form imagery geo-reference multimedium recently large geographic datum warehouse construct integrate thematic geographically reference datum multiple source construct spatial datum cube contain spatial dimension measure support spatial olap multidimensional spatial datum analysis spatial datum mining perform spatial datum warehouse spatial databasis geospatial datum repository popular topic geographic knowledge discovery spatial datum mining include mining spatial association co-location pattern spatial cluster spatial classification spatial modele spatial trend outlier analysis mining spatiotemporal datum move object spatiotemporal datum datum relate space time spatiotemporal datum mining refer process discover pattern knowledge spatiotemporal datum typical example spatiotemporal datum mining include discover evolutionary history city land uncover weather pattern predict earthquake hurricane determine global warm trend spatiotemporal datum mining become increasingly important far-reaching implication give popularity mobile phone gps device internet-based map service weather service digital earth well satellite rfid sensor wireless video technology among many kind spatiotemporal datum moving-object datum ( ie datum move object ) especially important example animal scientist attach telemetry equipment wildlife analyze ecological behavior mobility manager emb gps car better monitor guide vehicle meteorologist use weather satellite radar observe hurricane massive-scale moving-object datum become rich complex ubiquitous example moving-object datum mining include mining movement pattern multiple move object ( ie discovery relationship among multiple move object move cluster leader follower merge convoy swarm pincer well collective movement pattern ) example 
596 chapter 13 datum mining trend research frontier moving-object datum mining include mining periodic pattern one set move object mining trajectory pattern cluster model outlier mining cyber-physical system datum cyber-physical system ( cp ) typically consist large number interact physical information component cp system may interconnect form large heterogeneous cyber-physical network example cyber-physical network include patient care system link patient monitoring system network medical information emergency handle system transportation system link transportation monitoring network consist many sensor video camera traffic information control system battlefield commander system link reconnaissance network battlefield information analysis system clearly cyber-physical system network ubiquitous form critical component modern information infrastructure datum generate cyber-physical system dynamic volatile noisy inconsistent interdependent contain rich spatiotemporal information critically important real-time decision make comparison typical spatiotemporal datum mining mining cyber-physical datum require link current situation large information base perform real-time calculation return prompt response research area include rare-event detection anomaly analysis cyber-physical datum stream reliability trustworthiness cyber-physical datum analysis effective spatiotemporal datum analysis cyber-physical network integration stream datum mining real-time automate control process mining multimedium datum multimedium datum mining discovery interesting pattern multimedium databasis store manage large collection multimedium object include image datum video datum audio datum well sequence datum hypertext datum contain text text markup linkage multimedium datum mining interdisciplinary field integrate image process understand computer vision datum mining pattern recognition issue multimedium datum mining include content-based retrieval similarity search generalization multidimensional analysis multimedium datum cube contain additional dimension measure multimedium information topic multimedium mining include classification prediction analysis mining association video audio datum mining ( section 1323 ) mining text datum text mining interdisciplinary field draw information retrieval datum mining machine learn statistic computational linguistic substantial portion information store text news article technical paper book digital library email message blog web page hence research text mining active important goal derive high-quality information text 
131 mining complex datum type 597 typically do discovery pattern trend mean statistical pattern learn topic modele statistical language modele text mining usually require structuring input text ( eg parse along addition derive linguistic feature removal other subsequent insertion database ) follow derive pattern within structure datum evaluation interpretation output “ high quality ” text mining usually refer combination relevance novelty interestingness typical text mining task include text categorization text cluster entity extraction production granular taxonomy sentiment analysis document summarization entity-relation modele ( ie learn relation name entity ) example include multilingual datum mining multidimensional text analysis contextual text mining trust evolution analysis text datum well text mining application security biomedical literature analysis online medium analysis analytical customer relationship management various kind text mining analysis software tool available academic institution open-source forum industry text mining often also used wordnet sematic web wikipedia information source enhance understand mining text datum mining web datum world wide web serve huge widely distribute global information center news advertisement consumer information financial management education government e-commerce contain rich dynamic collection information web page content hypertext structure multimedium hyperlink information access usage information provide fertile source datum mining web mining application datum mining technique discover pattern structure knowledge web accord analysis target web mining organized three main area web content mining web structure mining web usage mining web content mining analyze web content text multimedium datum structure datum ( within web page link across web page ) do understand content web page provide scalable informative keyword-based page indexing concept resolution web page relevance ranking web page content summary valuable information related web search analysis web page reside either surface web deep web surface web portion web index typical search engine deep web ( hide web ) refer web content part surface web content provide underlie database engine web content mining study extensively researcher search engine web service company web content mining build link across multiple web page individual therefore potential inappropriately disclose personal information study privacy-preserve datum mining address concern development technique protect personal privacy web web structure mining process used graph network mining theory method analyze node connection structure web extract pattern hyperlink hyperlink structural component connect 
598 chapter 13 datum mining trend research frontier web page another location also mine document structure within page ( eg analyze treelike structure page structure describe html xml tag usage ) kind web structure mining help us understand web content may also help transform web content relatively structure datum set web usage mining process extract useful information ( eg user click stream ) server log find pattern related general particular group user understand user ’ search pattern trend association predict user look internet help improve search efficiency effectiveness well promote product related information different group user right time web search company routinely conduct web usage mining improve quality service mining datum stream stream datum refer datum flow system vast volume change dynamically possibly infinite contain multidimensional feature datum store traditional database system moreover system may able read stream sequential order pose great challenge effective mining stream datum substantial research lead progress development efficient method mining datum stream area mining frequent sequential pattern multidimensional analysis ( eg construction stream cube ) classification cluster outlier analysis online detection rare event datum stream general philosophy develop single-scan a-few-scan algorithms used limit compute storage capability include collect information stream datum slide window tilt time window ( recent datum register finest granularity distant datum register coarser granularity ) explore technique like microcluster limit aggregation approximation many application stream datum mining explored—for example real-time detection anomaly computer network traffic botnet text stream video stream power-grid flow web search sensor network cyber-physical system 132 methodology datum mining due broad scope datum mining large variety datum mining methodology methodology datum mining thoroughly cover book section briefly discuss several interesting methodology fully address previous chapter methodology list figure 133 1321 statistical datum mining datum mining technique describe book primarily draw computer science discipline include datum mining machine learn datum warehousing algorithms design efficient handle huge amount datum 
132 methodology datum mining h e r n n g e h l g e statistical datum mining foundation datum mining visual audio datum mining 599 regression generalized linear model analysis variance mixed-effect model factor analysis discriminant analysis survival analysis datum reduction datum compression probability statistical theory microeconomic view pattern discovery inductive database datum visualization datum mining result visualization datum mining process visualization interactive visual datum mining audio datum mining figure 133 datum mining methodology typically multidimensional possibly various complex type however many well-established statistical technique datum analysis particularly numeric datum technique apply extensively scientific datum ( eg datum experiment physics engineering manufacturing psychology medicine ) well datum economic social science technique principal component analysis ( chapter 3 ) cluster ( chapter 10 11 ) already address book thorough discussion major statistical method datum analysis beyond scope book however several method mentioned sake completeness pointer technique provide bibliographic note ( section 138 ) regression general method used predict value response ( dependent ) variable one predictor ( independent ) variable variable numeric various form regression linear multiple weight polynomial nonparametric robust ( robust method useful error fail satisfy normalcy condition datum contain significant outlier ) generalized linear model model generalization ( generalized additive model ) allow categorical ( nominal ) response variable ( transformation 
600 chapter 13 datum mining trend research frontier ) related set predictor variable manner similar modele numeric response variable used linear regression generalized linear model include logistic regression poisson regression analysis variance technique analyze experimental datum two population describe numeric response variable one categorical variable ( factor ) general anova ( single-factor analysis variance ) problem involve comparison k population treatment mean determine least two mean different complex anova problem also exist mixed-effect model model analyze group data—data classify accord one grouping variable typically describe relationship response variable covariate datum group accord one factor common area application include multilevel datum repeat measure datum block design longitudinal datum factor analysis method used determine variable combine generate give factor example many psychiatric datum possible measure certain factor interest directly ( eg intelligence ) however often possible measure quantity ( eg student test score ) reflect factor interest none variable designate dependent discriminant analysis technique used predict categorical response variable unlike generalized linear model assume independent variable follow multivariate normal distribution procedure attempt determine several discriminant function ( linear combination independent variable ) discriminate among group defined response variable discriminant analysis commonly used social science survival analysis several well-established statistical technique exist survival analysis technique originally design predict probability patient undergo medical treatment would survive least time t method survival analysis however also commonly apply manufacturing setting estimate life span industrial equipment popular method include kaplanmeier estimate survival cox proportional hazard regression model extension quality control various statistic used prepare chart quality control shewhart chart cusum chart ( display group summary statistic ) statistic include mean standard deviation range count move average move standard deviation move range 1322 view datum mining foundation research theoretical foundation datum mining yet mature solid systematic theoretical foundation important help provide coherent 
132 methodology datum mining 601 framework development evaluation practice datum mining technology several theory basis datum mining include follow datum reduction theory basis datum mining reduce datum representation datum reduction trade accuracy speed response need obtain quick approximate answer query large databasis datum reduction technique include singular value decomposition ( drive element behind principal component analysis ) wavelet regression log-linear model histogram cluster sampling construction index tree datum compression accord theory basis datum mining compress give datum encode term bit association rule decision tree cluster encode base minimum description length principle state “ best ” theory infer datum set one minimize length theory datum encode used theory predictor datum encode typically bit probability statistical theory accord theory basis datum mining discover joint probability distribution random variable example bayesian belief network hierarchical bayesian model microeconomic view microeconomic view consider datum mining task find pattern interesting extent used decision-make process enterprise ( eg regard marketing strategy production plan ) view one utility pattern consider interesting act enterprise regard face optimization problem object maximize utility value decision theory datum mining become nonlinear optimization problem pattern discovery inductive databasis theory basis datum mining discover pattern occur datum association classification model sequential pattern area machine learn neural network association mining sequential pattern mining cluster several subfield contribute theory knowledge base view database consist datum pattern user interact system query datum theory ( ie pattern ) knowledge base knowledge base actually inductive database theory mutually exclusive example pattern discovery also see form datum reduction datum compression ideally theoretical framework able model typical datum mining task ( eg association classification cluster ) probabilistic nature able handle different form datum consider iterative interactive essence datum mining effort require establish well-defined framework datum mining satisfy requirement 
602 chapter 13 datum mining trend research frontier 1323 visual audio datum mining visual datum mining discover implicit useful knowledge large datum set used datum or knowledge visualization technique human visual system controlled eye brain latter thought powerful highly parallel process reasoning engine contain large knowledge base visual datum mining essentially combine power component make highly attractive effective tool comprehension datum distribution pattern cluster outlier datum visual datum mining view integration two discipline datum visualization datum mining also closely related computer graphic multimedium system human–computer interaction pattern recognition high-performance compute general datum visualization datum mining integrate follow way datum visualization datum database datum warehouse view different granularity abstraction level different combination attribute dimension datum present various visual form boxplot 3-d cube datum distribution chart curf surface link graph show datum visualization section chapter figure 134 135 statsoft show figure 134 boxplot show multiple variable combination statsoft source wwwstatsoftcom 
132 methodology datum mining 603 figure 135 multidimensional datum distribution analysis statsoft source wwwstatsoftcom datum distribution multidimensional space visual display help give user clear impression overview datum characteristic large datum set datum mining result visualization visualization datum mining result presentation result knowledge obtain datum mining visual form form may include scatter plot boxplot ( chapter 2 ) well decision tree association rule cluster outlier generalized rule example scatter plot show figure 136 sas enterprise miner figure 137 mineset used plane associate set pillar describe set association rule mine database figure 138 also mineset present decision tree figure 139 ibm intelligent miner present set cluster property associate datum mining process visualization type visualization present various process datum mining visual form user see datum extract database datum warehouse extract well select datum clean integrate preprocessed mine moreover may also show method select datum mining result store may view figure 1310 show visual presentation datum mining process clementine datum mining system 
604 chapter 13 datum mining trend research frontier figure 136 visualization datum mining result sas enterprise miner interactive visual datum mining ( interactive ) visual datum mining visualization tool used datum mining process help user make smart datum mining decision example datum distribution set attribute display used colored sector ( whole space represent circle ) display help user determine sector first select classification good split point sector may example show figure 1311 output perception-based classification ( pbc ) system develop university munich audio datum mining used audio signal indicate pattern datum feature datum mining result although visual datum mining may disclose interesting pattern used graphical display require user concentrate watch pattern identify interesting novel feature within sometimes quite tiresome pattern transform sound music instead watch picture listen pitch rhythm tune melody identify anything interesting unusual may relieve burden visual concentration 
132 methodology datum mining figure 137 visualization association rule mineset figure 138 visualization decision tree mineset 605 
606 chapter 13 datum mining trend research frontier figure 139 visualization cluster grouping ibm intelligent miner figure 1310 visualization datum mining process clementine 
133 datum mining application 607 figure 1311 perception-based classification interactive visual mining approach relax visual mining therefore audio datum mining interesting complement visual mining 133 datum mining application book study principle method mining relational datum datum warehouse complex datum type datum mining relatively young discipline wide diverse application still nontrivial gap general principle datum mining application-specific effective datum mining tool section examine several application domain list figure discuss customize datum mining method tool develop application 1331 datum mining financial datum analysis bank financial institution offer wide variety banking investment credit service ( latter include business mortgage automobile loan credit card ) also offer insurance stock investment service 
608 chapter 13 datum mining trend research frontier financial datum analysis retail telecommunication industry science engineering datum mining application intrusion detection prevention recommender system figure 1312 common datum mining application domain financial datum collect banking financial industry often relatively complete reliable high quality facilitate systematic datum analysis datum mining present typical case design construction datum warehouse multidimensional datum analysis datum mining like many application datum warehouse need construct banking financial datum multidimensional datum analysis method used analyze general property datum example company ’ financial officer may want view debt revenue change month region sector factor along maximum minimum total average trend deviation statistical information datum warehouse datum cube ( include advanced datum cube concept multifeature discovery-driven regression prediction datum cube ) characterization class comparison cluster outlier analysis play important role financial datum analysis mining loan payment prediction customer credit policy analysis loan payment prediction customer credit analysis critical business bank many factor strongly weakly influence loan payment performance customer credit rating datum mining method attribute selection attribute relevance ranking may help identify important factor eliminate irrelevant one example factor related risk loan payment include loan-to-value ratio term loan debt ratio ( total amount monthly debt versus total monthly income ) payment-to-income ratio customer income level education level residence region credit history analysis customer payment history may find say payment-to-income ratio dominant factor education level debt ratio bank may decide adjust loan-grant policy 
133 datum mining application 609 grant loan customer whose application previously deny whose profile show relatively low risk accord critical factor analysis classification cluster customer target marketing classification cluster method used customer group identification target marketing example use classification identify crucial factor may influence customer ’ decision regard banking customer similar behavior regard loan payment may identify multidimensional cluster technique help identify customer group associate new customer appropriate customer group facilitate target marketing detection money launder financial crime detect money launder financial crime important integrate information multiple heterogeneous databasis ( eg bank transaction databasis federal state crime history databasis ) long potentially related study multiple datum analysis tool used detect unusual pattern large amount cash flow certain period certain group customer useful tool include datum visualization tool ( display transaction activity used graph time group customer ) linkage information network analysis tool ( identify link among different customer activity ) classification tool ( filter unrelated attribute rank highly related one ) cluster tool ( group different case ) outlier analysis tool ( detect unusual amount fund transfer activity ) sequential pattern analysis tool ( characterize unusual access sequence ) tool may identify important relationship pattern activity help investigator focus suspicious case detailed examination 1332 datum mining retail telecommunication industry retail industry well-fit application area datum mining since collect huge amount datum sale customer shopping history good transportation consumption service quantity datum collect continue expand rapidly especially due increase availability ease popularity business conduct web e-commerce today major chain store also web site customer make purchase online business amazoncom ( wwwamazoncom ) exist solely online without brick-and-mortar ( ie physical ) store location retail datum provide rich source datum mining retail datum mining help identify customer buy behavior discover customer shopping pattern trend improve quality customer service achieve better customer retention satisfaction enhance good consumption ratio design effective good transportation distribution policy reduce cost business example datum mining retail industry outlined follow design construction datum warehouse retail datum cover wide spectrum ( include sale customer employee good transportation consumption 
610 chapter 13 datum mining trend research frontier service ) many way design datum warehouse industry level detail include vary substantially outcome preliminary datum mining exercise used help guide design development datum warehouse structure involve decide dimension level include preprocess perform facilitate effective datum mining multidimensional analysis sale customer product time region retail industry require timely information regard customer need product sale trend fashion well quality cost profit service commodity therefore important provide powerful multidimensional analysis visualization tool include construction sophisticated datum cube accord need datum analysis advanced datum cube structure introduce chapter 5 useful retail datum analysis facilitate analysis multidimensional aggregate complex condition analysis effectiveness sale campaign retail industry conduct sale campaign used advertisement coupon various kind discount bonuse promote product attract customer careful analysis effectiveness sale campaign help improve company profit multidimensional analysis used purpose compare amount sale number transaction contain sale item sale period versus contain item sale campaign moreover association analysis may disclose item likely purchase together item sale especially comparison sale campaign customer retention—analysis customer loyalty use customer loyalty card information register sequence purchase particular customer customer loyalty purchase trend analyze systematically good purchase different period customer group sequence sequential pattern mining used investigate change customer consumption loyalty suggest adjustment pricing variety good help retain customer attract new one product recommendation cross-referencing item mining association sale record may discover customer buy digital camera likely buy another set item information used form product recommendation collaborative recommender system ( section 1335 ) use datum mining technique make personalize product recommendation live customer transaction base opinion customer product recommendation also advertised sale receipt weekly flyer web help improve customer service aid customer select item increase sale similarly information “ hot item week ” attractive deal display together associative information promote sale fraudulent analysis identification unusual pattern fraudulent activity cost retail industry million dollar per year important ( 1 ) identify potentially fraudulent user atypical usage pattern ( 2 ) detect attempt gain fraudulent entry unauthorized access individual organizational 
133 datum mining application 611 account ( 3 ) discover unusual pattern may need special attention many pattern discover multidimensional analysis cluster analysis outlier analysis another industry handle huge amount datum telecommunication industry quickly evolved offer local long-distance telephone service provide many comprehensive communication service include cellular phone smart phone internet access email text message image computer web datum transmission datum traffic integration telecommunication computer network internet numerous mean communication compute way change face telecommunication compute create great demand datum mining help understand business dynamic identify telecommunication pattern catch fraudulent activity make better use resource improve service quality datum mining task telecommunication share many similarity retail industry common task include construct large-scale datum warehouse perform multidimensional visualization olap in-depth analysis trend customer pattern sequential pattern task contribute business improvement cost reduction customer retention fraud analysis sharpen edge competition many datum mining task customize datum mining tool telecommunication flourishing expect play increasingly important role business datum mining popularly used many industry insurance manufacturing health care well analysis governmental institutional administration datum although industry characteristic datum set application demand share many common principle methodology therefore effective mining one industry may gain experience methodology transfer industrial application 1333 datum mining science engineering past many scientific datum analysis task tend handle relatively small homogeneous datum set datum typically analyze used “ formulate hypothesis build model evaluate result ” paradigm case statistical technique typically employ analysis ( see section 1321 ) massive datum collection storage technology recently change landscape scientific datum analysis today scientific datum amassed much higher speed lower cost result accumulation huge volume high-dimensional datum stream datum heterogenous datum contain rich spatial temporal information consequently scientific application shift “ hypothesize-and-test ” paradigm toward “ collect store datum mine new hypothesis confirm datum experimentation ” process shift bring new challenge datum mining vast amount datum collect scientific domain ( include geoscience astronomy meteorology geology biological science ) used sophisticated 
612 chapter 13 datum mining trend research frontier telescope multispectral high-resolution remote satellite sensor global position system new generation biological datum collection analysis technology large datum set also generate due fast numeric simulation various field climate ecosystem modele chemical engineering fluid dynamic structural mechanic look challenge bring emerge scientific application datum mining datum warehouse datum preprocess datum preprocess datum warehouse critical information exchange datum mining create warehouse often require find mean resolve inconsistent incompatible datum collect multiple environment different time period require reconcile semantic reference system geometry measurement accuracy precision method need integrate datum heterogeneous source identify event instance consider climate ecosystem datum spatial temporal require cross-referencing geospatial datum major problem analyze datum many event spatial domain temporal domain example el nino event occur every four seven year previous datum might collect systematically today method also need efficient computation sophisticated spatial aggregate handle spatial-related datum stream mining complex datum type scientific datum set heterogeneous nature typically involve semi-structure unstructured datum multimedium datum georeference stream datum well datum sophisticated deeply hide semantic ( eg genomic proteomic datum ) robust dedicate analysis method need handle spatiotemporal datum biological datum related concept hierarchy complex semantic relationship example bioinformatic research problem identify regulatory influence gene gene regulation refer gene cell switch ( ) determine cell ’ function different biological process involve different set gene act together precisely regulate pattern thus understand biological process need identify participate gene regulator require development sophisticated datum mining method analyze large biological datum set clue regulatory influence specific gene find dna segment ( “ regulatory sequence ” ) mediate influence graph-based network-based mining often difficult impossible model several physical phenomena process due limitation exist modele approach alternatively labele graph network may used capture many spatial topological geometric biological relational characteristic present scientific datum set graph network modele object mine represent vertex graph edge vertex represent relationship object example graph used model chemical structure biological pathway datum generate numeric 
133 datum mining application 613 simulation fluid-flow simulation success graph network modele however depend improvement scalability efficiency many graph-based datum mining task classification frequent pattern mining cluster visualization tool domain-specific knowledge high-level graphical user interface visualization tool require scientific datum mining system integrate exist domain-specific datum information system guide researcher general user search pattern interpret visualize discover pattern used discover knowledge decision make datum mining engineering share many similarity datum mining science practice often collect massive amount datum require datum preprocess datum warehousing scalable mining complex type datum typically use visualization make good use graph network moreover many engineering process need real-time response mining datum stream real time often become critical component massive amount human communication datum pour daily life communication exist many form include news blog article web page online discussion product reviews twitter message advertisement communication web various kind social network hence datum mining social science social study become increasingly popular moreover user reader feedback regard product speech article analyze deduce general opinion sentiment view society analysis result used predict trend improve work help decision make computer science generate unique kind datum example computer program long execution often generate huge-size trace computer network complex structure network flow dynamic massive sensor network may generate large amount datum varied reliability computer system databasis suffer various kind attack data access may raise security privacy concern unique kind datum provide fertile land datum mining datum mining computer science used help monitor system status improve system performance isolate software bug detect software plagiarism analyze computer system fault uncover network intrusion recognize system malfunction datum mining software system engineering operate static dynamic ( ie stream-based ) datum depend whether system dump trace beforehand postanalysis must react real time handle online datum various method develop domain integrate extend method machine learn datum mining system engineering pattern recognition statistic datum mining computer science active rich domain datum miner unique challenge require development sophisticated scalable real-time datum mining system engineering method 
614 chapter 13 datum mining trend research frontier 1334 datum mining intrusion detection prevention security computer system datum continual risk extensive growth internet increase availability tool trick intrude attack network prompt intrusion detection prevention become critical component networked system intrusion defined set action threaten integrity confidentiality availability network resource ( eg user account file system system kernel ) intrusion detection system intrusion prevention system monitor network traffic or system execution malicious activity however former produce report whereas latter place in-line able actively block intrusion detected main function intrusion prevention system identify malicious activity log information say activity attempt stop activity report activity majority intrusion detection prevention system use either signaturebased detection anomaly-based detection signature-based detection method detection utilize signature attack pattern preconfigured predetermine domain expert signature-based intrusion prevention system monitor network traffic match signature match find intrusion detection system report anomaly intrusion prevention system take additional appropriate action note since system usually quite dynamic signature need update laboriously whenever new software version arrive change network configuration situation occur another drawback detection mechanism identify case match signature unable detect new previously unknown intrusion trick anomaly-based detection method build model normal network behavior ( call profile ) used detect new pattern significantly deviate profile deviation may represent actual intrusion simply new behavior need add profile main advantage anomaly detection may detect novel intrusion yet observed typically human analyst must sort deviation ascertain represent real intrusion limit factor anomaly detection high percentage false positive new pattern intrusion add set signature enhance signature-based detection datum mining method help intrusion detection prevention system enhance performance various way follow new datum mining algorithms intrusion detection datum mining algorithms used signature-based anomaly-based detection signature-based detection training datum labele either “ normal ” “ ” classifier derive detect know intrusion research area 
133 datum mining application 615 include application classification algorithms association rule mining cost-sensitive modele anomaly-based detection build model normal behavior automatically detect significant deviation method include application cluster outlier analysis classification algorithms statistical approach technique used must efficient scalable capable handle network datum high volume dimensionality heterogeneity association correlation discriminative pattern analysis help select build discriminative classifier association correlation discriminative pattern mining apply find relationship system attribute describe network datum information provide insight regard selection useful attribute intrusion detection new attribute derive aggregate datum may also helpful summary count traffic match particular pattern analysis stream datum due transient dynamic nature intrusion malicious attack crucial perform intrusion detection datum stream environment moreover event may normal consider malicious view part sequence event thus necessary study sequence event frequently encounter together find sequential pattern identify outlier datum mining method find evolve cluster build dynamic classification model datum stream also necessary real-time intrusion detection distribute datum mining intrusion launch several different location target many different destination distribute datum mining method may used analyze network datum several network location detect distribute attack visualization query tool visualization tool available view anomalous pattern detected tool may include feature view association discriminative pattern cluster outlier intrusion detection system also graphical user interface allow security analyst pose query regard network datum intrusion detection result summary computer system continual risk break security datum mining technology used develop strong intrusion detection prevention system may employ signature-based anomaly-based detection 1335 datum mining recommender system today ’ consumer face million good service shopping online recommender system help consumer make product recommendation likely interest user book cds movie restaurant online news article service recommender system may use either contentbased approach collaborative approach hybrid approach combine content-based collaborative method 
616 chapter 13 datum mining trend research frontier content-based approach recommend item similar item user prefer query past rely product feature textual item description collaborative approach ( collaborative filter approach ) may consider user ’ social environment recommend item base opinion customer similar taste preference user recommender system use broad range technique information retrieval statistic machine learn datum mining search similarity among item customer preference consider example 131 example 131 scenario used recommender system suppose visit web site online bookstore ( eg amazon ) intention purchasing book want read type name book first time visit web site browse even make purchase last christmas web store remember previous visit store click stream information information regard past purchase system display description price book specify compare interest customer similar interest recommend additional book title say “ customer buy book specify also buy title ” survey list see another title spark interest decide purchase one well suppose go another online store intention purchasing digital camera system suggest additional item consider base previously mine sequential pattern “ customer buy kind digital camera likely buy particular brand printer memory card photo editing software within three ” decide buy camera without additional item week later receive coupon store regard additional item advantage recommender system provide personalization customer e-commerce promote one-to-one marketing amazon pioneer use collaborative recommender system offer “ personalize store every customer ” part marketing strategy personalization benefit consumer company involved accurate model customer company gain better understand customer need serve need result greater success regard cross-selling related product upsel product affinity one-to-one promotion larger basket customer retention recommendation problem consider set c user set item let u utility function measure usefulness item user c utility commonly represent rating initially defined item previously rate user example join movie recommendation system user typically ask rate several movie space c × possible user item huge recommendation system able extrapolate know unknown rating predict item–user combination item highest predict utility user recommend user 
133 datum mining application 617 “ utility item estimate user ” content-based method estimate base utility assign user item similar many system focus recommend item contain textual information web site article news message look commonality among item movie may look similar genre director actor article may look similar term content-based method root information theory make use keyword ( describe item ) user profile contain information user ’ taste need profile may obtain explicitly ( eg questionnaire ) learn user ’ transactional behavior time collaborative recommender system try predict utility item user u base item previously rate user similar u example recommend book collaborative recommender system try find user history agree u ( eg tend buy similar book give similar rating book ) collaborative recommender system either memory ( heuristic ) base model base memory-based method essentially use heuristic make rating prediction base entire collection item previously rate user unknown rating item–user combination estimate aggregate rating similar user item typically k-nearest-neighbor approach used find k user ( neighbor ) similar target user u various approach used compute similarity user popular approach use either pearson ’ correlation coefficient ( section 332 ) cosine similarity ( section 247 ) weight aggregate used adjust fact different user may use rating scale differently model-based collaborative recommender system use collection rating learn model used make rating prediction example probabilistic model cluster ( find cluster like-minded customer ) bayesian network machine learn technique used recommender system face major challenge scalability ensure quality recommendation consumer example regard scalability collaborative recommender system must able search million potential neighbor real time site used browse pattern indication product preference may thousand datum point customer ensure quality recommendation essential gain consumer ’ trust consumer follow system recommendation end liking product less likely use recommender system classification system recommender system make two type error false negative false positive false negative product system fail recommend although consumer would like false positive product recommend consumer like false positive less desirable annoy anger consumer content-based recommender system limit feature used describe item recommend 
618 chapter 13 datum mining trend research frontier another challenge content-based collaborative recommender system deal new user buy history yet available hybrid approach integrate content-based collaborative method achieve improve recommendation netflix prize open competition hold online dvd-rental service payout $ 1000000 best recommender algorithm predict user rating film base previous rating competition study show predictive accuracy recommender system substantially improve blending multiple predictor especially used ensemble many substantially different method rather refine single technique collaborative recommender system form intelligent query answer consist analyze intent query provide generalized neighborhood associate information relevant query example rather simply return book description price response customer ’ query return additional information related query explicitly ask ( eg book evaluation comment recommendation book sale statistic ) provide intelligent answer query 134 datum mining society us datum mining part daily life although may often unaware presence section 1341 look several example “ ubiquitous invisible ” datum mining affect everyday thing product stock local supermarket ad see surfing internet crime prevention datum mining offer individual many benefit improve customer service satisfaction well lifestyle general however also serious implication regard one ’ right privacy datum security issue topic section 1342 1341 ubiquitous invisible datum mining datum mining present many aspect daily life whether realize affect shop work search information even influence leisure time health well-being section look example ubiquitous ( ever-present ) datum mining several example also represent invisible datum mining “ smart ” software search engine customer-adaptive web service ( eg used recommender algorithms ) “ intelligent ” database system email manager ticket master incorporate datum mining functional component often unbeknownst user grocery store print personalize coupon customer receipt online store recommend additional item base customer interest datum mining innovatively influenced buy way shop experience shopping one example wal-mart hundred million customer visit ten thousand store every week wal-mart allow supplier access datum 
134 datum mining society 619 product perform analysis used datum mining software allow supplier identify customer buy pattern different store control inventory product placement identify new merchandize opportunity affect item ( many ) end store ’ shelves—something think next time wander aisle wal-mart datum mining shape online shopping experience many shopper routinely turn online store purchase book music movie toy recommender system discuss section 1335 offer personalize product recommendation base opinion customer amazoncom forefront used personalize datum mining–based approach marketing strategy observed traditional brick-and-mortar store hardest part get customer store customer likely buy something since cost go another store high therefore marketing brick-and-mortar store tend emphasize draw customer rather actual in-store customer experience contrast online store customer “ walk ” enter another online store click mouse amazoncom capitalize difference offer “ personalize store every ” use several datum mining technique identify customer ’ like make reliable recommendation topic shopping suppose lot buy credit card nowadays unusual receive phone call one ’ credit card company regard suspicious unusual pattern spending credit card company use datum mining detect fraudulent usage save billion dollar year many company increasingly use datum mining customer relationship management ( crm ) help provide customize personal service address individual customer ’ need lieu mass marketing study browse purchasing pattern web store company tailor advertisement promotion customer profile customer less likely annoyed unwanted mass mailing junk mail action result substantial cost saving company customer benefit likely notified offer actually interest result less waste personal time greater satisfaction datum mining greatly influenced way person use computer search information work get internet example decide check email unbeknownst several annoying email already delete thank spam filter used classification algorithms recognize spam process email go google ( wwwgooglecom ) provide access information billion web page index server google one popular widely used internet search engine used google search information become way life many person google popular even become new verb english language meaning “ search ( something ) internet used google search engine extension comprehensive search ” 1 decide type keyword 1 http open-dictionarycom 
620 chapter 13 datum mining trend research frontier topic interest google return list web site topic mine index organized set datum mining algorithms include pagerank moreover type “ boston new york ” google show bus train schedule boston new york however minor change “ boston paris ” lead flight schedule boston paris smart offering information service likely base frequent pattern mine click stream many previous query view result google query various ad pop relate query google ’ strategy tailor advertising match user ’ interest one typical service explore every internet search provider also make happier less likely pester irrelevant ad datum mining omnipresent see daily-encounter example can go scenario many case datum mining invisible user may unaware examine result return datum mining click actually fed new datum datum mining function datum mining become improve accept technology continue research development need many area mentioned challenge throughout book include efficiency scalability increase user interaction incorporation background knowledge visualization technique effective method find interesting pattern improve handle complex datum type stream datum realtime datum mining web mining addition integration datum mining exist business scientific technology provide domain-specific datum mining tool contribute advancement technology success datum mining solution tailor e-commerce application opposed generic datum mining system example 1342 privacy security social impact datum mining information accessible electronic form available web increasingly powerful datum mining tool develop put use increase concern datum mining may pose threat privacy datum security however important note many datum mining application even touch personal datum prominent example include application involve natural resource prediction flood drought meteorology astronomy geography geology biology scientific engineering datum furthermore study datum mining research focus development scalable algorithms involve personal datum focus datum mining technology discovery general statistically significant pattern specific information regard individual sense believe real privacy concern unconstrained access individual record especially access privacy-sensitive information credit card transaction record health-care record personal financial record biological trait justice investigation ethnicity datum mining application involve personal datum many case simple method remove sensitive id datum may protect privacy individual nevertheless privacy concern exist wherever 
134 datum mining society 621 personally identifiable information collect store digital form datum mining program able access datum even datum preparation improper nonexistent disclosure control root cause privacy issue handle concern numerous datum security-enhancing technique develop addition great deal recent effort develop privacypreserve datum mining method section look advance protect privacy datum security datum mining “ secure privacy individual collect mining datum ” many datum security–enhancing technique develop help protect datum databasis employ multilevel security model classify restrict datum accord various security level user permit access authorize level show however user execute specific query authorize security level still infer sensitive information similar possibility occur datum mining encryption another technique individual datum item may encode may involve blind signature ( build public key encryption ) biometric encryption ( eg image person ’ iris fingerprint used encode personal information ) anonymous databasis ( permit consolidation various databasis limit access personal information need know personal information encrypt store different location ) intrusion detection another active area research help protect privacy personal datum privacy-preserve datum mining area datum mining research response privacy protection datum mining also know privacy-enhance privacysensitive datum mining deal obtain valid datum mining result without disclose underlie sensitive datum value privacy-preserve datum mining method use form transformation datum perform privacy preservation typically method reduce granularity representation preserve privacy example may generalize datum individual customer customer group reduction granularity cause loss information possibly usefulness datum mining result natural trade-off information loss privacy privacy-preserve datum mining method classify follow category randomization method method add noise datum mask attribute value record noise add sufficiently large individual record value especially sensitive one re-cover however add skillfully final result datum mining basically preserve technique design derive aggregate distribution perturbed datum subsequently datum mining technique develop work aggregate distribution k-anonymity l-diversity method method alter individual record uniquely identify k-anonymity method granularity datum representation reduce sufficiently give record map onto least k record datum used technique like generalization suppression k-anonymity method weak homogeneity 
622 chapter 13 datum mining trend research frontier sensitive value within group value may infer alter record l-diversity model design handle weakness enforce intragroup diversity sensitive value ensure anonymization goal make sufficiently difficult adversary use combination record attribute exactly identify individual record distribute privacy preservation large datum set can partition distribute either horizontally ( ie datum set partition different subset record distribute across multiple site ) vertically ( ie datum set partition distribute attribute ) even combination individual site may want share entire datum set may consent limit information sharing use variety protocol overall effect method maintain privacy individual object derive aggregate result datum downgrading effectiveness datum mining result many case even though datum may available output datum mining ( eg association rule classification model ) may result violation privacy solution can downgrade effectiveness datum mining either modify datum mining result hiding association rule slightly distort classification model recently researcher propose new idea privacy-preserve datum mining notion differential privacy general idea two datum set close one another ( ie differ tiny datum set single element ) give differentially private algorithm behave approximately datum set definition give strong guarantee presence absence tiny datum set ( eg represent individual ) affect final output query significantly base notion set differential privacy-preserve datum mining algorithms develop research direction ongoing expect powerful privacy-preserve datum publish datum mining algorithms near future like technology datum mining misuse however must lose sight benefit datum mining research bring range insight gain medical scientific application increase customer satisfaction help company better suit client ’ need expect computer scientist policy expert counterterrorism expert continue work social scientist lawyer company consumer take responsibility build solution ensure datum privacy protection security way may continue reap benefit datum mining term time money saving discovery new knowledge 135 datum mining trend diversity datum datum mining task datum mining approach pose many challenge research issue datum mining development efficient effective datum 
135 datum mining trend 623 mining method system service interactive integrate datum mining environment key area study use datum mining technique solve large sophisticated application problem important task datum mining researcher datum mining system application developer section describe trend datum mining reflect pursuit challenge application exploration early datum mining application put lot effort help business gain competitive edge exploration datum mining business continue expand e-commerce e-marketing become mainstream retail industry datum mining increasingly used exploration application area web text analysis financial analysis industry government biomedicine science emerge application area include datum mining counterterrorism mobile ( wireless ) datum mining generic datum mining system may limitation deal application-specific problem may see trend toward development application-specific datum mining system tool well invisible datum mining function embed various kind service scalable interactive datum mining method contrast traditional datum analysis method datum mining must able handle huge amount datum efficiently possible interactively amount datum collect continue increase rapidly scalable algorithms individual integrate datum mining function become essential one important direction toward improve overall efficiency mining process increase user interaction constraint-based mining provide user add control allow specification use constraint guide datum mining system search interesting pattern knowledge integration datum mining search engine database system datum warehouse system cloud compute system search engine database system datum warehouse system cloud compute system mainstream information process compute system important ensure datum mining serve essential datum analysis component smoothly integrate information process environment datum mining service tightly couple system seamless unify framework invisible function ensure datum availability datum mining portability scalability high performance integrate information process environment multidimensional datum analysis exploration mining social information network mining social information network link analysis critical task network ubiquitous complex development scalable effective knowledge discovery method application large number network datum essential outlined section 1312 mining spatiotemporal moving-object cyber-physical system cyberphysical system well spatiotemporal datum mount rapidly due 
624 chapter 13 datum mining trend research frontier popular use cellular phone gps sensor wireless equipment outlined section 1313 many challenge research issue realize real-time effective knowledge discovery datum mining multimedium text web datum outlined section 1313 mining kind datum recent focus datum mining research great progress make yet still many open issue solve mining biological biomedical datum unique combination complexity richness size importance biological biomedical datum warrant special attention datum mining mining dna protein sequence mining highdimensional microarray datum biological pathway network analysis topic field area biological datum mining research include mining biomedical literature link analysis across heterogeneous biological datum information integration biological datum datum mining datum mining software engineering system engineering software program large computer system become increasingly bulky size sophisticated complexity tend originate integration multiple component develop different implementation team trend make increasingly challenge task ensure software robustness reliability analysis execution buggy software program essentially datum mining process—trace datum generate program execution may disclose important pattern outlier can lead eventual automate discovery software bug expect development datum mining methodology system debug enhance software robustness bring new vigor system engineering visual audio datum mining visual audio datum mining effective way integrate human ’ visual audio system discover knowledge huge amount datum systematic development technique facilitate promotion human participation effective efficient datum analysis distribute datum mining real-time datum stream mining traditional datum mining method design work centralize location work well many distribute compute environment present today ( eg internet intranet local area network high-speed wireless network sensor network cloud compute ) advance distribute datum mining method expect moreover many application involve stream datum ( eg e-commerce web mining stock analysis intrusion detection mobile datum mining datum mining counterterrorism ) require dynamic datum mining model build real time additional research need direction privacy protection information security datum mining abundance personal confidential information available electronic form couple increasingly powerful datum mining tool pose threat datum privacy security grow interest datum mining counterterrorism also add concern 
136 summary 625 development privacy-preserve datum mining method foresee collaboration technologist social scientist law expert government company need produce rigorous privacy security protection mechanism datum publish datum mining confidence look forward next generation datum mining technology benefit bring 136 summary mining complex datum type pose challenge issue many dedicate line research development chapter present high-level overview mining complex datum type include mining sequence datum time series symbolic sequence biological sequence mining graph network mining kind datum include spatiotemporal cyber-physical system datum multimedium text web datum datum stream several well-established statistical method propose datum analysis regression generalized linear model analysis variance mixed-effect model factor analysis discriminant analysis survival analysis quality control full coverage statistical datum analysis method beyond scope book interested reader refer statistical literature cite bibliographic note ( section 138 ) researcher strive build theoretical foundation datum mining several interesting proposal appear base datum reduction datum compression probability statistic theory microeconomic theory pattern discovery–based inductive databasis visual datum mining integrate datum mining datum visualization discover implicit useful knowledge large datum set visual datum mining include datum visualization datum mining result visualization datum mining process visualization interactive visual datum mining audio datum mining used audio signal indicate datum pattern feature datum mining result many customize datum mining tool develop domain-specific application include finance retail telecommunication industry science engineering intrusion detection prevention recommender system application domain-based study integrate domain-specific knowledge datum analysis technique provide mission-specific datum mining solution ubiquitous datum mining constant presence datum mining many aspect daily life influence shop work search information use computer well leisure time health well-being invisible datum mining “ smart ” software search engine customer-adaptive web service 
626 chapter 13 datum mining trend research frontier ( eg used recommender algorithms ) email manager incorporate datum mining functional component often unbeknownst user major social concern datum mining issue privacy datum security privacy-preserve datum mining deal obtain valid datum mining result without disclose underlie sensitive value goal ensure privacy protection security preserve overall quality datum mining result datum mining trend include effort toward exploration new application area improve scalable interactive constraint-based mining method integration datum mining web service database warehousing cloud compute system mining social information network trend include mining spatiotemporal cyber-physical system datum biological datum system engineering datum multimedium text datum addition web mining distribute real-time datum stream mining visual audio mining privacy security datum mining 137 exercise 131 sequence datum ubiquitous diverse application chapter present general overview sequential pattern mining sequence classification sequence similarity search trend analysis biological sequence alignment modele however cover sequence cluster present overview method sequence cluster 132 chapter present overview sequence pattern mining graph pattern mining method mining tree pattern partial order pattern also study research summarize method mining structure pattern include sequence tree graph partial order relationship examine kind structural pattern mining cover research propose application create new mining problem 133 many study analyze homogeneous information network ( eg social network consist friend link friend ) however many application involve heterogeneous information network ( ie network link multiple type object research paper conference author topic ) major difference methodology mining heterogeneous information network method homogeneous counterpart 134 research describe datum mining application present chapter discuss different form datum mining used application 135 establishment theoretical foundation important datum mining name describe main theoretical foundation propose datum mining comment satisfy ( fail satisfy ) requirement ideal theoretical framework datum mining 
137 exercise 627 136 ( research project ) build theory datum mining require set theoretical framework major datum mining function explain framework take one theory example ( eg datum compression theory ) examine major datum mining function fit framework function fit well current theoretical framework propose way extend framework explain function 137 strong linkage statistical datum analysis datum mining person think datum mining automate scalable method statistical datum analysis agree disagree perception present one statistical analysis method automate or scale nicely integration current datum mining methodology 138 difference visual datum mining datum visualization datum visualization may suffer datum abundance problem example easy visually discover interesting property network connection social network huge complex dense connection propose visualization method may help person see network topology interesting feature social network 139 propose implementation method audio datum mining integrate audio visual datum mining bring fun power datum mining possible develop video datum mining method state scenario solution make integrate audiovisual mining effective 1310 general-purpose computer domain-independent relational database system become large market last several decade however many person feel generic datum mining system prevail datum mining market think datum mining focus effort develop domain-independent datum mining tool develop domain-specific datum mining solution present reasoning 1311 recommender system way differ customer productbased cluster system differ typical classification predictive modele system outline one method collaborative filter discuss work limitation practice 1312 suppose local bank datum mining system bank study debit card usage pattern notice make many transaction home renovation store bank decide contact offer information regard special loan home improvement ( ) discuss may conflict right privacy ( b ) describe another situation feel datum mining infringe privacy ( c ) describe privacy-preserve datum mining method may allow bank perform customer pattern analysis without infringe customer ’ right privacy ( ) example datum mining can used help society think way can used may detrimental society 
628 chapter 13 datum mining trend research frontier 1313 major challenge face bring datum mining research market illustrate one datum mining research issue view may strong impact market society discuss approach research issue 1314 base view challenge research problem datum mining give number year good number researcher implementor would plan make good progress toward effective solution problem 1315 base experience knowledge suggest new frontier datum mining mentioned chapter 138 bibliographic note mining complex datum type many research paper book cover various theme list recent book well-cite survey research article reference time-series analysis study statistic computer science community decade many textbook box jenkin reinsel [ bjr08 ] brockwell davis [ bd02 ] chatfield [ cha03b ] hamilton [ ham94 ] shumway stoffer [ ss05 ] fast subsequence match method time-series databasis present faloutsos ranganathan manolopoulos [ frm94 ] agrawal lin sawhney shim [ alss95 ] develop method fast similarity search presence noise scaling translation time-series databasis shasha zhu present overview method high-performance discovery time series [ sz04 ] sequential pattern mining method study many researcher include agrawal srikant [ as95 ] zaki [ zak01 ] pei han mortazavi-asl et al [ + 04 ] yan han afshar [ yha03 ] study sequence classification include ji bailey dong [ jbd05 ] ye keogh [ yk09 ] survey xing pei keogh [ xpk10 ] dong pei [ dp07 ] provide overview sequence datum mining method method analysis biological sequence include markov chain hide markov model introduce many book tutorial waterman [ wat95 ] setubal meidanis [ sm97 ] durbin eddy krogh mitchison [ dekm98 ] baldi brunak [ bb01 ] krane raymer [ kr03 ] rabiner [ rab89 ] jone pevzner [ jp04 ] baxevanis ouellette [ bo04 ] information blast ( see also korf yandell bedell [ kyb03 ] ) find ncbi web site graph pattern mining study extensively include holder cook djoko [ hcd94 ] inokuchi washio motoda [ iwm98 ] kuramochi karypis [ kk01 ] yan han [ yh02 yh03a ] borgelt berthold [ bb02 ] huan wang bandyopadhyay et al [ + 04 ] gaston tool nijssen kok [ nk04 ] 
138 bibliographic note 629 great deal research social information network analysis include newman [ new10 ] easley kleinberg [ ek10 ] yu han faloutsos [ yhf10 ] wasserman faust [ wf94 ] watt [ wat03 ] newman barabasi watt [ nbw06 ] statistical modele network study popularly albert barbasi [ ab99 ] watt [ wat03 ] faloutsos faloutsos faloutsos [ fff99 ] kumar raghavan rajagopalan et al [ + 00 ] leskovec kleinberg faloutsos [ lkf05 ] datum clean integration validation information network analysis study many include bhattacharya getoor [ bg04 ] yin han yu [ yhy07 yhy08 ] cluster ranking classification network study extensively include brin page [ bp98 ] chakrabarti dom indyk [ cdi98 ] kleinberg [ kle99 ] getoor friedman koller taskar [ gfkt01 ] newman m girvan [ ng04 ] yin han yang yu [ yhyy04 ] yin han yu [ yhy05 ] xu yuruk feng schweiger [ xyfs07 ] kuli basu dhillon mooney [ kbdm09 ] sun han zhao et al [ + 09 ] neville gallaher eliassi-rad [ nge-r09 ] ji sun danilevsky et al [ + 10 ] role discovery link prediction information network study extensively well krebs [ kre02 ] kubica moore schneider [ kms03 ] liben-nowell kleinberg [ l-nk03 ] wang han jia et al [ + 10 ] similarity search olap information network study many include tian hankin patel [ thp08 ] chen yan zhu et al [ + 08 ] evolution social information network study many researcher chakrabarti kumar tomkin [ ckt06 ] chi song zhou et al [ + 07 ] tang liu zhang nazeri [ tlzn08 ] xu zhang yu long [ xzyl08 ] kim han [ kh09 ] sun tang han [ + 10 ] spatial spatiotemporal datum mining study extensively collection paper miller han [ mh09 ] introduce textbook shekhar chawla [ sc03 ] hsu lee wang [ hlw07 ] spatial cluster algorithms study extensively chapter 10 11 book research conduct spatial warehouse olap stefanovic han koperski [ shk00 ] spatial spatiotemporal datum mining koperski han [ kh95 ] mamouli cao kollio hadjieleftheriou et al [ + 04 ] tsoukatos gunopulos [ tg01 ] hadjieleftheriou kollio gunopulos tsotra [ hkgt03 ] mining moving-object datum study many vlachos gunopulos kollio [ vgk02 ] tao faloutsos papadia liu [ tfpl04 ] li han kim gonzalez [ lhkg07 ] lee han whang [ lhw07 ] li ding han et al [ + 10 ] bibliography temporal spatial spatiotemporal datum mining research see collection roddick hornsby spiliopoulou [ rhs01 ] multimedium datum mining deep root image process pattern recognition study extensively many textbook include gonzalez wood [ gw07 ] russ [ rus06 ] duda hart stork [ dhs01 ] z zhang r zhang [ zz09 ] search mining multimedium datum study many ( see eg fayyad smyth [ fs93 ] faloutsos lin [ fl95 ] natsev rastogi 
630 chapter 13 datum mining trend research frontier shim [ nrs99 ] zaı̈ane han zhu [ zhz00 ] ) overview image mining method give hsu lee zhang [ hlz02 ] text datum analysis study extensively information retrieval many textbook survey article croft metzler strohman [ cms09 ] s buttcher c clarke g cormack [ bcc10 ] man raghavan schutze [ mrs08 ] grossman frieder [ gr04 ] baeza-yate riberio-neto [ byrn11 ] zhai [ zha08 ] feldman sanger [ fs06 ] berry [ ber03 ] weis indurkhya zhang damerau [ wizd04 ] text mining fast-developing field numerous paper publish recent year cover many topic topic model ( eg blei lafferty [ bl09 ] ) sentiment analysis ( eg pang lee [ pl07 ] ) contextual text mining ( eg mei zhai [ mz06 ] ) web mining another focuse theme book like chakrabarti [ cha03a ] liu [ liu06 ] berry [ ber03 ] web mining substantially improve search engine influential milestone work brin page [ bp98 ] kleinberg [ kle99 ] chakrabarti dom kumar et al [ + 99 ] kleinberg tomkin [ kt99 ] numerous result generate since search log mining ( eg silvestri [ sil10 ] ) blog mining ( eg mei liu su zhai [ mlsz06 ] ) mining online forum ( eg cong wang lin et al [ + 08 ] ) book survey stream datum system stream datum process include babu widom [ bw01 ] babcock babu datar et al [ + 02 ] muthukrishnan [ mut05 ] aggarwal [ agg06 ] stream datum mining research cover stream cube model ( eg chen dong han et al [ + 02 ] ) stream frequent pattern mining ( eg manku motwani [ mm02 ] karp papadimitriou shenker [ kps03 ] ) stream classification ( eg domingo hulten [ dh00 ] wang fan yu han [ wfyh03 ] aggarwal han wang yu [ ahwy04b ] ) stream cluster ( eg guha mishra motwani ’ callaghan [ gmmo00 ] aggarwal han wang yu [ ahwy03 ] ) many book discuss datum mining application financial datum analysis financial modele see example benninga [ ben08 ] higgin [ hig08 ] retail datum mining customer relationship management see example book berry linoff [ bl04 ] berson smith thearle [ bst99 ] telecommunication-related datum mining see example horak [ hor08 ] also book scientific datum analysis grossman kamath kegelmeyer et al [ + 01 ] kamath [ kam09 ] issue theoretical foundation datum mining address many researcher example mannila present summary study foundation datum mining [ man00 ] datum reduction view datum mining summarize new jersey datum reduction report barbará dumouchel faloutos et al [ + 97 ] datum compression view find study minimum description length principle grunwald rissanen [ gr07 ] pattern discovery point view datum mining address numerous machine learn datum mining study range association mining decision tree induction sequential pattern mining cluster probability theory point view popular statistic machine learn literature 
138 bibliographic note 631 bayesian network hierarchical bayesian model chapter 9 probabilistic graph model ( eg koller friedman [ kf09 ] ) kleinberg papadimitriou raghavan [ kpr98 ] present microeconomic view treat datum mining optimization problem study inductive database view include imielinski mannila [ im96 ] de raedt gun nijssen [ rgn10 ] statistical method datum analysis describe many book hastie tibshirani friedman [ htf09 ] freedman pisani purf [ fpp07 ] devore [ dev03 ] kutner nachtsheim neter li [ knnl04 ] dobson [ dob01 ] breiman friedman olshen stone [ bfos84 ] pinheiro bate [ pb00 ] johnson wichern [ jw02b ] huberty [ hub94 ] shumway stoffer [ ss05 ] miller [ mil98 ] visual datum mining popular book visual display datum information include tufte [ tuf90 tuf97 tuf01 ] summary technique visualize datum present cleveland [ cle93 ] dedicate visual datum mining book visual datum mining technique tool datum visualization mining soukup davidson [ sd02 ] book information visualization datum mining knowledge discovery edit fayyad grinstein wierse [ fgw01 ] contain collection article visual datum mining method ubiquitous invisible datum mining discuss many text include john [ joh99 ] article book edit kargupta joshi sivakumar yesha [ kjsy04 ] book business @ speed thought succeed digital economy gate [ gat00 ] discuss e-commerce customer relationship management provide interesting perspective datum mining future mena [ men03 ] informative book use datum mining detect prevent crime cover many form criminal activity range fraud detection money launder insurance crime identity crime intrusion detection datum mining issue regard privacy datum security address popularly literature book privacy security datum mining include thuraisingham [ thu04 ] aggarwal yu [ ay08 ] vaidya clifton zhu [ vcz10 ] fung wang fu yu [ fwfy10 ] research article include agrawal srikant [ as00 ] evfimievski srikant agrawal gehrke [ esag02 ] vaidya clifton [ vc03 ] differential privacy introduce dwork [ dwo06 ] study many hay rastogi miklau suciu [ hrms10 ] many discussion trend research direction datum mining various forum several book collection article issue kargupta han yu et al [ + 08 ] 

hierarchical algorithms 
hierarchical cluster basic concept  hierarchical cluster     generate cluster hierarchy ( draw dendrogram ) require specify k number cluster step 0 step 2 step 3 step 4 abcde c iterative refinement e step 4 agglomerative ( agne ) ab b deterministic  two category algorithms step 1 cde de step 3 step 2 step 1 step 0 divisive ( diana ) agglomerative start singleton cluster continuously merge two cluster time build bottom-up hierarchy cluster  divisive start huge macro-cluster split continuously two group generate top-down hierarchy cluster  2 
dendrogram show cluster merged  dendrogram decompose set datum object tree cluster multi-level nest partition  cluster datum object obtain cut dendrogram desire level connect component form cluster hierarchical cluster generate dendrogram ( hierarchy cluster ) 3 

algorithms [ music ] > > session be go examine agglomerative cluster algorithms 
agglomerative cluster algorithm  agne ( agglomerative nest ) ( kaufmann rousseeuw 1990 ) use single-link method dissimilarity matrix continuously merge node least dissimilarity eventually node belong cluster    10 10 10 9 9 9 8 8 8 7 7 7 6 6 6 5 5 5 4 4 4 3 3 3 2 2 2 1 1 1 0 0 0 1 2 3 4 5 6 7 8 9 10 0 0 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9 10  agglomerative cluster vary different similarity measure among cluster single link ( nearest neighbor )  complete link ( diameter )  2 average link ( group average )  centroid link ( centroid similarity )  already introduce general concept know agglomerative divideditive cluster algorithms look computer science point view think agglomerative cluster essentially bottom cluster mean start single den cluster mean every single element treat one cluster try merge nearest neighbor bigger bigger cluster eventually merge one cluster one bottom agglomerative cluster kaufmann rousseeuw s 1990 book describe one algorithm call agne agglomerative nest algorithm used single-link method call nearest neighbor method dissimilarity matrix try continuously merge node least dissimilarity nearest neighbor eventually node merged one cluster okay mean start singleton try find everyone s nearest neighbor check closest nearest neighbor try merge okay merge bigger cluster be look cluster cluster nearest neighbor be single-link decide one merge eventually will merge everything one cluster actually rear agglomerative cluster algorithms vary different similarity measure example discuss agne used nearest neighbor call single-link measure also use farthest neighbor diameter s complete link use group average mean average everything base distance every pair element average calculate distance base average link use central link mean look distance centroid cluster let s examine little detail different link 
single link vs complete link hierarchical cluster  single link ( nearest neighbor ) similarity two cluster similarity similar ( nearest neighbor ) member  local similarity-based emphasize close region ignore overall structure cluster  capable cluster non-elliptical shape group object  sensitive noise outlier  complete link ( diameter )  similarity two cluster similarity dissimilar member  merge two cluster form one smallest diameter  nonlocal behavior obtain compact shape cluster  sensitive outlier  3 x x x x first look single link call nearest neighbor okay similarity distance two cluster defined base similar closest element two cluster okay give example suppose one us one cuba want find closest point likely us key west kind merge essentially examine neighbor close region know overall structure cluster context able find irregular shape non-elliptical shape group object link also sensitive noise outlier obviously get outlier close one may may decide merge another approach call complete link check diameter cluster idea try define similarity two cluster base farthest neighbor mean de-similar element two object example define distance us cuba probably complete link may pick farthest point alaska s pretty far apart mean actually try merge two cluster form one smallest diameter merge together want final one pretty compact examine non-local behavior obtain compact shape cluster measure also sensitive outlier okay probably even want think alaska may think hawaii guam know try merge s pretty far apart look average link average link two cluster actually rather expensive compute want calculate average distance element one cluster element cluster mean calculate pair two cluster try average example number cluster c sub n sub number element cluster c sub b n sub b want care distance essentially get total number pair n sub time n sub b s pretty big know average time easy way efficient way calculate distance two cluster actually use centroid link centroid link mean calculate centroid cluster c sub centroid cluster c sub b distance two cluster defined distance two centroid 
agglomerative cluster average vs centroid link  agglomerative cluster average link x x average link average distance element one cluster element ( ie pair two cluster ) cb nb ca na  expensive compute  agglomerative cluster centroid link x x  centroid link distance centroid two cluster  group averaged agglomerative cluster ( gaac )  let two cluster ca cb merged caub new centroid n ca + n b cb ca ∪b =  na cardinality cluster ca ca centroid ca n + nb  similarity measure gaac average distance  agglomerative cluster ward ’ criterion  ward ’ criterion increase value sse criterion cluster n nb obtain merge ca u cb w ( ca ∪b ca ∪b ) − w ( c c ) = ( ca cb )  4 n + nb may even want put weight example cluster c sub many many point versus c cluster c sub b want take consideration introduce gaac group averaged agglomerative cluster mean assume two cluster c sub c sub b merged one cluster c sub unit b centroid defined get centroid c sub a will time number element cluster get centroid c c sub b cluster c sub b bigger c sub b want also time number element finally normalize okay gaac measure another interesting person define one call ward s criterion s center measure increase merge remember merge two cluster c sub c sub b c sub union b sse sum square distance increase cluster become bigger okay s increase want see original cluster s difference s increase have get okay defined used criterium essentially look distance two centroid base weight calculate increase [ music ] 

algorithms 
divisive cluster  diana ( divisive analysis ) ( kaufmann rousseeuw1990 )  implement statistical analysis package eg splus  inverse order agne eventually node form cluster 10 10 10 9 9 9 8 8 8 7 7 7 6 6 6 5 5 5 4 4 4 3 3 3 2 2 2 1 1 1 0 0 0 0 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9 10  divisive cluster top-down approach     2 process start root point one cluster recursively split higher level cluster build dendrogram consider global approach efficient compare agglomerative cluster 
algorithm design divisive cluster  choose cluster split  check sum square error cluster choose one largest value  splitting criterion determine split  one may use ward ’ criterion chase greater reduction difference sse criterion result split  categorical datum gini-index used  handle noise  3 use threshold determine termination criterion ( generate cluster small contain mainly noise ) 

cluster [ music ] hi subsequent session go discuss several extension hierarchical cluster 
extension hierarchical cluster  major weakness hierarchical cluster method  never undo do previously  scale well  time complexity least ( n2 ) n number total object  hierarchical cluster algorithms 2  birch ( 1996 ) use cf-tree incrementally adjust quality sub-cluster  cure ( 1998 ) represent cluster used set well-scatter representative point  chameleon ( 1999 ) use graph partition method k-nearest neighbor graph datum hierarchical cluster method look simple hand weakness hierarchical cluster method first weakness master never undo do previously divide method first try figure divide one cluster two subcluster divide two subcluster work subcluster try splitting never able merge back try adjust particular element even agglomerative cluster philosophy similar sense try merge two cluster one subsequent know analysis treat one one unit never split try see whether subcluster obtain far merged know require every split merge must final s high requirement generate high-quality cluster second problem master may scale well every time try merge try check possible pair complexity least n square want split try many different possible choice try find best split complexity also high development hierarchical cluster algorithms lecture go introduce three one birch develop 1996 use micro-cluster macro-cluster idea used cluster feature tree incrementally adjust quality subcluster second one go introduce call cure develop method essentially represent cluster used set well-scatter representative point third one chameleon develop 1999 used graph partition method k-nearest neighbor graph datum introduce three one one [ music ] 

birch micro-clusteringbased approach [ sound ] session go introduce interesting extension hierarchical cluster method call birch micro-cluster base approach birch abbreviation balance iterative reduce cluster used hierarchy develop group researcher university wisconsin 1996 
birch ( balanced iterative reduce cluster used hierarchy )  multiphase cluster algorithm ( zhang ramakrishnan & livny sigmod ’ 96 )  incrementally construct cf ( cluster feature ) tree hierarchical datum structure multiphase cluster  phase 1 scan db build initial in-memory cf tree ( multi-level compression datum try preserve inherent cluster structure datum )  phase 2 use arbitrary cluster algorithm cluster leaf node cftree  key idea multi-level cluster  low-level micro-cluster reduce complexity increase scalability  high-level macro-cluster leave enough flexibility high-level cluster  scale linearly 2 find good cluster single scan improve quality additional scan general philosophy incrementally construct cf tree call cluster feature tree hierarchical datum structure multiphase cluster phase 1 essentially scan database construct initial in-memory cf tree multi-level compression datum try preserve inherent cluster structure datum phase 2 used arbitrary cluster algorithm cluster leaf node cf-tree key idea multilevel cluster low level micro-cluster therefore reduce complexity increase scalability high level macro-cluster leave enough flexibility high level cluster used different cluster methodology birch method scale linearly mean find good cluster single scan improve quality additional scan 
cluster feature vector birch  cluster feature ( cf )  cf = ( 5 ( 1630 ) ( 54190 ) ) cf = ( n ls ss ) n number datum point ( 34 ) ( 26 ) ( 45 ) ( 47 ) ( 38 ) 10 9 8 n   7 ls linear sum n point ∑ x 1 n ss square sum n point ∑ x 1 6 5 4 2 3 2 1 0 0 1 2 3 4 5 6 7 8 9 10  cluster feature 3  summary statistic give sub-cluster 0-th 1st 2nd moment sub-cluster statistical point view  register crucial measurement compute cluster utilize storage efficiently cluster feature first introduce cf vector birch birch cluster feature essentially suppose get five point one cluster okay suppose five point position okay cf vector contain three component one number datum point second linear sum point cluster third one square sum n point okay probably see first one 5 mean 5 point second one act linear sum dimension okay third one actually square sum dimension cluster feature essentially summary statistic give sub-cluster consider number zeroth first one linear second one second moment sub-cluster statistic point view mean register crucial measurement compute cluster utilize storage quite efficiently 
measure cluster centroid radius diameter   centroid x 0 x “ middle ” cluster  n number point cluster   x i-th point cluster  radius r  average distance member object centroid  square root average distance point cluster centroid  diameter  average pairwise distance within cluster  square root average mean square distance pair point cluster   ∑ xi n  x0 = n 4   2 ∑ ( xi − x 0 ) n = = n   2 ∑∑ ( xi − x j ) n n j n ( n − 1 ) look general concept centroid radius diameter okay centroid essentially center cluster okay suppose vector n dimension x sub i okay centroid essentially compute sum point cluster divide number point cluster get centroid cluster okay radius actually average distance member object centroid essentially every one get difference centroid use sum square distance divide number point cluster take square root essentially s square root average distance point cluster centroid diameter diameter essentially average pairwise distance within cluster mean x x j within cluster essentially want find total n time n minus 1 pair sum pairwise distance get square root s diameter 
cf tree structure birch  incremental insertion new point 5 b=7 root cf1 cf2 cf3 cf6 ( similar b+-tree ) child1 child2 child3 child6 l=6  point input  find closest leaf entry non-leaf node  add point leaf entry cf1 cf2 cf3 cf5 update cf child1 child2 child3 child5  entry diameter > max_diameter leaf node leaf node  split leaf possibly parent cf6 next cf4 next prev cf1 cf2 prev cf1 cf2  cf tree two parameter  branch factor maximum  cf tree height-balanced tree number child store cluster feature ( cfs )  maximum diameter sub non-leaf node store sum cfs cluster store leaf node child look cf tree structure birch cf tree structure essentially much like b+-tree incremental insertion new point mean new point come okay find closest leaf entry start root okay try traverse find closest entry add point leaf entry update cluster feature cf entry diameter greater maximum diameter will split leaf s possibly even able split parent base b+-tree algorithm cf tree two parameter one call branch factor mean maximum number child another maximum diameter sub-cluster store leaf node cf tree essentially height-balanced tree store cluster feature non-leaf node store sum cluster feature child 
birch scalable flexible cluster method  integration agglomerative cluster ( flexible ) cluster method  low-level micro-cluster  explore cp-feature birch tree structure  preserve inherent cluster structure datum  higher-level macro-cluster  provide sufficient flexibility integration cluster method  impact many cluster method application  concern 6  sensitive insertion order datum point  due fix size leaf node cluster may natural  cluster tend spherical give radius diameter measure see birch interesting algorithm integration agglomerative cluster flexible cluster method low level micro-cluster explore cf feature birch tree structure preserve inherent cluster structure datum high level macro-cluster provide sufficient flexibility integration cluster method method act impact many cluster method application large datum set concern one birch tree still sensitive insertion order datum point another since leaf node fix size cluster obtain may natural also cluster tend spherical give radius diameter measure major parameter still pretty interesting algorithm generate quite effective cluster [ music ] 

10 9 8 7 6 5 4 3 2 1 
author jiawei han bliss professor engineering department computer science university illinois urbana-champaign receive numerous award contribution research knowledge discovery datum mining include acm sigkdd innovation award ( 2004 ) ieee computer society technical achievement award ( 2005 ) ieee w wallace mcdowell award ( 2009 ) fellow acm ieee serve founding editor-in-chief acm transaction knowledge discovery datum ( 2006–2011 ) editorial board member several journal include ieee transaction knowledge datum engineering datum mining knowledge discovery micheline kamber master ’ degree computer science ( specialize artificial intelligence ) concordium university montreal quebec nserc scholar work researcher mcgill university simon fraser university switzerland background datum mining passion writing easyto-understand term help make text favorite professional instructor student jian pei currently associate professor school compute science simon fraser university british columbia receive degree compute science simon fraser university 2002 dr jiawei han ’ supervision publish prolifically premier academic forum datum mining databasis web search information retrieval actively serve academic community publication receive thousand citation several prestigious award associate editor several datum mining datum analytic journal xxxv 
2 get know datum ’ tempting jump straight mining first need get datum ready involve closer look attribute datum value real-world datum typically noisy enormous volume ( often several gigabyte ) may originate hodgepodge heterogenous source chapter get familiar datum knowledge datum useful datum preprocess ( see chapter 3 ) first major task datum mining process want know follow type attribute field make datum kind value attribute attribute discrete continuous-valu datum look like value distribute way visualize datum get better sense spot outlier measure similarity datum object respect other gain insight datum help subsequent analysis “ learn datum ’ helpful datum preprocess ” begin section 21 study various attribute type include nominal attribute binary attribute ordinal attribute numeric attribute basic statistical description used learn attribute ’ value describe section 22 give temperature attribute example determine mean ( average value ) median ( middle value ) mode ( common value ) measure central tendency give us idea “ middle ” center distribution know basic statistic regard attribute make easier fill miss value smooth noisy value spot outlier datum preprocess knowledge attribute attribute value also help fix inconsistency incur datum integration plot measure central tendency show us datum symmetric skewer quantile plot histogram scatter plot graphic display basic statistical description useful datum preprocess provide insight area mining field datum visualization provide many additional technique view datum graphical mean help identify relation trend biase “ hide ” unstructured datum set technique may simple scatter-plot matrix ( datum mining concept technique doi b978-0-12-381479-100002-2 c 2012 elsevier right re-serve 39 
40 chapter 2 get know datum two attribute map onto 2-d grid ) sophisticated method treemaps ( hierarchical partition screen display base attribute value ) datum visualization technique describe section 23 finally may want examine similar ( dissimilar ) datum object example suppose database datum object patient describe symptom may want find similarity dissimilarity individual patient information allow us find cluster like patient within datum set dissimilarity object may also used detect outlier datum perform nearest-neighbor classification ( cluster topic chapter 10 11 nearest-neighbor classification discuss chapter 9 ) many measure assess similarity dissimilarity general measure refer proximity measure think proximity two object function distance attribute value although proximity also calculate base probability rather actual distance measure datum proximity describe section 24 summary end chapter know different attribute type basic statistical measure describe central tendency dispersion ( spread ) attribute datum also know technique visualize attribute distribution compute similarity dissimilarity object 21 datum object attribute type datum set make datum object datum object represent entity—in sale database object may customer store item sale medical database object may patient university database object may student professor course datum object typically describe attribute datum object also refer sample example instance datum point object datum object store database datum tuple row database correspond datum object column correspond attribute section define attribute look various attribute type 211 attribute attribute datum field represent characteristic feature datum object noun attribute dimension feature variable often used interchangeably literature term dimension commonly used datum warehousing machine learn literature tend use term feature statistician prefer term variable datum mining database professional commonly use term attribute well attribute describe customer object include example customer id name address observed value give attribute know observation set attribute used describe give object call attribute vector ( feature vector ) distribution datum involve one attribute ( variable ) call univariate bivariate distribution involve two attribute 
21 datum object attribute type 41 type attribute determine set possible values—nominal binary ordinal numeric—the attribute follow subsection introduce type 212 nominal attribute nominal mean “ relate ” value nominal attribute symbol name thing value represent kind category code state nominal attribute also refer categorical value meaningful order computer science value also know enumeration example 21 nominal attribute suppose hair color marital status two attribute describe person object application possible value hair color black brown blond red auburn gray white attribute marital status take value single married divorce widow hair color marital status nominal attribute another example nominal attribute occupation value teacher dentist programmer farmer although say value nominal attribute symbol “ name thing ” possible represent symbol “ name ” number hair color instance assign code 0 black 1 brown another example customor id possible value numeric however case number intend used quantitatively mathematical operation value nominal attribute meaningful make sense subtract one customer id number another unlike say subtract age value another ( age numeric attribute ) even though nominal attribute may integer value consider numeric attribute integer meant used quantitatively say numeric attribute section 215 nominal attribute value meaningful order quantitative make sense find mean ( average ) value median ( middle ) value attribute give set object one thing interest however attribute ’ commonly occur value value know mode one measure central tendency learn measure central tendency section 22 213 binary attribute binary attribute nominal attribute two category state 0 1 0 typically mean attribute absent 1 mean present binary attribute refer boolean two state correspond true false example 22 binary attribute give attribute smoker describe patient object 1 indicate patient smoke 0 indicate patient similarly suppose 
42 chapter 2 get know datum patient undergo medical test two possible outcome attribute medical test binary value 1 mean result test patient positive 0 mean result negative binary attribute symmetric state equally valuable carry weight preference outcome code 0 one example can attribute gender state male female binary attribute asymmetric outcome state equally important positive negative outcome medical test hiv convention code important outcome usually rarest one 1 ( eg hiv positive ) 0 ( eg hiv negative ) 214 ordinal attribute ordinal attribute attribute possible value meaningful order ranking among magnitude successive value know example 23 ordinal attribute suppose drink size correspond size drink available fast-food restaurant nominal attribute three possible value small medium large value meaningful sequence ( correspond increase drink size ) however tell value much bigger say medium large example ordinal attribute include grade ( eg + a− + ) professional rank professional rank enumerate sequential order example assistant associate full professor private private first class specialist corporal sergeant army rank ordinal attribute useful register subjective assessment quality measure objectively thus ordinal attribute often used survey rating one survey participant ask rate satisfied customer customer satisfaction follow ordinal category 0 dissatisfied 1 somewhat dissatisfied 2 neutral 3 satisfied 4 satisfied ordinal attribute may also obtain discretization numeric quantity splitting value range finite number order category describe chapter 3 datum reduction central tendency ordinal attribute represent mode median ( middle value order sequence ) mean defined note nominal binary ordinal attribute qualitative describe feature object without give actual size quantity value qualitative attribute typically word represent category integer used represent computer code category opposed measurable quantity ( eg 0 small drink size 1 medium 2 large ) follow subsection look numeric attribute provide quantitative measurement object 
21 datum object attribute type 215 43 numeric attribute numeric attribute quantitative measurable quantity represent integer real value numeric attribute interval-scaled ratio-scale interval-scaled attribute interval-scaled attribute measure scale equal-size unit value interval-scaled attribute order positive 0 negative thus addition provide ranking value attribute allow us compare quantify difference value example 24 interval-scaled attribute temperature attribute interval-scaled suppose outdoor temperature value number different day day object order value obtain ranking object respect temperature addition quantify difference value example temperature 20◦ c five degree higher temperature 15◦ c calendar date another example instance year 2002 2010 eight year apart temperature celsius fahrenheit true zero-point neither 0◦ c 0◦ f indicate “ ” ( celsius scale example unit measurement 100 difference melt temperature boil temperature water atmospheric pressure ) although compute difference temperature value talk one temperature value multiple another without true zero say instance 10◦ c twice warm 5◦ c speak value term ratio similarly true zero-point calendar date ( year 0 correspond begin time ) bring us ratio-scale attribute true zero-point exit interval-scaled attribute numeric compute mean value addition median mode measure central tendency ratio-scale attribute ratio-scale attribute numeric attribute inherent zero-point measurement ratio-scale speak value multiple ( ratio ) another value addition value order also compute difference value well mean median mode example 25 ratio-scale attribute unlike temperature celsius fahrenheit kelvin ( k ) temperature scale consider true zero-point ( 0◦ k = −27315◦ c ) point particle comprise matter zero kinetic energy example ratio-scale attribute include count attribute year experience ( eg object employee ) number word ( eg object document ) additional example include attribute measure weight height latitude longitude 
44 chapter 2 get know datum coordinate ( eg cluster house ) monetary quantity ( eg 100 time richer $ 100 $ 1 ) 216 discrete versus continuous attribute presentation organized attribute nominal binary ordinal numeric type many way organize attribute type type mutually exclusive classification algorithms develop field machine learn often talk attribute either discrete continuous type may processed differently discrete attribute finite countably infinite set value may may represent integer attribute hair color smoker medical test drink size finite number value discrete note discrete attribute may numeric value 0 1 binary attribute value 0 110 attribute age attribute countably infinite set possible value infinite value put one-to-one correspondence natural number example attribute customer id countably infinite number customer grow infinity reality actual set value countable ( value put one-to-one correspondence set integer ) zip code another example attribute discrete continuous term numeric attribute continuous attribute often used interchangeably literature ( confuse classic sense continuous value real number whereas numeric value either integer real number ) practice real value represent used finite number digit continuous attribute typically represent floating-point variable 22 basic statistical description datum datum preprocess successful essential overall picture datum basic statistical description used identify property datum highlight datum value treat noise outlier section discuss three area basic statistical description start measure central tendency ( section 221 ) measure location middle center datum distribution intuitively speaking give attribute value fall particular discuss mean median mode midrange addition assess central tendency datum set also would like idea dispersion datum datum spread common datum dispersion measure range quartile interquartile range five-number summary boxplot variance standard deviation datum measure useful identify outlier describe section 222 finally use many graphic display basic statistical description visually inspect datum ( section 223 ) statistical graphical datum presentation software 
22 basic statistical description datum 45 package include bar chart pie chart line graph popular display datum summary distribution include quantile plot quantile–quantile plot histogram scatter plot 221 measure central tendency mean median mode section look various way measure central tendency datum suppose attribute x like salary record set object let x1 x2 xn set n observed value observation x value may also refer datum set ( x ) plot observation salary would value fall give us idea central tendency datum measure central tendency include mean median mode midrange common effective numeric measure “ center ” set datum ( arithmetic ) mean let x1 x2 xn set n value observation numeric attribute x like salary mean set value n x x̄ = xi i=1 n = x1 + x2 + · · · + xn n ( 21 ) correspond built-in aggregate function average ( avg ( ) sql ) provide relational database system example 26 mean suppose follow value salary ( thousand dollar ) show increase order 30 36 47 50 52 52 56 60 63 70 70 used eq ( 21 ) 30 + 36 + 47 + 50 + 52 + 52 + 56 + 60 + 63 + 70 + 70 + 110 12 696 = = 58 12 x̄ = thus mean salary $ 58000 sometimes value xi set may associate weight wi = 1 n weight reflect significance importance occurrence frequency attach respective value case compute n x x̄ = wi xi i=1 n x = w1 x1 + w2 x2 + · · · + wn xn w1 + w2 + · · · + wn wi i=1 call weight arithmetic mean weight average ( 22 ) 
46 chapter 2 get know datum although mean singlemost useful quantity describe datum set always best way measure center datum major problem mean sensitivity extreme ( eg outlier ) value even small number extreme value corrupt mean example mean salary company may substantially push highly paid manager similarly mean score class exam can pull quite bit low score offset effect cause small number extreme value instead use trim mean mean obtain chop value high low extreme example sort value observed salary remove top bottom 2 % compute mean avoid trimming large portion ( 20 % ) end result loss valuable information skewer ( asymmetric ) datum better measure center datum median middle value set order datum value value separate higher half datum set lower half probability statistic median generally apply numeric datum however may extend concept ordinal datum suppose give datum set n value attribute x sort increase order n odd median middle value order set n even median unique two middlemost value value x numeric attribute case convention median take average two middlemost value example 27 median let ’ find median datum example datum already sort increase order even number observation ( ie 12 ) therefore median unique value within two middlemost value 52 56 ( within sixth seventh value list ) convention assign = 108 average two middlemost value median 52+56 2 2 = thus median $ 54000 suppose first 11 value list give odd number value median middlemost value sixth value list value $ 52000 median expensive compute large number observation numeric attribute however easily approximate value assume datum group interval accord xi datum value frequency ( ie number datum value ) interval know example employee may group accord annual salary interval $ 10–20000 $ 20–30000 let interval contain median frequency median interval approximate median entire datum set ( eg median salary ) interpolation used formula  p n 2 − freq l median = l1 + width ( 23 ) freqmedian l1 lower median interval n number value  pboundary entire datum set freq l sum frequency interval 
22 basic statistical description datum 47 lower median interval freqmedian frequency median interval width width median interval mode another measure central tendency mode set datum value occur frequently set therefore determine qualitative quantitative attribute possible greatest frequency correspond several different value result one mode datum set one two three mode respectively call unimodal bimodal trimodal general datum set two mode multimodal extreme datum value occur mode example 28 mode datum example 26 bimodal two mode $ 52000 $ 70000 unimodal numeric datum moderately skewer ( asymmetrical ) follow empirical relation mean − mode ≈ 3 × ( mean − median ) ( 24 ) imply mode unimodal frequency curf moderately skewer easily approximate mean median value know midrange also used assess central tendency numeric datum set average largest smallest value set measure easy compute used sql aggregate function max ( ) min ( ) example 29 midrange midrange datum example 26 30000+110000 2 = $ 70000 unimodal frequency curve perfect symmetric datum distribution mean median mode center value show figure 21 ( ) datum real application symmetric may instead either positively skewer mode occur value smaller median ( figure 21b ) negatively skewer mode occur value greater median ( figure 21c ) mean median mode mode mean median ( ) symmetric datum ( b ) positively skewer datum mean mode median ( c ) negatively skewer datum figure 21 mean median mode symmetric versus positively negatively skewer datum 
48 chapter 2 get know datum 222 measure dispersion datum range quartile variance standard deviation interquartile range look measure assess dispersion spread numeric datum measure include range quantile quartile percentile interquartile range five-number summary display boxplot useful identify outlier variance standard deviation also indicate spread datum distribution range quartile interquartile range start let ’ study range quantile quartile percentile interquartile range measure datum dispersion let x1 x2 xn set observation numeric attribute x range set difference largest ( max ( ) ) smallest ( min ( ) ) value suppose datum attribute x sort increase numeric order imagine pick certain datum point split datum distribution equal-size consecutive set figure datum point call quantile quantile point take regular interval datum distribution divide essentially equalsize consecutive set ( say “ essentially ” may datum value x divide datum exactly equal-sized subset readability refer equal ) kth q-quantile give datum distribution value x q datum value less x ( q − k ) q datum value x k integer 0 < k < q q − 1 q-quantile 2-quantile datum point divide lower upper half datum distribution correspond median 4-quantiles three datum point split datum distribution four equal part part represent one-fourth datum distribution commonly refer quartile 100-quantile commonly refer percentile divide datum distribution 100 equal-sized consecutive set median quartile percentile widely used form quantile 25 % q1 q2 q3 median 75th 25th percentile percentile figure 22 plot datum distribution attribute x quantile plot quartile three quartile divide distribution four equal-size consecutive subset second quartile correspond median 
22 basic statistical description datum 49 quartile give indication distribution ’ center spread shape first quartile denote q1 25th percentile cut lowest 25 % datum third quartile denote q3 75th percentile—it cut lowest 75 % ( highest 25 % ) datum second quartile 50th percentile median give center datum distribution distance first third quartile simple measure spread give range cover middle half datum distance call interquartile range ( iqr ) defined iqr = q3 − q1 ( 25 ) example 210 interquartile range quartile three value split sort datum set four equal part datum example 26 contain 12 observation already sort increase order thus quartile datum third sixth ninth value respectively sort list therefore q1 = $ 47000 q3 $ 63000 thus interquartile range iqr = 63 − 47 = $ 16000 ( note sixth value median $ 52000 although datum set two median since number datum value even ) five-number summary boxplot outlier single numeric measure spread ( eg iqr ) useful describe skewer distribution look symmetric skewer datum distribution figure 21 symmetric distribution median ( measure central tendency ) split datum equal-size half occur skewer distribution therefore informative also provide two quartile q1 q3 along median common rule thumb identify suspect outlier single value fall least 15 × iqr third quartile first quartile q1 median q3 together contain information endpoint ( eg tail ) datum fuller summary shape distribution obtain provide lowest highest datum value well know five-number summary five-number summary distribution consist median ( q2 ) quartile q1 q3 smallest largest individual observation written order minimum q1 median q3 maximum boxplot popular way visualize distribution boxplot incorporate five-number summary follow typically end box quartile box length interquartile range median marked line within box two line ( call whisker ) outside box extend smallest ( minimum ) largest ( maximum ) observation 
50 chapter 2 get know datum 220 200 180 160 unit price ( $ ) 140 120 100 80 60 40 20 branch 1 branch 2 branch 3 branch 4 figure 23 boxplot unit price datum item sell four branch allelectronic give time period deal moderate number observation worthwhile plot potential outlier individually boxplot whisker extend extreme low high observation value less 15 × iqr beyond quartile otherwise whisker terminate extreme observation occur within 15 × iqr quartile remain case plot individually boxplot used comparison several set compatible datum example 211 boxplot figure 23 show boxplot unit price datum item sell four branch allelectronic give time period branch 1 see median price item sell $ 80 q1 $ 60 q3 $ 100 notice two outlying observation branch plot individually value 175 202 15 time iqr 40 boxplot compute ( n log n ) time approximate boxplot compute linear sublinear time depend quality guarantee require variance standard deviation variance standard deviation measure datum dispersion indicate spread datum distribution low standard deviation mean datum observation tend close mean high standard deviation indicate datum spread large range value 
22 basic statistical description datum variance n observation x1 x2 xn numeric attribute x n n x x 1 1 σ2 = ( xi − x̄ ) 2 = xi2 − x̄ 2 n n i=1 51 ( 26 ) i=1 x̄ mean value observation defined eq ( 21 ) standard deviation σ observation square root variance σ 2 example 212 variance standard deviation example 26 find x̄ = $ 58000 used eq ( 21 ) mean determine variance standard deviation datum example set n = 12 use eq ( 26 ) obtain 1 ( 302 + 362 + 472 + 1102 ) − 582 12 ≈ 37917 √ σ ≈ 37917 ≈ 1947 σ2 = basic property standard deviation σ measure spread follow σ measure spread mean consider mean choose measure center σ = 0 spread observation value otherwise σ > 0 importantly observation unlikely several standard deviation away mathematically used chebyshev ’ inequality show  mean  least 1 − k12 × 100 % observation k standard deviation mean therefore standard deviation good indicator spread datum set computation variance standard deviation scalable large databasis 223 graphic display basic statistical description datum section study graphic display basic statistical description include quantile plot quantile–quantile plot histogram scatter plot graph helpful visual inspection datum useful datum preprocess first three show univariate distribution ( ie datum one attribute ) scatter plot show bivariate distribution ( ie involve two attribute ) quantile plot follow subsection cover common graphic display datum distribution quantile plot simple effective way first look univariate datum distribution first display datum give attribute ( allow user 
52 chapter 2 get know datum assess overall behavior unusual occurrence ) second plot quantile information ( see section 222 ) let xi = 1 n datum sort increase order x1 smallest observation xn largest ordinal numeric attribute x observation xi pair percentage fi indicate approximately fi × 100 % datum value xi say “ approximately ” may value exactly fraction fi datum xi note 025 percentile correspond quartile q1 050 percentile median 075 percentile q3 let fi = − 05 n ( 27 ) 1 number increase equal step n range 2n ( slightly 1 0 ) 1 − 2n ( slightly 1 ) quantile plot xi graph fi allow us compare different distribution base quantile example give quantile plot sale datum two different time period compare q1 median q3 fi value glance example 213 quantile plot figure 24 show quantile plot unit price datum table 21 quantile–quantile plot unit price ( $ ) quantile–quantile plot q-q plot graph quantile one univariate distribution corresponding quantile another powerful visualization tool allow user view whether shift go one distribution another suppose two set observation attribute variable unit price take two different branch location let x1 xn datum first branch y1 ym datum second datum set sort increase order = n ( ie number point set ) simply plot yi xi yi xi ( − 05 ) n quantile respective datum set < n ( ie second branch fewer observation first ) point q-q plot yi ( − 05 ) m quantile 140 120 100 80 60 40 20 0 000 q3 median q1 025 050 f-value 075 figure 24 quantile plot unit price datum table 21 100 
22 basic statistical description datum 53 table 21 set unit price datum item sell branch allelectronic unit price ( $ ) count item sell 40 43 47 − 74 75 78 − 115 117 120 275 300 250 − 360 515 540 − 320 270 350 branch 2 ( unit price $ ) 120 110 q3 100 median 90 80 70 q1 60 50 40 40 50 60 70 80 90 branch 1 ( unit price $ ) 100 110 120 figure 25 q-q plot unit price datum two allelectronic branch datum plot ( − 05 ) m quantile x datum computation typically involve interpolation example 214 quantile–quantile plot figure 25 show quantile–quantile plot unit price datum item sell two branch allelectronic give time period point correspond quantile datum set show unit price item sell branch 1 versus branch 2 quantile ( aid comparison straight line represent case give quantile unit price branch darker point correspond datum q1 median q3 respectively ) see example q1 unit price item sell branch 1 slightly less branch word 25 % item sell branch 1 less 
54 chapter 2 get know datum equal $ 60 25 % item sell branch 2 less equal $ 64 50th percentile ( marked median also q2 ) see 50 % item sell branch 1 less $ 78 50 % item branch 2 less $ 85 general note shift distribution branch 1 respect branch 2 unit price item sell branch 1 tend lower branch 2 histogram histogram ( frequency histogram ) least century old widely used “ histos ” mean pole mast “ gram ” mean chart histogram chart pole plot histogram graphical method summarize distribution give attribute x x nominal automobile model item type pole vertical bar draw know value x height bar indicate frequency ( ie count ) x value result graph commonly know bar chart x numeric term histogram prefer range value x partition disjoint consecutive subrange subrange refer bucket bin disjoint subset datum distribution x range bucket know width typically bucket equal width example price attribute value range $ 1 $ 200 ( round nearest dollar ) partition subrange 1 20 21 40 41 60 subrange bar draw height represent total count item observed within subrange histogram partition rule discuss chapter 3 datum reduction example 215 histogram figure 26 show histogram datum set table 21 bucket ( bin ) defined equal-width range represent $ 20 increment frequency count item sell although histogram widely used may effective quantile plot q-q plot boxplot method compare group univariate observation scatter plot datum correlation scatter plot one effective graphical method determine appear relationship pattern trend two numeric attribute construct scatter plot pair value treat pair coordinate algebraic sense plot point plane figure 27 show scatter plot set datum table 21 scatter plot useful method provide first look bivariate datum see cluster point outlier explore possibility correlation relationship two attribute x correlated one attribute imply correlation positive negative null ( uncorrelated ) figure 28 show example positive negative correlation two attribute plot point pattern slope 
22 basic statistical description datum 55 6000 count item sell 5000 4000 3000 2000 1000 0 40–59 60–79 80–99 unit price ( $ ) 100–119 120–139 figure 26 histogram table 21 datum set 700 item sell 600 500 400 300 200 100 0 0 20 40 60 80 unit price ( $ ) 100 120 140 figure 27 scatter plot table 21 datum set ( ) ( b ) figure 28 scatter plot used find ( ) positive ( b ) negative correlation attribute 
56 chapter 2 get know datum figure 29 three case observed correlation two plot attribute datum set lower left upper right mean value x increase value increase suggest positive correlation ( figure 28a ) pattern plot point slope upper left lower right value x increase value decrease suggest negative correlation ( figure 28b ) line best fit draw study correlation variable statistical test correlation give chapter 3 datum integration ( eq ( 33 ) ) figure 29 show three case correlation relationship two attribute give datum set section 232 show scatter plot extend n attribute result scatter-plot matrix conclusion basic datum description ( eg measure central tendency measure dispersion ) graphic statistical display ( eg quantile plot histogram scatter plot ) provide valuable insight overall behavior datum help identify noise outlier especially useful datum clean 23 datum visualization convey datum user effectively datum visualization aim communicate datum clearly effectively graphical representation datum visualization used extensively many applications—for example work report manage business operation tracking progress task popularly take advantage visualization technique discover datum relationship otherwise easily observable look raw datum nowadays person also use datum visualization create fun interesting graphic section briefly introduce basic concept datum visualization start multidimensional datum store relational databasis discuss several representative approach include pixel-oriented technique geometric projection technique icon-based technique hierarchical graph-based technique discuss visualization complex datum relation 
23 datum visualization 231 57 pixel-oriented visualization technique simple way visualize value dimension use pixel color pixel reflect dimension ’ value datum set dimension pixel-oriented technique create window screen one dimension dimension value record map pixel corresponding position window color pixel reflect corresponding value inside window datum value arrange global order share window global order may obtain sort datum record way ’ meaningful task hand example 216 pixel-oriented visualization allelectronic maintain customer information table consist four dimension income credit limit transaction volume age analyze correlation income attribute visualization sort customer income-ascending order use order lay customer datum four visualization window show figure pixel color choose smaller value lighter shading used pixelbased visualization easily observe follow credit limit increase income increase customer whose income middle range likely purchase allelectronic clear correlation income age pixel-oriented technique datum record also order query-dependent way example give point query sort record descend order similarity point query fill window layer datum record linear way may work well wide window first pixel row far away last pixel previous row though next global order moreover pixel next one window even though two next global order solve problem lay datum record space-filling curve ( ) income ( b ) credit_limit ( c ) transaction_volume ( ) age figure 210 pixel-oriented visualization four attribute sort customer income ascend order 
58 chapter 2 get know datum ( ) hilbert curve ( b ) gray code ( c ) z-curve figure 211 frequently used 2-d space-filling curf one datum record dim 6 dim 6 dim 5 dim 1 dim 4 dim 2 dim 3 ( ) dim 5 dim 1 dim 4 dim 2 dim 3 ( b ) figure 212 circle segment technique ( ) represent datum record circle segment ( b ) layer pixel circle segment fill window space-filling curve curve range cover entire n-dimensional unit hypercube since visualization window 2-d use 2-d space-filling curve figure 211 show frequently used 2-d space-filling curf note window rectangular example circle segment technique used window shape segment circle illustrated figure 212 technique ease comparison dimension dimension window locate side side form circle 232 geometric projection visualization technique drawback pixel-oriented visualization technique help us much understand distribution datum multidimensional space example show whether dense area multidimensional subspace geometric 
23 datum visualization 59 80 70 60 50 40 30 20 10 0 0 10 20 30 40 x 50 60 70 80 figure 213 visualization 2-d datum set used scatter plot source rareevent-geoinformatica06pdf projection technique help user find interesting projection multidimensional datum set central challenge geometric projection technique try address visualize high-dimensional space 2-d display scatter plot display 2-d datum point used cartesian coordinate third dimension add used different color shape represent different datum point figure 213 show example x two spatial attribute third dimension represent different shape visualization see point type “ + ” “ × ” tend colocate 3-d scatter plot used three axe cartesian coordinate system also used color display 4-d datum point ( figure 214 ) datum set four dimension scatter plot usually ineffective scatter-plot matrix technique useful extension scatter plot ndimensional datum set scatter-plot matrix n × n grid 2-d scatter plot provide visualization dimension every dimension figure 215 show example visualize iris datum set datum set consist 450 sample three species iris flower five dimension datum set length width sepal petal species scatter-plot matrix become less effective dimensionality increase another popular technique call parallel coordinate handle higher dimensionality visualize n-dimensional datum point parallel coordinate technique draw n equally space axe one dimension parallel one display axe 
60 chapter 2 get know datum figure 214 visualization 3-d datum set used scatter plot source http scatter plotjpg datum record represent polygonal line intersect axis point corresponding associate dimension value ( figure 216 ) major limitation parallel coordinate technique effectively show datum set many record even datum set several thousand record visual clutter overlap often reduce readability visualization make pattern hard find 233 icon-based visualization technique icon-based visualization technique use small icon represent multidimensional datum value look two popular icon-based technique chernoff face stick figure chernoff face introduce 1973 statistician herman chernoff display multidimensional datum 18 variable ( dimension ) cartoon human face ( figure 217 ) chernoff face help reveal trend datum component face eye ears mouth nose represent value dimension shape size placement orientation example dimension map follow facial characteristic eye size eye spacing nose length nose width mouth curvature mouth width mouth openness pupil size eyebrow slant eye eccentricity head eccentricity chernoff face make use ability human mind recognize small difference facial characteristic assimilate many facial characteristic 
23 datum visualization 10 30 50 70 0 10 61 20 80 70 sepal length ( mm ) 60 50 40 70 50 petal length ( mm ) 30 10 45 40 35 30 25 20 sepal width ( mm ) 25 20 15 10 5 0 petal width ( mm ) 40 50 60 70 80 iris species 20 setosa 30 versicolor 40 virginica figure 215 visualization iris datum set used scatter-plot matrix source http gsgscmatgif view large table datum tedious condense datum chernoff face make datum easier user digest way facilitate visualization regularity irregularity present datum although power relate multiple relationship limit another limitation specific datum value show furthermore facial feature vary perceive importance mean similarity two face ( represent two multidimensional datum point ) vary depend order dimension assign facial characteristic therefore mapping carefully choose eye size eyebrow slant find important asymmetrical chernoff face propose extension original technique since face vertical symmetry ( along y-axis ) left right side face identical waste space asymmetrical chernoff face double number facial characteristic thus allow 36 dimension display stick figure visualization technique map multidimensional datum five-piece stick figure figure four limb body two dimension map display ( x ) axe remain dimension map angle 
62 chapter 2 get know datum 10 5 x 0 –5 –10 ⫻1 ⫻2 ⫻3 ⫻4 ⫻5 ⫻6 ⫻7 ⫻8 ⫻9 ⫻10 figure 216 visualization used parallel coordinate source parallel coordithml figure 217 chernoff face face represent n-dimensional datum point ( n ≤ 18 ) or length limb figure 218 show census datum age income map display axe remain dimension ( gender education ) map stick figure datum item relatively dense respect two display dimension result visualization show texture pattern reflect datum trend 
23 datum visualization 63 figure 218 census datum represent used stick figure source professor g grinstein department computer science university massachusett lowell 234 hierarchical visualization technique visualization technique discuss far focus visualize multiple dimension simultaneously however large datum set high dimensionality would difficult visualize dimension time hierarchical visualization technique partition dimension subset ( ie subspace ) subspace visualize hierarchical manner “ worlds-within-world ” also know n-vision representative hierarchical visualization method suppose want visualize 6-d datum set dimension f x1 x5 want observe dimension f change respect dimension first fix value dimension x3 x4 x5 select value say c3 c4 c5 visualize f x1 x2 used 3-d plot call world show figure position origin inner world locate point ( c3 c4 c5 ) outer world another 3-d plot used dimension x3 x4 x5 user interactively change outer world location origin inner world user view result change inner world moreover user vary dimension used inner world outer world give dimension level world used method call “ worlds-withinworld ” another example hierarchical visualization method tree-maps display hierarchical datum set nest rectangle example figure 220 show tree-map visualize google news story news story organized seven category show large rectangle unique color within category ( ie rectangle top level ) news story partition smaller subcategory 
64 chapter 2 get know datum figure 219 “ worlds-within-world ” ( also know n-vision ) source http 1dipstick5gif 235 visualize complex datum relation early day visualization technique mainly numeric datum recently non-numeric datum text social network become available visualize analyze datum attract lot interest many new visualization technique dedicate kind datum example many person web tag various object picture blog entry product reviews tag cloud visualization statistic user-generated tag often tag cloud tag list alphabetically user-preferred order importance tag indicated font size color figure 221 show tag cloud visualize popular tag used web site tag cloud often used two way first tag cloud single item use size tag represent number time tag apply item different user second visualize tag statistic multiple item use size tag represent number item tag apply popularity tag addition complex datum complex relation among datum entry also raise challenge visualization example figure 222 used disease influence graph visualize correlation disease node graph disease size node proportional prevalence corresponding disease two node link edge corresponding disease strong correlation width edge proportional strength correlation pattern two corresponding disease 
24 measure datum similarity dissimilarity 65 figure 220 newsmap use tree-maps visualize google news headline story source wwwcsumd newsmappng summary visualization provide effective tool explore datum introduce several popular method essential idea behind many exist tool method moreover visualization used datum mining various aspect addition visualize datum visualization used represent datum mining process pattern obtain mining method user interaction datum visual datum mining important research development direction 24 measure datum similarity dissimilarity datum mining application cluster outlier analysis nearest-neighbor classification need way assess alike unalike object comparison one another example store may want search cluster customer object result group customer similar characteristic ( eg similar income area residence age ) information used marketing cluster 
66 chapter 2 get know datum figure 221 used tag cloud visualize popular web site tag source snapshot january 23 2010 high blood pressure ( hb ) allergy ( al ) st li overweight ( ov ) en high cholesterol level ( hc ) ki arthritis ( ar ) trouble see ( tr ) li risk diabetes ( ri ) asthma ( ) ca th diabetes ( di ) hayfever ( ha ) hc thyroid problem ( th ) di heart disease ( ) em tr ar hb cancer ( cn ) os sleep disorder ( sl ) ov eczema ( ec ) chronic bronchitis ( ch ) cn osteoporosis ( os ) prostate ( pr ) cardiovascular ( ca ) ps glaucoma ( gl ) ec pr stroke ( st ) liver condition ( li ) ch psa test abnormal ( ps ) kidney ( ki ) endometriosis ( en ) emphysema ( em ) ha al ri sl gl figure 222 disease influence graph person least 20 year old nhane datum set collection datum object object within cluster similar one another dissimilar object cluster outlier analysis also employ clustering-based technique identify potential outlier object highly dissimilar other knowledge object similarity also used nearest-neighbor classification scheme give object ( eg patient ) assign class label ( relate say diagnosis ) base similarity toward object model 
24 measure datum similarity dissimilarity 67 section present similarity dissimilarity measure refer measure proximity similarity dissimilarity related similarity measure two object j typically return value 0 object unalike higher similarity value greater similarity object ( typically value 1 indicate complete similarity object identical ) dissimilarity measure work opposite way return value 0 object ( therefore far dissimilar ) higher dissimilarity value dissimilar two object section 241 present two datum structure commonly used type application datum matrix ( used store datum object ) dissimilarity matrix ( used store dissimilarity value pair object ) also switch different notation datum object previously used chapter since deal object describe one attribute discuss object dissimilarity compute object describe nominal attribute ( section 242 ) binary attribute ( section 243 ) numeric attribute ( section 244 ) ordinal attribute ( section 245 ) combination attribute type ( section 246 ) section 247 provide similarity measure long sparse datum vector term-frequency vector represent document information retrieval know compute dissimilarity useful study attribute also reference later topic cluster ( chapter 10 11 ) outlier analysis ( chapter 12 ) nearest-neighbor classification ( chapter 9 ) 241 datum matrix versus dissimilarity matrix section 22 look way study central tendency dispersion spread observed value attribute x object one-dimensional describe single attribute section talk object describe multiple attribute therefore need change notation suppose n object ( eg person item course ) describe p attribute ( also call measurement feature age height weight gender ) object x1 = ( x11 x12 x1p ) x2 = ( x21 x22 x2p ) xij value object xi jth attribute brevity hereafter refer object xi object i object may tuple relational database also refer datum sample feature vector main memory-based cluster nearest-neighbor algorithms typically operate either follow two datum structure datum matrix ( object-by-attribute structure ) structure store n datum object form relational table n-by-p matrix ( n object ×p attribute )   x11 · · · x1f · · · x1p ··· ··· ··· ··· ···     ( 28 )  xi1 · · · xif · · · xip    ··· ··· ··· ··· ··· xn1 · · · xnf · · · xnp 
68 chapter 2 get know datum row correspond object part notation may use f index p attribute dissimilarity matrix ( object-by-object structure ) structure store collection proximity available pair n object often represent n-by-n table   0  d ( 2 1 ) 0      d ( 3 1 ) ( 3 2 ) 0 ( 29 )       ( n 1 ) ( n 2 ) · · · · · · 0 ( j ) measure dissimilarity “ difference ” object j general ( j ) non-negative number close 0 object j highly similar “ near ” become larger differ note ( ) = 0 difference object furthermore ( j ) = ( j ) ( readability show ( j ) entry matrix symmetric ) measure dissimilarity discuss throughout remainder chapter measure similarity often expressed function measure dissimilarity example nominal datum sim ( j ) = 1 − ( j ) ( 210 ) sim ( j ) similarity object j throughout rest chapter also comment measure similarity datum matrix make two entity “ thing ” namely row ( object ) column ( attribute ) therefore datum matrix often call two-mode matrix dissimilarity matrix contain one kind entity ( dissimilarity ) call one-mode matrix many cluster nearest-neighbor algorithms operate dissimilarity matrix datum form datum matrix transform dissimilarity matrix apply algorithms 242 proximity measure nominal attribute nominal attribute take two state ( section 212 ) example map color nominal attribute may say five state red yellow green pink blue let number state nominal attribute m state denote letter symbol set integer 1 2 m notice integer used datum handle represent specific order 
24 measure datum similarity dissimilarity 69 “ dissimilarity compute object describe nominal attribute ” dissimilarity two object j compute base ratio mismatch ( j ) = p−m p ( 211 ) number match ( ie number attribute j state ) p total number attribute describe object weight assign increase effect assign greater weight match attribute larger number state example 217 dissimilarity nominal attribute suppose sample datum table 22 except object-identifier attribute test-1 available test-1 nominal ( use test-2 test-3 later example ) let ’ compute dissimilarity matrix ( eq 29 )   0  d ( 2 1 ) 0      d ( 3 1 ) ( 3 2 ) 0 ( 4 1 ) ( 4 2 ) ( 4 3 ) 0 since one nominal attribute test-1 set p = 1 eq ( 211 ) ( j ) evaluate 0 object j match 1 object differ thus get  0 1   1 0  0 1 1 0 1     0 see object dissimilar except object 1 4 ( ie ( 4 1 ) = 0 ) table 22 sample datum table contain attribute mixed type object identifier test-1 ( nominal ) test-2 ( ordinal ) test-3 ( numeric ) 1 2 3 4 code code b code c code excellent fair good excellent 45 22 64 28 
70 chapter 2 get know datum alternatively similarity compute sim ( j ) = 1 − ( j ) = p ( 212 ) proximity object describe nominal attribute compute used alternative encode scheme nominal attribute encode used asymmetric binary attribute create new binary attribute state object give state value binary attribute represent state set 1 remain binary attribute set example encode nominal attribute map color binary attribute create five color previously list object color yellow yellow attribute set 1 remain four attribute set proximity measure form encode calculate used method discuss next subsection 243 proximity measure binary attribute let ’ look dissimilarity similarity measure object describe either symmetric asymmetric binary attribute recall binary attribute one two state 0 1 0 mean attribute absent 1 mean present ( section 213 ) give attribute smoker describe patient instance 1 indicate patient smoke 0 indicate patient treat binary attribute numeric mislead therefore method specific binary datum necessary compute dissimilarity “ compute dissimilarity two binary attribute ” one approach involve compute dissimilarity matrix give binary datum binary attribute thought weight 2 × 2 contingency table table 23 q number attribute equal 1 object j r number attribute equal 1 object equal 0 object j number attribute equal 0 object equal 1 object j number attribute equal 0 object j total number attribute p p = q + r + + recall symmetric binary attribute state equally valuable dissimilarity base symmetric binary attribute call symmetric binary dissimilarity object j describe symmetric binary attribute table 23 contingency table binary attribute object j object 1 0 sum 1 q q+s 0 r r t sum q+r s+t p 
24 measure datum similarity dissimilarity 71 dissimilarity j ( j ) = r s q+r s+t ( 213 ) asymmetric binary attribute two state equally important positive ( 1 ) negative ( 0 ) outcome disease test give two asymmetric binary attribute agreement two 1s ( positive match ) consider significant two 0s ( negative match ) therefore binary attribute often consider “ monary ” ( one state ) dissimilarity base attribute call asymmetric binary dissimilarity number negative match consider unimportant thus ignore follow computation ( j ) = r s q+r s ( 214 ) complementarily measure difference two binary attribute base notion similarity instead dissimilarity example asymmetric binary similarity object j compute sim ( j ) = q = 1 − ( j ) q+r s ( 215 ) coefficient sim ( j ) eq ( 215 ) call jaccard coefficient popularly reference literature symmetric asymmetric binary attribute occur datum set mixed attribute approach describe section 246 apply example 218 dissimilarity binary attribute suppose patient record table ( table 24 ) contain attribute name gender fever cough test-1 test-2 test-3 test-4 name object identifier gender symmetric attribute remain attribute asymmetric binary asymmetric attribute value let value ( yes ) p ( positive ) set 1 value n ( negative ) set suppose distance object table 24 relational table patient describe binary attribute name gender fever cough test-1 test-2 test-3 test-4 jack jim mary f n n p n p n n n n n p n n n 
72 chapter 2 get know datum ( patient ) compute base asymmetric attribute accord eq ( 214 ) distance pair three patients—jack mary jim—is ( jack jim ) = 1+1 = 067 1+1+1 ( jack mary ) = 0+1 = 033 2+0+1 ( jim mary ) = 1+2 = 075 1+1+2 measurement suggest jim mary unlikely similar disease highest dissimilarity value among three pair three patient jack mary likely similar disease 244 dissimilarity numeric datum minkowski distance section describe distance measure commonly used compute dissimilarity object describe numeric attribute measure include euclidean manhattan minkowski distance case datum normalize apply distance calculation involve transform datum fall within smaller common range [ −1 1 ] [ 00 10 ] consider height attribute example can measure either meter inch general express attribute smaller unit lead larger range attribute thus tend give attribute greater effect “ weight ” normalize datum attempt give attribute equal weight may may useful particular application method normalize datum discuss detail chapter 3 datum preprocess popular distance measure euclidean distance ( ie straight line “ crow fly ” ) let = ( xi1 xi2 xip ) j = ( xj1 xj2 xjp ) two object describe p numeric attribute euclidean distance object j defined q ( j ) = ( xi1 − xj1 ) 2 + ( xi2 − xj2 ) 2 + · · · + ( xip − xjp ) 2 ( 216 ) another well-known measure manhattan ( city block ) distance name distance block two point city ( 2 block 3 block total 5 block ) defined ( j ) = xi1 − xj1 | + xi2 − xj2 | + · · · + xip − xjp | ( 217 ) euclidean manhattan distance satisfy follow mathematical property non-negativity ( j ) ≥ 0 distance non-negative number identity indiscernible ( ) = 0 distance object 0 
24 measure datum similarity dissimilarity 73 symmetry ( j ) = ( j ) distance symmetric function triangle inequality ( j ) ≤ ( k ) + ( k j ) go directly object object j space make detour object k measure satisfy condition know metric please note non-negativity property imply three property example 219 euclidean distance manhattan distance let x1 = ( 1 2 ) x2 = ( 3 5 ) represent √ two object show figure euclidean distance two 22 + 32 = manhattan distance two 2 + 3 = 5 minkowski distance generalization euclidean manhattan distance defined q ( j ) = h xi1 − xj1 h + xi2 − xj2 h + · · · + xip − xjp h ( 218 ) h real number h ≥ 1 ( distance also call lp norm literature symbol p refer notation h keep p number attribute consistent rest chapter ) represent manhattan distance h = 1 ( ie l1 norm ) euclidean distance h = 2 ( ie l2 norm ) supremum distance ( also refer lmax l∞ norm chebyshev distance ) generalization minkowski distance h → ∞ compute find attribute f give maximum difference value two object difference supremum distance defined formally  1 h p x p h  ( j ) = lim xif − xjf | = max xif − xjf | ( 219 ) h→∞ f 1 f l∞ norm also know uniform norm x2 = ( 3 5 ) 5 4 euclidean distance = ( 22 + 32 ) 2 = 361 3 3 2 x1 = ( 1 2 ) manhattan distance 2+3=5 supremum distance 5–2=3 2 1 1 2 3 figure 223 euclidean manhattan supremum distance two object 
74 chapter 2 get know datum example 220 supremum distance let ’ use two object x1 = ( 1 2 ) x2 = ( 3 5 ) figure second attribute give greatest difference value object 5 − 2 = supremum distance object attribute assign weight accord perceive importance weight euclidean distance compute q ( j ) = w1 xi1 − xj1 2 + w2 xi2 − xj2 2 + · · · + wm xip − xjp 2 ( 220 ) weighting also apply distance measure well 245 proximity measure ordinal attribute value ordinal attribute meaningful order ranking yet magnitude successive value unknown ( section 214 ) example include sequence small medium large size attribute ordinal attribute may also obtain discretization numeric attribute splitting value range finite number category category organized rank range numeric attribute map ordinal attribute f mf state example range interval-scaled attribute temperature ( celsius ) organized follow state −30 −10 −10 10 10 30 represent category cold temperature moderate temperature warm temperature respectively let represent number possible state ordinal attribute order state define ranking 1 mf “ ordinal attribute handled ” treatment ordinal attribute quite similar numeric attribute compute dissimilarity object suppose f attribute set ordinal attribute describe n object dissimilarity computation respect f involve follow step value f ith object xif f mf order state represent ranking 1 mf replace xif corresponding rank rif ∈ { 1 mf } since ordinal attribute different number state often necessary map range attribute onto [ 00 10 ] attribute equal weight perform datum normalization replace rank rif ith object f th attribute zif = rif − 1 mf − 1 ( 221 ) dissimilarity compute used distance measure describe section 244 numeric attribute used zif represent f value ith object 
24 measure datum similarity dissimilarity 75 example 221 dissimilarity ordinal attribute suppose sample datum show earlier table 22 except time object-identifier continuous ordinal attribute test-2 available three state test-2 fair good excellent mf = step 1 replace value test-2 rank four object assign rank 3 1 2 3 respectively step 2 normalizes ranking mapping rank 1 00 rank 2 05 rank 3 step 3 use say euclidean distance ( eq 216 ) result follow dissimilarity matrix  0 10 0   05 05 0 0 10 05      0 therefore object 1 2 dissimilar object 2 4 ( ie ( 2 1 ) = 10 ( 4 2 ) = 10 ) make intuitive sense since object 1 4 excellent object 2 fair opposite end range value test-2 similarity value ordinal attribute interpreted dissimilarity sim ( j ) = 1 − ( j ) 246 dissimilarity attribute mixed type section 242 245 discuss compute dissimilarity object describe attribute type type may either nominal symmetric binary asymmetric binary numeric ordinal however many real databasis object describe mixture attribute type general database contain attribute type “ compute dissimilarity object mixed attribute type ” one approach group type attribute together perform separate datum mining ( eg cluster ) analysis type feasible analysis derive compatible result however real application unlikely separate analysis per attribute type generate compatible result preferable approach process attribute type together perform single analysis one technique combine different attribute single dissimilarity matrix bring meaningful attribute onto common scale interval [ 00 10 ] suppose datum set contain p attribute mixed type dissimilarity ( j ) object j defined ( f ) ( f ) f 1 δij dij pp ( f ) f 1 δij pp ( j ) = ( 222 ) 
76 chapter 2 get know datum ( f ) indicator δij = 0 either ( 1 ) xif xjf miss ( ie measurement attribute f object object j ) ( 2 ) xif = xjf = 0 attribute ( f ) f asymmetric binary otherwise δij = contribution attribute f ( f ) dissimilarity j ( ie dij ) compute dependent type ( f ) f numeric dij = attribute f xif −xjf | maxh xhf −minh xhf h run nonmissing object ( f ) ( f ) f nominal binary dij = 0 xif = xjf otherwise dij = 1 f ordinal compute rank rif zif = rif −1 mf −1 treat zif numeric step identical already see individual attribute type difference numeric attribute normalize value map interval [ 00 10 ] thus dissimilarity object compute even attribute describe object different type example 222 dissimilarity attribute mixed type let ’ compute dissimilarity matrix object table consider attribute different type example 217 221 work dissimilarity matrix individual attribute procedure follow test-1 ( nominal ) test-2 ( ordinal ) outlined earlier process attribute mixed type therefore use dissimilarity matrix obtain test-1 test-2 later compute eq ( 222 ) first however need compute dissimilarity matrix third attribute test-3 ( numeric ) ( 3 ) must compute dij follow case numeric attribute let maxh xh = 64 minh xh = difference two used eq ( 222 ) normalize value dissimilarity matrix result dissimilarity matrix test-3  0 055   045 040  0 100 014 0 086     0 use dissimilarity matrix three attribute computation ( f ) eq ( 222 ) indicator δij = 1 three attribute f get example ( 3 1 ) = 1 ( 1 ) 1 ( 050 ) 1 ( 045 ) 3 = result dissimilarity matrix obtain 
24 measure datum similarity dissimilarity 77 datum describe three attribute mixed type  0 085   065 013  0 083 071 0 079     0 table 22 intuitively guess object 1 4 similar base value test-1 test-2 confirm dissimilarity matrix ( 4 1 ) lowest value pair different object similarly matrix indicate object 1 2 least similar 247 cosine similarity document represent thousand attribute record frequency particular word ( keyword ) phrase document thus document object represent call term-frequency vector example table 25 see document1 contain five instance word team hockey occur three time word coach absent entire document indicated count value datum highly asymmetric term-frequency vector typically long sparse ( ie many 0 value ) application used structure include information retrieval text document cluster biological taxonomy gene feature mapping traditional distance measure study chapter work well sparse numeric datum example two term-frequency vector may many 0 value common meaning corresponding document share many word make similar need measure focus word two document common occurrence frequency word word need measure numeric datum ignore zero-match cosine similarity measure similarity used compare document say give ranking document respect give vector query word let x two vector comparison used cosine measure table 25 document vector term-frequency vector document team coach hockey baseball soccer penalty score win loss season document1 document2 document3 document4 5 3 0 0 0 0 7 1 3 2 0 0 0 0 2 0 2 1 1 1 0 1 0 2 0 0 0 2 2 1 3 0 0 0 0 3 0 1 0 0 
78 chapter 2 get know datum similarity function sim ( x ) = x·y | ( 223 ) | euclidean norm vector x = ( x1 x2 xp ) defined q x12 + x22 + · · · + xp2 conceptually length vector similarly | euclidean norm vector y measure compute cosine angle vector x y cosine value 0 mean two vector 90 degree ( orthogonal ) match closer cosine value 1 smaller angle greater match vector note cosine similarity measure obey property section 244 define metric measure refer nonmetric measure example 223 cosine similarity two term-frequency vector suppose x first two term-frequency vector table x = ( 5 0 3 0 2 0 0 2 0 0 ) = ( 3 0 2 0 1 1 0 1 0 1 ) similar x used eq ( 223 ) compute cosine similarity two vector get xt · = 5 × 3 + 0 × 0 + 3 × 2 + 0 × 0 + 2 × 1 + 0 × 1 + 0 × 0 + 2 × 1 + 0 × 0 + 0 × 1 = 25 p | = 52 + 02 + 32 + 02 + 22 + 02 + 02 + 22 + 02 + 02 = 648 p | = 32 + 02 + 22 + 02 + 12 + 12 + 02 + 12 + 02 + 12 = 412 sim ( x ) = 094 therefore used cosine similarity measure compare document would consider quite similar attribute binary-valu cosine similarity function interpreted term share feature attribute suppose object x possess ith attribute xi = xt · number attribute possessed ( ie share ) x | geometric mean number attribute possessed x number possessed y thus sim ( x ) measure relative possession common attribute simple variation cosine similarity precede scenario sim ( x ) = x·y x·x+y·y−x·y ( 224 ) ratio number attribute share x number attribute possessed x y function know tanimoto coefficient tanimoto distance frequently used information retrieval biology taxonomy 
26 exercise 25 79 summary datum set make datum object datum object represent entity datum object describe attribute attribute nominal binary ordinal numeric value nominal ( categorical ) attribute symbol name thing value represent kind category code state binary attribute nominal attribute two possible state ( 1 0 true false ) two state equally important attribute symmetric otherwise asymmetric ordinal attribute attribute possible value meaningful order ranking among magnitude successive value know numeric attribute quantitative ( ie measurable quantity ) represent integer real value numeric attribute type interval-scaled ratioscale value interval-scaled attribute measure fix equal unit ratio-scale attribute numeric attribute inherent zero-point measurement ratio-scale speak value order magnitude larger unit measurement basic statistical description provide analytical foundation datum preprocess basic statistical measure datum summarization include mean weight mean median mode measure central tendency datum range quantile quartile interquartile range variance standard deviation measure dispersion datum graphical representation ( eg boxplot quantile plot quantile– quantile plot histogram scatter plot ) facilitate visual inspection datum thus useful datum preprocess mining datum visualization technique may pixel-oriented geometric-based icon-based hierarchical method apply multidimensional relational datum additional technique propose visualization complex datum text social network measure object similarity dissimilarity used datum mining application cluster outlier analysis nearest-neighbor classification measure proximity compute attribute type study chapter combination attribute example include jaccard coefficient asymmetric binary attribute euclidean manhattan minkowski supremum distance numeric attribute application involve sparse numeric datum vector term-frequency vector cosine measure tanimoto coefficient often used assessment similarity 26 exercise 21 give three additional commonly used statistical measure already illustrated chapter characterization datum dispersion discuss compute efficiently large databasis 
80 chapter 2 get know datum 22 suppose datum analysis include attribute age age value datum tuple ( increase order ) 13 15 16 16 19 20 20 21 22 22 25 25 25 25 30 33 33 35 35 35 35 36 40 45 46 52 70 ( ) mean datum median ( b ) mode datum comment datum ’ modality ( ie bimodal trimodal etc ) ( c ) midrange datum ( ) find ( roughly ) first quartile ( q1 ) third quartile ( q3 ) datum ( e ) give five-number summary datum ( f ) show boxplot datum ( g ) quantile–quantile plot different quantile plot 23 suppose value give set datum group interval interval corresponding frequency follow age 1–5 6–15 16–20 21–50 51–80 81–110 frequency 200 450 300 1500 700 44 compute approximate median value datum 24 suppose hospital test age body fat datum 18 randomly select adult follow result age % fat 23 95 23 265 27 78 27 178 39 314 41 259 47 274 49 272 50 312 age % fat 52 346 54 425 54 288 56 334 57 302 58 341 58 329 60 412 61 357 ( ) calculate mean median standard deviation age % fat ( b ) draw boxplot age % fat ( c ) draw scatter plot q-q plot base two variable 25 briefly outline compute dissimilarity object describe follow ( ) nominal attribute ( b ) asymmetric binary attribute 
27 bibliographic note 81 ( c ) numeric attribute ( ) term-frequency vector 26 give two object represent tuple ( 22 1 42 10 ) ( 20 0 36 8 ) ( ) ( b ) ( c ) ( ) compute euclidean distance two object compute manhattan distance two object compute minkowski distance two object used q = 3 compute supremum distance two object 27 median one important holistic measure datum analysis propose several method median approximation analyze respective complexity different parameter setting decide extent real value approximate moreover suggest heuristic strategy balance accuracy complexity apply method give 28 important define select similarity measure datum analysis however commonly accept subjective similarity measure result vary depend similarity measure used nonetheless seemingly different similarity measure may equivalent transformation suppose follow 2-d datum set x1 x2 x3 x4 x5 a1 15 2 16 12 15 a2 17 19 18 15 10 ( ) consider datum 2-d datum point give new datum point x = ( 14 16 ) query rank database point base similarity query used euclidean distance manhattan distance supremum distance cosine similarity ( b ) normalize datum set make norm datum point equal use euclidean distance transform datum rank datum point 27 bibliographic note method descriptive datum summarization study statistic literature long onset computer good summary statistical descriptive datum mining method include freedman pisani purf [ fpp07 ] devore [ dev95 ] 
82 chapter 2 get know datum statistics-based visualization datum used boxplot quantile plot quantile–quantile plot scatter plot loess curf see cleveland [ cle93 ] pioneer work datum visualization technique describe visual display quantitative information [ tuf83 ] envision information [ tuf90 ] visual explanation image quantity evidence narrative [ tuf97 ] tufte addition graphic graphic information process bertin [ ber81 ] visualize datum cleveland [ cle93 ] information visualization datum mining knowledge discovery edit fayyad grinstein wierse [ fgw01 ] major conference symposium visualization include acm human factor compute system ( chi ) visualization international symposium information visualization research visualization also publish transaction visualization computer graphic journal computational graphical statistic ieee computer graphic application many graphical user interface visualization tool develop find various datum mining product several book datum mining ( eg datum mining solution westphal blaxton [ wb98 ] ) present many good example visual snapshot survey visualization technique see “ visual technique explore databasis ” keim [ kei97 ] similarity distance measure among various variable introduce many textbook study cluster analysis include hartigan [ har75 ] jain dube [ jd88 ] kaufman rousseeuw [ kr90 ] arabie hubert de soete [ ahs96 ] method combine attribute different type single dissimilarity matrix introduce kaufman rousseeuw [ kr90 ] 
10 cluster analysis basic concept method imagine director customer relationship allelectronic five manager work would like organize company ’ customer five group group assign different manager strategically would like customer group similar possible moreover two give customer different business pattern place group intention behind business strategy develop customer relationship campaign specifically target group base common feature share customer per group kind datum mining technique help accomplish task unlike classification class label ( group id ) customer unknown need discover grouping give large number customer many attribute describe customer profile costly even infeasible human study datum manually come way partition customer strategic group need cluster tool help cluster process grouping set datum object multiple group cluster object within cluster high similarity dissimilar object cluster dissimilarity similarity assessed base attribute value describe object often involve distance measures1 cluster datum mining tool root many application area biology security business intelligence web search chapter present basic concept method cluster analysis section 101 introduce topic study requirement cluster method massive amount datum various application learn several basic cluster technique organized follow category partition method ( section 102 ) hierarchical method ( section 103 ) density-based method ( section 104 ) grid-based method ( section 105 ) section 106 briefly discuss evaluate 1 datum similarity dissimilarity discuss detail section may want refer section quick review datum mining concept technique doi b978-0-12-381479-100010-1 c 2012 elsevier right re-serve 443 
444 chapter 10 cluster analysis basic concept method cluster method discussion advanced method cluster re-serve chapter 11 101 cluster analysis section set groundwork study cluster analysis section 1011 define cluster analysis present example useful section 1012 learn aspect compare cluster method well requirement cluster overview basic cluster technique present section 1013 1011 cluster analysis cluster analysis simply cluster process partition set datum object ( observation ) subset subset cluster object cluster similar one another yet dissimilar object cluster set cluster result cluster analysis refer cluster context different cluster method may generate different clustering datum set partition perform human cluster algorithm hence cluster useful lead discovery previously unknown group within datum cluster analysis widely used many application business intelligence image pattern recognition web search biology security business intelligence cluster used organize large number customer group customer within group share strong similar characteristic facilitate development business strategy enhance customer relationship management moreover consider consultant company large number project improve project management cluster apply partition project category base similarity project audit diagnosis ( improve project delivery outcome ) conduct effectively image recognition cluster used discover cluster “ subclass ” handwritten character recognition system suppose datum set handwritten digit digit labele either 1 2 3 note large variance way person write digit take number 2 example person may write small circle left bottom part other may use cluster determine subclass “ 2 ” represent variation way 2 written used multiple model base subclass improve overall recognition accuracy cluster also find many application web search example keyword search may often return large number hit ( ie page relevant search ) due extremely large number web page cluster used organize search result group present result concise easily accessible way moreover cluster technique develop cluster document topic commonly used information retrieval practice 
101 cluster analysis 445 datum mining function cluster analysis used standalone tool gain insight distribution datum observe characteristic cluster focus particular set cluster analysis alternatively may serve preprocess step algorithms characterization attribute subset selection classification would operate detected cluster select attribute feature cluster collection datum object similar one another within cluster dissimilar object cluster cluster datum object treat implicit class sense cluster sometimes call automatic classification critical difference cluster automatically find grouping distinct advantage cluster analysis cluster also call datum segmentation application cluster partition large datum set group accord similarity cluster also used outlier detection outlier ( value “ far away ” cluster ) may interesting common case application outlier detection include detection credit card fraud monitoring criminal activity electronic commerce example exceptional case credit card transaction expensive infrequent purchase may interest possible fraudulent activity outlier detection subject chapter 12 datum cluster vigorous development contribute area research include datum mining statistic machine learn spatial database technology information retrieval web search biology marketing many application area owing huge amount datum collect databasis cluster analysis recently become highly active topic datum mining research branch statistic cluster analysis extensively study main focus distance-based cluster analysis cluster analysis tool base k-mean k-medoid several method also build many statistical analysis software package system s-plus spss sas machine learn recall classification know supervised learn class label information give learn algorithm supervised tell class membership training tuple cluster know unsupervised learn class label information present reason cluster form learn observation rather learn example datum mining effort focuse find method efficient effective cluster analysis large databasis active theme research focus scalability cluster method effectiveness method cluster complex shape ( eg nonconvex ) type datum ( eg text graph image ) high-dimensional cluster technique ( eg cluster object thousand feature ) method cluster mixed numerical nominal datum large databasis 1012 requirement cluster analysis cluster challenge research field section learn requirement cluster datum mining tool well aspect used compare cluster method 
446 chapter 10 cluster analysis basic concept method follow typical requirement cluster datum mining scalability many cluster algorithms work well small datum set contain fewer several hundred datum object however large database may contain million even billion object particularly web search scenario cluster sample give large datum set may lead bias result therefore highly scalable cluster algorithms need ability deal different type attribute many algorithms design cluster numeric ( interval-based ) datum however application may require cluster datum type binary nominal ( categorical ) ordinal datum mixture datum type recently application need cluster technique complex datum type graph sequence image document discovery cluster arbitrary shape many cluster algorithms determine cluster base euclidean manhattan distance measure ( chapter 2 ) algorithms base distance measure tend find spherical cluster similar size density however cluster can shape consider sensor example often deploy environment surveillance cluster analysis sensor reading detect interesting phenomena may want use cluster find frontier run forest fire often spherical important develop algorithms detect cluster arbitrary shape requirement domain knowledge determine input parameter many cluster algorithms require user provide domain knowledge form input parameter desire number cluster consequently cluster result may sensitive parameter parameter often hard determine especially high-dimensionality datum set user yet grasp deep understand datum require specification domain knowledge burden user also make quality cluster difficult control ability deal noisy datum real-world datum set contain outlier or miss unknown erroneous datum sensor reading example often noisy—some reading may inaccurate due sense mechanism reading may erroneous due interference surround transient object cluster algorithms sensitive noise may produce poor-quality cluster therefore need cluster method robust noise incremental cluster insensitivity input order many application incremental update ( represent newer datum ) may arrive time cluster algorithms incorporate incremental update exist cluster structure instead recompute new cluster scratch cluster algorithms may also sensitive input datum order give set datum object cluster algorithms may return dramatically different clustering depend order object present incremental cluster algorithms algorithms insensitive input order need 
101 cluster analysis 447 capability cluster high-dimensionality datum datum set contain numerous dimension attribute cluster document example keyword regard dimension often thousand keyword cluster algorithms good handle low-dimensional datum datum set involve two three dimension find cluster datum object highdimensional space challenge especially consider datum sparse highly skewer constraint-based cluster real-world application may need perform cluster various kind constraint suppose job choose location give number new automatic teller machine ( atms ) city decide upon may cluster household consider constraint city ’ river highway network type number customer per cluster challenge task find datum group good cluster behavior satisfy specify constraint interpretability usability user want cluster result interpretable comprehensible usable cluster may need tie specific semantic interpretation application important study application goal may influence selection cluster feature cluster method follow orthogonal aspect cluster method compare partition criterium method object partition hierarchy exist among cluster cluster level conceptually method useful example partition customer group group manager alternatively method partition datum object hierarchically cluster form different semantic level example text mining may want organize corpus document multiple general topic “ politic ” “ sport ” may subtopic instance “ football ” “ basketball ” “ baseball ” “ hockey ” exist subtopic “ ” latter four subtopic lower level hierarchy “ sport ” separation cluster method partition datum object mutually exclusive cluster cluster customer group group take care one manager customer may belong one group situation cluster may exclusive datum object may belong one cluster example cluster document topic document may related multiple topic thus topic cluster may exclusive similarity measure method determine similarity two object distance distance defined euclidean space 
448 chapter 10 cluster analysis basic concept method road network vector space space method similarity may defined connectivity base density contiguity may rely absolute distance two object similarity measure play fundamental role design cluster method distance-based method often take advantage optimization technique - continuity-based method often find cluster arbitrary shape cluster space many cluster method search cluster within entire give datum space method useful low-dimensionality datum set highdimensional datum however many irrelevant attribute make similarity measurement unreliable consequently cluster find full space often meaningless ’ often better instead search cluster within different subspace datum set subspace cluster discover cluster subspace ( often low dimensionality ) manifest object similarity conclude cluster algorithms several requirement factor include scalability ability deal different type attribute noisy datum incremental update cluster arbitrary shape constraint interpretability usability also important addition cluster method differ respect partition level whether cluster mutually exclusive similarity measure used whether subspace cluster perform 1013 overview basic cluster method many cluster algorithms literature difficult provide crisp categorization cluster method category may overlap method may feature several category nevertheless useful present relatively organized picture cluster method general major fundamental cluster method classify follow category discuss rest chapter partition method give set n object partition method construct k partition datum partition represent cluster k ≤ n divide datum k group group must contain least one object word partition method conduct one-level partition datum set basic partition method typically adopt exclusive cluster separation object must belong exactly one group requirement may relax example fuzzy partition technique reference technique give bibliographic note ( section 109 ) partition method distance-based give k number partition construct partition method create initial partition used iterative relocation technique attempt improve partition move object one group another general criterion good partition object cluster “ close ” related whereas object different cluster “ far apart ” different various kind 
101 cluster analysis 449 criterium judge quality partition traditional partition method extend subspace cluster rather search full datum space useful many attribute datum sparse achieve global optimality partitioning-based cluster often computationally prohibitive potentially require exhaustive enumeration possible partition instead application adopt popular heuristic method greedy approach like k-mean k-medoid algorithms progressively improve cluster quality approach local optimum heuristic cluster method work well find spherical-shap cluster - medium-size databasis find cluster complex shape large datum set partitioning-based method need extend partitioning-based cluster method study depth section 102 hierarchical method hierarchical method create hierarchical decomposition give set datum object hierarchical method classify either agglomerative divisive base hierarchical decomposition form agglomerative approach also call bottom-up approach start object form separate group successively merge object group close one another group merged one ( topmost level hierarchy ) termination condition hold divisive approach also call top-down approach start object cluster successive iteration cluster split smaller cluster eventually object one cluster termination condition hold hierarchical cluster method distance-based - continuitybased various extension hierarchical method consider cluster subspace well hierarchical method suffer fact step ( merge split ) do never undo rigidity useful lead smaller computation cost worry combinatorial number different choice technique correct erroneous decision however method improve quality hierarchical cluster propose hierarchical cluster method study section 103 density-based method partition method cluster object base distance object method find spherical-shap cluster encounter difficulty discover cluster arbitrary shape cluster method develop base notion density general idea continue grow give cluster long density ( number object datum point ) “ neighborhood ” exceed threshold example datum point within give cluster neighborhood give radius contain least minimum number point method used filter noise outlier discover cluster arbitrary shape density-based method divide set object multiple exclusive cluster hierarchy cluster typically density-based method consider exclusive cluster consider fuzzy cluster moreover density-based method extend full space subspace cluster density-based cluster method study section 104 
450 chapter 10 cluster analysis basic concept method grid-based method grid-based method quantize object space finite number cell form grid structure cluster operation perform grid structure ( ie quantized space ) main advantage approach fast process time typically independent number datum object dependent number cell dimension quantized space used grid often efficient approach many spatial datum mining problem include cluster therefore grid-based method integrate cluster method density-based method hierarchical method gridbase cluster study section 105 method briefly summarize figure cluster algorithms integrate idea several cluster method sometimes difficult classify give algorithm uniquely belong one cluster method category furthermore application may cluster criterium require integration several cluster technique follow section examine cluster method detail advanced cluster method related issue discuss chapter general notation used follow let datum set n object cluster object describe variable variable also call attribute dimension method partition method general characteristic – find mutually exclusive cluster spherical shape – distance-based – may use mean medoid ( etc ) represent cluster center – effective - medium-size datum set hierarchical method – cluster hierarchical decomposition ( ie multiple level ) – correct erroneous merge split – may incorporate technique like microcluster consider object “ linkage ” density-based method – find arbitrarily shape cluster – cluster dense region object space separated low-density region – cluster density point must minimum number point within “ neighborhood ” – may filter outlier grid-based method – use multiresolution grid datum structure – fast process time ( typically independent number datum object yet dependent grid size ) figure 101 overview cluster method discuss chapter note algorithms may combine various method 
102 partition method 451 therefore may also refer point d-dimensional object space object represent bold italic font ( eg p ) 102 partition method simplest fundamental version cluster analysis partition organize object set several exclusive group cluster keep problem specification concise assume number cluster give background knowledge parameter start point partition method formally give datum set n object k number cluster form partition algorithm organize object k partition ( k ≤ n ) partition represent cluster cluster form optimize objective partition criterion dissimilarity function base distance object within cluster “ similar ” one another “ dissimilar ” object cluster term datum set attribute section learn well-known commonly used partition methods—k-mean ( section 1021 ) k-medoid ( section 1022 ) also learn several variation classic partition method scale handle large datum set 1021 k-mean centroid-based technique suppose datum set contain n object euclidean space partition method distribute object k cluster c1 ck ci ⊂ ci ∩ cj = ∅ ( 1 ≤ j ≤ k ) objective function used assess partition quality object within cluster similar one another dissimilar object cluster objective function aim high intracluster similarity low intercluster similarity centroid-based partition technique used centroid cluster ci represent cluster conceptually centroid cluster center point centroid defined various way mean medoid object ( point ) assign cluster difference object p ∈ ci ci representative cluster measure dist ( p ci ) dist ( x ) euclidean distance two point x y quality cluster ci measure withincluster variation sum square error object ci centroid ci defined = k x x dist ( p ci ) 2 ( 101 ) i=1 p∈ci e sum square error object datum set p point space represent give object ci centroid cluster ci ( p ci multidimensional ) word object cluster distance 
452 chapter 10 cluster analysis basic concept method object cluster center square distance sum objective function try make result k cluster compact separate possible optimize within-cluster variation computationally challenge worst case would enumerate number possible partitioning exponential number cluster check within-cluster variation value show problem np-hard general euclidean space even two cluster ( ie k = 2 ) moreover problem np-hard general number cluster k even 2-d euclidean space number cluster k dimensionality space fix problem solve time ( ndk+1 log n ) n number object overcome prohibitive computational cost exact solution greedy approach often used practice prime example k-mean algorithm simple commonly used “ k-mean algorithm work ” k-mean algorithm define centroid cluster mean value point within cluster proceed follow first randomly select k object initially represent cluster mean center remain object object assign cluster similar base euclidean distance object cluster mean k-mean algorithm iteratively improve within-cluster variation cluster compute new mean used object assign cluster previous iteration object reassign used update mean new cluster center iteration continue assignment stable cluster form current round form previous round k-mean procedure summarize figure 102 algorithm k-mean k-mean algorithm partition cluster ’ center represent mean value object cluster input k number cluster datum set contain n object output set k cluster method ( 1 ) arbitrarily choose k object initial cluster center ( 2 ) repeat ( 3 ) ( ) assign object cluster object similar base mean value object cluster ( 4 ) update cluster mean calculate mean value object cluster ( 5 ) change figure 102 k-mean partition algorithm 
102 partition method 453 + + + + + ( ) initial cluster + ( b ) iterate + + + ( c ) final cluster figure 103 cluster set object used k-mean method ( b ) update cluster center reassign object accordingly ( mean cluster marked + ) example 101 cluster k-mean partition consider set object locate 2-d space depict figure 103 ( ) let k = 3 user would like object partition three cluster accord algorithm figure 102 arbitrarily choose three object three initial cluster center cluster center marked + object assign cluster base cluster center nearest distribution form silhouette encircle dot curf show figure 103 ( ) next cluster center update mean value cluster recalculate base current object cluster used new cluster center object redistribute cluster base cluster center nearest redistribution form new silhouette encircle dash curf show figure 103 ( b ) process iterate lead figure 103 ( c ) process iteratively reassigning object cluster improve partition refer iterative relocation eventually reassignment object cluster occur process terminate result cluster return cluster process k-mean method guarantee converge global optimum often terminate local optimum result may depend initial random selection cluster center ( ask give example show exercise ) obtain good result practice common run k-mean algorithm multiple time different initial cluster center time complexity k-mean algorithm ( nkt ) n total number object k number cluster number iteration normally k n n therefore method relatively scalable efficient process large datum set several variant k-mean method differ selection initial k-mean calculation dissimilarity strategy calculate cluster mean 
454 chapter 10 cluster analysis basic concept method k-mean method apply mean set object defined may case application datum nominal attribute involved k-mode method variant k-mean extend k-mean paradigm cluster nominal datum replace mean cluster mode used new dissimilarity measure deal nominal object frequency-based method update mode cluster k-mean k-mode method integrate cluster datum mixed numeric nominal value necessity user specify k number cluster advance see disadvantage study overcome difficulty however provide approximate range k value used analytical technique determine best k compare cluster result obtain different k value k-mean method suitable discover cluster nonconvex shape cluster different size moreover sensitive noise outlier datum point small number datum substantially influence mean value “ make k-mean algorithm scalable ” one approach make k-mean method efficient large datum set use good-sized set sample cluster another employ filter approach used spatial hierarchical datum index save cost compute mean third approach explore microcluster idea first group nearby object “ microcluster ” perform k-mean cluster microcluster microcluster discuss section 103 1022 k-medoid representative object-based technique k-mean algorithm sensitive outlier object far away majority datum thus assign cluster dramatically distort mean value cluster inadvertently affect assignment object cluster effect particularly exacerbate due use squared-error function eq ( 101 ) observed example 102 example 102 drawback k-mean consider six point 1-d space value 1 2 3 8 9 10 25 respectively intuitively visual inspection may imagine point partition cluster { 1 2 3 } { 8 9 10 } point 25 exclude appear outlier would k-mean partition value apply k-mean used k = 2 eq ( 101 ) partition { { 1 2 3 } { 8 9 10 25 } } within-cluster variation ( 1 − 2 ) 2 + ( 2 − 2 ) 2 + ( 3 − 2 ) 2 + ( 8 − 13 ) 2 + ( 9 − 13 ) 2 + ( 10 − 13 ) 2 + ( 25 − 13 ) 2 = 196 give mean cluster { 1 2 3 } 2 mean { 8 9 10 25 } compare partition { { 1 2 3 8 } { 9 10 25 } } k-mean compute withincluster variation ( 1 − 35 ) 2 + ( 2 − 35 ) 2 + ( 3 − 35 ) 2 + ( 8 − 35 ) 2 + ( 9 − 1467 ) 2 + ( 10 − 1467 ) 2 + ( 25 − 1467 ) 2 = 18967 
102 partition method 455 give 35 mean cluster { 1 2 3 8 } 1467 mean cluster { 9 10 25 } latter partition lowest within-cluster variation therefore k-mean method assign value 8 cluster different contain 9 10 due outlier point moreover center second cluster 1467 substantially far member cluster “ modify k-mean algorithm diminish sensitivity outlier ” instead take mean value object cluster reference point pick actual object represent cluster used one representative object per cluster remain object assign cluster representative object similar partition method perform base principle minimize sum dissimilarity object p corresponding representative object absolute-error criterion used defined = k x x dist ( p oi ) ( 102 ) i=1 p∈ci e sum absolute error object p datum set oi representative object ci basis k-medoid method group n object k cluster minimize absolute error ( eq 102 ) k = 1 find exact median ( n2 ) time however k general positive number k-medoid problem np-hard partition around medoid ( pam ) algorithm ( see figure 105 later ) popular realization k-medoid cluster tackle problem iterative greedy way like k-mean algorithm initial representative object ( call seed ) choose arbitrarily consider whether replace representative object nonrepresentative object would improve cluster quality possible replacement try iterative process replace representative object object continue quality result cluster improve replacement quality measure cost function average dissimilarity object representative object cluster specifically let o1 ok current set representative object ( ie medoid ) determine whether nonrepresentative object denote orandom good replacement current medoid oj ( 1 ≤ j ≤ k ) calculate distance every object p closest object set { o1 oj−1 orandom oj+1 ok } use distance update cost function reassignment object { o1 oj−1 orandom oj+1 ok } simple suppose object p currently assign cluster represent medoid oj ( figure 104a b ) need reassign p different cluster oj replace orandom object p need reassign either orandom cluster represent oi ( = j ) whichever closest example figure 104 ( ) p closest oi therefore reassign oi figure 104 ( b ) however p closest orandom reassign orandom instead p currently assign cluster represent object oi = j 
456 chapter 10 cluster analysis basic concept method oi p oj orandom ( ) reassign oi oi oj p oi oj oi oj p orandom ( b ) reassign orandom ( c ) change orandom p orandom datum object cluster center swap swap ( ) reassign orandom figure 104 four case cost function k-medoid cluster object remain assign cluster represent oi long still closer oi orandom ( figure 104c ) otherwise reassign orandom ( figure 104d ) time reassignment occur difference absolute error e contribute cost function therefore cost function calculate difference absolute-error value current representative object replace nonrepresentative object total cost swap sum cost incur nonrepresentative object total cost negative oj replace swap orandom actual absolute-error e reduce total cost positive current representative object oj consider acceptable nothing change iteration “ method robust—k-mean k-medoid ” k-medoid method robust k-mean presence noise outlier medoid less influenced outlier extreme value mean however complexity iteration k-medoid algorithm ( k ( n − k ) 2 ) large value n k computation become costly much costly k-mean method method require user specify k number cluster “ scale k-medoid method ” typical k-medoid partition algorithm like pam ( figure 105 ) work effectively small datum set scale well large datum set deal larger datum set sampling-based method call clara ( cluster large application ) used instead take whole datum set consideration clara used random sample datum set pam algorithm apply compute best medoid sample ideally sample closely represent original datum set many case large sample work well create object equal probability select sample representative object ( medoid ) choose likely similar would choose whole datum set clara build clustering multiple random sample return best cluster output complexity compute medoid random sample ( ks 2 + k ( n − k ) ) size sample k number cluster n total number object clara deal larger datum set pam effectiveness clara depend sample size notice pam search best k-medoid among give datum set whereas clara search best k-medoid among select sample datum set clara find good cluster best sample medoid far best k-medoid object 
103 hierarchical method 457 algorithm k-medoid pam k-medoid algorithm partition base medoid central object input k number cluster datum set contain n object output set k cluster method ( 1 ) arbitrarily choose k object initial representative object seed ( 2 ) repeat ( 3 ) assign remain object cluster nearest representative object ( 4 ) randomly select nonrepresentative object orandom ( 5 ) compute total cost swap representative object oj orandom ( 6 ) < 0 swap oj orandom form new set k representative object ( 7 ) change figure 105 pam k-medoid partition algorithm one best k-medoid select sampling clara never find best cluster ( ask provide example demonstrate exercise ) “ might improve quality scalability clara ” recall search better medoid pam examine every object datum set every current medoid whereas clara confine candidate medoid random sample datum set randomize algorithm call claran ( cluster large application base upon randomize search ) present trade-off cost effectiveness used sample obtain cluster first randomly select k object datum set current medoid randomly select current medoid x object one current medoid replace x improve absolute-error criterion yes replacement make claran conduct randomize search l time set current medoid l step consider local optimum claran repeat randomize process time return best local optimal final result 103 hierarchical method partition method meet basic cluster requirement organize set object number exclusive group situation may want partition datum group different level hierarchy hierarchical cluster method work grouping datum object hierarchy “ tree ” cluster represent datum object form hierarchy useful datum summarization visualization example manager human resource allelectronic 
458 chapter 10 cluster analysis basic concept method may organize employee major group executive manager staff partition group smaller subgroup instance general group staff divide subgroup senior officer officer trainee group form hierarchy easily summarize characterize datum organized hierarchy used find say average salary manager officer consider handwritten character recognition another example set handwriting sample may first partition general group group correspond unique character group partition subgroup since character may written multiple substantially different way necessary hierarchical partition continue recursively desire granularity reach previous example although partition datum hierarchically assume datum hierarchical structure ( eg manager level allelectronic hierarchy staff ) use hierarchy summarize represent underlie datum compress way hierarchy particularly useful datum visualization alternatively application may believe datum bear underlie hierarchical structure want discover example hierarchical cluster may uncover hierarchy allelectronic employee structure say salary study evolution hierarchical cluster may group animal accord biological feature uncover evolutionary path hierarchy species another example grouping configuration strategic game ( eg chess checker ) hierarchical way may help develop game strategy used train player section study hierarchical cluster method section 1031 begin discussion agglomerative versus divisive hierarchical cluster organize object hierarchy used bottom-up top-down strategy respectively agglomerative method start individual object cluster iteratively merged form larger cluster conversely divisive method initially let give object form one cluster iteratively split smaller cluster hierarchical cluster method encounter difficulty regard selection merge split point decision critical group object merged split process next step operate newly generate cluster neither undo do previously perform object swap cluster thus merge split decision well choose may lead low-quality cluster moreover method scale well decision merge split need examine evaluate many object cluster promising direction improve cluster quality hierarchical method integrate hierarchical cluster cluster technique result multiple-phase ( multiphase ) cluster introduce two method namely birch chameleon birch ( section 1033 ) begin partition object hierarchically used tree structure leaf low-level nonleaf node view “ microcluster ” depend resolution scale apply 
103 hierarchical method 459 cluster algorithms perform macrocluster microcluster chameleon ( section 1034 ) explore dynamic modele hierarchical cluster several orthogonal way categorize hierarchical cluster method instance may categorize algorithmic method probabilistic method bayesian method agglomerative divisive multiphase method algorithmic meaning consider datum object deterministic compute cluster accord deterministic distance object probabilistic method use probabilistic model capture cluster measure quality cluster fitness model discuss probabilistic hierarchical cluster section bayesian method compute distribution possible clustering instead output single deterministic cluster datum set return group cluster structure probability conditional give datum bayesian method consider advanced topic discuss book 1031 agglomerative versus divisive hierarchical cluster hierarchical cluster method either agglomerative divisive depend whether hierarchical decomposition form bottom-up ( merge ) topdown ( splitting ) fashion let ’ closer look strategy agglomerative hierarchical cluster method used bottom-up strategy typically start let object form cluster iteratively merge cluster larger larger cluster object single cluster certain termination condition satisfied single cluster become hierarchy ’ root merge step find two cluster closest ( accord similarity measure ) combine two form one cluster two cluster merged per iteration cluster contain least one object agglomerative method require n iteration divisive hierarchical cluster method employ top-down strategy start place object one cluster hierarchy ’ root divide root cluster several smaller subcluster recursively partition cluster smaller one partition process continue cluster lowest level coherent enough—either contain one object object within cluster sufficiently similar either agglomerative divisive hierarchical cluster user specify desire number cluster termination condition example 103 agglomerative versus divisive hierarchical cluster figure 106 show application agne ( agglomerative nest ) agglomerative hierarchical cluster method diana ( divisive analysis ) divisive hierarchical cluster method datum set five object { b c e } initially agne agglomerative method place object cluster cluster merged step-by-step accord criterion example cluster c1 c2 may merged object c1 object c2 form minimum euclidean distance two object 
chapter 10 cluster analysis basic concept method agglomerative ( agne ) step 0 step 1 step 2 step 3 step 4 ab b abcde c cde de e step 4 step 3 step 2 step 1 divisive ( diana ) step 0 figure 106 agglomerative divisive hierarchical cluster datum object { b c e } level l=0 b c e 10 l=1 l=2 06 l=3 04 l=4 02 08 similarity scale 460 00 figure 107 dendrogram representation hierarchical cluster datum object { b c e } different cluster single-linkage approach cluster represent object cluster similarity two cluster measure similarity closest pair datum point belong different cluster cluster-merge process repeat object eventually merged form one cluster diana divisive method proceed contrast way object used form one initial cluster cluster split accord principle maximum euclidean distance closest neighboring object cluster cluster-split process repeat eventually new cluster contain single object tree structure call dendrogram commonly used represent process hierarchical cluster show object group together ( agglomerative method ) partition ( divisive method ) step-by-step figure 107 show dendrogram five object present figure 106 l = 0 show five object singleton cluster level l = 1 object b group together form 
103 hierarchical method 461 first cluster stay together subsequent level also use vertical axis show similarity scale cluster example similarity two group object { b } { c e } roughly 016 merged together form single cluster challenge divisive method partition large cluster several smaller one example 2n−1 − 1 possible way partition set n object two exclusive subset n number object n large computationally prohibitive examine possibility consequently divisive method typically used heuristic partition lead inaccurate result sake efficiency divisive method typically backtrack partition decision make cluster partition alternative partition cluster consider due challenge divisive method many agglomerative method divisive method 1032 distance measure algorithmic method whether used agglomerative method divisive method core need measure distance two cluster cluster generally set object four widely used measure distance cluster follow p − p0 | distance two object point p p0 mi mean cluster ci ni number object ci also know linkage measure minimum distance distmin ( ci cj ) = maximum distance distmax ( ci cj ) = mean distance average distance min { p − p0 | } ( 103 ) max { p − p0 | } ( 104 ) p∈ci p0 ∈cj p∈ci p0 ∈cj distmean ( ci cj ) = mi − mj | distavg ( ci cj ) = 1 ni nj x ( 105 ) p − p0 | ( 106 ) p∈ci p0 ∈cj algorithm used minimum distance dmin ( ci cj ) measure distance cluster sometimes call nearest-neighbor cluster algorithm moreover cluster process terminate distance nearest cluster exceed user-defined threshold call single-linkage algorithm view datum point node graph edge form path node cluster merge two cluster ci cj correspond add edge nearest pair node ci cj edge link cluster always go distinct cluster result graph generate tree thus agglomerative hierarchical cluster algorithm used minimum distance measure also call 
462 chapter 10 cluster analysis basic concept method minimal span tree algorithm span tree graph tree connect vertex minimal span tree one least sum edge weight algorithm used maximum distance dmax ( ci cj ) measure distance cluster sometimes call farthest-neighbor cluster algorithm cluster process terminate maximum distance nearest cluster exceed user-defined threshold call complete-linkage algorithm view datum point node graph edge link node think cluster complete subgraph edge connect node cluster distance two cluster determine distant node two cluster farthest-neighbor algorithms tend minimize increase diameter cluster iteration true cluster rather compact approximately equal size method produce high-quality cluster otherwise cluster produce meaningless previous minimum maximum measure represent two extreme measure distance cluster tend overly sensitive outlier noisy datum use mean average distance compromise minimum maximum distance overcome outlier sensitivity problem whereas mean distance simplest compute average distance advantageous handle categoric well numeric datum computation mean vector categoric datum difficult impossible define example 104 single versus complete linkage let us apply hierarchical cluster datum set figure 108 ( ) figure 108 ( b ) show dendrogram used single linkage figure 108 ( c ) show case used complete linkage edge cluster { b j h } { c g f e } omitted ease presentation example show used single linkage find hierarchical cluster defined local proximity whereas complete linkage tend find cluster opt global closeness variation four essential linkage measure discuss example measure distance two cluster distance centroid ( ie central object ) cluster 1033 birch multiphase hierarchical cluster used cluster feature tree balanced iterative reduce cluster used hierarchy ( birch ) design cluster large amount numeric datum integrate hierarchical cluster ( initial microcluster stage ) cluster method iterative partition ( later macrocluster stage ) overcome two difficulty agglomerative cluster method ( 1 ) scalability ( 2 ) inability undo do previous step birch used notion cluster feature summarize cluster cluster feature tree ( cf-tree ) represent cluster hierarchy structure help 
103 hierarchical method b c 463 e j h g f ( ) datum set b c e j h g f b c e f g h j c ( b ) cluster used single linkage b c e j h g f b h j e f g ( c ) cluster used complete linkage figure 108 hierarchical cluster used single complete linkage cluster method achieve good speed scalability large even stream databasis also make effective incremental dynamic cluster incoming object consider cluster n d-dimensional datum object point cluster feature ( cf ) cluster 3-d vector summarize information cluster object defined cf = hn ls ssi ( 107 ) p ls linear n point ( ie ni=1 xi ) ss square sum pn sum datum point ( ie i=1 xi 2 ) cluster feature essentially summary statistic give cluster used cluster feature easily derive many useful statistic cluster example cluster ’ centroid x0 radius r diameter n p x0 = i=1 n xi = ls n ( 108 ) 
464 chapter 10 cluster analysis basic concept method = = v u n ux u ( xi − x0 ) 2 u i=1 n = v ux n x n u u ( xi − xj ) 2 u i=1 j=1 n ( n − 1 ) nss − 2ls2 + nls n2 = 2nss − 2ls2 n ( n − 1 ) ( 109 ) ( 1010 ) r average distance member object centroid average pairwise distance within cluster r reflect tightness cluster around centroid summarize cluster used cluster feature avoid store detailed information individual object point instead need constant size space store cluster feature key birch efficiency space moreover cluster feature additive two disjoint cluster c1 c2 cluster feature cf1 = hn1 ls1 ss1 cf2 = hn2 ls2 ss2 respectively cluster feature cluster form merge c1 c2 simply cf1 + cf2 = hn1 + n2 ls1 + ls2 ss1 + ss2 ( 1011 ) example 105 cluster feature suppose three point ( 2 5 ) ( 3 2 ) ( 4 3 ) cluster c1 cluster feature c1 cf1 = h3 ( 2 + 3 + 4 5 + 2 + 3 ) ( 22 + 32 + 42 52 + 22 + 32 ) = h3 ( 9 10 ) ( 29 38 ) suppose c1 disjoint second cluster c2 cf2 = h3 ( 35 36 ) ( 417 440 ) cluster feature new cluster c3 form merge c1 c2 derive add cf1 cf2 cf3 = h3 + 3 ( 9 + 35 10 + 36 ) ( 29 + 417 38 + 440 ) = h6 ( 44 46 ) ( 446 478 ) cf-tree height-balanced tree store cluster feature hierarchical cluster example show figure definition nonleaf node tree descendant “ ” nonleaf node store sum cfs child thus summarize cluster information child cf-tree two parameter branch factor b threshold t branch factor specify maximum number child per nonleaf node threshold parameter specify maximum diameter subcluster store leaf node tree two parameter implicitly control result tree ’ size give limit amount main memory important consideration birch minimize time require output ( o ) birch apply multiphase cluster technique single scan datum set yield basic good cluster 
103 hierarchical method cf1 cf11 cf12 cf2 cf1k cfk 465 root level first level figure 109 cf-tree structure one additional scan optionally used improve quality primary phase phase 1 birch scan database build initial in-memory cf-tree view multilevel compression datum try preserve datum ’ inherent cluster structure phase 2 birch apply ( select ) cluster algorithm cluster leaf node cf-tree remove sparse cluster outlier group dense cluster larger one phase 1 cf-tree build dynamically object insert thus method incremental object insert closest leaf entry ( subcluster ) diameter subcluster store leaf node insertion larger threshold value leaf node possibly node split insertion new object information object pass toward root tree size cf-tree change modify threshold size memory need store cf-tree larger size main memory larger threshold value specify cf-tree rebuild rebuild process perform build new tree leaf node old tree thus process rebuild tree do without necessity reread object point similar insertion node split construction b+-tree therefore build tree datum read heuristic method introduce deal outlier improve quality cf-tree additional scan datum cf-tree build cluster algorithm typical partition algorithm used cf-tree phase 2 “ effective birch ” time complexity algorithm ( n ) n number object cluster experiment show linear scalability algorithm respect number object good quality cluster datum however since node cf-tree hold limit number entry due size cf-tree node always correspond user may consider natural cluster moreover cluster spherical shape birch perform well used notion radius diameter control boundary cluster 
466 chapter 10 cluster analysis basic concept method idea cluster feature cf-tree apply beyond birch idea borrow many other tackle problem cluster stream dynamic datum 1034 chameleon multiphase hierarchical cluster used dynamic modele chameleon hierarchical cluster algorithm used dynamic modele determine similarity pair cluster chameleon cluster similarity assessed base ( 1 ) well connect object within cluster ( 2 ) proximity cluster two cluster merged interconnectivity high close together thus chameleon depend static user-supplied model automatically adapt internal characteristic cluster merged merge process facilitate discovery natural homogeneous cluster apply datum type long similarity function specify figure 1010 illustrate chameleon work chameleon used k-nearest-neighbor graph approach construct sparse graph vertex graph represent datum object exist edge two vertex ( object ) one object among k-most similar object edge weight reflect similarity object chameleon used graph partition algorithm partition k-nearest-neighbor graph large number relatively small subcluster minimize edge cut cluster c partition subcluster ci cj minimize weight edge would cut c bisect ci cj assess absolute interconnectivity cluster ci cj chameleon used agglomerative hierarchical cluster algorithm iteratively merge subcluster base similarity determine pair similar subcluster take account interconnectivity closeness cluster specifically chameleon determine similarity pair cluster ci cj accord relative interconnectivity ri ( ci cj ) relative closeness rc ( ci cj ) relative interconnectivity ri ( ci cj ) two cluster ci cj defined absolute interconnectivity ci cj normalize respect k-nearest-neighbor graph datum set construct sparse graph partition graph final cluster merge partition figure 1010 chameleon hierarchical cluster base k-nearest neighbor dynamic modele source base karypis han kumar [ khk99 ] 
103 hierarchical method 467 internal interconnectivity two cluster ci cj ri ( ci cj ) = ec { ci cj } | 1 2 ( ecci | + eccj | ) ( 1012 ) ec { ci cj } edge cut previously defined cluster contain ci cj similarly ecci ( eccj ) minimum sum cut edge partition ci ( cj ) two roughly equal part relative closeness rc ( ci cj ) pair cluster ci cj absolute closeness ci cj normalize respect internal closeness two cluster ci cj defined rc ( ci cj ) = sec { ci cj } ci | ci cj | ec ci c | j + ci c sec cj | ( 1013 ) sec { ci cj } average weight edge connect vertex ci vertex cj sec ci ( sec cj ) average weight edge belong mincut bisector cluster ci ( cj ) chameleon show greater power discover arbitrarily shape cluster high quality several well-known algorithms birch densitybased dbscan ( section 1041 ) however process cost high-dimensional datum may require ( n2 ) time n object worst case 1035 probabilistic hierarchical cluster algorithmic hierarchical cluster method used linkage measure tend easy understand often efficient cluster commonly used many cluster analysis application however algorithmic hierarchical cluster method suffer several drawback first choose good distance measure hierarchical cluster often far trivial second apply algorithmic method datum object miss attribute value case datum partially observed ( ie attribute value object miss ) easy apply algorithmic hierarchical cluster method distance computation conduct third algorithmic hierarchical cluster method heuristic step locally search good splitting decision consequently optimization goal result cluster hierarchy unclear probabilistic hierarchical cluster aim overcome disadvantage used probabilistic model measure distance cluster one way look cluster problem regard set datum object cluster sample underlie datum generation mechanism analyze formally generative model example conduct cluster analysis set marketing survey assume survey collect sample opinion possible customer datum generation mechanism probability 
468 chapter 10 cluster analysis basic concept method distribution opinion respect different customer obtain directly completely task cluster estimate generative model accurately possible used observed datum object cluster practice assume datum generative model adopt common distribution function gaussian distribution bernoulli distribution govern parameter task learn generative model reduce find parameter value model best fit observed datum set example 106 generative model suppose give set 1-d point x = { x1 xn } cluster analysis let us assume datum point generate gaussian distribution n ( µ σ 2 ) = √ 2 1 2π σ 2 e − ( x−µ ) 2 2σ ( 1014 ) parameter µ ( mean ) σ 2 ( variance ) probability point xi ∈ x generate model ( x −µ ) 2 1 − e 2σ 2 p ( xi µ σ 2 ) = √ 2π σ 2 ( 1015 ) consequently likelihood x generate model l ( n ( µ σ 2 ) x ) = p ( x|µ σ 2 ) = n i=1 √ 1 2π σ 2 e − ( xi −µ ) 2 2σ 2 ( 1016 ) task learn generative model find parameter µ σ 2 likelihood l ( n ( µ σ 2 ) x ) maximize find n ( µ0 σ02 ) = arg max { l ( n ( µ σ 2 ) x ) } ( 1017 ) max { l ( n ( µ σ 2 ) x ) } call maximum likelihood give set object quality cluster form object measure maximum likelihood set object partition cluster c1 cm quality measure q ( { c1 cm } ) = i=1 p ( ci ) ( 1018 ) 
103 hierarchical method 469 p ( ) maximum likelihood merge two cluster cj1 cj2 cluster cj1 ∪ cj2 change quality overall cluster q ( ( { c1 cm } − { cj1 cj2 } ) ∪ { cj1 ∪ cj2 } ) − q ( { c1 cm } ) qm p ( ci ) · p ( cj1 ∪ cj2 ) = i=1 − p ( ci ) p ( cj1 ) p ( cj2 ) i=1 = i=1  p ( cj1 ∪ cj2 ) −1 p ( ci ) p ( cj1 ) p ( cj2 )  ( 1019 ) q choose merge two cluster hierarchical cluster i=1 p ( ci ) constant pair cluster therefore give cluster c1 c2 distance measure dist ( ci cj ) = − log p ( c1 ∪ c2 ) p ( c1 ) p ( c2 ) ( 1020 ) probabilistic hierarchical cluster method adopt agglomerative cluster framework use probabilistic model ( eq 1020 ) measure distance cluster upon close observation eq ( 1019 ) see merge two cluster may p ( c ∪c ) always lead improvement cluster quality p ( cj j1 ) p ( cj2j ) may less 1 2 example assume gaussian distribution function used model figure although merge cluster c1 c2 result cluster better fit gaussian distribution merge cluster c3 c4 lower cluster quality gaussian function fit merged cluster well base observation probabilistic hierarchical cluster scheme start one cluster per object merge two cluster ci cj distance negative iteration try find ci cj maximize p ( c ∪c ) p ( c ∪c ) j j log p ( ci ) p ( c iteration continue long log p ( ci ) p ( c > 0 long j ) j ) improvement cluster quality pseudocode give figure 1012 probabilistic hierarchical cluster method easy understand generally efficiency algorithmic agglomerative hierarchical cluster method fact share framework probabilistic model interpretable sometimes less flexible distance metric probabilistic model handle partially observed datum example give multidimensional datum set object miss value dimension learn gaussian model dimension independently used observed value dimension result cluster hierarchy accomplish optimization goal fitting datum select probabilistic model drawback used probabilistic hierarchical cluster output one hierarchy respect choose probabilistic model handle uncertainty cluster hierarchy give datum set may exist multiple hierarchy 
470 chapter 10 cluster analysis basic concept method c1 c2 ( ) c3 c4 ( b ) ( c ) figure 1011 merge cluster probabilistic hierarchical cluster ( ) merge cluster c1 c2 lead increase overall cluster quality merge cluster ( b ) c3 ( c ) c4 algorithm probabilistic hierarchical cluster algorithm input = { o1 } datum set contain n object output hierarchy cluster method ( 1 ) create cluster object ci = { oi } 1 ≤ ≤ n ( 2 ) = 1 n p ( c ∪c ) ( 3 ) j find pair cluster ci cj ci cj = arg maxi6=j log p ( c ) p ( c ) ( 4 ) j log p ( c ) p ( c ) > 0 merge ci cj ( 5 ) else stop p ( c ∪c ) j j figure 1012 probabilistic hierarchical cluster algorithm fit observed datum neither algorithmic approach probabilistic approach find distribution hierarchy recently bayesian tree-structure model develop handle problem bayesian sophisticated probabilistic cluster method consider advanced topic cover book 
104 density-based method 104 471 density-based method partition hierarchical method design find spherical-shap cluster difficulty find cluster arbitrary shape “ ” shape oval cluster figure give datum would likely inaccurately identify convex region noise outlier include cluster find cluster arbitrary shape alternatively model cluster dense region datum space separated sparse region main strategy behind density-based cluster method discover cluster nonspherical shape section learn basic technique density-based cluster study three representative method namely dbscan ( section 1041 ) optic ( section 1042 ) denclue ( section 1043 ) 1041 dbscan density-based cluster base connect region high density “ find dense region density-based cluster ” density object measure number object close o dbscan ( density-based spatial cluster application noise ) find core object object dense neighborhood connect core object neighborhood form dense region cluster “ dbscan quantify neighborhood object ” user-specified parameter  > 0 used specify radius neighborhood consider every object -neighborhood object space within radius  center due fix neighborhood size parameterized  density neighborhood measure simply number object neighborhood determine whether neighborhood dense dbscan used another user-specified figure 1013 cluster arbitrary shape 
472 chapter 10 cluster analysis basic concept method parameter minpt specify density threshold dense region object core object -neighborhood object contain least minpt object core object pillar dense region give set object identify core object respect give parameter  minpt cluster task therein reduce used core object neighborhood form dense region dense region cluster core object q object p say p directly density-reachable q ( respect  minpt ) p within -neighborhood q clearly object p directly density-reachable another object q q core object p -neighborhood q used directly density-reachable relation core object “ bring ” object -neighborhood dense region “ assemble large dense region used small dense region center core object ” dbscan p density-reachable q ( respect  minpt ) chain object p1 pn p1 = q pn = p pi+1 directly density-reachable pi respect  minpt 1 ≤ ≤ n pi ∈ d note density-reachability equivalence relation symmetric o1 o2 core object o1 density-reachable o2 o2 density-reachable o1 however o2 core object o1 o1 may density-reachable o2 vice versa connect core object well neighbor dense region dbscan used notion density-connectedness two object p1 p2 ∈ density-connect respect  minpt object q ∈ p1 p2 densityreachable q respect  minpt unlike density-reachability densityconnectedness equivalence relation easy show object o1 o2 o3 o1 o2 density-connect o2 o3 density-connect o1 o3 example 107 density-reachability density-connectivity consider figure 1014 give  represent radius circle say let minpt = 3 labele point p r core object -neighborhood contain least three point object q directly density-reachable m object directly density-reachable p vice versa object q ( indirectly ) density-reachable p q directly densityreachable directly density-reachable p however p densityreachable q q core object similarly r density-reachable density-reachable r thus r density-connect use closure density-connectedness find connect dense region cluster close set density-based cluster subset c ⊆ cluster ( 1 ) two object o1 o2 ∈ c o1 o2 density-connect ( 2 ) exist object ∈ c another object o0 ∈ ( − c ) o0 densityconnect 
104 density-based method 473 q p r figure 1014 density-reachability density-connectivity density-based cluster source base ester kriegel sander xu [ eksx96 ] “ dbscan find cluster ” initially object give datum set marked “ ” dbscan randomly select unvisite object p mark p “ visit ” check whether -neighborhood p contain least minpt object p marked noise point otherwise new cluster c create p object -neighborhood p add candidate set n dbscan iteratively add c object n belong cluster process object p0 n carry label “ unvisite ” dbscan mark “ visit ” check -neighborhood -neighborhood p0 least minpt object object -neighborhood p0 add n dbscan continue add object c c longer expand n empty time cluster c complete thus output find next cluster dbscan randomly select unvisite object remain one cluster process continue object visit pseudocode dbscan algorithm give figure 1015 spatial index used computational complexity dbscan ( n log n ) n number database object otherwise complexity ( n2 ) appropriate setting user-defined parameter  minpt algorithm effective find arbitrary-shap cluster 1042 optic order point identify cluster structure although dbscan cluster object give input parameter  ( maximum radius neighborhood ) minpt ( minimum number point require neighborhood core object ) encumber user responsibility select parameter value lead discovery acceptable cluster problem associate many cluster algorithms parameter setting 
474 chapter 10 cluster analysis basic concept method algorithm dbscan density-based cluster algorithm input datum set contain n object  radius parameter minpt neighborhood density threshold output set density-based cluster method ( 1 ) mark object unvisite ( 2 ) ( 3 ) randomly select unvisite object p ( 4 ) mark p visit ( 5 ) -neighborhood p least minpt object ( 6 ) create new cluster c add p c ( 7 ) let n set object -neighborhood p ( 8 ) point p0 n ( 9 ) p0 unvisite ( 10 ) mark p0 visit ( 11 ) -neighborhood p0 least minpt point add point n ( 12 ) p0 yet member cluster add p0 c ( 13 ) end ( 14 ) output c ( 15 ) else mark p noise ( 16 ) object unvisite figure 1015 dbscan algorithm usually empirically set difficult determine especially real-world highdimensional datum set algorithms sensitive parameter value slightly different setting may lead different clustering datum moreover real-world high-dimensional datum set often skewer distribution intrinsic cluster structure may well characterize single set global density parameter note density-based cluster monotonic respect neighborhood threshold dbscan fix minpt value two neighborhood threshold 1 < 2 cluster c respect 1 minpt must subset cluster c 0 respect 2 minpt mean two object density-based cluster must also cluster lower density requirement overcome difficulty used one set global parameter cluster analysis cluster analysis method call optic propose optic explicitly produce datum set cluster instead output cluster order linear list 
104 density-based method 475 object analysis represent density-based cluster structure datum object denser cluster list closer cluster order order equivalent density-based cluster obtain wide range parameter setting thus optic require user provide specific density threshold cluster order used extract basic cluster information ( eg cluster center arbitrary-shap cluster ) derive intrinsic cluster structure well provide visualization cluster construct different clustering simultaneously object processed specific order order select object density-reachable respect lowest  value cluster higher density ( lower  ) finished first base idea optic need two important piece information per object core-distance object p smallest value  0  0 neighborhood p least minpt object  0 minimum distance threshold make p core object p core object respect  minpt core-distance p undefined reachability-distance object p q minimum radius value make p density-reachable q accord definition density-reachability q core object p must neighborhood q therefore reachability-distance q p max { core-distance ( q ) dist ( p q ) } q core object respect  minpt reachability-distance p q undefined object p may directly reachable multiple core object therefore p may multiple reachability-distance respect different core object smallest reachability-distance p particular interest give shortest path p connect dense cluster example 108 core-distance reachability-distance figure 1016 illustrate concept coredistance reachability-distance suppose  = 6 mm minpt = coredistance p distance  0 p fourth closest datum object p reachability-distance q1 p core-distance p ( ie  0 = 3 mm ) greater euclidean distance p q1 reachability-distance q2 respect p euclidean distance p q2 greater core-distance p optic compute order object give database object database store core-distance suitable reachability-distance optic maintain list call orderseed generate output order object orderseed sort reachability-distance respective closest core object smallest reachability-distance object optic begin arbitrary object input database current object p retrieve -neighborhood p determine core-distance set reachability-distance undefined current object p written output 
476 chapter 10 cluster analysis basic concept method = 6 mm p = 3 mm = 6 mm  p q1 q2 core-distance p reachability-distance ( p q1 ) = = 3 mm reachability-distance ( p q2 ) = dist ( p q2 ) figure 1016 optic terminology source base ankerst breunig kriegel sander [ abks99 ] p core object optic simply move next object orderseed list ( input database orderseed empty ) p core object object q -neighborhood p optic update reachability-distance p insert q orderseed q yet processed iteration continue input fully consume orderseed empty datum set ’ cluster order represent graphically help visualize understand cluster structure datum set example figure 1017 reachability plot simple 2-d datum set present general overview datum structure cluster datum object plot cluster order ( horizontal axis ) together respective reachability-distance ( vertical axis ) three gaussian “ bump ” plot reflect three cluster datum set method also develop view cluster structure high-dimensional datum various level detail structure optic algorithm similar dbscan consequently two algorithms time complexity complexity ( n log n ) spatial index used ( n2 ) otherwise n number object 1043 denclue cluster base density distribution function density estimation core issue density-based cluster method denclue ( density-based cluster ) cluster method base set density distribution function first give background density estimation describe denclue algorithm probability statistic density estimation estimation unobservable underlie probability density function base set observed datum context density-based cluster unobservable underlie probability density function true distribution population possible object analyze observed datum set regard random sample population 
104 density-based method 477 reachability-distance undefined cluster order object figure 1017 cluster order optic source adapt ankerst breunig kriegel sander [ abks99 ] 1 2 figure 1018 subtlety density estimation dbscan optic increase neighborhood radius slightly 1 2 result much higher density dbscan optic density calculate count number object neighborhood defined radius parameter  density estimate highly sensitive radius value used example figure 1018 density change significantly radius increase small amount overcome problem kernel density estimation used nonparametric density estimation approach statistic general idea behind kernel density estimation simple treat observed object indicator 
478 chapter 10 cluster analysis basic concept method high-probability density surround region probability density point depend distance point observed object formally let x1 xn independent identically distribute sample random variable f kernel density approximation probability density function   n x − xi 1 x ( 1021 ) k fˆh ( x ) = nh h i=1 k ( ) kernel h bandwidth serve smooth parameter kernel regard function modele influence sample point within neighborhood technically kernel k ( ) isra non-negative real-valu integrable func+∞ tion satisfy two requirement −∞ k ( u ) du = 1 k ( −u ) = k ( u ) value u frequently used kernel standard gaussian function mean 0 variance 1   x − xi 1 − ( x − 2xi ) 2 2h k ( 1022 ) √ e h 2π denclue used gaussian kernel estimate density base give set object cluster point x∗ call density attractor local maximum estimate density function avoid trivial local maximum point denclue used noise threshold ξ consider density attractor x∗ fˆ ( x∗ ) ≥ ξ nontrivial density attractor center cluster object analysis assign cluster density attractor used stepwise hill-climb procedure object x hill-climb procedure start x guide gradient estimate density function density attractor x compute x0 = x xj+1 = xj + δ ∇ fˆ ( xj ) ∇ fˆ ( xj ) | ( 1023 ) δ parameter control speed convergence ∇ fˆ ( x ) = hd+2 n 1   x − xi ( x − x ) k i=1 h pn ( 1024 ) hill-climb procedure stop step k > 0 fˆ ( xk+1 ) < fˆ ( xk ) assign x density attractor x∗ = xk object x outlier noise converge hillclimb procedure local maximum x∗ fˆ ( x∗ ) < ξ cluster denclue set density attractor x set input object c object c assign density attractor x exist path every pair density attractor density ξ used multiple density attractor connect path denclue find cluster arbitrary shape 
105 grid-based method 479 denclue several advantage regard generalization several well-known cluster method single-linkage approach dbscan moreover denclue invariant noise kernel density estimation effectively reduce influence noise uniformly distribute noise input datum 105 grid-based method cluster method discuss far data-driven—they partition set object adapt distribution object embedding space alternatively grid-based cluster method take space-driven approach partition embedding space cell independent distribution input object grid-based cluster approach used multiresolution grid datum structure quantize object space finite number cell form grid structure operation cluster perform main advantage approach fast process time typically independent number datum object yet dependent number cell dimension quantized space section illustrate grid-based cluster used two typical example sting ( section 1051 ) explore statistical information store grid cell clique ( section 1052 ) represent - density-based approach subspace cluster high-dimensional datum space 1051 sting statistical information grid sting grid-based multiresolution cluster technique embedding spatial area input object divide rectangular cell space divide hierarchical recursive way several level rectangular cell correspond different level resolution form hierarchical structure cell high level partition form number cell next lower level statistical information regard attribute grid cell mean maximum minimum value precompute store statistical parameter statistical parameter useful query process datum analysis task figure 1019 show hierarchical structure sting cluster statistical parameter higher-level cell easily compute parameter lower-level cell parameter include follow attribute-independent parameter count attribute-dependent parameter mean stdev ( standard deviation ) min ( minimum ) max ( maximum ) type distribution attribute value cell follow normal uniform exponential none ( distribution unknown ) attribute select measure analysis price house object datum load database parameter count mean stdev min max bottom-level cell calculate directly datum value distribution may either assign user distribution type know 
480 chapter 10 cluster analysis basic concept method first layer ( – 1 ) st layer ith layer figure 1019 hierarchical structure sting cluster beforehand obtain hypothesis test χ 2 test type distribution higher-level cell compute base majority distribution type corresponding lower-level cell conjunction threshold filter process distribution lower-level cell disagree fail threshold test distribution type high-level cell set none “ statistical information useful query answer ” statistical parameter used top-down grid-based manner follow first layer within hierarchical structure determine query-answer process start layer typically contain small number cell cell current layer compute confidence interval ( estimate probability range ) reflect cell ’ relevancy give query irrelevant cell remove consideration process next lower level examine remain relevant cell process repeat bottom layer reach time query specification meet region relevant cell satisfy query return otherwise datum fall relevant cell retrieve processed meet query ’ requirement interesting property sting approach cluster result dbscan granularity approach 0 ( ie toward low-level datum ) word used count cell size information dense cluster identify approximately used sting therefore sting also regard density-based cluster method “ advantage sting offer cluster method ” sting offer several advantage ( 1 ) grid-based computation query-independent statistical information store cell represent summary information datum grid cell independent query ( 2 ) grid structure facilitate parallel process incremental update ( 3 ) method ’ efficiency major advantage sting go database compute statistical parameter cell hence time complexity generate cluster ( n ) n total number object generate hierarchical structure query process time 
105 grid-based method 481 ( g ) g total number grid cell lowest level usually much smaller n sting used multiresolution approach cluster analysis quality sting cluster depend granularity lowest level grid structure granularity fine cost process increase substantially however bottom level grid structure coarse may reduce quality cluster analysis moreover sting consider spatial relationship child neighboring cell construction parent cell result shape result cluster isothetic cluster boundary either horizontal vertical diagonal boundary detected may lower quality accuracy cluster despite fast process time technique 1052 clique apriori-like subspace cluster method datum object often ten attribute many may irrelevant value attribute may vary considerably factor make difficult locate cluster span entire datum space may meaningful instead search cluster within different subspace datum example consider healthinformatic application patient record contain extensive attribute describe personal information numerous symptom condition family history find nontrivial group patient even attribute strongly agree unlikely bird flu patient instance age gender job attribute may vary dramatically within wide range value thus difficult find cluster within entire datum space instead search subspace may find cluster similar patient lower-dimensional space ( eg patient similar one respect symptom like high fever cough runny nose age 3 16 ) clique ( cluster quest ) simple grid-based method find densitybased cluster subspace clique partition dimension nonoverlapping interval thereby partition entire embedding space datum object cell used density threshold identify dense cell sparse one cell dense number object map exceed density threshold main strategy behind clique identify candidate search space used monotonicity dense cell respect dimensionality base apriori property used frequent pattern association rule mining ( chapter 6 ) context cluster subspace monotonicity say follow k-dimensional cell c ( k > 1 ) least l point every ( k − 1 ) dimensional projection c cell ( k − 1 ) dimensional subspace least l point consider figure 1020 embedding datum space contain three dimension age salary vacation 2-d cell say subspace form age salary contain l point projection cell every dimension age salary respectively contain least l point clique perform cluster two step first step clique partition d-dimensional datum space nonoverlapping rectangular unit identify dense unit among clique find dense cell subspace 
482 chapter 10 cluster analysis basic concept method 7 salary ( $ 10000 ) 6 5 4 3 2 1 0 20 30 40 50 60 age 30 40 50 60 age 7 vacation ( week ) 6 5 4 3 2 1 vacation 0 20 50 age sa la ry 30 figure 1020 dense unit find respect age dimension salary vacation intersected provide candidate search space dense unit higher dimensionality 
106 evaluation cluster 483 clique partition every dimension interval identify interval contain least l point l density threshold clique iteratively join two k-dimensional dense cell c1 c2 subspace ( di1 dik ) ( dj1 djk ) respectively di1 = dj1 dik−1 = djk−1 c1 c2 share interval dimension join operation generate new ( k + 1 ) dimensional candidate cell c space ( di1 dik−1 dik djk ) clique check whether number point c pass density threshold iteration terminate candidate generate candidate cell dense second step clique used dense cell subspace assemble cluster arbitrary shape idea apply minimum description length ( mdl ) principle ( chapter 8 ) use maximal region cover connect dense cell maximal region hyperrectangle every cell fall region dense region extend dimension subspace find best description cluster general np-hard thus clique adopt simple greedy approach start arbitrary dense cell find maximal region cover cell work remain dense cell yet cover greedy method terminate dense cell cover “ effective clique ” clique automatically find subspace highest dimensionality high-density cluster exist subspace insensitive order input object presume canonical datum distribution scale linearly size input good scalability number dimension datum increase however obtain meaningful cluster dependent proper tune grid size ( stable structure ) density threshold difficult practice grid size density threshold used across combination dimension datum set thus accuracy cluster result may degraded expense method ’ simplicity moreover give dense region projection region onto lower-dimensionality subspace also dense result large overlap among report dense region furthermore difficult find cluster rather different density within different dimensional subspace several extension approach follow similar philosophy example think grid set fix bin instead used fix bin dimension use adaptive data-driven strategy dynamically determine bin dimension base datum distribution statistic alternatively instead used density threshold may use entropy ( chapter 8 ) measure quality subspace cluster 106 evaluation cluster learn cluster know several popular cluster method may ask “ try cluster method datum set evaluate whether cluster result good ” general cluster evaluation assess 
484 chapter 10 cluster analysis basic concept method feasibility cluster analysis datum set quality result generate cluster method major task cluster evaluation include follow assess cluster tendency task give datum set assess whether nonrandom structure exist datum blindly apply cluster method datum set return cluster however cluster mine may mislead cluster analysis datum set meaningful nonrandom structure datum determine number cluster datum set algorithms k-mean require number cluster datum set parameter moreover number cluster regard interesting important summary statistic datum set therefore desirable estimate number even cluster algorithm used derive detailed cluster measure cluster quality apply cluster method datum set want assess good result cluster number measure used method measure well cluster fit datum set other measure well cluster match ground truth truth available also measure score clustering thus compare two set cluster result datum set rest section discuss three topic 1061 assess cluster tendency cluster tendency assessment determine whether give datum set non-random structure may lead meaningful cluster consider datum set non-random structure set uniformly distribute point datum space even though cluster algorithm may return cluster datum cluster random meaningful example 109 cluster require nonuniform distribution datum figure 1021 show datum set uniformly distribute 2-d datum space although cluster algorithm may still artificially partition point group group unlikely mean anything significant application due uniform distribution datum “ assess cluster tendency datum set ” intuitively try measure probability datum set generate uniform datum distribution achieve used statistical test spatial randomness illustrate idea let ’ look simple yet effective statistic call hopkin statistic hopkin statistic spatial statistic test spatial randomness variable distribute space give datum set regard sample 
106 evaluation cluster 485 figure 1021 datum set uniformly distribute datum space random variable want determine far away uniformly distribute datum space calculate hopkin statistic follow sample n point p1 pn uniformly d point probability include sample point pi find nearest neighbor pi ( 1 ≤ ≤ n ) let xi distance pi nearest neighbor d xi = min { dist ( pi v ) } v∈d ( 1025 ) sample n point q1 qn uniformly d qi ( 1 ≤ ≤ n ) find nearest neighbor qi − { qi } let yi distance qi nearest neighbor − { qi } yi = min { dist ( qi v ) } v∈d v6=qi ( 1026 ) calculate hopkin statistic h pn h = pn i=1 xi i=1 yi + pn i=1 yi ( 1027 ) “ hopkin statistic tell us likely datum set follow pn uniform distribution datum space ” uniformly distribute i=1 yi pn x would close thus h would 05 however i=1 p p highly skewer ni=1 yi would substantially smaller ni=1 xi expectation thus h would close 0 
486 chapter 10 cluster analysis basic concept method null hypothesis homogeneous hypothesis—that uniformly distribute thus contain meaningful cluster nonhomogeneous hypothesis ( ie uniformly distribute thus contain cluster ) alternative hypothesis conduct hopkin statistic test iteratively used 05 threshold reject alternative hypothesis h > 05 unlikely statistically significant cluster 1062 determine number cluster determine “ right ” number cluster datum set important cluster algorithms like k-mean require parameter also appropriate number cluster control proper granularity cluster analysis regard find good balance compressibility accuracy cluster analysis consider two extreme case treat entire datum set cluster would maximize compression datum cluster analysis value hand treat object datum set cluster give finest cluster resolution ( ie accurate due zero distance object corresponding cluster center ) method like k-mean even achieve best cost however one object per cluster enable datum summarization determine number cluster far easy often “ right ” number ambiguous figure right number cluster often depend distribution ’ shape scale datum set well cluster resolution require user many possible way estimate number cluster briefly introduce simple yet popular effective method q simple method set number cluster n2 datum set n √ point expectation cluster 2n point elbow method base observation increase number cluster help reduce sum within-cluster variance cluster cluster allow one capture finer group datum object similar however marginal effect reduce sum within-cluster variance may drop many cluster form splitting cohesive cluster two give small reduction consequently heuristic select right number cluster use turn point curve sum within-cluster variance respect number cluster technically give number k > 0 form k cluster datum set question used cluster algorithm like k-mean calculate sum within-cluster variance var ( k ) plot curve var respect k first ( significant ) turn point curve suggest “ right ” number advanced method determine number cluster used information criterium information theoretic approach please refer bibliographic note information ( section 109 ) 
106 evaluation cluster 487 “ right ” number cluster datum set also determine crossvalidation technique often used classification ( chapter 8 ) first divide give datum set part next use − 1 part build cluster model use remain part test quality cluster example point test set find closest centroid consequently use sum square distance point test set closest centroid measure well cluster model fit test set integer k > 0 repeat process time derive clustering k cluster used part turn test set average quality measure take overall quality measure compare overall quality measure respect different value k find number cluster best fit datum 1063 measure cluster quality suppose assessed cluster tendency give datum set may also try predetermine number cluster set apply one multiple cluster method obtain clustering datum set “ good cluster generate method compare clustering generate different method ” method choose measure quality cluster general method categorize two group accord whether ground truth available ground truth ideal cluster often build used human expert ground truth available used extrinsic method compare cluster group truth measure ground truth unavailable use intrinsic method evaluate goodness cluster consider well cluster separated ground truth consider supervision form “ cluster ” hence extrinsic method also know supervised method intrinsic method unsupervised method let ’ look simple method category extrinsic method ground truth available compare cluster assess cluster thus core task extrinsic method assign score q ( c cg ) cluster c give ground truth cg whether extrinsic method effective largely depend measure q used general measure q cluster quality effective satisfy follow four essential criterium cluster homogeneity require pure cluster cluster better cluster suppose ground truth say object datum set belong category l1 ln consider cluster c1 wherein cluster c ∈ c1 contain object two category li lj ( 1 ≤ < j ≤ n ) also 
488 chapter 10 cluster analysis basic concept method consider cluster c2 identical c1 except c2 split two cluster contain object li lj respectively cluster quality measure q respect cluster homogeneity give higher score c2 c1 q ( c2 cg ) > q ( c1 cg ) cluster completeness counterpart cluster homogeneity cluster completeness require cluster two object belong category accord ground truth assign cluster cluster completeness require cluster assign object belong category ( accord ground truth ) cluster consider cluster c1 contain cluster c1 c2 member belong category accord ground truth let cluster c2 identical c1 except c1 c2 merged one cluster c2 cluster quality measure q respect cluster completeness give higher score c2 q ( c2 cg ) > q ( c1 cg ) rag bag many practical scenario often “ rag bag ” category contain object merged object category often call “ miscellaneous ” “ ” rag bag criterion state putt heterogeneous object pure cluster penalize putt rag bag consider cluster c1 cluster c ∈ c1 object c except one denote belong category accord ground truth consider cluster c2 identical c1 except assign cluster c 0 = c c2 c 0 contain object various category accord ground truth thus noisy word c 0 c2 rag bag cluster quality measure q respect rag bag criterion give higher score c2 q ( c2 cg ) > q ( c1 cg ) small cluster preservation small category split small piece cluster small piece may likely become noise thus small category discover cluster small cluster preservation criterion state splitting small category piece harmful splitting large category piece consider extreme case let datum set n + 2 object accord ground truth n object denote o1 belong one category two object denote on+1 on+2 belong another category suppose cluster c1 three cluster c1 = { o1 } c2 = { on+1 } c3 = { on+2 } let cluster c2 three cluster namely c1 = { o1 on−1 } c2 = { } c3 = { on+1 on+2 } word c1 split small category c2 split big category cluster quality measure q preserve small cluster give higher score c2 q ( c2 cg ) > q ( c1 cg ) many cluster quality measure satisfy four criterium introduce bcube precision recall metric satisfy four criterium bcube evaluate precision recall every object cluster give datum set accord ground truth precision object indicate many object cluster belong category object recall 
106 evaluation cluster 489 object reflect many object category assign cluster formally let = { o1 } set object c cluster d let l ( oi ) ( 1 ≤ ≤ n ) category oi give ground truth c ( oi ) cluster id oi c two object oi oj ( 1 ≤ j ≤ n = j ) correctness relation oi oj cluster c give ( 1 l ( oi ) = l ( oj ) ⇔ c ( oi ) = c ( oj ) correctness ( oi oj ) = 0 otherwise ( 1028 ) bcube precision defined x n x oj i6=j c ( oi ) c ( oj ) precision bcube = correctness ( oi oj ) k { oj i = j c ( oi ) = c ( oj ) } k i=1 n ( 1029 ) bcube recall defined x n x oj i6=j l ( oi ) l ( oj ) recall bcube = i=1 correctness ( oi oj ) k { oj i = j l ( oi ) = l ( oj ) } k n ( 1030 ) intrinsic method ground truth datum set available use intrinsic method assess cluster quality general intrinsic method evaluate cluster examine well cluster separated compact cluster many intrinsic method advantage similarity metric object datum set silhouette coefficient measure datum set n object suppose partition k cluster c1 ck object ∈ calculate ( ) average distance object cluster belong similarly b ( ) minimum average distance cluster belong formally suppose ∈ ci ( 1 ≤ ≤ k ) p ( ) = o0 ∈ci o6=o0 dist ( ) ci | − 1 0 ( 1031 ) 
490 chapter 10 cluster analysis basic concept method ( p b ( ) = min cj 1≤j≤k j6=i 0 ) o0 ∈cj dist ( ) cj | ( 1032 ) silhouette coefficient defined ( ) = b ( ) − ( ) max { ( ) b ( ) } ( 1033 ) value silhouette coefficient −1 value ( ) reflect compactness cluster belong smaller value compact cluster value b ( ) capture degree separated cluster larger b ( ) separated cluster therefore silhouette coefficient value approach 1 cluster contain compact far away cluster preferable case however silhouette coefficient value negative ( ie b ( ) < ( ) ) mean expectation closer object another cluster object cluster many case bad situation avoid measure cluster ’ fitness within cluster compute average silhouette coefficient value object cluster measure quality cluster use average silhouette coefficient value object datum set silhouette coefficient intrinsic measure also used elbow method heuristically derive number cluster datum set replace sum within-cluster variance 107 summary cluster collection datum object similar one another within cluster dissimilar object cluster process grouping set physical abstract object class similar object call cluster cluster analysis extensive application include business intelligence image pattern recognition web search biology security cluster analysis used standalone datum mining tool gain insight datum distribution preprocess step datum mining algorithms operate detected cluster cluster dynamic field research datum mining related unsupervised learn machine learn cluster challenge field typical requirement include scalability ability deal different type datum attribute discovery cluster arbitrary shape minimal requirement domain knowledge determine input parameter ability deal noisy datum incremental cluster 
108 exercise 491 insensitivity input order capability cluster high-dimensionality datum constraint-based cluster well interpretability usability many cluster algorithms develop categorize several orthogonal aspect regard partition criterium separation cluster similarity measure used cluster space chapter discuss major fundamental cluster method follow category partition method hierarchical method density-based method grid-based method algorithms may belong one category partition method first create initial set k partition parameter k number partition construct used iterative relocation technique attempt improve partition move object one group another typical partition method include k-mean k-medoid claran hierarchical method create hierarchical decomposition give set datum object method classify either agglomerative ( bottom-up ) divisive ( top-down ) base hierarchical decomposition form compensate rigidity merge split quality hierarchical agglomeration improve analyze object linkage hierarchical partition ( eg chameleon ) first perform microcluster ( grouping object “ microcluster ” ) operate microcluster cluster technique iterative relocation ( birch ) density-based method cluster object base notion density grow cluster either accord density neighborhood object ( eg dbscan ) accord density function ( eg denclue ) optic density-based method generate augment order datum ’ cluster structure grid-based method first quantize object space finite number cell form grid structure perform cluster grid structure sting typical example grid-based method base statistical information store grid cell clique grid-based subspace cluster algorithm cluster evaluation assess feasibility cluster analysis datum set quality result generate cluster method task include assess cluster tendency determine number cluster measure cluster quality 108 exercise 101 briefly describe give example follow approach cluster partition method hierarchical method density-based method grid-based method 
492 chapter 10 cluster analysis basic concept method 102 suppose datum mining task cluster point ( ( x ) represent location ) three cluster point a1 ( 2 10 ) a2 ( 2 5 ) a3 ( 8 4 ) b1 ( 5 8 ) b2 ( 7 5 ) b3 ( 6 4 ) c1 ( 1 2 ) c2 ( 4 9 ) distance function euclidean distance suppose initially assign a1 b1 c1 center cluster respectively use k-mean algorithm show ( ) three cluster center first round execution ( b ) final three cluster 103 use example show k-mean algorithm may find global optimum optimize within-cluster variation 104 k-mean algorithm interesting note choose initial cluster center carefully may able speed algorithm ’ convergence also guarantee quality final cluster + algorithm variant k-mean choose initial center follow first select one center uniformly random object datum set iteratively object p choose center choose object new center object choose random probability proportional dist ( p ) 2 dist ( p ) distance p closest center already choose iteration continue k center select explain method speed convergence k-mean algorithm also guarantee quality final cluster result 105 provide pseudocode object reassignment step pam algorithm 106 k-mean k-medoid algorithms perform effective cluster ( ) illustrate strength weakness k-mean comparison k-medoid ( b ) illustrate strength weakness scheme comparison hierarchical cluster scheme ( eg agne ) 107 prove dbscan density-connectedness equivalence relation 108 prove dbscan fix minpt value two neighborhood threshold 1 < 2 cluster c respect 1 minpt must subset cluster c 0 respect 2 minpt 109 provide pseudocode optic algorithm 1010 birch encounter difficulty find cluster arbitrary shape optic propose modification birch help find cluster arbitrary shape 1011 provide pseudocode step clique find dense cell subspace 
108 exercise 493 1012 present condition density-based cluster suitable partitioning-based cluster hierarchical cluster give application example support argument 1013 give example specific cluster method integrate example one cluster algorithm used preprocess step another addition provide reasoning integration two method may sometimes lead improve cluster quality efficiency 1014 cluster recognize important datum mining task broad application give one application example follow case ( ) application used cluster major datum mining function ( b ) application used cluster preprocess tool datum preparation datum mining task 1015 datum cube multidimensional databasis contain nominal ordinal numeric datum hierarchical aggregate form base learn cluster method design cluster method find cluster large datum cube effectively efficiently 1016 describe follow cluster algorithms term follow criterium ( 1 ) shape cluster determine ( 2 ) input parameter must specify ( 3 ) limitation ( ) ( b ) ( c ) ( ) ( e ) ( f ) k-mean k-medoid clara birch chameleon dbscan 1017 human eye fast effective judge quality cluster method 2-d datum design datum visualization method may help human visualize datum cluster judge cluster quality 3-d datum even higher-dimensional datum 1018 suppose allocate number automatic teller machine ( atms ) give region satisfy number constraint household workplace may cluster typically one atm assign per cluster cluster however may constrain two factor ( 1 ) obstacle object ( ie bridge river highway affect atm accessibility ) ( 2 ) additional user-specified constraint atm serve least 10000 household cluster algorithm k-mean modify quality cluster constraint 1019 constraint-based cluster aside minimum number customer cluster ( atm allocation ) constraint many kind 
494 chapter 10 cluster analysis basic concept method constraint example constraint can form maximum number customer per cluster average income customer per cluster maximum distance every two cluster categorize kind constraint impose cluster produce discuss perform cluster efficiently kind constraint 1020 design privacy-preserve cluster method datum owner would able ask third party mine datum quality cluster without worry potential inappropriate disclosure certain private sensitive information store datum 1021 show bcube metric satisfy four essential requirement extrinsic cluster evaluation method 109 bibliographic note cluster extensively study 40 year across many discipline due broad application book pattern classification machine learn contain chapter cluster analysis unsupervised learn several textbook dedicate method cluster analysis include hartigan [ har75 ] jain dube [ jd88 ] kaufman rousseeuw [ kr90 ] arabie hubert de sorte [ ahs96 ] also many survey article different aspect cluster method recent one include jain murty flynn [ jmf99 ] parson haque liu [ phl04 ] jain [ jai10 ] partition method k-mean algorithm first introduce lloyd [ llo57 ] macqueen [ mac67 ] arthur vassilvitskii [ av07 ] present + algorithm filter algorithm used spatial hierarchical datum index speed computation cluster mean give kanungo mount netanyahu et al [ + 02 ] k-medoid algorithms pam clara propose kaufman rousseeuw [ kr90 ] k-mode ( cluster nominal datum ) k-prototype ( cluster hybrid datum ) algorithms propose huang [ hua98 ] k-mode cluster algorithm also propose independently chaturvedi green carroll [ cgc94 cgc01 ] claran algorithm propose ng han [ nh94 ] ester kriegel xu [ ekx95 ] propose technique improvement performance claran used efficient spatial access method r∗-tree focuse technique k-means-based scalable cluster algorithm propose bradley fayyad reina [ bfr98 ] early survey agglomerative hierarchical cluster algorithms conduct day edelsbrunner [ de84 ] agglomerative hierarchical cluster agne divisive hierarchical cluster diana introduce kaufman rousseeuw [ kr90 ] interesting direction improve cluster quality hierarchical cluster method integrate hierarchical cluster distance-based iterative relocation nonhierarchical cluster method example birch zhang ramakrishnan livny [ zrl96 ] first perform hierarchical cluster 
109 bibliographic note 495 cf-tree apply technique hierarchical cluster also perform sophisticated linkage analysis transformation nearest-neighbor analysis cure guha rastogi shim [ grs98 ] rock ( cluster nominal attribute ) guha rastogi shim [ grs99 ] chameleon karypis han kumar [ khk99 ] probabilistic hierarchical cluster framework follow normal linkage algorithms used probabilistic model define cluster similarity develop friedman [ fri03 ] heller ghahramani [ hg05 ] density-based cluster method dbscan propose ester kriegel sander xu [ eksx96 ] ankerst breunig kriegel sander [ abks99 ] develop optic cluster-order method facilitate density-based cluster without worry parameter specification denclue algorithm base set density distribution function propose hinneburg keim [ hk98 ] hinneburg gabriel [ hg07 ] develop denclue 20 include new hill-climb procedure gaussian kernel adjust step size automatically sting grid-based multiresolution approach collect statistical information grid cell propose wang yang muntz [ wym97 ] wavecluster develop sheikholeslami chatterjee zhang [ scz98 ] multiresolution cluster approach transform original feature space wavelet transform scalable method cluster nominal datum study gibson kleinberg raghavan [ gkr98 ] guha rastogi shim [ grs99 ] ganti gehrke ramakrishnan [ ggr99 ] also many cluster paradigm example fuzzy cluster method discuss kaufman rousseeuw [ kr90 ] bezdek [ bez81 ] bezdek pal [ bp92 ] high-dimensional cluster apriori-based dimension-growth subspace cluster algorithm call clique propose agrawal gehrke gunopulos raghavan [ aggr98 ] integrate density-based grid-based cluster method recent study proceed cluster stream datum babcock badu datar et al [ + 02 ] k-median-based datum stream cluster algorithm propose guha mishra motwani ’ callaghan [ gmmo00 ] ’ callaghan et al [ + 02 ] method cluster evolve datum stream propose aggarwal han wang yu [ ahwy03 ] framework project cluster high-dimensional datum stream propose aggarwal han wang yu [ ahwy04a ] cluster evaluation discuss monograph survey article jain dube [ jd88 ] halkidi batistakis vazirgiannis [ hbv01 ] extrinsic method cluster quality evaluation extensively explore recent study include meilǎ [ mei03 mei05 ] amigó gonzalo artile verdejo [ agav09 ] four essential criterium introduce chapter formulate amigó gonzalo artile verdejo [ agav09 ] individual criterium also mentioned earlier example meilǎ [ mei03 ] rosenberg hirschberg [ rh07 ] bagga baldwin [ bb98 ] introduce bcube metric silhouette coefficient describe kaufman rousseeuw [ kr90 ] 
11 advanced cluster analysis learn fundamental cluster analysis chapter chapter discuss advanced topic cluster analysis specifically investigate four major perspective probabilistic model-based cluster section 111 introduce general framework method derive cluster object assign probability belong cluster probabilistic model-based cluster widely used many datum mining application text mining cluster high-dimensional datum dimensionality high conventional distance measure dominate noise section 112 introduce fundamental method cluster analysis high-dimensional datum cluster graph network datum graph network datum increasingly popular application online social network world wide web digital library section 113 study key issue cluster graph network datum include similarity measurement cluster method cluster constraint discussion far assume constraint cluster application however various constraint may exist constraint may rise background knowledge spatial distribution object learn conduct cluster analysis different kind constraint section 114 end chapter good grasp issue technique regard advanced cluster analysis 111 probabilistic model-based cluster cluster analysis method discuss far datum object assign one number cluster cluster assignment rule require application assign customer marketing manager however datum mining concept technique doi b978-0-12-381479-100011-3 c 2012 elsevier right re-serve 497 
498 chapter 11 advanced cluster analysis application rigid requirement may desirable section demonstrate need fuzzy flexible cluster assignment application introduce general method compute probabilistic cluster assignment “ situation may datum object belong one cluster ” consider example 111 example 111 cluster product reviews allelectronic online store customer purchase online also create reviews product every product receive reviews instead product may many reviews many other none moreover review may involve multiple product thus review editor allelectronic task cluster reviews ideally cluster topic example group product service issue highly related assign review one cluster exclusively would work well task suppose cluster “ camera camcorder ” another “ ” review talk compatibility camcorder computer review relate cluster however exclusively belong either cluster would like use cluster method allow review belong one cluster review indeed involve one topic reflect strength review belong cluster want assignment review cluster carry weight represent partial membership scenario object may belong multiple cluster occur often many application illustrated example 112 example 112 cluster study user search intent allelectronic online store record customer browse purchasing behavior log important datum mining task use log datum categorize understand user search intent example consider user session ( short period user interact online store ) user search product make comparison among different product look customer support information cluster analysis help difficult predefine user behavior pattern thoroughly cluster contain similar user browse trajectory may represent similar user behavior however every session belong one cluster example suppose user session involve purchase digital camera form one cluster user session compare laptop computer form another cluster user one session make order digital camera time compare several laptop computer session belong cluster extent section systematically study theme cluster allow object belong one cluster start notion fuzzy cluster section generalize concept probabilistic model-based cluster section section 1113 introduce expectation-maximization algorithm general framework mining cluster 
111 probabilistic model-based cluster 499 1111 fuzzy cluster give set object x = { x1 xn } fuzzy set subset x allow object x membership degree 0 formally fuzzy set modeled function fs x → [ 0 1 ] example 113 fuzzy set digital camera unit sell popular camera allelectronic use follow formula compute degree popularity digital camera give sale pop ( ) = ( 1 1000 1000 unit sell ( < 1000 ) unit sell ( 111 ) function pop ( ) define fuzzy set popular digital camera example suppose sale digital camera allelectronic show table fuzzy set popular digital camera { ( 005 ) b ( 1 ) c ( 086 ) ( 027 ) } degree membership written parenthesis apply fuzzy set idea cluster give set object cluster fuzzy set object cluster call fuzzy cluster consequently cluster contain multiple fuzzy cluster formally give set object o1 fuzzy cluster k fuzzy cluster c1 ck represent used partition matrix = [ wij ] ( 1 ≤ ≤ n 1 ≤ j ≤ k ) wij membership degree oi fuzzy cluster cj partition matrix satisfy follow three requirement object oi cluster cj 0 ≤ wij ≤ requirement enforce fuzzy cluster fuzzy set object oi k x wij = requirement ensure every object - j=1 pate cluster equivalently table 111 set digital camera sale allelectronic camera sale ( unit ) b c 50 1320 860 270 
500 chapter 11 advanced cluster analysis cluster cj 0 < n x wij < n requirement ensure every cluster i=1 least one object membership value nonzero example 114 fuzzy cluster suppose allelectronic online store six reviews keyword contain reviews list table 112 group reviews two fuzzy cluster c1 c2 c1 “ digital camera ” “ lens ” c2 “ ” partition matrix  1 1  1  = 2 3  0 0  0 0  0  1  3 1 1 use keyword “ digital camera ” “ lens ” feature cluster c1 “ computer ” feature cluster c2 review ri cluster cj ( 1 ≤ ≤ 6 1 ≤ j ≤ 2 ) wij defined wij = ri ∩ cj | ri ∩ cj | = ri ∩ ( c1 ∪ c2 ) | ri ∩ { digital camera lens computer } | fuzzy cluster review r4 belong cluster c1 c2 membership degree 23 31 respectively “ evaluate well fuzzy cluster describe datum set ” consider set object o1 fuzzy cluster c k cluster c1 ck let = [ wij ] ( 1 ≤ ≤ n 1 ≤ j ≤ k ) partition matrix let c1 ck center cluster c1 ck respectively center defined either mean medoid way specific application discuss chapter 10 distance similarity object center cluster object assign used measure well table 112 set reviews keyword used review id keyword r1 r2 r3 r4 r5 r6 digital camera lens digital camera lens digital camera lens computer computer cpu computer computer game 
111 probabilistic model-based cluster 501 object belong cluster idea extend fuzzy cluster object oi cluster cj wij > 0 dist ( oi cj ) measure well oi represent cj thus belong cluster cj object participate one cluster sum distance corresponding cluster center weight degree membership capture well object fit cluster formally object oi sum square error ( sse ) give sse ( oi ) = k x p wij dist ( oi cj ) 2 ( 112 ) j=1 parameter p ( p ≥ 1 ) control influence degree membership larger value p larger influence degree membership orthogonally sse cluster cj sse ( cj ) = n x p wij dist ( oi cj ) 2 ( 113 ) i=1 finally sse cluster defined sse ( c ) = n x k x p wij dist ( oi cj ) 2 ( 114 ) i=1 j=1 sse used measure well fuzzy cluster fit datum set fuzzy cluster also call soft cluster allow object belong one cluster easy see traditional ( rigid ) cluster enforce object belong one cluster exclusively special case fuzzy cluster defer discussion compute fuzzy cluster section 1113 1112 probabilistic model-based cluster “ fuzzy cluster ( section 1111 ) provide flexibility allow object participate multiple cluster general framework specify clustering object may participate multiple cluster probabilistic way ” section introduce general notion probabilistic model-based cluster answer question discuss chapter 10 conduct cluster analysis datum set assume object datum set fact belong different inherent category recall cluster tendency analysis ( section 1061 ) used examine whether datum set contain object may lead meaningful cluster inherent category hide datum latent mean directly observed instead infer used datum observed example topic hide set reviews allelectronic online store latent one read topic directly however topic infer reviews review one multiple topic 
502 chapter 11 advanced cluster analysis therefore goal cluster analysis find hide category datum set subject cluster analysis regard sample possible instance hide category without category label cluster derive cluster analysis infer used datum set design approach hide category statistically assume hide category distribution datum space mathematically represent used probability density function ( distribution function ) call hide category probabilistic cluster probabilistic cluster c probability density function f point datum space f ( ) relative likelihood instance c appear example 115 probabilistic cluster suppose digital camera sell allelectronic divide two category c1 consumer line ( eg point-and-shoot camera ) c2 professional line ( eg single-len reflex camera ) respective probability density function f1 f2 show figure 111 respect attribute price price value say $ 1000 f1 ( 1000 ) relative likelihood price consumer-line camera $ 1000 similarly f2 ( 1000 ) relative likelihood price professional-line camera $ 1000 probability density function f1 f2 observed directly instead allelectronic infer distribution analyze price digital camera sell moreover camera often come well-determine category ( eg “ consumer line ” “ professional line ” ) instead category typically base user background knowledge vary example camera prosumer segment may regard high end consumer line customer low end professional line other analyst allelectronic consider category probabilistic cluster conduct cluster analysis price camera approach category probability consumer line professional line price 1000 figure 111 probability density function two probabilistic cluster 
111 probabilistic model-based cluster 503 suppose want find k probabilistic cluster c1 ck cluster analysis datum set n object regard finite sample possible instance cluster conceptually assume form follow cluster cj ( 1 ≤ j ≤ k ) associate probability ωj instance sample cluster often assume ω1 ωk give part problem set p kj=1 ωj = 1 ensure object generate k cluster parameter ωj capture background knowledge relative population cluster cj run follow two step generate object d step execute n time total generate n object o1 choose cluster cj accord probability ω1 ωk choose instance cj accord probability density function fj datum generation process basic assumption mixture model formally mixture model assume set observed object mixture instance multiple probabilistic cluster conceptually observed object generate independently two step first choose probabilistic cluster accord probability cluster choose sample accord probability density function choose cluster give datum set k number cluster require task probabilistic model-based cluster analysis infer set k probabilistic cluster likely generate used datum generation process important question remain measure likelihood set k probabilistic cluster probability generate observed datum set consider set c k probabilistic cluster c1 ck probability density function f1 fk respectively probability ω1 ωk object probability generate cluster cj ( 1 ≤ j ≤ k ) give p ( o|cj ) = ωj fj ( ) therefore probability generate set c cluster p ( o|c ) = k x ωj fj ( ) ( 115 ) j=1 since object assume generate independently datum set = { o1 } n object p ( d|c ) = n i=1 p ( oi c ) = k n x ωj fj ( oi ) ( 116 ) i=1 j=1 clear task probabilistic model-based cluster analysis datum set find set c k probabilistic cluster p ( d|c ) maximize maximize p ( d|c ) often intractable general probability density function 
504 chapter 11 advanced cluster analysis cluster take arbitrarily complicate form make probabilistic model-based cluster computationally feasible often compromise assume probability density function parameterized distribution formally let o1 n observed object 21 2k parameter k distribution denote = { o1 } 2 = { 21 2k } respectively object oi ∈ ( 1 ≤ ≤ n ) eq ( 115 ) rewrite p ( oi 2 ) = k x ωj pj ( oi 2j ) ( 117 ) j=1 pj ( oi 2j ) probability oi generate jth distribution used parameter 2j consequently eq ( 116 ) rewrite p ( o|2 ) = n x k ωj pj ( oi 2j ) ( 118 ) i=1 j=1 used parameterized probability distribution model task probabilistic model-based cluster analysis infer set parameter 2 maximize eq ( 118 ) example 116 univariate gaussian mixture model let ’ use univariate gaussian distribution example assume probability density function cluster follow 1-d gaussian distribution suppose k cluster two parameter probability density function cluster center µj standard deviation σj ( 1 ≤ j ≤ k ) denote parameter 2j = ( µj σj ) 2 = { 21 2k } let datum set = { o1 } oi ( 1 ≤ ≤ n ) real number point oi ∈ 1 e p ( oi 2j ) = √ 2π σj − ( oi −µj ) 2 2σ 2 ( 119 ) assume cluster probability ω1 = ω2 = · · · = ωk = k1 plug eq ( 119 ) eq ( 117 ) k 2 ( oi −µj ) 1x 1 − p ( oi 2 ) = e 2σ 2 √ k 2π σj ( 1110 ) j=1 apply eq ( 118 ) n p ( o|2 ) = k 2 ( oi −µj ) 1 yx 1 − e 2σ 2 √ k 2π σj ( 1111 ) i=1 j=1 task probabilistic model-based cluster analysis used univariate gaussian mixture model infer 2 eq ( 1111 ) maximize 
111 probabilistic model-based cluster 505 1113 expectation-maximization algorithm “ compute fuzzy clustering probabilistic model-based clustering ” section introduce principled approach let ’ start review k-mean cluster problem k-mean algorithm study chapter 10 easily show k-mean cluster special case fuzzy cluster ( exercise 111 ) k-mean algorithm iterate cluster improve iteration consist two step expectation step ( e-step ) give current cluster center object assign cluster center closest object object expect belong closest cluster maximization step ( m-step ) give cluster assignment cluster algorithm adjust center sum distance object assign cluster new center minimize similarity object assign cluster maximize generalize two-step method tackle fuzzy cluster probabilistic model-based cluster general expectation-maximization ( em ) algorithm framework approach maximum likelihood maximum posteriori estimate parameter statistical model context fuzzy probabilistic model-based cluster em algorithm start initial set parameter iterate cluster improve cluster converge change sufficiently small ( less preset threshold ) iteration also consist two step expectation step assign object cluster accord current fuzzy cluster parameter probabilistic cluster maximization step find new cluster parameter maximize sse fuzzy cluster ( eq 114 ) expect likelihood probabilistic model-based cluster example 117 fuzzy cluster used em algorithm consider six point figure 112 coordinate point also show let ’ compute two fuzzy cluster used em algorithm randomly select two point say c1 = c2 = b initial center two cluster first iteration conduct expectation step maximization step follow e-step point calculate membership degree cluster point assign c1 c2 membership weight 1 dist ( c1 ) 2 1 1 + 2 dist ( c1 ) dist ( c2 ) 2 = dist ( c2 ) 2 dist ( c1 ) 2 dist ( c1 ) 2 + dist ( c2 ) 2 dist ( c1 ) 2 + dist ( c2 ) 2 
506 chapter 11 advanced cluster analysis e ( 18 11 ) b ( 4 10 ) ( 14 8 ) c ( 9 6 ) f ( 21 7 ) ( 3 3 ) x figure 112 datum set fuzzy cluster table 113 intermediate result first three iteration example 117 ’ em algorithm iteration 1 2 3 e-step ` 1 0 = 0 1 ` 073 mt = 027 ` 080 mt = 020 048 052 042 058 m-step 041 059 # 047 053 049 051 091 009 026 074 033 067 076 024 099 001 002 098 014 086 c1 = ( 847 512 ) c2 = ( 1042 899 ) # 042 058 # 023 077 c1 = ( 851 611 ) c2 = ( 1442 869 ) c1 = ( 640 624 ) c2 = ( 1655 864 ) respectively dist ( ) euclidean distance rationale close c1 dist ( c1 ) small membership degree respect c1 high also normalize membership degree sum degree object equal 1 point wa c1 = 1 wa c2 = exclusively belong c1 41 = 048 point b wb c1 = 0 wb c2 = point c wc c1 = 45+41 45 wc c2 = 45+41 = degree membership point show partition matrix table 113 m-step recalculate centroid accord partition matrix minimize sse give eq ( 114 ) new centroid adjust x 2 wo c j point cj = ( 1112 ) x 2 wo c j point j = 1 2 
111 probabilistic model-based cluster 507 example 12 × 3 + 02 × 4 + 0482 × 9 + 0422 × 14 + 0412 × 18 + 0472 × 21 12 + 02 + 0482 + 0422 + 0412 + 0472  12 × 3 + 02 × 10 + 0482 × 6 + 0422 × 8 + 0412 × 11 + 0472 × 7 12 + 02 + 0482 + 0422 + 0412 + 0472  c1 = = ( 847 512 )  c2 = 02 × 3 + 12 × 4 + 0522 × 9 + 0582 × 14 + 0592 × 18 + 0532 × 21 02 + 12 + 0522 + 0582 + 0592 + 0532  02 × 3 + 12 × 10 + 0522 × 6 + 0582 × 8 + 0592 × 11 + 0532 × 7 02 + 12 + 0522 + 0582 + 0592 + 0532 = ( 1042 899 ) repeat iteration iteration contain e-step m-step table 113 show result first three iteration algorithm stop cluster center converge change small enough “ apply em algorithm compute probabilistic model-based cluster ” let ’ use univariate gaussian mixture model ( example 116 ) illustrate example 118 used em algorithm mixture model give set object = { o1 } want mine set parameter 2 = { 21 2k } p ( o|2 ) eq ( 1111 ) maximize 2j = ( µj σj ) mean standard deviation respectively jth univariate gaussian distribution ( 1 ≤ j ≤ k ) apply em algorithm assign random value parameter 2 initial value iteratively conduct e-step m-step follow parameter converge change sufficiently small e-step object oi ∈ ( 1 ≤ ≤ n ) calculate probability oi belong distribution p ( oi 2j ) p ( 2j oi 2 ) = pk l=1 p ( oi 2l ) ( 1113 ) m-step adjust parameter 2 expect likelihood p ( o|2 ) eq ( 1111 ) maximize achieve set pn n p ( 2j oi 2 ) 1x 1 i=1 oi p ( 2j oi 2 ) µj = oi pn = pn k k l=1 p ( 2j ol 2 ) i=1 p ( 2j oi 2 ) i=1 ( 1114 ) 
508 chapter 11 advanced cluster analysis σj = sp n 2 i=1 p ( 2j oi 2 ) ( oi − uj ) pn i=1 p ( 2j oi 2 ) ( 1115 ) many application probabilistic model-based cluster show effective general partition method fuzzy cluster method distinct advantage appropriate statistical model used capture latent cluster em algorithm commonly used handle many learn problem datum mining statistic due simplicity note general em algorithm may converge optimal solution may instead converge local maximum many heuristic explore avoid example can run em process multiple time used different random initial value furthermore em algorithm costly number distribution large datum set contain observed datum point 112 cluster high-dimensional datum cluster method study far work well dimensionality high less 10 attribute however important application high dimensionality “ conduct cluster analysis high-dimensional datum ” section study approach cluster high-dimensional datum section 1121 start overview major challenge approach used method high-dimensional datum cluster divide two category subspace cluster method ( section 1122 ) dimensionality reduction method ( section 1123 ) 1121 cluster high-dimensional datum problem challenge major methodology present specific method cluster high-dimensional datum let ’ first demonstrate need cluster analysis high-dimensional datum used example examine challenge call new method categorize major method accord whether search cluster subspace original space whether create new lower-dimensionality space search cluster application datum object may describe 10 attribute object refer high-dimensional datum space example 119 high-dimensional datum cluster allelectronic keep track product purchase every customer customer-relationship manager want cluster customer group accord purchase allelectronic 
112 cluster high-dimensional datum 509 table 114 customer purchase datum customer p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 ada bob cathy 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 customer purchase datum high dimensionality allelectronic carry ten thousand product therefore customer ’ purchase profile vector product carry company ten thousand dimension “ traditional distance measure frequently used low-dimensional cluster analysis also effective high-dimensional datum ” consider customer table 114 10 product p1 p10 used demonstration customer purchase product 1 set corresponding bit otherwise 0 appear let ’ calculate euclidean distance ( eq 216 ) among ada bob cathy easy see dist ( ada bob ) = dist ( bob cathy ) = dist ( ada cathy ) = √ 2 accord euclidean distance three customer equivalently similar ( dissimilar ) however close look tell us ada similar cathy bob ada cathy share one common purchase item p1 show example 119 traditional distance measure ineffective high-dimensional datum distance measure may dominate noise many dimension therefore cluster full high-dimensional space unreliable find cluster may meaningful “ kind cluster meaningful high-dimensional datum ” cluster analysis high-dimensional datum still want group similar object together however datum space often big messy additional challenge need find cluster cluster set attribute manifest cluster word cluster high-dimensional datum often defined used small set attribute instead full datum space essentially cluster high-dimensional datum return group object cluster ( conventional cluster analysis ) addition cluster set attribute characterize cluster example table 114 characterize similarity ada cathy p1 may return attribute ada cathy purchase p1 cluster high-dimensional datum search cluster space exist thus two major kind method subspace cluster approach search cluster exist subspace give high-dimensional datum space subspace defined used subset attribute full space subspace cluster approach discuss section 1122 
510 chapter 11 advanced cluster analysis dimensionality reduction approach try construct much lower-dimensional space search cluster space often method may construct new dimension combine dimension original datum dimensionality reduction method topic section 1124 general cluster high-dimensional datum raise several new challenge addition conventional cluster major issue create appropriate model cluster high-dimensional datum unlike conventional cluster low-dimensional space cluster hide high-dimensional datum often significantly smaller example cluster customer-purchase datum would expect many user similar purchase pattern search small meaningful cluster like find needle haystack show conventional distance measure ineffective instead often consider various sophisticated technique model correlation consistency among object subspace typically exponential number possible subspace dimensionality reduction option thus optimal solution often computationally prohibitive example original datum space 1000 dimension want 1000 find cluster dimensionality 10 = 263 × 1023 possible 10 subspace 1122 subspace cluster method “ find subspace cluster high-dimensional datum ” many method propose generally categorize three major group subspace search method correlation-based cluster method bicluster method subspace search method subspace search method search various subspace cluster cluster subset object similar subspace similarity often capture conventional measure distance density example clique algorithm introduce section 1052 subspace cluster method enumerate subspace cluster subspace dimensionality-increas order apply antimonotonicity prune subspace cluster may exist major challenge subspace search method face search series subspace effectively efficiently generally two kind strategy bottom-up approach start low-dimensional subspace search higherdimensional subspace may cluster higher-dimensional 
112 cluster high-dimensional datum 511 subspace various prune technique explore reduce number higherdimensional subspace need search clique example bottom-up approach top-down approach start full space search smaller smaller subspace recursively top-down approach effective locality assumption hold require subspace cluster determine local neighborhood example 1110 proclus top-down subspace approach proclus k-medoid-like method first generate k potential cluster center high-dimensional datum set used sample datum set refine subspace cluster iteratively iteration current k-medoid proclus consider local neighborhood medoid whole datum set identify subspace cluster minimize standard deviation distance point neighborhood medoid dimension subspace medoid determine point datum set assign closest medoid accord corresponding subspace cluster possible outlier identify next iteration new medoid replace exist one improve cluster quality correlation-based cluster method subspace search method search cluster similarity measure used conventional metric like distance density correlation-based approach discover cluster defined advanced correlation model example 1111 correlation-based approach used pca example pca-based approach first apply pca ( principal component analysis see chapter 3 ) derive set new uncorrelated dimension mine cluster new space subspace addition pca space transformation may used hough transform fractal dimension additional detail subspace search method correlation-based cluster method please refer bibliographic note ( section 117 ) bicluster method application want cluster object attribute simultaneously result cluster know bicluster meet four requirement ( 1 ) small set object participate cluster ( 2 ) cluster involve small number attribute ( 3 ) object may participate multiple cluster participate cluster ( 4 ) attribute may involved multiple cluster involved cluster section 1123 discuss bicluster detail 
512 chapter 11 advanced cluster analysis 1123 bicluster cluster analysis discuss far cluster object accord attribute value object attribute treat way however application object attribute defined symmetric way datum analysis involve search datum matrix submatrix show unique pattern cluster kind cluster technique belong category bicluster section first introduce two motivate application example biclustering— gene expression recommender system learn different type bicluster last present bicluster method application example bicluster technique first propose address need analyze gene expression datum gene unit passing-on trait live organism offspr typically gene reside segment dna gene critical live thing specify protein functional rna chain hold information build maintain live organism ’ cell pass genetic trait offspr synthesis functional gene product either rna protein rely process gene expression genotype genetic makeup cell organism individual phenotype observable characteristic organism gene expression fundamental level genetic genotype cause phenotype used dna chip ( also know dna microarray ) biological engineering technique measure expression level large number ( possibly ) organism ’ gene number different experimental condition condition may correspond different time point experiment sample different organ roughly speaking gene expression datum dna microarray datum conceptually condition matrix row correspond one gene column correspond one sample condition element matrix real number record expression level gene specific condition figure 113 show illustration cluster viewpoint interesting issue gene expression datum matrix analyze two dimensions—the gene dimension condition dimension analyze gene dimension treat gene object treat condition attribute mining gene dimension may find pattern share multiple gene cluster gene group example may find group gene express similarly highly interesting bioinformatic find pathway analyze condition dimension treat condition object treat gene attribute way may find pattern condition cluster condition group example may find difference gene expression compare group tumor sample nontumor sample 
112 cluster high-dimensional datum 513 condition gene w11 w12 w1m w21 w22 w2m w31 w32 w3m wn1 wn2 wnm figure 113 microarrary datum matrix example 1112 gene expression gene expression matrix popular bioinformatic research development example important task classify new gene used expression datum gene gene know class symmetrically may classify new sample ( eg new patient ) used expression datum sample sample know class ( eg tumor nontumor ) task invaluable understand mechanism disease clinical treatment see many gene expression datum mining problem highly related cluster analysis however challenge instead cluster one dimension ( eg gene condition ) many case need cluster two dimension simultaneously ( eg gene condition ) moreover unlike cluster model discuss far cluster gene expression datum matrix submatrix usually follow characteristic small set gene participate cluster cluster involve small subset condition gene may participate multiple cluster may participate cluster condition may involved multiple cluster may involved cluster find cluster condition matrix need new cluster technique meet follow requirement bicluster cluster gene defined used subset condition cluster condition defined used subset gene 
514 chapter 11 advanced cluster analysis cluster neither exclusive ( eg one gene participate multiple cluster ) exhaustive ( eg gene may participate cluster ) bicluster useful bioinformatic also application well consider recommender system example example 1113 used bicluster recommender system allelectronic collect datum customer ’ evaluation product used datum recommend product customer datum modeled customer-product matrix row represent customer column represent product element matrix represent customer ’ evaluation product may score ( eg like like somewhat like ) purchase behavior ( eg buy ) figure 114 illustrate structure customer-product matrix analyze two dimension customer dimension product dimension treat customer object product attribute allelectronic find customer group similar preference purchase pattern used product object customer attribute allelectronic mine product group similar customer interest moreover allelectronic mine cluster customer product simultaneously cluster contain subset customer involve subset product example allelectronic highly interested find group customer like group product cluster submatrix customer-product matrix element high value used cluster allelectronic make recommendation two direction first company recommend product new customer similar customer cluster second company recommend customer new product similar involved cluster bicluster gene expression datum matrix bicluster customerproduct matrix usually follow characteristic small set customer participate cluster cluster involve small subset product customer participate multiple cluster may participate cluster customer w11 w21 ··· wn1 product w12 · · · w22 · · · ··· ··· wn2 · · · figure 114 customer–product matrix w1m w2m ··· wnm 
112 cluster high-dimensional datum 515 product may involved multiple cluster may involved cluster bicluster apply customer-product matrix mine cluster satisfying requirement type bicluster “ model bicluster mine ” let ’ start basic notation sake simplicity use “ gene ” “ condition ” refer two dimension discussion discussion easily extend application example simply replace “ gene ” “ condition ” “ customer ” “ product ” tackle customer-product bicluster problem let = { a1 } set gene b = { b1 bm } set condition let e = [ eij ] gene expression datum matrix gene-condition matrix 1 ≤ ≤ n 1 ≤ j ≤ m submatrix × j defined subset ⊆ gene subset j ⊆ b condition example matrix show figure 115 { a1 a33 a86 } × { b6 b12 b36 b99 } submatrix bicluster submatrix gene condition follow consistent pattern define different type bicluster base pattern simplest case submatrix × j ( ⊆ j ⊆ b ) bicluster constant value ∈ j ∈ j eij = c c constant example submatrix { a1 a33 a86 } × { b6 b12 b36 b99 } figure 115 bicluster constant value bicluster interesting row constant value though different row may different value bicluster constant value row submatrix × j ∈ j ∈ j eij = c + αi αi adjustment row i example figure 116 show bicluster constant value row symmetrically bicluster constant value column submatrix × j ∈ j ∈ j eij = c + βj βj adjustment column j a1 ··· a33 ··· a86 ··· ··· ··· ··· ··· ··· ··· ··· b6 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b12 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b36 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b99 · · · 60 · · · ··· ··· 60 · · · ··· ··· 60 · · · ··· ··· figure 115 gene-condition matrix submatrix bicluster 
516 chapter 11 advanced cluster analysis generally bicluster interesting row change synchronize way respect column vice versa mathematically bicluster coherent value ( also know pattern-based cluster ) submatrix × j ∈ j ∈ j eij = c + αi + βj αi βj adjustment row column j respectively example figure 117 show bicluster coherent value show × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 moreover instead used addition define bicluster coherent value used multiplication eij = c · αi · βj clearly bicluster constant value row column special case bicluster coherent value application may interested - down-regulate change across gene condition without constrain exact value bicluster coherent evolution row submatrix × j i1 i2 ∈ j1 j2 ∈ j ( ei1 j1 − ei1 j2 ) ( ei2 j1 − ei2 j2 ) ≥ example figure 118 show bicluster coherent evolution row symmetrically define bicluster coherent evolution column next study mine bicluster 10 20 50 0 10 20 50 0 10 20 50 0 10 20 50 0 10 20 50 0 figure 116 bicluster constant value row 10 20 50 0 50 60 90 40 30 40 70 20 70 80 110 60 20 30 60 10 figure 117 bicluster coherent value 10 20 50 0 50 100 100 80 30 50 90 20 70 1000 120 100 20 30 80 10 figure 118 bicluster coherent evolution row 
112 cluster high-dimensional datum 517 bicluster method previous specification type bicluster consider ideal case real datum set perfect bicluster rarely exist exist usually small instead random noise affect reading eij thus prevent bicluster nature appear perfect shape two major type method discover bicluster datum may come noise optimization-based method conduct iterative search iteration submatrix highest significance score identify bicluster process terminate user-specified condition meet due cost concern computation greedy search often employ find local optimal bicluster enumeration method use tolerance threshold specify degree noise allow bicluster mine try enumerate submatrix bicluster satisfy requirement use δ-cluster maple algorithms example illustrate idea optimization used δ-cluster algorithm submatrix × j mean ith row 1 x eij = eij | ( 1116 ) j∈j symmetrically mean jth column 1 x eij = eij | ( 1117 ) i∈i mean element submatrix 1 x 1 x 1 x eij = eij = eij eij = | | | i∈i j∈j i∈i ( 1118 ) j∈j quality submatrix bicluster measure mean-squared residue value 1 x h ( × j ) = ( eij − eij − eij + eij ) 2 ( 1119 ) | i∈i j∈j submatrix × j δ-bicluster h ( × j ) ≤ δ δ ≥ 0 threshold δ = 0 × j perfect bicluster coherent value set δ > 0 user specify tolerance average noise per element perfect bicluster eq ( 1119 ) residue element residue ( eij ) = eij − eij − eij + eij ( 1120 ) maximal δ-bicluster δ-bicluster × j exist another δ-bicluster 0 × j 0 ⊆ 0 j ⊆ j 0 least one inequality hold find 
518 chapter 11 advanced cluster analysis maximal δ-bicluster largest size computationally costly therefore use heuristic greedy search method obtain local optimal cluster algorithm work two phase deletion phase start whole matrix mean-squared residue matrix δ iteratively remove row column iteration row compute mean-squared residue 1 x ( ) = ( eij − eij − eij + eij ) 2 ( 1121 ) | j∈j moreover column j compute mean-squared residue 1 x ( eij − eij − eij + eij ) 2 ( j ) = | ( 1122 ) i∈i remove row column largest mean-squared residue end phase obtain submatrix × j δ-bicluster however submatrix may maximal addition phase iteratively expand δ-bicluster × j obtain deletion phase long δ-bicluster requirement maintain iteration consider row column involved current bicluster × j calculate mean-squared residue row column smallest mean-squared residue add current δ-bicluster greedy algorithm find one δ-bicluster find multiple bicluster heavy overlap run algorithm multiple time execution δ-bicluster output replace element output bicluster random number although greedy algorithm may find neither optimal bicluster bicluster fast even large matrix enumerate bicluster used maple mentioned submatrix × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 2 × 2 submatrix × j define p-score ei1 j1 ei1 j2 p-score = | ( ei1 j1 − ei2 j1 ) − ( ei1 j2 − ei2 j2 ) | ( 1123 ) ei2 j1 ei2 j2 submatrix × j δ-pcluster ( pattern-based cluster ) p-score every 2 × 2 submatrix × j δ δ ≥ 0 threshold specify user ’ tolerance noise perfect bicluster p-score control noise every element bicluster mean-squared residue capture average noise interesting property δ-pcluster × j δ-pcluster every x × ( x ≥ 2 ) submatrix × j also δ-pcluster monotonicity enable 
112 cluster high-dimensional datum 519 us obtain succinct representation nonredundant δ-pcluster δ-pcluster maximal row column add cluster maintain δ-pcluster property avoid redundancy instead find δ-pcluster need compute maximal δ-pcluster maple algorithm enumerate maximal δ-pcluster systematically enumerate every combination condition used set enumeration tree depthfirst search enumeration framework pattern-growth method frequent pattern mining ( chapter 6 ) consider gene expression datum condition combination j maple find maximal subset gene × j δ-pcluster × j submatrix another δ-pcluster × j maximal δ-pcluster may huge number condition combination maple prune many unfruitful combination used monotonicity δ-pcluster condition combination j exist set gene × j δ-pcluster need consider superset j moreover consider × j candidate δ-pcluster every ( | − 1 ) subset j 0 j × j 0 δ-pcluster maple also employ several prune technique speed search retain completeness return maximal δ-pcluster example examine current δ-pcluster × j maple collect gene condition may add expand cluster candidate gene condition together j form submatrix δ-pcluster already find search × j superset j prune interested reader may refer bibliographic note additional information maple algorithm ( section 117 ) interesting observation search maximal δ-pcluster maple somewhat similar mining frequent close itemset consequently maple borrow depth-first search framework idea prune technique pattern-growth method frequent pattern mining example frequent pattern mining cluster analysis may share similar technique idea advantage maple algorithms enumerate bicluster guarantee completeness result miss overlapping bicluster however challenge enumeration algorithms may become time consume matrix become large customer-purchase matrix hundred thousand customer million product 1124 dimensionality reduction method spectral cluster subspace cluster method try find cluster subspace original datum space situation effective construct new space instead used subspace original datum motivation behind dimensionality reduction method cluster high-dimensional datum example 1114 cluster derive space consider three cluster point figure possible cluster point subspace original space x × 
520 chapter 11 advanced cluster analysis − 0707x + 0707y x figure 119 cluster derive space may effective three cluster would end project onto overlapping area x √ √ 2 2 axe instead construct new dimension − 2 x + 2 ( show dash line figure ) project point onto new dimension three cluster become apparent although example 1114 involve two dimension idea construct new space ( cluster structure hide datum become well manifest ) extend high-dimensional datum preferably newly construct space low dimensionality many dimensionality reduction method straightforward approach apply feature selection extraction method datum set discuss chapter however method may able detect cluster structure therefore method combine feature extraction cluster prefer section introduce spectral cluster group method effective highdimensional datum application figure 1110 show general framework spectral cluster approach ng-jordan-weiss algorithm spectral cluster method let ’ look step framework also note special condition apply ng-jordan-weiss algorithm example give set object o1 distance pair object dist ( oi oj ) ( 1 ≤ j ≤ n ) desire number k cluster spectral cluster approach work follow used distance measure calculate affinity matrix w wij = e − dist ( oi oj ) σ2 σ scaling parameter control fast affinity wij decrease dist ( oi oj ) increase ng-jordan-weiss algorithm wii set 0 
112 cluster high-dimensional datum datum affinity matrix [ wij ] compute lead k eigenvector cluster new space 521 project back cluster original datum av = λv = f ( w ) figure 1110 framework spectral cluster approach source adapt slide 8 http micued08 azran used affinity matrix w derive matrix = f ( w ) way do vary ng-jordan-weiss algorithm define matrix diagonal matrix dii sum ith row w dii = n x wij ( 1124 ) j=1 set 1 1 = d− 2 wd− 2 ( 1125 ) find k lead eigenvector a recall eigenvector square matrix nonzero vector remain proportional original vector multiply matrix mathematically vector v eigenvector matrix av = λv λ call corresponding eigenvalue step derive k new dimension base affinity matrix w typically k much smaller dimensionality original datum ng-jordan-weiss algorithm compute k eigenvector largest eigenvalue x1 xk used k lead eigenvector project original datum new space defined k lead eigenvector run cluster algorithm k-mean find k cluster ng-jordan-weiss algorithm stack k largest eigenvector column form matrix x = [ x1 x2 · · · xk ] ∈ rn×k algorithm form matrix renormalize row x unit length xij yij = qp k 2 j=1 xij ( 1126 ) algorithm treat row point k-dimensional space rk run k-mean ( algorithm serve partition purpose ) cluster point k cluster 
522 chapter 11 advanced cluster analysis v = [ v1 v2 v3 ] w 05 u = [ u1 u2 u3 ] 0 −05 0 10 20 30 40 50 60 1 05 0 −05 −1 0 10 20 30 40 50 60 1 05 05 0 0 0 04 02 0 −02 0 10 10 20 20 30 30 40 40 50 50 60 60 0 1 05 0 −05 0 10 10 20 20 30 30 40 40 50 50 60 60 figure 1111 new dimension cluster result ng-jordan-weiss algorithm source adapt slide 9 http micued08 azran assign original datum point cluster accord transform point assign cluster obtain step 4 ng-jordan-weiss algorithm original object oi assign jth cluster matrix ’ row assign jth cluster result step 4 spectral cluster method dimensionality new space set desire number cluster set expect new dimension able manifest cluster example 1115 ng-jordan-weiss algorithm consider set point figure datum set affinity matrix three largest eigenvector normalize vector show note three new dimension ( form three largest eigenvector ) cluster easily detected spectral cluster effective high-dimensional application image process theoretically work well certain condition apply scalability however challenge compute eigenvector large matrix costly spectral cluster combine cluster method bicluster additional information dimensionality reduction cluster method kernel pca find bibliographic note ( section 117 ) 113 cluster graph network datum cluster analysis graph network datum extract valuable knowledge information datum increasingly popular many application discuss application challenge cluster graph network datum section similarity measure form cluster give section learn graph cluster method section 1133 general term graph network used interchangeably rest section mainly use term graph 
113 cluster graph network datum 523 1131 application challenge customer relationship manager allelectronic notice lot datum relate customer purchase behavior preferably modeled used graph example 1116 bipartite graph customer purchase behavior allelectronic represent bipartite graph bipartite graph vertex divide two disjoint set edge connect vertex one set vertex set allelectronic customer purchase datum one set vertex represent customer one customer per vertex set represent product one product per vertex edge connect customer product represent purchase product customer figure 1112 show illustration “ kind knowledge obtain cluster analysis customer-product bipartite graph ” cluster customer customer buy similar set product place one group customer relationship manager make product recommendation example suppose ada belong customer cluster customer purchase digital camera last 12 month ada yet purchase one manager decide recommend digital camera alternatively cluster product product purchase similar set customer group together cluster information also used product recommendation example digital camera high-speed flash memory card belong product cluster customer purchase digital camera recommend high-speed flash memory card bipartite graph widely used many application consider another example example 1117 web search engine web search engine search log archive record user query corresponding click-through information ( click-through information tell us page give result search user click ) query click-through information represent used bipartite graph two set customer product figure 1112 bipartite graph represent customer-purchase datum 
524 chapter 11 advanced cluster analysis vertex correspond query web page respectively edge link query web page user click web page ask query valuable information obtain cluster analysis query–web page bipartite graph instance may identify query pose different language mean thing click-through information query similar another example web page web form direct graph also know web graph web page vertex hyperlink edge point source page destination page cluster analysis web graph disclose community find hub authoritative web page detect web spam addition bipartite graph cluster analysis also apply type graph include general graph elaborate example 1118 example 1118 social network social network social structure represent graph vertex individual organization link interdependency vertex represent friendship common interest collaborative activity allelectronic ’ customer form social network customer vertex edge link two customer know customer relationship manager interested find useful information derive allelectronic ’ social network cluster analysis obtain cluster network customer cluster know friend common customer within cluster may influence one another regard purchase decision make moreover communication channel design inform “ head ” cluster ( ie “ best ” connect person cluster ) promotional information spread quickly thus may use customer cluster promote sale allelectronic another example author scientific publication form social network author vertex two author connect edge coauthor publication network general weight graph edge two author carry weight represent strength collaboration many publication two author ( end vertex ) coauthor cluster coauthor network provide insight community author pattern collaboration “ challenge specific cluster analysis graph network datum ” cluster method discuss far object represent used set attribute unique feature graph network datum object ( vertex ) relationship ( edge ) give dimension attribute explicitly defined conduct cluster analysis graph network datum two major new challenge “ measure similarity two object graph accordingly ” typically use conventional distance measure euclidean distance instead need develop new measure quantify similarity 
113 cluster graph network datum 525 measure often metric thus raise new challenge regard development efficient cluster method similarity measure graph discuss section 1132 “ design cluster model method effective graph network datum ” graph network datum often complicate carry topological structure sophisticated traditional cluster analysis application many graph datum set large web graph contain least ten billion web page publicly indexable web graph also sparse average vertex connect small number vertex graph discover accurate useful knowledge hide deep datum good cluster method accommodate factor cluster method graph network datum introduce section 1133 1132 similarity measure “ measure similarity distance two vertex graph ” discussion examine two type measure geodesic distance distance base random walk geodesic distance simple measure distance two vertex graph shortest path vertex formally geodesic distance two vertex length term number edge shortest path vertex two vertex connect graph geodesic distance defined infinite used geodesic distance define several useful measurement graph analysis cluster give graph g = ( v e ) v set vertex e set edge define follow vertext v ∈ v eccentricity v denote eccen ( v ) largest geodesic distance v vertex u ∈ v − { v } eccentricity v capture far away v remotest vertex graph radius graph g minimum eccentricity vertex r = min eccen ( v ) v∈v ( 1127 ) radius capture distance “ central point ” “ farthest border ” graph diameter graph g maximum eccentricity vertex = max eccen ( v ) v∈v diameter represent largest distance pair vertex peripheral vertex vertex achieve diameter ( 1128 ) 
526 chapter 11 advanced cluster analysis b c e figure 1113 graph g vertex c e peripheral example 1119 measurement base geodesic distance consider graph g figure eccentricity 2 eccen ( ) = 2 eccen ( b ) = 2 eccen ( c ) = eccen ( ) = eccen ( e ) = thus radius g 2 diameter note necessary = 2 × r vertex c e peripheral vertex simrank similarity base random walk structural context application geodesic distance may inappropriate measure similarity vertex graph introduce simrank similarity measure base random walk structural context graph mathematics random walk trajectory consist take successive random step example 1120 similarity person social network let ’ consider measure similarity two vertex allelectronic customer social network example 1118 similarity explain closeness two participant network close two person term relationship represent social network “ well geodesic distance measure similarity closeness network ” suppose ada bob two customer network network undirected geodesic distance ( ie length shortest path ada bob ) shortest path message pass ada bob vice versa however information useful allelectronic ’ customer relationship management company typically want send specific message one customer another therefore geodesic distance suit application “ similarity mean social network ” consider two way define similarity two customer consider similar one another similar neighbor social network heuristic intuitive practice two person receive recommendation good number common friend often make similar decision kind similarity base local structure ( ie neighborhood ) vertex thus call structural context–based similarity 
113 cluster graph network datum 527 suppose allelectronic send promotional information ada bob social network ada bob may randomly forward information friend ( neighbor ) network closeness ada bob measure likelihood customer simultaneously receive promotional information originally send ada bob kind similarity base random walk reachability network thus refer similarity base random walk let ’ closer look meant similarity base structural context similarity base random walk intuition behind similarity base structural context two vertex graph similar connect similar vertex measure similarity need define notion individual neighborhood direct graph g = ( v e ) v set vertex e ⊆ v × v set edge vertex v ∈ v individual in-neighborhood v defined ( 1129 ) ( v ) = { | ( u v ) ∈ e } symmetrically define individual out-neighborhood v ( 1130 ) ( v ) = { | ( v w ) ∈ e } follow intuition illustrated example 1120 define simrank structural-context similarity value 0 1 pair vertex vertex v ∈ v similarity vertex ( v v ) = 1 neighborhood identical vertex u v ∈ v u = v define x x c ( u v ) = ( x ) ( 1131 ) i ( u ) i ( v ) | x∈i ( u ) y∈i ( v ) c constant 0 vertex may in-neighbor thus define eq ( 1131 ) 0 either ( u ) ( v ) ∅ parameter c specify rate decay similarity propagate across edge “ compute simrank ” straightforward method iteratively evaluate eq ( 1131 ) fix point reach let si ( u v ) simrank score calculate ith round begin set ( 0 u = v s0 ( u v ) = ( 1132 ) 1 u = v use eq ( 1131 ) compute si+1 si si+1 ( u v ) = x c i ( u ) i ( v ) | x x∈i ( u ) y∈i ( v ) si ( x ) ( 1133 ) 
528 chapter 11 advanced cluster analysis show lim si ( u v ) = ( u v ) additional method approximate i→∞ simrank give bibliographic note ( section 117 ) let ’ consider similarity base random walk direct graph strongly connect two node u v path u v another path v u strongly connect graph g = ( v e ) two vertex u v ∈ v define expect distance u v ( u v ) = x u p [ ] l ( ) ( 1134 ) v u v path start u end v may contain cycle reach v end travele tour = w1 → w2 → · · · → wk length l ( ) = k − probability tour defined ( q k−1 1 i=1 o ( wi ) | l ( ) > 0 ( 1135 ) p [ ] = 0 l ( ) = 0 measure probability vertex w receive message originated simultaneously u v extend expect distance notion expect meeting distance x ( u v ) = p [ ] l ( ) ( 1136 ) ( x x ) ( u v ) ( u v ) ( x x ) pair tour u x v x length used constant c 0 1 define expect meeting probability p ( u v ) = x ( u v ) p [ ] c l ( ) ( 1137 ) ( x x ) similarity measure base random walk parameter c specify probability continue walk step trajectory show ( u v ) = p ( u v ) two vertex u v simrank base structural context random walk 1133 graph cluster method let ’ consider conduct cluster graph first describe intuition behind graph cluster discuss two general category graph cluster method find cluster graph imagine cut graph piece piece cluster vertex within cluster well connect vertex different cluster connect much weaker way formally graph g = ( v e ) 
113 cluster graph network datum 529 cut c = ( ) partition set vertex v g v = ∪ ∩ = ∅ cut set cut set edge { ( u v ) ∈ e|u ∈ v ∈ } size cut number edge cut set weight graph size cut sum weight edge cut set “ kind cut good derive cluster graph ” graph theory network application minimum cut importance cut minimum cut ’ size greater cut ’ size polynomial time algorithms compute minimum cut graph use algorithms graph cluster example 1121 cut cluster consider graph g figure graph two cluster { b c e f } { g h j k } one outlier vertex l consider cut c1 = ( { b c e f g h j k } { l } ) one edge namely ( e l ) cross two partition create c1 therefore cut set c1 { ( e l ) } size c1 1 ( note size cut connect graph smaller 1 ) minimum cut c1 lead good cluster separate outlier vertex l rest graph cut c2 = ( { b c e f l } { g h j k } ) lead much better cluster c1 edge cut set c2 connect two “ natural cluster ” graph specifically edge ( h ) ( e k ) cut set edge connect h e k belong one cluster example 1121 indicate used minimum cut unlikely lead good cluster better choose cut vertex u involved edge cut set edge connect u belong one cluster formally let deg ( u ) degree u number edge connect u sparsity cut c = ( ) defined = cut size min { | | } ( 1138 ) sparsest cut c2 b c g f h e k minimum cut c1 l figure 1114 graph g two cut j 
530 chapter 11 advanced cluster analysis cut sparsest sparsity greater sparsity cut may one sparsest cut example 1121 figure 1114 c2 sparsest cut used sparsity objective function sparsest cut try minimize number edge cross partition balance partition size consider cluster graph g = ( v e ) partition graph k cluster modularity cluster assess quality cluster defined = k x i=1   di 2 li − | | ( 1139 ) li number edge vertex ith cluster di sum degree vertex ith cluster modularity cluster graph difference fraction edge fall individual cluster fraction would graph vertex randomly connect optimal cluster graph maximize modularity theoretically many graph cluster problem regard find good cut sparsest cut graph practice however number challenge exist high computational cost many graph cut problem computationally expensive sparsest cut problem example np-hard therefore find optimal solution large graph often impossible good trade-off scalability quality achieve sophisticated graph graph sophisticated one describe involve weight or cycle high dimensionality graph many vertex similarity matrix vertex represent vector ( row matrix ) dimensionality number vertex graph therefore graph cluster method must handle high dimensionality sparsity large graph often sparse meaning vertex average connect small number vertex similarity matrix large sparse graph also sparse two kind method cluster graph datum address challenge one used cluster method high-dimensional datum design specifically cluster graph first group method base generic cluster method highdimensional datum extract similarity matrix graph used similarity measure discuss section generic cluster method apply similarity matrix discover cluster cluster method 
113 cluster graph network datum 531 high-dimensional datum typically employ example many scenario similarity matrix obtain spectral cluster method ( section 1124 ) apply spectral cluster approximate optimal graph cut solution additional information please refer bibliographic note ( section 117 ) second group method specific graph search graph find well-connected component cluster let ’ look method call scan ( structural cluster algorithm network ) example give undirected graph g = ( v e ) vertex u ∈ v neighborhood u 0 ( u ) = { | ( u v ) ∈ e } ∪ { u } used idea structural-context similarity scan measure similarity two vertex u v ∈ v normalize common neighborhood size 0 ( u ) ∩ 0 ( v ) | σ ( u v ) = √ 0 ( u ) 0 ( v ) | ( 1140 ) larger value compute similar two vertex scan used similarity threshold ε define cluster membership vertex u ∈ v ε-neighborhood u defined nε ( u ) = { v ∈ 0 ( u ) σ ( u v ) ≥ ε } ε-neighborhood u contain neighbor u structural-context similarity u least ε scan core vertex vertex inside cluster u ∈ v core vertex nε ( u ) | ≥ µ µ popularity threshold scan grow cluster core vertex vertex v ε-neighborhood core u v assign cluster u process grow cluster continue cluster grow process similar density-based cluster method dbscan ( chapter 10 ) formally vertex v directly reach core u v ∈ nε ( u ) transitively vertex v reach core u exist vertex w1 wn w1 reach u wi reach wi−1 1 < ≤ n v reach wn moreover two vertex u v ∈ v may may core say connect exist core w u v reach w vertex cluster connect cluster maximum set vertex every pair set connect vertex may belong cluster vertex u hub neighborhood 0 ( u ) u contain vertex one cluster vertex belong cluster hub outlier scan algorithm show figure search framework closely resemble cluster-find process dbscan scan find cut graph cluster set vertex connect base transitive similarity structural context advantage scan time complexity linear respect number edge large sparse graph number edge scale number vertex therefore scan expect good scalability cluster large graph 
532 chapter 11 advanced cluster analysis algorithm scan cluster graph datum input graph g = ( v e ) similarity threshold ε population threshold µ output set cluster method set vertex v unlabeled unlabeled vertex u u core generate new cluster-id c insert v ∈ nε ( u ) queue q q = w ← first vertex q r ← set vertex directly reach w ∈ r unlabeled labele nonmember assign current cluster-id c endif unlabeled insert queue q endif endfor remove w q end else label u nonmember endif endfor vertex u labele nonmember ∃x ∈ 0 ( u ) x different cluster-id label u hub else label u outlier endif endfor figure 1115 scan algorithm cluster analysis graph datum 114 cluster constraint user often background knowledge want integrate cluster analysis may also application-specific requirement information modeled cluster constraint approach topic cluster constraint two step section 1141 categorize type constraint cluster graph datum method cluster constraint introduce section 1142 
114 cluster constraint 533 1141 categorization constraint section study categorize constraint used cluster analysis specifically categorize constraint accord subject set strongly constraint enforce discuss chapter 10 cluster analysis involve three essential aspect object instance cluster cluster group object similarity among object therefore first method discuss categorize constraint accord apply thus three type constraint instance constraint cluster constraint similarity measurement constraint instance constraint instance specify pair set instance group cluster analysis two common type constraint category include must-link constraint must-link constraint specify two object x x group one cluster output cluster analysis must-link constraint transitive must-link ( x ) must-link ( z ) must-link ( x z ) link constraint link constraint opposite must-link constraint link constraint specify two object x output cluster analysis x belong different cluster link constraint entail link ( x ) must-link ( x x 0 ) must-link ( 0 ) link ( x 0 0 ) constraint instance defined used specific instance alternatively also defined used instance variable attribute instance example constraint constraint ( x ) must-link ( x ) dist ( x ) ≤  used distance object specify must-link constraint constraint cluster constraint cluster specify requirement cluster possibly used attribute cluster example constraint may specify minimum number object cluster maximum diameter cluster shape cluster ( eg convex ) number cluster specify partition cluster method regard constraint cluster constraint similarity measurement often similarity measure euclidean distance used measure similarity object cluster analysis application exception apply constraint similarity measurement specify requirement similarity calculation must respect example cluster person move object plaza euclidean distance used give 
534 chapter 11 advanced cluster analysis walking distance two point constraint similarity measurement trajectory implement shortest distance cross wall one way express constraint depend category example specify constraint cluster constraint1 diameter cluster larger requirement also expressed used constraint instance constraint10 link ( x ) dist ( x ) > ( 1141 ) example 1122 constraint instance cluster similarity measurement allelectronic cluster customer group customer assign customer relationship manager suppose want specify customer address place group would allow comprehensive service family expressed used must-link constraint instance constraintfamily ( x ) must-link ( x ) xaddress = yaddress allelectronic eight customer relationship manager ensure similar workload place constraint cluster eight cluster cluster least 10 % customer 15 % customer calculate spatial distance two customer used drive distance two however two customer live different country use flight distance instead constraint similarity measurement another way categorize cluster constraint consider firmly constraint respect constraint hard cluster violate constraint unacceptable constraint soft cluster violate constraint preferable acceptable better solution find soft constraint also call preference example 1123 hard soft constraint allelectronic constraintfamily example 1122 hard constraint splitting family different cluster can prevent company provide comprehensive service family lead poor customer satisfaction constraint number cluster ( correspond number customer relationship manager company ) also hard example 1122 also constraint balance size cluster satisfying constraint strongly prefer company flexible willing assign senior capable customer relationship manager oversee larger cluster therefore constraint soft ideally specific datum set set constraint clustering satisfy constraint however possible may cluster datum set 
114 cluster constraint 535 satisfy constraint trivially two constraint set conflict cluster satisfy time example 1124 conflict constraint consider constraint must-link ( x ) dist ( x ) < 5 link ( x ) dist ( x ) > 3 datum set two object x dist ( x ) = 4 cluster satisfy constraint simultaneously consider two constraint must-link ( x ) dist ( x ) < 5 must-link ( x ) dist ( x ) < 3 second constraint redundant give first moreover datum set distance two object least 5 every possible cluster object satisfy constraint “ measure quality usefulness set constraint ” general consider either informativeness coherence informativeness amount information carry constraint beyond cluster model give datum set cluster method set constraint c informativeness c respect measure fraction constraint c unsatisfied cluster compute d higher informativeness specific requirement background knowledge constraint carry coherence set constraint degree agreement among constraint measure redundancy among constraint 1142 method cluster constraint although categorize cluster constraint application may different constraint specific form consequently various technique need handle specific constraint section discuss general principle handle hard soft constraint handle hard constraint general strategy handle hard constraint strictly respect constraint cluster assignment process illustrate idea use partition cluster example 
536 chapter 11 advanced cluster analysis give datum set set constraint instance ( ie must-link link constraint ) extend k-mean method satisfy constraint cop-k-mean algorithm work follow generate superinstance must-link constraint compute transitive closure must-link constraint must-link constraint treat equivalence relation closure give one multiple subset object object subset must assign one cluster represent subset replace object subset mean superinstance also carry weight number object represent step must-link constraint always satisfied conduct modify k-mean cluster recall k-mean object assign closest center nearest-center assignment violate link constraint respect link constraint modify center assignment process k-mean nearest feasible center assignment object assign center sequence step make sure assignment far violate link constraint object assign nearest center assignment respect link constraint cop-k-mean ensure constraint violate every step require backtracking greedy algorithm generate cluster satisfy constraint provide conflict exist among constraint handle soft constraint cluster soft constraint optimization problem cluster violate soft constraint penalty impose cluster therefore optimization goal cluster contain two part optimize cluster quality minimize constraint violation penalty overall objective function combination cluster quality score penalty score illustrate use partition cluster example give datum set set soft constraint instance cvqe ( constrain vector quantization error ) algorithm conduct k-mean cluster enforce constraint violation penalty objective function used cvqe sum distance used k-mean adjust constraint violation penalty calculate follow penalty must-link violation must-link constraint object x assign two different center c1 c2 respectively constraint violate result dist ( c1 c2 ) distance c1 c2 add objective function penalty penalty link violation link constraint object x assign common center c constraint violate 
114 cluster constraint 537 distance dist ( c c 0 ) c c 0 add objective function penalty speeding constrain cluster constraint similarity measurement lead heavy cost cluster consider follow cluster obstacle problem cluster person move object plaza euclidean distance used measure walking distance two point however constraint similarity measurement trajectory implement shortest distance cross wall ( section 1141 ) obstacle may occur object distance two object may derive geometric computation ( eg involve triangulation ) computational cost high large number object obstacle involved cluster obstacle problem represent used graphical notation first point p visible another point q region r straight line join p q intersect obstacle visibility graph graph vg = ( v e ) vertex obstacle corresponding node v two node v1 v2 v joined edge e corresponding vertex represent visible let vg 0 = ( v 0 e 0 ) visibility graph create vg add two additional point p q v 0 e 0 contain edge join two point v 0 two point mutually visible shortest path two point p q subpath vg 0 show figure 1116 ( ) see begin edge p either v1 v2 v3 go path vg end edge either v4 v5 q reduce cost distance computation two pair object point several preprocess optimization technique used one method group point close together microcluster do first triangulating region r triangle grouping nearby point triangle microcluster used method similar birch dbscan show figure 1116 ( b ) process microcluster rather individual point overall computation reduce precomputation perform build two v4 v1 p v2 o1 v3 o2 vg q v5 vg ( ) ( b ) figure 1116 cluster obstacle object ( o1 o2 ) ( ) visibility graph ( b ) triangulation region microcluster source adapt tung hou han [ thh01 ] 
538 chapter 11 advanced cluster analysis kind join index base computation shortest path ( 1 ) vv index pair obstacle vertex ( 2 ) mv index pair microcluster obstacle vertex use index help optimize overall performance used precomputation optimization strategy distance two point ( granularity level microcluster ) compute efficiently thus cluster process perform manner similar typical efficient k-medoid algorithm claran achieve good cluster quality large datum set 115 summary conventional cluster analysis object assign one cluster exclusively however application need assign object one cluster fuzzy probabilistic way fuzzy cluster probabilistic model-based cluster allow object belong one cluster partition matrix record membership degree object belong cluster probabilistic model-based cluster assume cluster parameterized distribution used datum cluster observed sample estimate parameter cluster mixture model assume set observed object mixture instance multiple probabilistic cluster conceptually observed object generate independently first choose probabilistic cluster accord probability cluster choose sample accord probability density function choose cluster expectation-maximization algorithm framework approach maximum likelihood maximum posteriori estimate parameter statistical model expectation-maximization algorithms used compute fuzzy cluster probabilistic model-based cluster high-dimensional datum pose several challenge cluster analysis include model high-dimensional cluster search cluster two major category cluster method high-dimensional datum subspace cluster method dimensionality reduction method subspace cluster method search cluster subspace original space example include subspace search method correlation-based cluster method bicluster method dimensionality reduction method create new space lower dimensionality search cluster bicluster method cluster object attribute simultaneously type bicluster include bicluster constant value constant value column coherent value coherent evolution column two major type bicluster method optimization-based method enumeration method 
116 exercise 539 spectral cluster dimensionality reduction method general idea construct new dimension used affinity matrix cluster graph network datum many application social network analysis challenge include measure similarity object graph design cluster model method graph network datum geodesic distance number edge two vertex graph used measure similarity alternatively similarity graph social network measure used structural context random walk simrank similarity measure base structural context random walk graph cluster modeled compute graph cut sparsest cut may lead good cluster modularity used measure cluster quality scan graph cluster algorithm search graph identify well-connected component cluster constraint used express application-specific requirement background knowledge cluster analysis constraint cluster categorize constraint instance cluster similarity measurement constraint instance include must-link link constraint constraint hard soft hard constraint cluster enforce strictly respect constraint cluster assignment process cluster soft constraint consider optimization problem heuristic used speed constrain cluster 116 exercise 111 traditional cluster method rigid require object belong exclusively one cluster explain special case fuzzy cluster may use k-mean example 112 allelectronic carry 1000 product p1 p1000 consider customer ada bob cathy ada bob purchase three product common p1 p2 p3 997 product ada bob independently purchase seven randomly cathy purchase 10 product randomly select 1000 product euclidean distance probability dist ( ada bob ) > dist ( ada cathy ) jaccard similarity ( chapter 2 ) used learn example 113 show × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 114 compare maple algorithm ( section 1123 ) frequent close itemset mining algorithm closet ( pei han mao [ phm00 ] ) major similarity difference 
540 chapter 11 advanced cluster analysis 115 simrank similarity measure cluster graph network datum ( ) prove lim si ( u v ) = ( u v ) simrank computation i→∞ ( b ) show ( u v ) = p ( u v ) simrank 116 large sparse graph average node low degree similarity matrix used simrank still sparse sense deliberate answer 117 compare scan algorithm ( section 1133 ) dbscan ( section 1041 ) similarity difference 118 consider partition cluster follow constraint cluster number object cluster must nk ( 1 − δ ) nk ( 1 + δ ) n total number object datum set k number cluster desire δ [ 0 1 ) parameter extend k-mean method handle constraint discuss situation constraint hard soft 117 bibliographic note höppner klawonn kruse runkler [ hkkr99 ] provide thorough discussion fuzzy cluster fuzzy c-mean algorithm ( example 117 base ) propose bezdek [ bez81 ] fraley raftery [ fr02 ] give comprehensive overview model-based cluster analysis probabilistic model mclachlan basford [ mb88 ] present systematic introduction mixture model application cluster analysis dempster laird rubin [ dlr77 ] recognize first introduce em algorithm give name however idea em algorithm “ propose many time special circumstance ” admit dempster laird rubin [ dlr77 ] wu [ wu83 ] give correct analysis em algorithm mixture model em algorithms used extensively many datum mining application introduction model-based cluster mixture model em algorithms find recent textbook machine learn statistical learning—for example bishop [ bis06 ] marsland [ mar09 ] alpaydin [ alp11 ] increase dimensionality severe effect distance function indicated beyer et al [ bgrs99 ] also dramatic impact various technique classification cluster semisupervised learn ( radovanović nanopoulos ivanović [ rni09 ] ) kriegel kröger zimek [ kkz09 ] present comprehensive survey method cluster high-dimensional datum clique algorithm develop agrawal gehrke gunopulos raghavan [ aggr98 ] proclus algorithm propose aggawal procopiuc wolf et al [ + 99 ] technique bicluster initially propose hartigan [ har72 ] term bicluster coin mirkin [ mir98 ] cheng church [ cc00 ] introduce 
117 bibliographic note 541 bicluster gene expression datum analysis many study bicluster model method notion δ-pcluster introduce wang wang yang yu [ wwyy02 ] informative survey see madeira oliveira [ mo04 ] tanay sharan shamir [ tss04 ] chapter introduce δ-cluster algorithm cheng church [ cc00 ] maple pei zhang cho et al [ + 03 ] example optimization-based method enumeration method bicluster respectively donath hoffman [ dh73 ] fiedler [ fie73 ] pioneer spectral cluster chapter use algorithm propose ng jordan weis [ njw01 ] example thorough tutorial spectral cluster see luxburg [ lux07 ] cluster graph network datum important fast-growing topic schaeffer [ sch07 ] provide survey simrank measure similarity develop jeh widom [ jw02a ] xu et al [ xyfs07 ] propose scan algorithm arora rao vazirani [ arv09 ] discuss sparsest cut approximation algorithms cluster constraint extensively study davidson wagstaff basu [ dwb06 ] propose measure informativeness coherence copk-mean algorithm give wagstaff et al [ wcrs01 ] cvqe algorithm propose davidson ravi [ dr05 ] tung han lakshmanan ng [ thln01 ] present framework constraint-based cluster base user-specified constraint efficient method constraint-based spatial cluster existence physical obstacle constraint propose tung hou han [ thh01 ] 
13 datum mining trend research frontier young research field datum mining make significant progress cover broad spectrum application since 1980s today datum mining used vast array area numerous commercial datum mining system service available many challenge however still remain final chapter introduce mining complex datum type prelude in-depth study reader may choose addition focus trend research frontier datum mining section 131 present overview methodology mining complex datum type extend concept task introduce book mining include mining time-series sequential pattern biological sequence graph network spatiotemporal datum include geospatial datum moving-object datum cyber-physical system datum multimedium datum text datum web datum datum stream section 132 briefly introduce approach datum mining include statistical method theoretical foundation visual audio datum mining section 133 learn datum mining application business science include financial retail telecommunication industry science engineering recommender system social impact datum mining discuss section 134 include ubiquitous invisible datum mining privacy-preserve datum mining finally section 135 speculate current expect datum mining trend arise response new challenge field 131 mining complex datum type section outline major development research effort mining complex datum type complex datum type summarize figure section 1311 cover mining sequence datum time-series symbolic sequence biological sequence section 1312 discuss mining graph social information network section 1313 address mining kind datum include spatial datum spatiotemporal datum moving-object datum cyber-physical system datum multimedium datum text datum datum mining concept technique doi b978-0-12-381479-100013-7 c 2012 elsevier right re-serve 585 
586 chapter 13 datum mining trend research frontier c p l e x p e f sequence datum graph network mining kind datum time-series datum ( eg stock market datum ) symbolic sequence ( eg customer shopping sequence web click stream ) biological sequence ( eg dna protein sequence ) homogeneous ( link type ) heterogeneous ( link different type ) example graph social information network etc spatial datum spatiotemporal datum cyber-physical system datum multimedium datum text datum web datum datum stream figure 131 complex datum type mining web datum datum stream due broad scope theme section present high-level overview topic discuss in-depth book 1311 mining sequence datum time-series symbolic sequence biological sequence sequence order list event sequence may categorize three group base characteristic event describe ( 1 ) time-series datum ( 2 ) symbolic sequence datum ( 3 ) biological sequence let ’ consider type time-series datum sequence datum consist long sequence numeric datum record equal time interval ( eg per minute per hour per day ) time-series datum generate many natural economic process stock market scientific medical natural observation symbolic sequence datum consist long sequence event nominal datum typically observed equal time interval many sequence gap ( ie lapse record event ) matter much example include customer shopping sequence web click stream well sequence event science engineering natural social development biological sequence include dna protein sequence sequence typically long carry important complicate hide semantic meaning gap usually important let ’ look datum mining sequence datum type 
131 mining complex datum type 587 similarity search time-series datum time-series datum set consist sequence numeric value obtain repeat measurement time value typically measure equal time interval ( eg every minute hour day ) time-series databasis popular many application stock market analysis economic sale forecasting budgetary analysis utility study inventory study yield projection workload projection process quality control also useful study natural phenomena ( eg atmosphere temperature wind earthquake ) scientific engineering experiment medical treatment unlike normal database query find datum match give query exactly similarity search find datum sequence differ slightly give query sequence many time-series similarity query require subsequence match find set sequence contain subsequence similar give query sequence similarity search often necessary first perform datum dimensionality reduction transformation time-series datum typical dimensionality reduction technique include ( 1 ) discrete fourier transform ( dft ) ( 2 ) discrete wavelet transform ( dwt ) ( 3 ) singular value decomposition ( svd ) base principle component analysis ( pca ) touch concept chapter 3 thorough explanation beyond scope book go great detail technique datum signal map signal transform space small subset “ strongest ” transform coefficient save feature feature form feature space projection transform space index construct original transform time-series datum speed search query-based similarity search technique include normalization transformation atomic match ( ie find pair gap-free window small length similar ) window stitching ( ie stitching similar window form pair large similar subsequence allow gap atomic match ) subsequence order ( ie linearly order subsequence match determine whether enough similar piece exist ) numerous software package exist similarity search time-series datum recently researcher propose transform time-series datum piecewise aggregate approximation datum view sequence symbolic representation problem similarity search transform one match subsequence symbolic sequence datum identify motif ( ie frequently occur sequential pattern ) build index hashing mechanism efficient search base motif experiment show approach fast simple comparable search quality dft dwt dimensionality reduction method regression trend analysis time-series datum regression analysis time-series datum study substantially field statistic signal analysis however one may often need go beyond pure regression 
chapter 13 datum mining trend research frontier price 588 allelectronic stock 10-day move average time figure 132 time-series datum stock price allelectronic time trend show dash curve calculate move average analysis perform trend analysis many practical application trend analysis build integrate model used follow four major component movement characterize time-series datum trend long-term movement indicate general direction time-series graph move time example used weight move average least square method find trend curf dash curve indicated figure 132 cyclic movement long-term oscillation trend line curve seasonal variation nearly identical pattern time series appear follow corresponding season successive year holiday shopping season effective trend analysis datum often need “ deseasonalize ” base seasonal index compute autocorrelation random movement characterize sporadic change due chance event labor dispute announce personnel change within company trend analysis also used time-series forecasting find mathematical function approximately generate historic pattern time series used make long-term short-term prediction future value arima ( auto-regressive integrate move average ) long-memory time-series modele autoregression popular method analysis sequential pattern mining symbolic sequence symbolic sequence consist order set element event record without concrete notion time many application involve datum 
131 mining complex datum type 589 symbolic sequence customer shopping sequence web click stream program execution sequence biological sequence sequence event science engineering natural social development biological sequence carry complicate semantic meaning pose many challenge research issue investigation conduct field bioinformatic sequential pattern mining focuse extensively mining symbolic sequence sequential pattern frequent subsequence exist single sequence set sequence sequence α = ha1 a2 · · · subsequence another sequence β = hb1 b2 · · · bm exist integer 1 ≤ j1 < j2 < · · · < jn ≤ a1 ⊆ bj1 a2 ⊆ bj2 ⊆ bjn example α = h { ab } di β = h { abc } { } { de } ai b c e item α subsequence mining sequential pattern consist mining set subsequence frequent one sequence set sequence many scalable algorithms develop result extensive study area alternatively mine set close sequential pattern sequential pattern close exist sequential pattern 0 proper subsequence 0 0 ( frequency ) support s similar frequent pattern mining counterpart also study efficient mining multidimensional multilevel sequential pattern constraint-based frequent pattern mining user-specified constraint used reduce search space sequential pattern mining derive pattern interest user refer constraint-based sequential pattern mining moreover may relax constraint enforce additional constraint problem sequential pattern mining derive different kind pattern sequence datum example enforce gap constraint pattern derive contain consecutive subsequence subsequence small gap alternatively may derive periodic sequential pattern fold event proper-size window find recur subsequence window another approach derive partial order pattern relax requirement strict sequential order mining subsequence pattern besides mining partial order pattern sequential pattern mining methodology also extend mining tree lattice episode order pattern sequence classification classification method perform model construction base feature vector however sequence explicit feature even sophisticated feature selection technique dimensionality potential feature still high sequential nature feature difficult capture make sequence classification challenge task sequence classification method organized three category ( 1 ) featurebased classification transform sequence feature vector apply conventional classification method ( 2 ) sequence distance–based classification distance function measure similarity sequence determine 
590 chapter 13 datum mining trend research frontier quality classification significantly ( 3 ) model-based classification used hide markov model ( hmm ) statistical model classify sequence time-series numeric-valu datum feature selection technique symbolic sequence easily apply time-series datum without discretization however discretization cause information loss recently propose time-series shapelet method used time-series subsequence maximally represent class feature achieve quality classification result alignment biological sequence biological sequence generally refer sequence nucleotide amino acid biological sequence analysis compare align index analyze biological sequence thus play crucial role bioinformatic modern biology sequence alignment base fact live organism related evolution imply nucleotide ( dna rna ) protein sequence species closer evolution exhibit similarity alignment process line sequence achieve maximal identity level also express degree similarity sequence two sequence homologous share common ancestor degree similarity obtain sequence alignment useful determine possibility homology two sequence alignment also help determine relative position multiple species evolution tree call phylogenetic tree problem alignment biological sequence describe follow give two input biological sequence identify similar sequence long conserve subsequence number sequence align exactly two problem know pairwise sequence alignment otherwise multiple sequence alignment sequence compare align either nucleotide ( rna ) amino acid ( protein ) nucleotide two symbol align identical however amino acid two symbol align identical one derive substitution likely occur nature two kind alignment local alignment global alignment former mean portion sequence align whereas latter require alignment entire length sequence either nucleotide amino acid insertion deletion substitution occur nature different probability substitution matrix used represent probability substitution nucleotide amino acid probability insertion deletion usually use gap character − indicate position preferable align two symbol evaluate quality alignment score mechanism typically defined usually count identical similar symbol positive score gap negative one algebraic sum score take alignment measure goal alignment achieve maximal score among possible alignment however expensive ( exactly np-hard problem ) find optimal alignment therefore various heuristic method develop find suboptimal alignment 
131 mining complex datum type 591 dynamic programming approach commonly used sequence alignment among many available analysis package blast ( basic local alignment search tool ) one popular tool biosequence analysis hide markov model biological sequence analysis give biological sequence biologist would like analyze sequence represent represent structure statistical regularity sequence class biologist construct various probabilistic model markov chain hide markov model model probability state depend previous state therefore particularly useful analysis biological sequence datum common method construct hide markov model forward algorithm viterbi algorithm baum-welch algorithm give sequence symbol x forward algorithm find probability obtain x model viterbi algorithm find probable path ( corresponding x ) model whereas baum-welch algorithm learn adjust model parameter best explain set training sequence 1312 mining graph network graph represent general class structure set sequence lattice tree broad range graph application web social network information network biological network bioinformatic chemical informatic computer vision multimedium text retrieval hence graph network mining become increasingly important heavily research overview follow major theme ( 1 ) graph pattern mining ( 2 ) statistical modele network ( 3 ) datum clean integration validation network analysis ( 4 ) cluster classification graph homogeneous network ( 5 ) cluster ranking classification heterogeneous network ( 6 ) role discovery link prediction information network ( 7 ) similarity search olap information network ( 8 ) evolution information network graph pattern mining graph pattern mining mining frequent subgraph ( also call ( sub ) graph pattern ) one set graph method mining graph pattern categorize apriori-based pattern growth–base approach alternatively mine set close graph graph g close exist proper supergraph g 0 carry support count g moreover many variant graph pattern include approximate frequent graph coherent graph dense graph user-specified constraint push deep graph pattern mining process improve mining efficiency graph pattern mining many interesting application example used generate compact effective graph index structure base concept 
592 chapter 13 datum mining trend research frontier frequent discriminative graph pattern approximate structure similarity search achieve explore graph index structure multiple graph feature moreover classification graph also perform effectively used frequent discriminative subgraph feature statistical modele network network consist set node corresponding object associate set property set edge ( link ) connect node represent relationship object network homogeneous node link type friend network coauthor network web page network network heterogeneous node link different type publication network ( link together author conference paper content ) health-care network ( link together doctor nurse patient disease treatment ) researcher propose multiple statistical model modele homogeneous network well-known generative model random graph model ( ie erdös-rényi model ) watts-strogatz model scale-free model scalefree model assume network follow power law distribution ( also know pareto distribution heavy-tailed distribution ) large-scale social network small-world phenomenon observed network characterize high degree local cluster small fraction node ( ie node interconnect one another ) degree separation remain node social network exhibit certain evolutionary characteristic tend follow densification power law state network become increasingly dense time shrink diameter another characteristic effective diameter often decrease network grow node out-degree in-degree typically follow heavytailed distribution datum clean integration validation information network analysis real-world datum often incomplete noisy uncertain unreliable information redundancy may exist among multiple piece datum interconnect large network information redundancy explore network perform quality datum clean datum integration information validation trustability analysis network analysis example distinguish author share name examine networked connection heterogeneous object coauthor publication venue term addition identify inaccurate author information present bookseller explore network build base author information provide multiple bookseller sophisticated information network analysis method develop direction many case portion datum serve “ training ” relatively clean reliable datum consensus datum multiple information 
131 mining complex datum type 593 provider used help consolidate remain unreliable portion datum reduce costly effort labele datum hand training massive dynamic real-world datum set cluster classification graph homogeneous network large graph network cohesive structure often hide among massive interconnect node link cluster analysis method develop large network uncover network structure discover hide community hub outlier base network topological structure associate property various kind network cluster method develop categorize either partition hierarchical density-based algorithms moreover give human-labele training datum discovery network structure guide human-specify heuristic constraint supervised classification semi-supervised classification network recent hot topic datum mining research community cluster ranking classification heterogeneous network heterogeneous network contain interconnect node link different type interconnect structure contain rich information used mutually enhance node link propagate knowledge one type another cluster ranking heterogeneous network perform hand-inhand context highly rank link cluster may contribute lower-rank counterpart evaluation cohesiveness cluster cluster may help consolidate high ranking link dedicate cluster mutual enhancement ranking cluster prompt development algorithm call rankclus moreover user may specify different ranking rule present labele link certain datum type knowledge one type propagate type propagation reach link type via heterogeneous-type connection algorithms develop supervised learn semi-supervised learn heterogeneous network role discovery link prediction information network exist many hide role relationship among different link heterogeneous network example include advisor–advisee leader–follower relationship research publication network discover hide role relationship expert specify constraint base background knowledge enforce constraint may help crosscheck validation large interconnect network information redundancy network often used help weed link follow constraint 
594 chapter 13 datum mining trend research frontier similarly link prediction perform base assessment ranking expect relationship among candidate link example may predict paper author may write read cite base author ’ recent publication history trend research similar topic study often require analyze proximity network link trend connection similar neighbor roughly speaking person refer link prediction link mining however link mining cover additional task include link-based object classification object type prediction link type prediction link existence prediction link cardinality estimation object reconciliation ( predict whether two object fact ) also include group detection ( cluster object ) well subgraph identification ( find characteristic subgraph within network ) metadata mining ( uncover schema-type information regard unstructured datum ) similarity search olap information network similarity search primitive operation database web search engine heterogeneous information network consist multityped interconnect object example include bibliographic network social medium network two object consider similar link similar way multityped object general object similarity within network determine base network structure object property similarity measure moreover network cluster hierarchical network structure help organize object network identify subcommunity well facilitate similarity search furthermore similarity defined differently per user consider different linkage path derive various similarity semantic network know path-based similarity organize network base notion similarity cluster generate multiple hierarchy within network online analytical process ( olap ) perform example drill dice information network base different level abstraction different angle view olap operation may generate multiple interrelate network relationship among network may disclose interesting hide semantic evolution social information network network dynamic constantly evolve detect evolve community evolve regularity anomaly homogeneous heterogeneous network help person better understand structural evolution network predict trend irregularity evolve network homogeneous network evolve community discover subnetwork consist object type set friend coauthor however heterogeneous network community discover subnetwork consist object different type connect set paper author venue term also derive set evolve object type like evolve author theme 
131 mining complex datum type 595 1313 mining kind datum addition sequence graph many kind semi-structure unstructured datum spatiotemporal multimedium hypertext datum interesting application datum carry various kind semantic either store dynamically stream system call specialize datum mining methodology thus mining multiple kind datum include spatial datum spatiotemporal datum cyber-physical system datum multimedium datum text datum web datum datum stream increasingly important task datum mining subsection overview methodology mining kind datum mining spatial datum spatial datum mining discover pattern knowledge spatial datum spatial datum many case refer geospace-related datum store geospatial datum repository datum “ vector ” “ raster ” format form imagery geo-reference multimedium recently large geographic datum warehouse construct integrate thematic geographically reference datum multiple source construct spatial datum cube contain spatial dimension measure support spatial olap multidimensional spatial datum analysis spatial datum mining perform spatial datum warehouse spatial databasis geospatial datum repository popular topic geographic knowledge discovery spatial datum mining include mining spatial association co-location pattern spatial cluster spatial classification spatial modele spatial trend outlier analysis mining spatiotemporal datum move object spatiotemporal datum datum relate space time spatiotemporal datum mining refer process discover pattern knowledge spatiotemporal datum typical example spatiotemporal datum mining include discover evolutionary history city land uncover weather pattern predict earthquake hurricane determine global warm trend spatiotemporal datum mining become increasingly important far-reaching implication give popularity mobile phone gps device internet-based map service weather service digital earth well satellite rfid sensor wireless video technology among many kind spatiotemporal datum moving-object datum ( ie datum move object ) especially important example animal scientist attach telemetry equipment wildlife analyze ecological behavior mobility manager emb gps car better monitor guide vehicle meteorologist use weather satellite radar observe hurricane massive-scale moving-object datum become rich complex ubiquitous example moving-object datum mining include mining movement pattern multiple move object ( ie discovery relationship among multiple move object move cluster leader follower merge convoy swarm pincer well collective movement pattern ) example 
596 chapter 13 datum mining trend research frontier moving-object datum mining include mining periodic pattern one set move object mining trajectory pattern cluster model outlier mining cyber-physical system datum cyber-physical system ( cp ) typically consist large number interact physical information component cp system may interconnect form large heterogeneous cyber-physical network example cyber-physical network include patient care system link patient monitoring system network medical information emergency handle system transportation system link transportation monitoring network consist many sensor video camera traffic information control system battlefield commander system link reconnaissance network battlefield information analysis system clearly cyber-physical system network ubiquitous form critical component modern information infrastructure datum generate cyber-physical system dynamic volatile noisy inconsistent interdependent contain rich spatiotemporal information critically important real-time decision make comparison typical spatiotemporal datum mining mining cyber-physical datum require link current situation large information base perform real-time calculation return prompt response research area include rare-event detection anomaly analysis cyber-physical datum stream reliability trustworthiness cyber-physical datum analysis effective spatiotemporal datum analysis cyber-physical network integration stream datum mining real-time automate control process mining multimedium datum multimedium datum mining discovery interesting pattern multimedium databasis store manage large collection multimedium object include image datum video datum audio datum well sequence datum hypertext datum contain text text markup linkage multimedium datum mining interdisciplinary field integrate image process understand computer vision datum mining pattern recognition issue multimedium datum mining include content-based retrieval similarity search generalization multidimensional analysis multimedium datum cube contain additional dimension measure multimedium information topic multimedium mining include classification prediction analysis mining association video audio datum mining ( section 1323 ) mining text datum text mining interdisciplinary field draw information retrieval datum mining machine learn statistic computational linguistic substantial portion information store text news article technical paper book digital library email message blog web page hence research text mining active important goal derive high-quality information text 
131 mining complex datum type 597 typically do discovery pattern trend mean statistical pattern learn topic modele statistical language modele text mining usually require structuring input text ( eg parse along addition derive linguistic feature removal other subsequent insertion database ) follow derive pattern within structure datum evaluation interpretation output “ high quality ” text mining usually refer combination relevance novelty interestingness typical text mining task include text categorization text cluster entity extraction production granular taxonomy sentiment analysis document summarization entity-relation modele ( ie learn relation name entity ) example include multilingual datum mining multidimensional text analysis contextual text mining trust evolution analysis text datum well text mining application security biomedical literature analysis online medium analysis analytical customer relationship management various kind text mining analysis software tool available academic institution open-source forum industry text mining often also used wordnet sematic web wikipedia information source enhance understand mining text datum mining web datum world wide web serve huge widely distribute global information center news advertisement consumer information financial management education government e-commerce contain rich dynamic collection information web page content hypertext structure multimedium hyperlink information access usage information provide fertile source datum mining web mining application datum mining technique discover pattern structure knowledge web accord analysis target web mining organized three main area web content mining web structure mining web usage mining web content mining analyze web content text multimedium datum structure datum ( within web page link across web page ) do understand content web page provide scalable informative keyword-based page indexing concept resolution web page relevance ranking web page content summary valuable information related web search analysis web page reside either surface web deep web surface web portion web index typical search engine deep web ( hide web ) refer web content part surface web content provide underlie database engine web content mining study extensively researcher search engine web service company web content mining build link across multiple web page individual therefore potential inappropriately disclose personal information study privacy-preserve datum mining address concern development technique protect personal privacy web web structure mining process used graph network mining theory method analyze node connection structure web extract pattern hyperlink hyperlink structural component connect 
598 chapter 13 datum mining trend research frontier web page another location also mine document structure within page ( eg analyze treelike structure page structure describe html xml tag usage ) kind web structure mining help us understand web content may also help transform web content relatively structure datum set web usage mining process extract useful information ( eg user click stream ) server log find pattern related general particular group user understand user ’ search pattern trend association predict user look internet help improve search efficiency effectiveness well promote product related information different group user right time web search company routinely conduct web usage mining improve quality service mining datum stream stream datum refer datum flow system vast volume change dynamically possibly infinite contain multidimensional feature datum store traditional database system moreover system may able read stream sequential order pose great challenge effective mining stream datum substantial research lead progress development efficient method mining datum stream area mining frequent sequential pattern multidimensional analysis ( eg construction stream cube ) classification cluster outlier analysis online detection rare event datum stream general philosophy develop single-scan a-few-scan algorithms used limit compute storage capability include collect information stream datum slide window tilt time window ( recent datum register finest granularity distant datum register coarser granularity ) explore technique like microcluster limit aggregation approximation many application stream datum mining explored—for example real-time detection anomaly computer network traffic botnet text stream video stream power-grid flow web search sensor network cyber-physical system 132 methodology datum mining due broad scope datum mining large variety datum mining methodology methodology datum mining thoroughly cover book section briefly discuss several interesting methodology fully address previous chapter methodology list figure 133 1321 statistical datum mining datum mining technique describe book primarily draw computer science discipline include datum mining machine learn datum warehousing algorithms design efficient handle huge amount datum 
132 methodology datum mining h e r n n g e h l g e statistical datum mining foundation datum mining visual audio datum mining 599 regression generalized linear model analysis variance mixed-effect model factor analysis discriminant analysis survival analysis datum reduction datum compression probability statistical theory microeconomic view pattern discovery inductive database datum visualization datum mining result visualization datum mining process visualization interactive visual datum mining audio datum mining figure 133 datum mining methodology typically multidimensional possibly various complex type however many well-established statistical technique datum analysis particularly numeric datum technique apply extensively scientific datum ( eg datum experiment physics engineering manufacturing psychology medicine ) well datum economic social science technique principal component analysis ( chapter 3 ) cluster ( chapter 10 11 ) already address book thorough discussion major statistical method datum analysis beyond scope book however several method mentioned sake completeness pointer technique provide bibliographic note ( section 138 ) regression general method used predict value response ( dependent ) variable one predictor ( independent ) variable variable numeric various form regression linear multiple weight polynomial nonparametric robust ( robust method useful error fail satisfy normalcy condition datum contain significant outlier ) generalized linear model model generalization ( generalized additive model ) allow categorical ( nominal ) response variable ( transformation 
600 chapter 13 datum mining trend research frontier ) related set predictor variable manner similar modele numeric response variable used linear regression generalized linear model include logistic regression poisson regression analysis variance technique analyze experimental datum two population describe numeric response variable one categorical variable ( factor ) general anova ( single-factor analysis variance ) problem involve comparison k population treatment mean determine least two mean different complex anova problem also exist mixed-effect model model analyze group data—data classify accord one grouping variable typically describe relationship response variable covariate datum group accord one factor common area application include multilevel datum repeat measure datum block design longitudinal datum factor analysis method used determine variable combine generate give factor example many psychiatric datum possible measure certain factor interest directly ( eg intelligence ) however often possible measure quantity ( eg student test score ) reflect factor interest none variable designate dependent discriminant analysis technique used predict categorical response variable unlike generalized linear model assume independent variable follow multivariate normal distribution procedure attempt determine several discriminant function ( linear combination independent variable ) discriminate among group defined response variable discriminant analysis commonly used social science survival analysis several well-established statistical technique exist survival analysis technique originally design predict probability patient undergo medical treatment would survive least time t method survival analysis however also commonly apply manufacturing setting estimate life span industrial equipment popular method include kaplanmeier estimate survival cox proportional hazard regression model extension quality control various statistic used prepare chart quality control shewhart chart cusum chart ( display group summary statistic ) statistic include mean standard deviation range count move average move standard deviation move range 1322 view datum mining foundation research theoretical foundation datum mining yet mature solid systematic theoretical foundation important help provide coherent 
132 methodology datum mining 601 framework development evaluation practice datum mining technology several theory basis datum mining include follow datum reduction theory basis datum mining reduce datum representation datum reduction trade accuracy speed response need obtain quick approximate answer query large databasis datum reduction technique include singular value decomposition ( drive element behind principal component analysis ) wavelet regression log-linear model histogram cluster sampling construction index tree datum compression accord theory basis datum mining compress give datum encode term bit association rule decision tree cluster encode base minimum description length principle state “ best ” theory infer datum set one minimize length theory datum encode used theory predictor datum encode typically bit probability statistical theory accord theory basis datum mining discover joint probability distribution random variable example bayesian belief network hierarchical bayesian model microeconomic view microeconomic view consider datum mining task find pattern interesting extent used decision-make process enterprise ( eg regard marketing strategy production plan ) view one utility pattern consider interesting act enterprise regard face optimization problem object maximize utility value decision theory datum mining become nonlinear optimization problem pattern discovery inductive databasis theory basis datum mining discover pattern occur datum association classification model sequential pattern area machine learn neural network association mining sequential pattern mining cluster several subfield contribute theory knowledge base view database consist datum pattern user interact system query datum theory ( ie pattern ) knowledge base knowledge base actually inductive database theory mutually exclusive example pattern discovery also see form datum reduction datum compression ideally theoretical framework able model typical datum mining task ( eg association classification cluster ) probabilistic nature able handle different form datum consider iterative interactive essence datum mining effort require establish well-defined framework datum mining satisfy requirement 
602 chapter 13 datum mining trend research frontier 1323 visual audio datum mining visual datum mining discover implicit useful knowledge large datum set used datum or knowledge visualization technique human visual system controlled eye brain latter thought powerful highly parallel process reasoning engine contain large knowledge base visual datum mining essentially combine power component make highly attractive effective tool comprehension datum distribution pattern cluster outlier datum visual datum mining view integration two discipline datum visualization datum mining also closely related computer graphic multimedium system human–computer interaction pattern recognition high-performance compute general datum visualization datum mining integrate follow way datum visualization datum database datum warehouse view different granularity abstraction level different combination attribute dimension datum present various visual form boxplot 3-d cube datum distribution chart curf surface link graph show datum visualization section chapter figure 134 135 statsoft show figure 134 boxplot show multiple variable combination statsoft source wwwstatsoftcom 
132 methodology datum mining 603 figure 135 multidimensional datum distribution analysis statsoft source wwwstatsoftcom datum distribution multidimensional space visual display help give user clear impression overview datum characteristic large datum set datum mining result visualization visualization datum mining result presentation result knowledge obtain datum mining visual form form may include scatter plot boxplot ( chapter 2 ) well decision tree association rule cluster outlier generalized rule example scatter plot show figure 136 sas enterprise miner figure 137 mineset used plane associate set pillar describe set association rule mine database figure 138 also mineset present decision tree figure 139 ibm intelligent miner present set cluster property associate datum mining process visualization type visualization present various process datum mining visual form user see datum extract database datum warehouse extract well select datum clean integrate preprocessed mine moreover may also show method select datum mining result store may view figure 1310 show visual presentation datum mining process clementine datum mining system 
604 chapter 13 datum mining trend research frontier figure 136 visualization datum mining result sas enterprise miner interactive visual datum mining ( interactive ) visual datum mining visualization tool used datum mining process help user make smart datum mining decision example datum distribution set attribute display used colored sector ( whole space represent circle ) display help user determine sector first select classification good split point sector may example show figure 1311 output perception-based classification ( pbc ) system develop university munich audio datum mining used audio signal indicate pattern datum feature datum mining result although visual datum mining may disclose interesting pattern used graphical display require user concentrate watch pattern identify interesting novel feature within sometimes quite tiresome pattern transform sound music instead watch picture listen pitch rhythm tune melody identify anything interesting unusual may relieve burden visual concentration 
132 methodology datum mining figure 137 visualization association rule mineset figure 138 visualization decision tree mineset 605 
606 chapter 13 datum mining trend research frontier figure 139 visualization cluster grouping ibm intelligent miner figure 1310 visualization datum mining process clementine 
133 datum mining application 607 figure 1311 perception-based classification interactive visual mining approach relax visual mining therefore audio datum mining interesting complement visual mining 133 datum mining application book study principle method mining relational datum datum warehouse complex datum type datum mining relatively young discipline wide diverse application still nontrivial gap general principle datum mining application-specific effective datum mining tool section examine several application domain list figure discuss customize datum mining method tool develop application 1331 datum mining financial datum analysis bank financial institution offer wide variety banking investment credit service ( latter include business mortgage automobile loan credit card ) also offer insurance stock investment service 
608 chapter 13 datum mining trend research frontier financial datum analysis retail telecommunication industry science engineering datum mining application intrusion detection prevention recommender system figure 1312 common datum mining application domain financial datum collect banking financial industry often relatively complete reliable high quality facilitate systematic datum analysis datum mining present typical case design construction datum warehouse multidimensional datum analysis datum mining like many application datum warehouse need construct banking financial datum multidimensional datum analysis method used analyze general property datum example company ’ financial officer may want view debt revenue change month region sector factor along maximum minimum total average trend deviation statistical information datum warehouse datum cube ( include advanced datum cube concept multifeature discovery-driven regression prediction datum cube ) characterization class comparison cluster outlier analysis play important role financial datum analysis mining loan payment prediction customer credit policy analysis loan payment prediction customer credit analysis critical business bank many factor strongly weakly influence loan payment performance customer credit rating datum mining method attribute selection attribute relevance ranking may help identify important factor eliminate irrelevant one example factor related risk loan payment include loan-to-value ratio term loan debt ratio ( total amount monthly debt versus total monthly income ) payment-to-income ratio customer income level education level residence region credit history analysis customer payment history may find say payment-to-income ratio dominant factor education level debt ratio bank may decide adjust loan-grant policy 
133 datum mining application 609 grant loan customer whose application previously deny whose profile show relatively low risk accord critical factor analysis classification cluster customer target marketing classification cluster method used customer group identification target marketing example use classification identify crucial factor may influence customer ’ decision regard banking customer similar behavior regard loan payment may identify multidimensional cluster technique help identify customer group associate new customer appropriate customer group facilitate target marketing detection money launder financial crime detect money launder financial crime important integrate information multiple heterogeneous databasis ( eg bank transaction databasis federal state crime history databasis ) long potentially related study multiple datum analysis tool used detect unusual pattern large amount cash flow certain period certain group customer useful tool include datum visualization tool ( display transaction activity used graph time group customer ) linkage information network analysis tool ( identify link among different customer activity ) classification tool ( filter unrelated attribute rank highly related one ) cluster tool ( group different case ) outlier analysis tool ( detect unusual amount fund transfer activity ) sequential pattern analysis tool ( characterize unusual access sequence ) tool may identify important relationship pattern activity help investigator focus suspicious case detailed examination 1332 datum mining retail telecommunication industry retail industry well-fit application area datum mining since collect huge amount datum sale customer shopping history good transportation consumption service quantity datum collect continue expand rapidly especially due increase availability ease popularity business conduct web e-commerce today major chain store also web site customer make purchase online business amazoncom ( wwwamazoncom ) exist solely online without brick-and-mortar ( ie physical ) store location retail datum provide rich source datum mining retail datum mining help identify customer buy behavior discover customer shopping pattern trend improve quality customer service achieve better customer retention satisfaction enhance good consumption ratio design effective good transportation distribution policy reduce cost business example datum mining retail industry outlined follow design construction datum warehouse retail datum cover wide spectrum ( include sale customer employee good transportation consumption 
610 chapter 13 datum mining trend research frontier service ) many way design datum warehouse industry level detail include vary substantially outcome preliminary datum mining exercise used help guide design development datum warehouse structure involve decide dimension level include preprocess perform facilitate effective datum mining multidimensional analysis sale customer product time region retail industry require timely information regard customer need product sale trend fashion well quality cost profit service commodity therefore important provide powerful multidimensional analysis visualization tool include construction sophisticated datum cube accord need datum analysis advanced datum cube structure introduce chapter 5 useful retail datum analysis facilitate analysis multidimensional aggregate complex condition analysis effectiveness sale campaign retail industry conduct sale campaign used advertisement coupon various kind discount bonuse promote product attract customer careful analysis effectiveness sale campaign help improve company profit multidimensional analysis used purpose compare amount sale number transaction contain sale item sale period versus contain item sale campaign moreover association analysis may disclose item likely purchase together item sale especially comparison sale campaign customer retention—analysis customer loyalty use customer loyalty card information register sequence purchase particular customer customer loyalty purchase trend analyze systematically good purchase different period customer group sequence sequential pattern mining used investigate change customer consumption loyalty suggest adjustment pricing variety good help retain customer attract new one product recommendation cross-referencing item mining association sale record may discover customer buy digital camera likely buy another set item information used form product recommendation collaborative recommender system ( section 1335 ) use datum mining technique make personalize product recommendation live customer transaction base opinion customer product recommendation also advertised sale receipt weekly flyer web help improve customer service aid customer select item increase sale similarly information “ hot item week ” attractive deal display together associative information promote sale fraudulent analysis identification unusual pattern fraudulent activity cost retail industry million dollar per year important ( 1 ) identify potentially fraudulent user atypical usage pattern ( 2 ) detect attempt gain fraudulent entry unauthorized access individual organizational 
133 datum mining application 611 account ( 3 ) discover unusual pattern may need special attention many pattern discover multidimensional analysis cluster analysis outlier analysis another industry handle huge amount datum telecommunication industry quickly evolved offer local long-distance telephone service provide many comprehensive communication service include cellular phone smart phone internet access email text message image computer web datum transmission datum traffic integration telecommunication computer network internet numerous mean communication compute way change face telecommunication compute create great demand datum mining help understand business dynamic identify telecommunication pattern catch fraudulent activity make better use resource improve service quality datum mining task telecommunication share many similarity retail industry common task include construct large-scale datum warehouse perform multidimensional visualization olap in-depth analysis trend customer pattern sequential pattern task contribute business improvement cost reduction customer retention fraud analysis sharpen edge competition many datum mining task customize datum mining tool telecommunication flourishing expect play increasingly important role business datum mining popularly used many industry insurance manufacturing health care well analysis governmental institutional administration datum although industry characteristic datum set application demand share many common principle methodology therefore effective mining one industry may gain experience methodology transfer industrial application 1333 datum mining science engineering past many scientific datum analysis task tend handle relatively small homogeneous datum set datum typically analyze used “ formulate hypothesis build model evaluate result ” paradigm case statistical technique typically employ analysis ( see section 1321 ) massive datum collection storage technology recently change landscape scientific datum analysis today scientific datum amassed much higher speed lower cost result accumulation huge volume high-dimensional datum stream datum heterogenous datum contain rich spatial temporal information consequently scientific application shift “ hypothesize-and-test ” paradigm toward “ collect store datum mine new hypothesis confirm datum experimentation ” process shift bring new challenge datum mining vast amount datum collect scientific domain ( include geoscience astronomy meteorology geology biological science ) used sophisticated 
612 chapter 13 datum mining trend research frontier telescope multispectral high-resolution remote satellite sensor global position system new generation biological datum collection analysis technology large datum set also generate due fast numeric simulation various field climate ecosystem modele chemical engineering fluid dynamic structural mechanic look challenge bring emerge scientific application datum mining datum warehouse datum preprocess datum preprocess datum warehouse critical information exchange datum mining create warehouse often require find mean resolve inconsistent incompatible datum collect multiple environment different time period require reconcile semantic reference system geometry measurement accuracy precision method need integrate datum heterogeneous source identify event instance consider climate ecosystem datum spatial temporal require cross-referencing geospatial datum major problem analyze datum many event spatial domain temporal domain example el nino event occur every four seven year previous datum might collect systematically today method also need efficient computation sophisticated spatial aggregate handle spatial-related datum stream mining complex datum type scientific datum set heterogeneous nature typically involve semi-structure unstructured datum multimedium datum georeference stream datum well datum sophisticated deeply hide semantic ( eg genomic proteomic datum ) robust dedicate analysis method need handle spatiotemporal datum biological datum related concept hierarchy complex semantic relationship example bioinformatic research problem identify regulatory influence gene gene regulation refer gene cell switch ( ) determine cell ’ function different biological process involve different set gene act together precisely regulate pattern thus understand biological process need identify participate gene regulator require development sophisticated datum mining method analyze large biological datum set clue regulatory influence specific gene find dna segment ( “ regulatory sequence ” ) mediate influence graph-based network-based mining often difficult impossible model several physical phenomena process due limitation exist modele approach alternatively labele graph network may used capture many spatial topological geometric biological relational characteristic present scientific datum set graph network modele object mine represent vertex graph edge vertex represent relationship object example graph used model chemical structure biological pathway datum generate numeric 
133 datum mining application 613 simulation fluid-flow simulation success graph network modele however depend improvement scalability efficiency many graph-based datum mining task classification frequent pattern mining cluster visualization tool domain-specific knowledge high-level graphical user interface visualization tool require scientific datum mining system integrate exist domain-specific datum information system guide researcher general user search pattern interpret visualize discover pattern used discover knowledge decision make datum mining engineering share many similarity datum mining science practice often collect massive amount datum require datum preprocess datum warehousing scalable mining complex type datum typically use visualization make good use graph network moreover many engineering process need real-time response mining datum stream real time often become critical component massive amount human communication datum pour daily life communication exist many form include news blog article web page online discussion product reviews twitter message advertisement communication web various kind social network hence datum mining social science social study become increasingly popular moreover user reader feedback regard product speech article analyze deduce general opinion sentiment view society analysis result used predict trend improve work help decision make computer science generate unique kind datum example computer program long execution often generate huge-size trace computer network complex structure network flow dynamic massive sensor network may generate large amount datum varied reliability computer system databasis suffer various kind attack data access may raise security privacy concern unique kind datum provide fertile land datum mining datum mining computer science used help monitor system status improve system performance isolate software bug detect software plagiarism analyze computer system fault uncover network intrusion recognize system malfunction datum mining software system engineering operate static dynamic ( ie stream-based ) datum depend whether system dump trace beforehand postanalysis must react real time handle online datum various method develop domain integrate extend method machine learn datum mining system engineering pattern recognition statistic datum mining computer science active rich domain datum miner unique challenge require development sophisticated scalable real-time datum mining system engineering method 
614 chapter 13 datum mining trend research frontier 1334 datum mining intrusion detection prevention security computer system datum continual risk extensive growth internet increase availability tool trick intrude attack network prompt intrusion detection prevention become critical component networked system intrusion defined set action threaten integrity confidentiality availability network resource ( eg user account file system system kernel ) intrusion detection system intrusion prevention system monitor network traffic or system execution malicious activity however former produce report whereas latter place in-line able actively block intrusion detected main function intrusion prevention system identify malicious activity log information say activity attempt stop activity report activity majority intrusion detection prevention system use either signaturebased detection anomaly-based detection signature-based detection method detection utilize signature attack pattern preconfigured predetermine domain expert signature-based intrusion prevention system monitor network traffic match signature match find intrusion detection system report anomaly intrusion prevention system take additional appropriate action note since system usually quite dynamic signature need update laboriously whenever new software version arrive change network configuration situation occur another drawback detection mechanism identify case match signature unable detect new previously unknown intrusion trick anomaly-based detection method build model normal network behavior ( call profile ) used detect new pattern significantly deviate profile deviation may represent actual intrusion simply new behavior need add profile main advantage anomaly detection may detect novel intrusion yet observed typically human analyst must sort deviation ascertain represent real intrusion limit factor anomaly detection high percentage false positive new pattern intrusion add set signature enhance signature-based detection datum mining method help intrusion detection prevention system enhance performance various way follow new datum mining algorithms intrusion detection datum mining algorithms used signature-based anomaly-based detection signature-based detection training datum labele either “ normal ” “ ” classifier derive detect know intrusion research area 
133 datum mining application 615 include application classification algorithms association rule mining cost-sensitive modele anomaly-based detection build model normal behavior automatically detect significant deviation method include application cluster outlier analysis classification algorithms statistical approach technique used must efficient scalable capable handle network datum high volume dimensionality heterogeneity association correlation discriminative pattern analysis help select build discriminative classifier association correlation discriminative pattern mining apply find relationship system attribute describe network datum information provide insight regard selection useful attribute intrusion detection new attribute derive aggregate datum may also helpful summary count traffic match particular pattern analysis stream datum due transient dynamic nature intrusion malicious attack crucial perform intrusion detection datum stream environment moreover event may normal consider malicious view part sequence event thus necessary study sequence event frequently encounter together find sequential pattern identify outlier datum mining method find evolve cluster build dynamic classification model datum stream also necessary real-time intrusion detection distribute datum mining intrusion launch several different location target many different destination distribute datum mining method may used analyze network datum several network location detect distribute attack visualization query tool visualization tool available view anomalous pattern detected tool may include feature view association discriminative pattern cluster outlier intrusion detection system also graphical user interface allow security analyst pose query regard network datum intrusion detection result summary computer system continual risk break security datum mining technology used develop strong intrusion detection prevention system may employ signature-based anomaly-based detection 1335 datum mining recommender system today ’ consumer face million good service shopping online recommender system help consumer make product recommendation likely interest user book cds movie restaurant online news article service recommender system may use either contentbased approach collaborative approach hybrid approach combine content-based collaborative method 
616 chapter 13 datum mining trend research frontier content-based approach recommend item similar item user prefer query past rely product feature textual item description collaborative approach ( collaborative filter approach ) may consider user ’ social environment recommend item base opinion customer similar taste preference user recommender system use broad range technique information retrieval statistic machine learn datum mining search similarity among item customer preference consider example 131 example 131 scenario used recommender system suppose visit web site online bookstore ( eg amazon ) intention purchasing book want read type name book first time visit web site browse even make purchase last christmas web store remember previous visit store click stream information information regard past purchase system display description price book specify compare interest customer similar interest recommend additional book title say “ customer buy book specify also buy title ” survey list see another title spark interest decide purchase one well suppose go another online store intention purchasing digital camera system suggest additional item consider base previously mine sequential pattern “ customer buy kind digital camera likely buy particular brand printer memory card photo editing software within three ” decide buy camera without additional item week later receive coupon store regard additional item advantage recommender system provide personalization customer e-commerce promote one-to-one marketing amazon pioneer use collaborative recommender system offer “ personalize store every customer ” part marketing strategy personalization benefit consumer company involved accurate model customer company gain better understand customer need serve need result greater success regard cross-selling related product upsel product affinity one-to-one promotion larger basket customer retention recommendation problem consider set c user set item let u utility function measure usefulness item user c utility commonly represent rating initially defined item previously rate user example join movie recommendation system user typically ask rate several movie space c × possible user item huge recommendation system able extrapolate know unknown rating predict item–user combination item highest predict utility user recommend user 
133 datum mining application 617 “ utility item estimate user ” content-based method estimate base utility assign user item similar many system focus recommend item contain textual information web site article news message look commonality among item movie may look similar genre director actor article may look similar term content-based method root information theory make use keyword ( describe item ) user profile contain information user ’ taste need profile may obtain explicitly ( eg questionnaire ) learn user ’ transactional behavior time collaborative recommender system try predict utility item user u base item previously rate user similar u example recommend book collaborative recommender system try find user history agree u ( eg tend buy similar book give similar rating book ) collaborative recommender system either memory ( heuristic ) base model base memory-based method essentially use heuristic make rating prediction base entire collection item previously rate user unknown rating item–user combination estimate aggregate rating similar user item typically k-nearest-neighbor approach used find k user ( neighbor ) similar target user u various approach used compute similarity user popular approach use either pearson ’ correlation coefficient ( section 332 ) cosine similarity ( section 247 ) weight aggregate used adjust fact different user may use rating scale differently model-based collaborative recommender system use collection rating learn model used make rating prediction example probabilistic model cluster ( find cluster like-minded customer ) bayesian network machine learn technique used recommender system face major challenge scalability ensure quality recommendation consumer example regard scalability collaborative recommender system must able search million potential neighbor real time site used browse pattern indication product preference may thousand datum point customer ensure quality recommendation essential gain consumer ’ trust consumer follow system recommendation end liking product less likely use recommender system classification system recommender system make two type error false negative false positive false negative product system fail recommend although consumer would like false positive product recommend consumer like false positive less desirable annoy anger consumer content-based recommender system limit feature used describe item recommend 
618 chapter 13 datum mining trend research frontier another challenge content-based collaborative recommender system deal new user buy history yet available hybrid approach integrate content-based collaborative method achieve improve recommendation netflix prize open competition hold online dvd-rental service payout $ 1000000 best recommender algorithm predict user rating film base previous rating competition study show predictive accuracy recommender system substantially improve blending multiple predictor especially used ensemble many substantially different method rather refine single technique collaborative recommender system form intelligent query answer consist analyze intent query provide generalized neighborhood associate information relevant query example rather simply return book description price response customer ’ query return additional information related query explicitly ask ( eg book evaluation comment recommendation book sale statistic ) provide intelligent answer query 134 datum mining society us datum mining part daily life although may often unaware presence section 1341 look several example “ ubiquitous invisible ” datum mining affect everyday thing product stock local supermarket ad see surfing internet crime prevention datum mining offer individual many benefit improve customer service satisfaction well lifestyle general however also serious implication regard one ’ right privacy datum security issue topic section 1342 1341 ubiquitous invisible datum mining datum mining present many aspect daily life whether realize affect shop work search information even influence leisure time health well-being section look example ubiquitous ( ever-present ) datum mining several example also represent invisible datum mining “ smart ” software search engine customer-adaptive web service ( eg used recommender algorithms ) “ intelligent ” database system email manager ticket master incorporate datum mining functional component often unbeknownst user grocery store print personalize coupon customer receipt online store recommend additional item base customer interest datum mining innovatively influenced buy way shop experience shopping one example wal-mart hundred million customer visit ten thousand store every week wal-mart allow supplier access datum 
134 datum mining society 619 product perform analysis used datum mining software allow supplier identify customer buy pattern different store control inventory product placement identify new merchandize opportunity affect item ( many ) end store ’ shelves—something think next time wander aisle wal-mart datum mining shape online shopping experience many shopper routinely turn online store purchase book music movie toy recommender system discuss section 1335 offer personalize product recommendation base opinion customer amazoncom forefront used personalize datum mining–based approach marketing strategy observed traditional brick-and-mortar store hardest part get customer store customer likely buy something since cost go another store high therefore marketing brick-and-mortar store tend emphasize draw customer rather actual in-store customer experience contrast online store customer “ walk ” enter another online store click mouse amazoncom capitalize difference offer “ personalize store every ” use several datum mining technique identify customer ’ like make reliable recommendation topic shopping suppose lot buy credit card nowadays unusual receive phone call one ’ credit card company regard suspicious unusual pattern spending credit card company use datum mining detect fraudulent usage save billion dollar year many company increasingly use datum mining customer relationship management ( crm ) help provide customize personal service address individual customer ’ need lieu mass marketing study browse purchasing pattern web store company tailor advertisement promotion customer profile customer less likely annoyed unwanted mass mailing junk mail action result substantial cost saving company customer benefit likely notified offer actually interest result less waste personal time greater satisfaction datum mining greatly influenced way person use computer search information work get internet example decide check email unbeknownst several annoying email already delete thank spam filter used classification algorithms recognize spam process email go google ( wwwgooglecom ) provide access information billion web page index server google one popular widely used internet search engine used google search information become way life many person google popular even become new verb english language meaning “ search ( something ) internet used google search engine extension comprehensive search ” 1 decide type keyword 1 http open-dictionarycom 
620 chapter 13 datum mining trend research frontier topic interest google return list web site topic mine index organized set datum mining algorithms include pagerank moreover type “ boston new york ” google show bus train schedule boston new york however minor change “ boston paris ” lead flight schedule boston paris smart offering information service likely base frequent pattern mine click stream many previous query view result google query various ad pop relate query google ’ strategy tailor advertising match user ’ interest one typical service explore every internet search provider also make happier less likely pester irrelevant ad datum mining omnipresent see daily-encounter example can go scenario many case datum mining invisible user may unaware examine result return datum mining click actually fed new datum datum mining function datum mining become improve accept technology continue research development need many area mentioned challenge throughout book include efficiency scalability increase user interaction incorporation background knowledge visualization technique effective method find interesting pattern improve handle complex datum type stream datum realtime datum mining web mining addition integration datum mining exist business scientific technology provide domain-specific datum mining tool contribute advancement technology success datum mining solution tailor e-commerce application opposed generic datum mining system example 1342 privacy security social impact datum mining information accessible electronic form available web increasingly powerful datum mining tool develop put use increase concern datum mining may pose threat privacy datum security however important note many datum mining application even touch personal datum prominent example include application involve natural resource prediction flood drought meteorology astronomy geography geology biology scientific engineering datum furthermore study datum mining research focus development scalable algorithms involve personal datum focus datum mining technology discovery general statistically significant pattern specific information regard individual sense believe real privacy concern unconstrained access individual record especially access privacy-sensitive information credit card transaction record health-care record personal financial record biological trait justice investigation ethnicity datum mining application involve personal datum many case simple method remove sensitive id datum may protect privacy individual nevertheless privacy concern exist wherever 
134 datum mining society 621 personally identifiable information collect store digital form datum mining program able access datum even datum preparation improper nonexistent disclosure control root cause privacy issue handle concern numerous datum security-enhancing technique develop addition great deal recent effort develop privacypreserve datum mining method section look advance protect privacy datum security datum mining “ secure privacy individual collect mining datum ” many datum security–enhancing technique develop help protect datum databasis employ multilevel security model classify restrict datum accord various security level user permit access authorize level show however user execute specific query authorize security level still infer sensitive information similar possibility occur datum mining encryption another technique individual datum item may encode may involve blind signature ( build public key encryption ) biometric encryption ( eg image person ’ iris fingerprint used encode personal information ) anonymous databasis ( permit consolidation various databasis limit access personal information need know personal information encrypt store different location ) intrusion detection another active area research help protect privacy personal datum privacy-preserve datum mining area datum mining research response privacy protection datum mining also know privacy-enhance privacysensitive datum mining deal obtain valid datum mining result without disclose underlie sensitive datum value privacy-preserve datum mining method use form transformation datum perform privacy preservation typically method reduce granularity representation preserve privacy example may generalize datum individual customer customer group reduction granularity cause loss information possibly usefulness datum mining result natural trade-off information loss privacy privacy-preserve datum mining method classify follow category randomization method method add noise datum mask attribute value record noise add sufficiently large individual record value especially sensitive one re-cover however add skillfully final result datum mining basically preserve technique design derive aggregate distribution perturbed datum subsequently datum mining technique develop work aggregate distribution k-anonymity l-diversity method method alter individual record uniquely identify k-anonymity method granularity datum representation reduce sufficiently give record map onto least k record datum used technique like generalization suppression k-anonymity method weak homogeneity 
622 chapter 13 datum mining trend research frontier sensitive value within group value may infer alter record l-diversity model design handle weakness enforce intragroup diversity sensitive value ensure anonymization goal make sufficiently difficult adversary use combination record attribute exactly identify individual record distribute privacy preservation large datum set can partition distribute either horizontally ( ie datum set partition different subset record distribute across multiple site ) vertically ( ie datum set partition distribute attribute ) even combination individual site may want share entire datum set may consent limit information sharing use variety protocol overall effect method maintain privacy individual object derive aggregate result datum downgrading effectiveness datum mining result many case even though datum may available output datum mining ( eg association rule classification model ) may result violation privacy solution can downgrade effectiveness datum mining either modify datum mining result hiding association rule slightly distort classification model recently researcher propose new idea privacy-preserve datum mining notion differential privacy general idea two datum set close one another ( ie differ tiny datum set single element ) give differentially private algorithm behave approximately datum set definition give strong guarantee presence absence tiny datum set ( eg represent individual ) affect final output query significantly base notion set differential privacy-preserve datum mining algorithms develop research direction ongoing expect powerful privacy-preserve datum publish datum mining algorithms near future like technology datum mining misuse however must lose sight benefit datum mining research bring range insight gain medical scientific application increase customer satisfaction help company better suit client ’ need expect computer scientist policy expert counterterrorism expert continue work social scientist lawyer company consumer take responsibility build solution ensure datum privacy protection security way may continue reap benefit datum mining term time money saving discovery new knowledge 135 datum mining trend diversity datum datum mining task datum mining approach pose many challenge research issue datum mining development efficient effective datum 
135 datum mining trend 623 mining method system service interactive integrate datum mining environment key area study use datum mining technique solve large sophisticated application problem important task datum mining researcher datum mining system application developer section describe trend datum mining reflect pursuit challenge application exploration early datum mining application put lot effort help business gain competitive edge exploration datum mining business continue expand e-commerce e-marketing become mainstream retail industry datum mining increasingly used exploration application area web text analysis financial analysis industry government biomedicine science emerge application area include datum mining counterterrorism mobile ( wireless ) datum mining generic datum mining system may limitation deal application-specific problem may see trend toward development application-specific datum mining system tool well invisible datum mining function embed various kind service scalable interactive datum mining method contrast traditional datum analysis method datum mining must able handle huge amount datum efficiently possible interactively amount datum collect continue increase rapidly scalable algorithms individual integrate datum mining function become essential one important direction toward improve overall efficiency mining process increase user interaction constraint-based mining provide user add control allow specification use constraint guide datum mining system search interesting pattern knowledge integration datum mining search engine database system datum warehouse system cloud compute system search engine database system datum warehouse system cloud compute system mainstream information process compute system important ensure datum mining serve essential datum analysis component smoothly integrate information process environment datum mining service tightly couple system seamless unify framework invisible function ensure datum availability datum mining portability scalability high performance integrate information process environment multidimensional datum analysis exploration mining social information network mining social information network link analysis critical task network ubiquitous complex development scalable effective knowledge discovery method application large number network datum essential outlined section 1312 mining spatiotemporal moving-object cyber-physical system cyberphysical system well spatiotemporal datum mount rapidly due 
624 chapter 13 datum mining trend research frontier popular use cellular phone gps sensor wireless equipment outlined section 1313 many challenge research issue realize real-time effective knowledge discovery datum mining multimedium text web datum outlined section 1313 mining kind datum recent focus datum mining research great progress make yet still many open issue solve mining biological biomedical datum unique combination complexity richness size importance biological biomedical datum warrant special attention datum mining mining dna protein sequence mining highdimensional microarray datum biological pathway network analysis topic field area biological datum mining research include mining biomedical literature link analysis across heterogeneous biological datum information integration biological datum datum mining datum mining software engineering system engineering software program large computer system become increasingly bulky size sophisticated complexity tend originate integration multiple component develop different implementation team trend make increasingly challenge task ensure software robustness reliability analysis execution buggy software program essentially datum mining process—trace datum generate program execution may disclose important pattern outlier can lead eventual automate discovery software bug expect development datum mining methodology system debug enhance software robustness bring new vigor system engineering visual audio datum mining visual audio datum mining effective way integrate human ’ visual audio system discover knowledge huge amount datum systematic development technique facilitate promotion human participation effective efficient datum analysis distribute datum mining real-time datum stream mining traditional datum mining method design work centralize location work well many distribute compute environment present today ( eg internet intranet local area network high-speed wireless network sensor network cloud compute ) advance distribute datum mining method expect moreover many application involve stream datum ( eg e-commerce web mining stock analysis intrusion detection mobile datum mining datum mining counterterrorism ) require dynamic datum mining model build real time additional research need direction privacy protection information security datum mining abundance personal confidential information available electronic form couple increasingly powerful datum mining tool pose threat datum privacy security grow interest datum mining counterterrorism also add concern 
136 summary 625 development privacy-preserve datum mining method foresee collaboration technologist social scientist law expert government company need produce rigorous privacy security protection mechanism datum publish datum mining confidence look forward next generation datum mining technology benefit bring 136 summary mining complex datum type pose challenge issue many dedicate line research development chapter present high-level overview mining complex datum type include mining sequence datum time series symbolic sequence biological sequence mining graph network mining kind datum include spatiotemporal cyber-physical system datum multimedium text web datum datum stream several well-established statistical method propose datum analysis regression generalized linear model analysis variance mixed-effect model factor analysis discriminant analysis survival analysis quality control full coverage statistical datum analysis method beyond scope book interested reader refer statistical literature cite bibliographic note ( section 138 ) researcher strive build theoretical foundation datum mining several interesting proposal appear base datum reduction datum compression probability statistic theory microeconomic theory pattern discovery–based inductive databasis visual datum mining integrate datum mining datum visualization discover implicit useful knowledge large datum set visual datum mining include datum visualization datum mining result visualization datum mining process visualization interactive visual datum mining audio datum mining used audio signal indicate datum pattern feature datum mining result many customize datum mining tool develop domain-specific application include finance retail telecommunication industry science engineering intrusion detection prevention recommender system application domain-based study integrate domain-specific knowledge datum analysis technique provide mission-specific datum mining solution ubiquitous datum mining constant presence datum mining many aspect daily life influence shop work search information use computer well leisure time health well-being invisible datum mining “ smart ” software search engine customer-adaptive web service 
626 chapter 13 datum mining trend research frontier ( eg used recommender algorithms ) email manager incorporate datum mining functional component often unbeknownst user major social concern datum mining issue privacy datum security privacy-preserve datum mining deal obtain valid datum mining result without disclose underlie sensitive value goal ensure privacy protection security preserve overall quality datum mining result datum mining trend include effort toward exploration new application area improve scalable interactive constraint-based mining method integration datum mining web service database warehousing cloud compute system mining social information network trend include mining spatiotemporal cyber-physical system datum biological datum system engineering datum multimedium text datum addition web mining distribute real-time datum stream mining visual audio mining privacy security datum mining 137 exercise 131 sequence datum ubiquitous diverse application chapter present general overview sequential pattern mining sequence classification sequence similarity search trend analysis biological sequence alignment modele however cover sequence cluster present overview method sequence cluster 132 chapter present overview sequence pattern mining graph pattern mining method mining tree pattern partial order pattern also study research summarize method mining structure pattern include sequence tree graph partial order relationship examine kind structural pattern mining cover research propose application create new mining problem 133 many study analyze homogeneous information network ( eg social network consist friend link friend ) however many application involve heterogeneous information network ( ie network link multiple type object research paper conference author topic ) major difference methodology mining heterogeneous information network method homogeneous counterpart 134 research describe datum mining application present chapter discuss different form datum mining used application 135 establishment theoretical foundation important datum mining name describe main theoretical foundation propose datum mining comment satisfy ( fail satisfy ) requirement ideal theoretical framework datum mining 
137 exercise 627 136 ( research project ) build theory datum mining require set theoretical framework major datum mining function explain framework take one theory example ( eg datum compression theory ) examine major datum mining function fit framework function fit well current theoretical framework propose way extend framework explain function 137 strong linkage statistical datum analysis datum mining person think datum mining automate scalable method statistical datum analysis agree disagree perception present one statistical analysis method automate or scale nicely integration current datum mining methodology 138 difference visual datum mining datum visualization datum visualization may suffer datum abundance problem example easy visually discover interesting property network connection social network huge complex dense connection propose visualization method may help person see network topology interesting feature social network 139 propose implementation method audio datum mining integrate audio visual datum mining bring fun power datum mining possible develop video datum mining method state scenario solution make integrate audiovisual mining effective 1310 general-purpose computer domain-independent relational database system become large market last several decade however many person feel generic datum mining system prevail datum mining market think datum mining focus effort develop domain-independent datum mining tool develop domain-specific datum mining solution present reasoning 1311 recommender system way differ customer productbased cluster system differ typical classification predictive modele system outline one method collaborative filter discuss work limitation practice 1312 suppose local bank datum mining system bank study debit card usage pattern notice make many transaction home renovation store bank decide contact offer information regard special loan home improvement ( ) discuss may conflict right privacy ( b ) describe another situation feel datum mining infringe privacy ( c ) describe privacy-preserve datum mining method may allow bank perform customer pattern analysis without infringe customer ’ right privacy ( ) example datum mining can used help society think way can used may detrimental society 
628 chapter 13 datum mining trend research frontier 1313 major challenge face bring datum mining research market illustrate one datum mining research issue view may strong impact market society discuss approach research issue 1314 base view challenge research problem datum mining give number year good number researcher implementor would plan make good progress toward effective solution problem 1315 base experience knowledge suggest new frontier datum mining mentioned chapter 138 bibliographic note mining complex datum type many research paper book cover various theme list recent book well-cite survey research article reference time-series analysis study statistic computer science community decade many textbook box jenkin reinsel [ bjr08 ] brockwell davis [ bd02 ] chatfield [ cha03b ] hamilton [ ham94 ] shumway stoffer [ ss05 ] fast subsequence match method time-series databasis present faloutsos ranganathan manolopoulos [ frm94 ] agrawal lin sawhney shim [ alss95 ] develop method fast similarity search presence noise scaling translation time-series databasis shasha zhu present overview method high-performance discovery time series [ sz04 ] sequential pattern mining method study many researcher include agrawal srikant [ as95 ] zaki [ zak01 ] pei han mortazavi-asl et al [ + 04 ] yan han afshar [ yha03 ] study sequence classification include ji bailey dong [ jbd05 ] ye keogh [ yk09 ] survey xing pei keogh [ xpk10 ] dong pei [ dp07 ] provide overview sequence datum mining method method analysis biological sequence include markov chain hide markov model introduce many book tutorial waterman [ wat95 ] setubal meidanis [ sm97 ] durbin eddy krogh mitchison [ dekm98 ] baldi brunak [ bb01 ] krane raymer [ kr03 ] rabiner [ rab89 ] jone pevzner [ jp04 ] baxevanis ouellette [ bo04 ] information blast ( see also korf yandell bedell [ kyb03 ] ) find ncbi web site graph pattern mining study extensively include holder cook djoko [ hcd94 ] inokuchi washio motoda [ iwm98 ] kuramochi karypis [ kk01 ] yan han [ yh02 yh03a ] borgelt berthold [ bb02 ] huan wang bandyopadhyay et al [ + 04 ] gaston tool nijssen kok [ nk04 ] 
138 bibliographic note 629 great deal research social information network analysis include newman [ new10 ] easley kleinberg [ ek10 ] yu han faloutsos [ yhf10 ] wasserman faust [ wf94 ] watt [ wat03 ] newman barabasi watt [ nbw06 ] statistical modele network study popularly albert barbasi [ ab99 ] watt [ wat03 ] faloutsos faloutsos faloutsos [ fff99 ] kumar raghavan rajagopalan et al [ + 00 ] leskovec kleinberg faloutsos [ lkf05 ] datum clean integration validation information network analysis study many include bhattacharya getoor [ bg04 ] yin han yu [ yhy07 yhy08 ] cluster ranking classification network study extensively include brin page [ bp98 ] chakrabarti dom indyk [ cdi98 ] kleinberg [ kle99 ] getoor friedman koller taskar [ gfkt01 ] newman m girvan [ ng04 ] yin han yang yu [ yhyy04 ] yin han yu [ yhy05 ] xu yuruk feng schweiger [ xyfs07 ] kuli basu dhillon mooney [ kbdm09 ] sun han zhao et al [ + 09 ] neville gallaher eliassi-rad [ nge-r09 ] ji sun danilevsky et al [ + 10 ] role discovery link prediction information network study extensively well krebs [ kre02 ] kubica moore schneider [ kms03 ] liben-nowell kleinberg [ l-nk03 ] wang han jia et al [ + 10 ] similarity search olap information network study many include tian hankin patel [ thp08 ] chen yan zhu et al [ + 08 ] evolution social information network study many researcher chakrabarti kumar tomkin [ ckt06 ] chi song zhou et al [ + 07 ] tang liu zhang nazeri [ tlzn08 ] xu zhang yu long [ xzyl08 ] kim han [ kh09 ] sun tang han [ + 10 ] spatial spatiotemporal datum mining study extensively collection paper miller han [ mh09 ] introduce textbook shekhar chawla [ sc03 ] hsu lee wang [ hlw07 ] spatial cluster algorithms study extensively chapter 10 11 book research conduct spatial warehouse olap stefanovic han koperski [ shk00 ] spatial spatiotemporal datum mining koperski han [ kh95 ] mamouli cao kollio hadjieleftheriou et al [ + 04 ] tsoukatos gunopulos [ tg01 ] hadjieleftheriou kollio gunopulos tsotra [ hkgt03 ] mining moving-object datum study many vlachos gunopulos kollio [ vgk02 ] tao faloutsos papadia liu [ tfpl04 ] li han kim gonzalez [ lhkg07 ] lee han whang [ lhw07 ] li ding han et al [ + 10 ] bibliography temporal spatial spatiotemporal datum mining research see collection roddick hornsby spiliopoulou [ rhs01 ] multimedium datum mining deep root image process pattern recognition study extensively many textbook include gonzalez wood [ gw07 ] russ [ rus06 ] duda hart stork [ dhs01 ] z zhang r zhang [ zz09 ] search mining multimedium datum study many ( see eg fayyad smyth [ fs93 ] faloutsos lin [ fl95 ] natsev rastogi 
630 chapter 13 datum mining trend research frontier shim [ nrs99 ] zaı̈ane han zhu [ zhz00 ] ) overview image mining method give hsu lee zhang [ hlz02 ] text datum analysis study extensively information retrieval many textbook survey article croft metzler strohman [ cms09 ] s buttcher c clarke g cormack [ bcc10 ] man raghavan schutze [ mrs08 ] grossman frieder [ gr04 ] baeza-yate riberio-neto [ byrn11 ] zhai [ zha08 ] feldman sanger [ fs06 ] berry [ ber03 ] weis indurkhya zhang damerau [ wizd04 ] text mining fast-developing field numerous paper publish recent year cover many topic topic model ( eg blei lafferty [ bl09 ] ) sentiment analysis ( eg pang lee [ pl07 ] ) contextual text mining ( eg mei zhai [ mz06 ] ) web mining another focuse theme book like chakrabarti [ cha03a ] liu [ liu06 ] berry [ ber03 ] web mining substantially improve search engine influential milestone work brin page [ bp98 ] kleinberg [ kle99 ] chakrabarti dom kumar et al [ + 99 ] kleinberg tomkin [ kt99 ] numerous result generate since search log mining ( eg silvestri [ sil10 ] ) blog mining ( eg mei liu su zhai [ mlsz06 ] ) mining online forum ( eg cong wang lin et al [ + 08 ] ) book survey stream datum system stream datum process include babu widom [ bw01 ] babcock babu datar et al [ + 02 ] muthukrishnan [ mut05 ] aggarwal [ agg06 ] stream datum mining research cover stream cube model ( eg chen dong han et al [ + 02 ] ) stream frequent pattern mining ( eg manku motwani [ mm02 ] karp papadimitriou shenker [ kps03 ] ) stream classification ( eg domingo hulten [ dh00 ] wang fan yu han [ wfyh03 ] aggarwal han wang yu [ ahwy04b ] ) stream cluster ( eg guha mishra motwani ’ callaghan [ gmmo00 ] aggarwal han wang yu [ ahwy03 ] ) many book discuss datum mining application financial datum analysis financial modele see example benninga [ ben08 ] higgin [ hig08 ] retail datum mining customer relationship management see example book berry linoff [ bl04 ] berson smith thearle [ bst99 ] telecommunication-related datum mining see example horak [ hor08 ] also book scientific datum analysis grossman kamath kegelmeyer et al [ + 01 ] kamath [ kam09 ] issue theoretical foundation datum mining address many researcher example mannila present summary study foundation datum mining [ man00 ] datum reduction view datum mining summarize new jersey datum reduction report barbará dumouchel faloutos et al [ + 97 ] datum compression view find study minimum description length principle grunwald rissanen [ gr07 ] pattern discovery point view datum mining address numerous machine learn datum mining study range association mining decision tree induction sequential pattern mining cluster probability theory point view popular statistic machine learn literature 
138 bibliographic note 631 bayesian network hierarchical bayesian model chapter 9 probabilistic graph model ( eg koller friedman [ kf09 ] ) kleinberg papadimitriou raghavan [ kpr98 ] present microeconomic view treat datum mining optimization problem study inductive database view include imielinski mannila [ im96 ] de raedt gun nijssen [ rgn10 ] statistical method datum analysis describe many book hastie tibshirani friedman [ htf09 ] freedman pisani purf [ fpp07 ] devore [ dev03 ] kutner nachtsheim neter li [ knnl04 ] dobson [ dob01 ] breiman friedman olshen stone [ bfos84 ] pinheiro bate [ pb00 ] johnson wichern [ jw02b ] huberty [ hub94 ] shumway stoffer [ ss05 ] miller [ mil98 ] visual datum mining popular book visual display datum information include tufte [ tuf90 tuf97 tuf01 ] summary technique visualize datum present cleveland [ cle93 ] dedicate visual datum mining book visual datum mining technique tool datum visualization mining soukup davidson [ sd02 ] book information visualization datum mining knowledge discovery edit fayyad grinstein wierse [ fgw01 ] contain collection article visual datum mining method ubiquitous invisible datum mining discuss many text include john [ joh99 ] article book edit kargupta joshi sivakumar yesha [ kjsy04 ] book business @ speed thought succeed digital economy gate [ gat00 ] discuss e-commerce customer relationship management provide interesting perspective datum mining future mena [ men03 ] informative book use datum mining detect prevent crime cover many form criminal activity range fraud detection money launder insurance crime identity crime intrusion detection datum mining issue regard privacy datum security address popularly literature book privacy security datum mining include thuraisingham [ thu04 ] aggarwal yu [ ay08 ] vaidya clifton zhu [ vcz10 ] fung wang fu yu [ fwfy10 ] research article include agrawal srikant [ as00 ] evfimievski srikant agrawal gehrke [ esag02 ] vaidya clifton [ vc03 ] differential privacy introduce dwork [ dwo06 ] study many hay rastogi miklau suciu [ hrms10 ] many discussion trend research direction datum mining various forum several book collection article issue kargupta han yu et al [ + 08 ] 

cure cluster used wellscatter representative 
cure cluster used representative  cure ( cluster used representative ) ( s guha r rastogi k shim 1998 ) represent cluster used set well-scatter representative point  cluster distance minimum distance representative point choose  incorporate feature single link average link  shrink factor α point shrink towards centroid factor α  far away point shrink towards center robust outlier  choose scatter point help cure capture cluster arbitrary shape  courtesy kyuseok shim @ snukr 2 

graph datum 
chameleon hierarchical cluster used dynamic modele  chameleon graph partition approach ( g karypis e h han v kumar 1999 )  measure similarity base dynamic model   two cluster merged interconnectivity closeness ( proximity ) two cluster high relative internal interconnectivity cluster closeness item within cluster graph-based two-phase algorithm use graph-partition algorithm cluster object large number relatively small sub-cluster use agglomerative hierarchical cluster algorithm find genuine cluster repeatedly combine sub-cluster 2 
overall framework chameleon construct ( k-nn ) partition graph sparse graph datum set k-nn graph point p q connect q among top-k closest neighbor p merge partition final cluster 3 relative interconnectivity connectivity c1 c2 internal connectivity relative closeness closeness c1 c2 internal closeness 
knn graph interconnectivity  k-nearest neighbor ( knn ) graph original datum 2d  ec { ci cj } absolute interconnectivity ci cj  sum weight edge connect vertex ci vertex cj  internal interconnectivity cluster ci size min-cut bisector ecci ( ie weight sum edge partition graph two roughly equal part )  relative interconnectivity ( ri ) 4 
relative closeness & merge sub-cluster  relative closeness pair cluster ci cj absolute closeness ci cj normalize wrt internal closeness two cluster ci cj  average weight edge belong min-cut bisector cluster ci cj respectively average weight edge connect vertex ci vertex cj  merge sub-cluster 5  merge pair cluster whose ri rc userspecified threshold  merge maximize function combine ri rc 
chameleon cluster complex object chameleon capable generate quality cluster cluster complex object 6 

cluster 
probabilistic hierarchical cluster  algorithmic hierarchical cluster  nontrivial choose good distance measure  hard handle miss attribute value  optimization goal clear heuristic local search  probabilistic hierarchical cluster  use probabilistic model measure distance cluster  generative model regard set datum object cluster sample underlie datum generation mechanism analyze  easy understand efficiency algorithmic agglomerative cluster method handle partially observed datum  practice assume generative model adopt common distribution function 2 eg gaussian distribution bernoulli distribution govern parameter 
generative model  give set 1-d point x = { x1 … xn } cluster analysis & assume generate gaussian distribution  probability point xi ∈ x generate model  likelihood x generate model  task learn generative model find parameter μ σ2 3 maximum likelihood 
gaussian distribution bean machine drop ball pin 1-d gaussian 4 2-d gaussian wikipedia http homedeipolimiit 
probabilistic hierarchical cluster algorithm  set object partition cluster c1 cm quality measure p ( ) maximum likelihood  merge two cluster cj1 cj2 cluster cj1∪cj2 change quality overall cluster  distance cluster c1 c2  dist ( ci cj ) < 0 merge ci cj 5 
recommend reading 6  a k jain r c dube algorithms cluster datum prentice hall 1988  l kaufman p j rousseeuw find group datum introduction cluster analysis john wiley & son 1990  t zhang r ramakrishnan m livny birch efficient datum cluster method large databasis sigmod96  s guha r rastogi k shim cure efficient cluster algorithm large databasis sigmod ’ 98  g karypis han v kumar chameleon hierarchical cluster algorithm used dynamic modele computer 32 ( 8 ) 68-75 1999  jiawei han micheline kamber jian pei datum mining concept technique morgan kaufmann 3rd ed 2011 ( chap 10 )  c k reddy b vinzamuri survey partitional hierarchical cluster algorithms ( chap 4 ) aggarwal reddy ( ed ) datum cluster algorithms application crc press 2014  m j zaki w meira datum mining analysis fundamental concept algorithms cambridge univ press 2014 

10 9 8 7 6 5 4 3 2 1 
author jiawei han bliss professor engineering department computer science university illinois urbana-champaign receive numerous award contribution research knowledge discovery datum mining include acm sigkdd innovation award ( 2004 ) ieee computer society technical achievement award ( 2005 ) ieee w wallace mcdowell award ( 2009 ) fellow acm ieee serve founding editor-in-chief acm transaction knowledge discovery datum ( 2006–2011 ) editorial board member several journal include ieee transaction knowledge datum engineering datum mining knowledge discovery micheline kamber master ’ degree computer science ( specialize artificial intelligence ) concordium university montreal quebec nserc scholar work researcher mcgill university simon fraser university switzerland background datum mining passion writing easyto-understand term help make text favorite professional instructor student jian pei currently associate professor school compute science simon fraser university british columbia receive degree compute science simon fraser university 2002 dr jiawei han ’ supervision publish prolifically premier academic forum datum mining databasis web search information retrieval actively serve academic community publication receive thousand citation several prestigious award associate editor several datum mining datum analytic journal xxxv 
2 get know datum ’ tempting jump straight mining first need get datum ready involve closer look attribute datum value real-world datum typically noisy enormous volume ( often several gigabyte ) may originate hodgepodge heterogenous source chapter get familiar datum knowledge datum useful datum preprocess ( see chapter 3 ) first major task datum mining process want know follow type attribute field make datum kind value attribute attribute discrete continuous-valu datum look like value distribute way visualize datum get better sense spot outlier measure similarity datum object respect other gain insight datum help subsequent analysis “ learn datum ’ helpful datum preprocess ” begin section 21 study various attribute type include nominal attribute binary attribute ordinal attribute numeric attribute basic statistical description used learn attribute ’ value describe section 22 give temperature attribute example determine mean ( average value ) median ( middle value ) mode ( common value ) measure central tendency give us idea “ middle ” center distribution know basic statistic regard attribute make easier fill miss value smooth noisy value spot outlier datum preprocess knowledge attribute attribute value also help fix inconsistency incur datum integration plot measure central tendency show us datum symmetric skewer quantile plot histogram scatter plot graphic display basic statistical description useful datum preprocess provide insight area mining field datum visualization provide many additional technique view datum graphical mean help identify relation trend biase “ hide ” unstructured datum set technique may simple scatter-plot matrix ( datum mining concept technique doi b978-0-12-381479-100002-2 c 2012 elsevier right re-serve 39 
40 chapter 2 get know datum two attribute map onto 2-d grid ) sophisticated method treemaps ( hierarchical partition screen display base attribute value ) datum visualization technique describe section 23 finally may want examine similar ( dissimilar ) datum object example suppose database datum object patient describe symptom may want find similarity dissimilarity individual patient information allow us find cluster like patient within datum set dissimilarity object may also used detect outlier datum perform nearest-neighbor classification ( cluster topic chapter 10 11 nearest-neighbor classification discuss chapter 9 ) many measure assess similarity dissimilarity general measure refer proximity measure think proximity two object function distance attribute value although proximity also calculate base probability rather actual distance measure datum proximity describe section 24 summary end chapter know different attribute type basic statistical measure describe central tendency dispersion ( spread ) attribute datum also know technique visualize attribute distribution compute similarity dissimilarity object 21 datum object attribute type datum set make datum object datum object represent entity—in sale database object may customer store item sale medical database object may patient university database object may student professor course datum object typically describe attribute datum object also refer sample example instance datum point object datum object store database datum tuple row database correspond datum object column correspond attribute section define attribute look various attribute type 211 attribute attribute datum field represent characteristic feature datum object noun attribute dimension feature variable often used interchangeably literature term dimension commonly used datum warehousing machine learn literature tend use term feature statistician prefer term variable datum mining database professional commonly use term attribute well attribute describe customer object include example customer id name address observed value give attribute know observation set attribute used describe give object call attribute vector ( feature vector ) distribution datum involve one attribute ( variable ) call univariate bivariate distribution involve two attribute 
21 datum object attribute type 41 type attribute determine set possible values—nominal binary ordinal numeric—the attribute follow subsection introduce type 212 nominal attribute nominal mean “ relate ” value nominal attribute symbol name thing value represent kind category code state nominal attribute also refer categorical value meaningful order computer science value also know enumeration example 21 nominal attribute suppose hair color marital status two attribute describe person object application possible value hair color black brown blond red auburn gray white attribute marital status take value single married divorce widow hair color marital status nominal attribute another example nominal attribute occupation value teacher dentist programmer farmer although say value nominal attribute symbol “ name thing ” possible represent symbol “ name ” number hair color instance assign code 0 black 1 brown another example customor id possible value numeric however case number intend used quantitatively mathematical operation value nominal attribute meaningful make sense subtract one customer id number another unlike say subtract age value another ( age numeric attribute ) even though nominal attribute may integer value consider numeric attribute integer meant used quantitatively say numeric attribute section 215 nominal attribute value meaningful order quantitative make sense find mean ( average ) value median ( middle ) value attribute give set object one thing interest however attribute ’ commonly occur value value know mode one measure central tendency learn measure central tendency section 22 213 binary attribute binary attribute nominal attribute two category state 0 1 0 typically mean attribute absent 1 mean present binary attribute refer boolean two state correspond true false example 22 binary attribute give attribute smoker describe patient object 1 indicate patient smoke 0 indicate patient similarly suppose 
42 chapter 2 get know datum patient undergo medical test two possible outcome attribute medical test binary value 1 mean result test patient positive 0 mean result negative binary attribute symmetric state equally valuable carry weight preference outcome code 0 one example can attribute gender state male female binary attribute asymmetric outcome state equally important positive negative outcome medical test hiv convention code important outcome usually rarest one 1 ( eg hiv positive ) 0 ( eg hiv negative ) 214 ordinal attribute ordinal attribute attribute possible value meaningful order ranking among magnitude successive value know example 23 ordinal attribute suppose drink size correspond size drink available fast-food restaurant nominal attribute three possible value small medium large value meaningful sequence ( correspond increase drink size ) however tell value much bigger say medium large example ordinal attribute include grade ( eg + a− + ) professional rank professional rank enumerate sequential order example assistant associate full professor private private first class specialist corporal sergeant army rank ordinal attribute useful register subjective assessment quality measure objectively thus ordinal attribute often used survey rating one survey participant ask rate satisfied customer customer satisfaction follow ordinal category 0 dissatisfied 1 somewhat dissatisfied 2 neutral 3 satisfied 4 satisfied ordinal attribute may also obtain discretization numeric quantity splitting value range finite number order category describe chapter 3 datum reduction central tendency ordinal attribute represent mode median ( middle value order sequence ) mean defined note nominal binary ordinal attribute qualitative describe feature object without give actual size quantity value qualitative attribute typically word represent category integer used represent computer code category opposed measurable quantity ( eg 0 small drink size 1 medium 2 large ) follow subsection look numeric attribute provide quantitative measurement object 
21 datum object attribute type 215 43 numeric attribute numeric attribute quantitative measurable quantity represent integer real value numeric attribute interval-scaled ratio-scale interval-scaled attribute interval-scaled attribute measure scale equal-size unit value interval-scaled attribute order positive 0 negative thus addition provide ranking value attribute allow us compare quantify difference value example 24 interval-scaled attribute temperature attribute interval-scaled suppose outdoor temperature value number different day day object order value obtain ranking object respect temperature addition quantify difference value example temperature 20◦ c five degree higher temperature 15◦ c calendar date another example instance year 2002 2010 eight year apart temperature celsius fahrenheit true zero-point neither 0◦ c 0◦ f indicate “ ” ( celsius scale example unit measurement 100 difference melt temperature boil temperature water atmospheric pressure ) although compute difference temperature value talk one temperature value multiple another without true zero say instance 10◦ c twice warm 5◦ c speak value term ratio similarly true zero-point calendar date ( year 0 correspond begin time ) bring us ratio-scale attribute true zero-point exit interval-scaled attribute numeric compute mean value addition median mode measure central tendency ratio-scale attribute ratio-scale attribute numeric attribute inherent zero-point measurement ratio-scale speak value multiple ( ratio ) another value addition value order also compute difference value well mean median mode example 25 ratio-scale attribute unlike temperature celsius fahrenheit kelvin ( k ) temperature scale consider true zero-point ( 0◦ k = −27315◦ c ) point particle comprise matter zero kinetic energy example ratio-scale attribute include count attribute year experience ( eg object employee ) number word ( eg object document ) additional example include attribute measure weight height latitude longitude 
44 chapter 2 get know datum coordinate ( eg cluster house ) monetary quantity ( eg 100 time richer $ 100 $ 1 ) 216 discrete versus continuous attribute presentation organized attribute nominal binary ordinal numeric type many way organize attribute type type mutually exclusive classification algorithms develop field machine learn often talk attribute either discrete continuous type may processed differently discrete attribute finite countably infinite set value may may represent integer attribute hair color smoker medical test drink size finite number value discrete note discrete attribute may numeric value 0 1 binary attribute value 0 110 attribute age attribute countably infinite set possible value infinite value put one-to-one correspondence natural number example attribute customer id countably infinite number customer grow infinity reality actual set value countable ( value put one-to-one correspondence set integer ) zip code another example attribute discrete continuous term numeric attribute continuous attribute often used interchangeably literature ( confuse classic sense continuous value real number whereas numeric value either integer real number ) practice real value represent used finite number digit continuous attribute typically represent floating-point variable 22 basic statistical description datum datum preprocess successful essential overall picture datum basic statistical description used identify property datum highlight datum value treat noise outlier section discuss three area basic statistical description start measure central tendency ( section 221 ) measure location middle center datum distribution intuitively speaking give attribute value fall particular discuss mean median mode midrange addition assess central tendency datum set also would like idea dispersion datum datum spread common datum dispersion measure range quartile interquartile range five-number summary boxplot variance standard deviation datum measure useful identify outlier describe section 222 finally use many graphic display basic statistical description visually inspect datum ( section 223 ) statistical graphical datum presentation software 
22 basic statistical description datum 45 package include bar chart pie chart line graph popular display datum summary distribution include quantile plot quantile–quantile plot histogram scatter plot 221 measure central tendency mean median mode section look various way measure central tendency datum suppose attribute x like salary record set object let x1 x2 xn set n observed value observation x value may also refer datum set ( x ) plot observation salary would value fall give us idea central tendency datum measure central tendency include mean median mode midrange common effective numeric measure “ center ” set datum ( arithmetic ) mean let x1 x2 xn set n value observation numeric attribute x like salary mean set value n x x̄ = xi i=1 n = x1 + x2 + · · · + xn n ( 21 ) correspond built-in aggregate function average ( avg ( ) sql ) provide relational database system example 26 mean suppose follow value salary ( thousand dollar ) show increase order 30 36 47 50 52 52 56 60 63 70 70 used eq ( 21 ) 30 + 36 + 47 + 50 + 52 + 52 + 56 + 60 + 63 + 70 + 70 + 110 12 696 = = 58 12 x̄ = thus mean salary $ 58000 sometimes value xi set may associate weight wi = 1 n weight reflect significance importance occurrence frequency attach respective value case compute n x x̄ = wi xi i=1 n x = w1 x1 + w2 x2 + · · · + wn xn w1 + w2 + · · · + wn wi i=1 call weight arithmetic mean weight average ( 22 ) 
46 chapter 2 get know datum although mean singlemost useful quantity describe datum set always best way measure center datum major problem mean sensitivity extreme ( eg outlier ) value even small number extreme value corrupt mean example mean salary company may substantially push highly paid manager similarly mean score class exam can pull quite bit low score offset effect cause small number extreme value instead use trim mean mean obtain chop value high low extreme example sort value observed salary remove top bottom 2 % compute mean avoid trimming large portion ( 20 % ) end result loss valuable information skewer ( asymmetric ) datum better measure center datum median middle value set order datum value value separate higher half datum set lower half probability statistic median generally apply numeric datum however may extend concept ordinal datum suppose give datum set n value attribute x sort increase order n odd median middle value order set n even median unique two middlemost value value x numeric attribute case convention median take average two middlemost value example 27 median let ’ find median datum example datum already sort increase order even number observation ( ie 12 ) therefore median unique value within two middlemost value 52 56 ( within sixth seventh value list ) convention assign = 108 average two middlemost value median 52+56 2 2 = thus median $ 54000 suppose first 11 value list give odd number value median middlemost value sixth value list value $ 52000 median expensive compute large number observation numeric attribute however easily approximate value assume datum group interval accord xi datum value frequency ( ie number datum value ) interval know example employee may group accord annual salary interval $ 10–20000 $ 20–30000 let interval contain median frequency median interval approximate median entire datum set ( eg median salary ) interpolation used formula  p n 2 − freq l median = l1 + width ( 23 ) freqmedian l1 lower median interval n number value  pboundary entire datum set freq l sum frequency interval 
22 basic statistical description datum 47 lower median interval freqmedian frequency median interval width width median interval mode another measure central tendency mode set datum value occur frequently set therefore determine qualitative quantitative attribute possible greatest frequency correspond several different value result one mode datum set one two three mode respectively call unimodal bimodal trimodal general datum set two mode multimodal extreme datum value occur mode example 28 mode datum example 26 bimodal two mode $ 52000 $ 70000 unimodal numeric datum moderately skewer ( asymmetrical ) follow empirical relation mean − mode ≈ 3 × ( mean − median ) ( 24 ) imply mode unimodal frequency curf moderately skewer easily approximate mean median value know midrange also used assess central tendency numeric datum set average largest smallest value set measure easy compute used sql aggregate function max ( ) min ( ) example 29 midrange midrange datum example 26 30000+110000 2 = $ 70000 unimodal frequency curve perfect symmetric datum distribution mean median mode center value show figure 21 ( ) datum real application symmetric may instead either positively skewer mode occur value smaller median ( figure 21b ) negatively skewer mode occur value greater median ( figure 21c ) mean median mode mode mean median ( ) symmetric datum ( b ) positively skewer datum mean mode median ( c ) negatively skewer datum figure 21 mean median mode symmetric versus positively negatively skewer datum 
48 chapter 2 get know datum 222 measure dispersion datum range quartile variance standard deviation interquartile range look measure assess dispersion spread numeric datum measure include range quantile quartile percentile interquartile range five-number summary display boxplot useful identify outlier variance standard deviation also indicate spread datum distribution range quartile interquartile range start let ’ study range quantile quartile percentile interquartile range measure datum dispersion let x1 x2 xn set observation numeric attribute x range set difference largest ( max ( ) ) smallest ( min ( ) ) value suppose datum attribute x sort increase numeric order imagine pick certain datum point split datum distribution equal-size consecutive set figure datum point call quantile quantile point take regular interval datum distribution divide essentially equalsize consecutive set ( say “ essentially ” may datum value x divide datum exactly equal-sized subset readability refer equal ) kth q-quantile give datum distribution value x q datum value less x ( q − k ) q datum value x k integer 0 < k < q q − 1 q-quantile 2-quantile datum point divide lower upper half datum distribution correspond median 4-quantiles three datum point split datum distribution four equal part part represent one-fourth datum distribution commonly refer quartile 100-quantile commonly refer percentile divide datum distribution 100 equal-sized consecutive set median quartile percentile widely used form quantile 25 % q1 q2 q3 median 75th 25th percentile percentile figure 22 plot datum distribution attribute x quantile plot quartile three quartile divide distribution four equal-size consecutive subset second quartile correspond median 
22 basic statistical description datum 49 quartile give indication distribution ’ center spread shape first quartile denote q1 25th percentile cut lowest 25 % datum third quartile denote q3 75th percentile—it cut lowest 75 % ( highest 25 % ) datum second quartile 50th percentile median give center datum distribution distance first third quartile simple measure spread give range cover middle half datum distance call interquartile range ( iqr ) defined iqr = q3 − q1 ( 25 ) example 210 interquartile range quartile three value split sort datum set four equal part datum example 26 contain 12 observation already sort increase order thus quartile datum third sixth ninth value respectively sort list therefore q1 = $ 47000 q3 $ 63000 thus interquartile range iqr = 63 − 47 = $ 16000 ( note sixth value median $ 52000 although datum set two median since number datum value even ) five-number summary boxplot outlier single numeric measure spread ( eg iqr ) useful describe skewer distribution look symmetric skewer datum distribution figure 21 symmetric distribution median ( measure central tendency ) split datum equal-size half occur skewer distribution therefore informative also provide two quartile q1 q3 along median common rule thumb identify suspect outlier single value fall least 15 × iqr third quartile first quartile q1 median q3 together contain information endpoint ( eg tail ) datum fuller summary shape distribution obtain provide lowest highest datum value well know five-number summary five-number summary distribution consist median ( q2 ) quartile q1 q3 smallest largest individual observation written order minimum q1 median q3 maximum boxplot popular way visualize distribution boxplot incorporate five-number summary follow typically end box quartile box length interquartile range median marked line within box two line ( call whisker ) outside box extend smallest ( minimum ) largest ( maximum ) observation 
50 chapter 2 get know datum 220 200 180 160 unit price ( $ ) 140 120 100 80 60 40 20 branch 1 branch 2 branch 3 branch 4 figure 23 boxplot unit price datum item sell four branch allelectronic give time period deal moderate number observation worthwhile plot potential outlier individually boxplot whisker extend extreme low high observation value less 15 × iqr beyond quartile otherwise whisker terminate extreme observation occur within 15 × iqr quartile remain case plot individually boxplot used comparison several set compatible datum example 211 boxplot figure 23 show boxplot unit price datum item sell four branch allelectronic give time period branch 1 see median price item sell $ 80 q1 $ 60 q3 $ 100 notice two outlying observation branch plot individually value 175 202 15 time iqr 40 boxplot compute ( n log n ) time approximate boxplot compute linear sublinear time depend quality guarantee require variance standard deviation variance standard deviation measure datum dispersion indicate spread datum distribution low standard deviation mean datum observation tend close mean high standard deviation indicate datum spread large range value 
22 basic statistical description datum variance n observation x1 x2 xn numeric attribute x n n x x 1 1 σ2 = ( xi − x̄ ) 2 = xi2 − x̄ 2 n n i=1 51 ( 26 ) i=1 x̄ mean value observation defined eq ( 21 ) standard deviation σ observation square root variance σ 2 example 212 variance standard deviation example 26 find x̄ = $ 58000 used eq ( 21 ) mean determine variance standard deviation datum example set n = 12 use eq ( 26 ) obtain 1 ( 302 + 362 + 472 + 1102 ) − 582 12 ≈ 37917 √ σ ≈ 37917 ≈ 1947 σ2 = basic property standard deviation σ measure spread follow σ measure spread mean consider mean choose measure center σ = 0 spread observation value otherwise σ > 0 importantly observation unlikely several standard deviation away mathematically used chebyshev ’ inequality show  mean  least 1 − k12 × 100 % observation k standard deviation mean therefore standard deviation good indicator spread datum set computation variance standard deviation scalable large databasis 223 graphic display basic statistical description datum section study graphic display basic statistical description include quantile plot quantile–quantile plot histogram scatter plot graph helpful visual inspection datum useful datum preprocess first three show univariate distribution ( ie datum one attribute ) scatter plot show bivariate distribution ( ie involve two attribute ) quantile plot follow subsection cover common graphic display datum distribution quantile plot simple effective way first look univariate datum distribution first display datum give attribute ( allow user 
52 chapter 2 get know datum assess overall behavior unusual occurrence ) second plot quantile information ( see section 222 ) let xi = 1 n datum sort increase order x1 smallest observation xn largest ordinal numeric attribute x observation xi pair percentage fi indicate approximately fi × 100 % datum value xi say “ approximately ” may value exactly fraction fi datum xi note 025 percentile correspond quartile q1 050 percentile median 075 percentile q3 let fi = − 05 n ( 27 ) 1 number increase equal step n range 2n ( slightly 1 0 ) 1 − 2n ( slightly 1 ) quantile plot xi graph fi allow us compare different distribution base quantile example give quantile plot sale datum two different time period compare q1 median q3 fi value glance example 213 quantile plot figure 24 show quantile plot unit price datum table 21 quantile–quantile plot unit price ( $ ) quantile–quantile plot q-q plot graph quantile one univariate distribution corresponding quantile another powerful visualization tool allow user view whether shift go one distribution another suppose two set observation attribute variable unit price take two different branch location let x1 xn datum first branch y1 ym datum second datum set sort increase order = n ( ie number point set ) simply plot yi xi yi xi ( − 05 ) n quantile respective datum set < n ( ie second branch fewer observation first ) point q-q plot yi ( − 05 ) m quantile 140 120 100 80 60 40 20 0 000 q3 median q1 025 050 f-value 075 figure 24 quantile plot unit price datum table 21 100 
22 basic statistical description datum 53 table 21 set unit price datum item sell branch allelectronic unit price ( $ ) count item sell 40 43 47 − 74 75 78 − 115 117 120 275 300 250 − 360 515 540 − 320 270 350 branch 2 ( unit price $ ) 120 110 q3 100 median 90 80 70 q1 60 50 40 40 50 60 70 80 90 branch 1 ( unit price $ ) 100 110 120 figure 25 q-q plot unit price datum two allelectronic branch datum plot ( − 05 ) m quantile x datum computation typically involve interpolation example 214 quantile–quantile plot figure 25 show quantile–quantile plot unit price datum item sell two branch allelectronic give time period point correspond quantile datum set show unit price item sell branch 1 versus branch 2 quantile ( aid comparison straight line represent case give quantile unit price branch darker point correspond datum q1 median q3 respectively ) see example q1 unit price item sell branch 1 slightly less branch word 25 % item sell branch 1 less 
54 chapter 2 get know datum equal $ 60 25 % item sell branch 2 less equal $ 64 50th percentile ( marked median also q2 ) see 50 % item sell branch 1 less $ 78 50 % item branch 2 less $ 85 general note shift distribution branch 1 respect branch 2 unit price item sell branch 1 tend lower branch 2 histogram histogram ( frequency histogram ) least century old widely used “ histos ” mean pole mast “ gram ” mean chart histogram chart pole plot histogram graphical method summarize distribution give attribute x x nominal automobile model item type pole vertical bar draw know value x height bar indicate frequency ( ie count ) x value result graph commonly know bar chart x numeric term histogram prefer range value x partition disjoint consecutive subrange subrange refer bucket bin disjoint subset datum distribution x range bucket know width typically bucket equal width example price attribute value range $ 1 $ 200 ( round nearest dollar ) partition subrange 1 20 21 40 41 60 subrange bar draw height represent total count item observed within subrange histogram partition rule discuss chapter 3 datum reduction example 215 histogram figure 26 show histogram datum set table 21 bucket ( bin ) defined equal-width range represent $ 20 increment frequency count item sell although histogram widely used may effective quantile plot q-q plot boxplot method compare group univariate observation scatter plot datum correlation scatter plot one effective graphical method determine appear relationship pattern trend two numeric attribute construct scatter plot pair value treat pair coordinate algebraic sense plot point plane figure 27 show scatter plot set datum table 21 scatter plot useful method provide first look bivariate datum see cluster point outlier explore possibility correlation relationship two attribute x correlated one attribute imply correlation positive negative null ( uncorrelated ) figure 28 show example positive negative correlation two attribute plot point pattern slope 
22 basic statistical description datum 55 6000 count item sell 5000 4000 3000 2000 1000 0 40–59 60–79 80–99 unit price ( $ ) 100–119 120–139 figure 26 histogram table 21 datum set 700 item sell 600 500 400 300 200 100 0 0 20 40 60 80 unit price ( $ ) 100 120 140 figure 27 scatter plot table 21 datum set ( ) ( b ) figure 28 scatter plot used find ( ) positive ( b ) negative correlation attribute 
56 chapter 2 get know datum figure 29 three case observed correlation two plot attribute datum set lower left upper right mean value x increase value increase suggest positive correlation ( figure 28a ) pattern plot point slope upper left lower right value x increase value decrease suggest negative correlation ( figure 28b ) line best fit draw study correlation variable statistical test correlation give chapter 3 datum integration ( eq ( 33 ) ) figure 29 show three case correlation relationship two attribute give datum set section 232 show scatter plot extend n attribute result scatter-plot matrix conclusion basic datum description ( eg measure central tendency measure dispersion ) graphic statistical display ( eg quantile plot histogram scatter plot ) provide valuable insight overall behavior datum help identify noise outlier especially useful datum clean 23 datum visualization convey datum user effectively datum visualization aim communicate datum clearly effectively graphical representation datum visualization used extensively many applications—for example work report manage business operation tracking progress task popularly take advantage visualization technique discover datum relationship otherwise easily observable look raw datum nowadays person also use datum visualization create fun interesting graphic section briefly introduce basic concept datum visualization start multidimensional datum store relational databasis discuss several representative approach include pixel-oriented technique geometric projection technique icon-based technique hierarchical graph-based technique discuss visualization complex datum relation 
23 datum visualization 231 57 pixel-oriented visualization technique simple way visualize value dimension use pixel color pixel reflect dimension ’ value datum set dimension pixel-oriented technique create window screen one dimension dimension value record map pixel corresponding position window color pixel reflect corresponding value inside window datum value arrange global order share window global order may obtain sort datum record way ’ meaningful task hand example 216 pixel-oriented visualization allelectronic maintain customer information table consist four dimension income credit limit transaction volume age analyze correlation income attribute visualization sort customer income-ascending order use order lay customer datum four visualization window show figure pixel color choose smaller value lighter shading used pixelbased visualization easily observe follow credit limit increase income increase customer whose income middle range likely purchase allelectronic clear correlation income age pixel-oriented technique datum record also order query-dependent way example give point query sort record descend order similarity point query fill window layer datum record linear way may work well wide window first pixel row far away last pixel previous row though next global order moreover pixel next one window even though two next global order solve problem lay datum record space-filling curve ( ) income ( b ) credit_limit ( c ) transaction_volume ( ) age figure 210 pixel-oriented visualization four attribute sort customer income ascend order 
58 chapter 2 get know datum ( ) hilbert curve ( b ) gray code ( c ) z-curve figure 211 frequently used 2-d space-filling curf one datum record dim 6 dim 6 dim 5 dim 1 dim 4 dim 2 dim 3 ( ) dim 5 dim 1 dim 4 dim 2 dim 3 ( b ) figure 212 circle segment technique ( ) represent datum record circle segment ( b ) layer pixel circle segment fill window space-filling curve curve range cover entire n-dimensional unit hypercube since visualization window 2-d use 2-d space-filling curve figure 211 show frequently used 2-d space-filling curf note window rectangular example circle segment technique used window shape segment circle illustrated figure 212 technique ease comparison dimension dimension window locate side side form circle 232 geometric projection visualization technique drawback pixel-oriented visualization technique help us much understand distribution datum multidimensional space example show whether dense area multidimensional subspace geometric 
23 datum visualization 59 80 70 60 50 40 30 20 10 0 0 10 20 30 40 x 50 60 70 80 figure 213 visualization 2-d datum set used scatter plot source rareevent-geoinformatica06pdf projection technique help user find interesting projection multidimensional datum set central challenge geometric projection technique try address visualize high-dimensional space 2-d display scatter plot display 2-d datum point used cartesian coordinate third dimension add used different color shape represent different datum point figure 213 show example x two spatial attribute third dimension represent different shape visualization see point type “ + ” “ × ” tend colocate 3-d scatter plot used three axe cartesian coordinate system also used color display 4-d datum point ( figure 214 ) datum set four dimension scatter plot usually ineffective scatter-plot matrix technique useful extension scatter plot ndimensional datum set scatter-plot matrix n × n grid 2-d scatter plot provide visualization dimension every dimension figure 215 show example visualize iris datum set datum set consist 450 sample three species iris flower five dimension datum set length width sepal petal species scatter-plot matrix become less effective dimensionality increase another popular technique call parallel coordinate handle higher dimensionality visualize n-dimensional datum point parallel coordinate technique draw n equally space axe one dimension parallel one display axe 
60 chapter 2 get know datum figure 214 visualization 3-d datum set used scatter plot source http scatter plotjpg datum record represent polygonal line intersect axis point corresponding associate dimension value ( figure 216 ) major limitation parallel coordinate technique effectively show datum set many record even datum set several thousand record visual clutter overlap often reduce readability visualization make pattern hard find 233 icon-based visualization technique icon-based visualization technique use small icon represent multidimensional datum value look two popular icon-based technique chernoff face stick figure chernoff face introduce 1973 statistician herman chernoff display multidimensional datum 18 variable ( dimension ) cartoon human face ( figure 217 ) chernoff face help reveal trend datum component face eye ears mouth nose represent value dimension shape size placement orientation example dimension map follow facial characteristic eye size eye spacing nose length nose width mouth curvature mouth width mouth openness pupil size eyebrow slant eye eccentricity head eccentricity chernoff face make use ability human mind recognize small difference facial characteristic assimilate many facial characteristic 
23 datum visualization 10 30 50 70 0 10 61 20 80 70 sepal length ( mm ) 60 50 40 70 50 petal length ( mm ) 30 10 45 40 35 30 25 20 sepal width ( mm ) 25 20 15 10 5 0 petal width ( mm ) 40 50 60 70 80 iris species 20 setosa 30 versicolor 40 virginica figure 215 visualization iris datum set used scatter-plot matrix source http gsgscmatgif view large table datum tedious condense datum chernoff face make datum easier user digest way facilitate visualization regularity irregularity present datum although power relate multiple relationship limit another limitation specific datum value show furthermore facial feature vary perceive importance mean similarity two face ( represent two multidimensional datum point ) vary depend order dimension assign facial characteristic therefore mapping carefully choose eye size eyebrow slant find important asymmetrical chernoff face propose extension original technique since face vertical symmetry ( along y-axis ) left right side face identical waste space asymmetrical chernoff face double number facial characteristic thus allow 36 dimension display stick figure visualization technique map multidimensional datum five-piece stick figure figure four limb body two dimension map display ( x ) axe remain dimension map angle 
62 chapter 2 get know datum 10 5 x 0 –5 –10 ⫻1 ⫻2 ⫻3 ⫻4 ⫻5 ⫻6 ⫻7 ⫻8 ⫻9 ⫻10 figure 216 visualization used parallel coordinate source parallel coordithml figure 217 chernoff face face represent n-dimensional datum point ( n ≤ 18 ) or length limb figure 218 show census datum age income map display axe remain dimension ( gender education ) map stick figure datum item relatively dense respect two display dimension result visualization show texture pattern reflect datum trend 
23 datum visualization 63 figure 218 census datum represent used stick figure source professor g grinstein department computer science university massachusett lowell 234 hierarchical visualization technique visualization technique discuss far focus visualize multiple dimension simultaneously however large datum set high dimensionality would difficult visualize dimension time hierarchical visualization technique partition dimension subset ( ie subspace ) subspace visualize hierarchical manner “ worlds-within-world ” also know n-vision representative hierarchical visualization method suppose want visualize 6-d datum set dimension f x1 x5 want observe dimension f change respect dimension first fix value dimension x3 x4 x5 select value say c3 c4 c5 visualize f x1 x2 used 3-d plot call world show figure position origin inner world locate point ( c3 c4 c5 ) outer world another 3-d plot used dimension x3 x4 x5 user interactively change outer world location origin inner world user view result change inner world moreover user vary dimension used inner world outer world give dimension level world used method call “ worlds-withinworld ” another example hierarchical visualization method tree-maps display hierarchical datum set nest rectangle example figure 220 show tree-map visualize google news story news story organized seven category show large rectangle unique color within category ( ie rectangle top level ) news story partition smaller subcategory 
64 chapter 2 get know datum figure 219 “ worlds-within-world ” ( also know n-vision ) source http 1dipstick5gif 235 visualize complex datum relation early day visualization technique mainly numeric datum recently non-numeric datum text social network become available visualize analyze datum attract lot interest many new visualization technique dedicate kind datum example many person web tag various object picture blog entry product reviews tag cloud visualization statistic user-generated tag often tag cloud tag list alphabetically user-preferred order importance tag indicated font size color figure 221 show tag cloud visualize popular tag used web site tag cloud often used two way first tag cloud single item use size tag represent number time tag apply item different user second visualize tag statistic multiple item use size tag represent number item tag apply popularity tag addition complex datum complex relation among datum entry also raise challenge visualization example figure 222 used disease influence graph visualize correlation disease node graph disease size node proportional prevalence corresponding disease two node link edge corresponding disease strong correlation width edge proportional strength correlation pattern two corresponding disease 
24 measure datum similarity dissimilarity 65 figure 220 newsmap use tree-maps visualize google news headline story source wwwcsumd newsmappng summary visualization provide effective tool explore datum introduce several popular method essential idea behind many exist tool method moreover visualization used datum mining various aspect addition visualize datum visualization used represent datum mining process pattern obtain mining method user interaction datum visual datum mining important research development direction 24 measure datum similarity dissimilarity datum mining application cluster outlier analysis nearest-neighbor classification need way assess alike unalike object comparison one another example store may want search cluster customer object result group customer similar characteristic ( eg similar income area residence age ) information used marketing cluster 
66 chapter 2 get know datum figure 221 used tag cloud visualize popular web site tag source snapshot january 23 2010 high blood pressure ( hb ) allergy ( al ) st li overweight ( ov ) en high cholesterol level ( hc ) ki arthritis ( ar ) trouble see ( tr ) li risk diabetes ( ri ) asthma ( ) ca th diabetes ( di ) hayfever ( ha ) hc thyroid problem ( th ) di heart disease ( ) em tr ar hb cancer ( cn ) os sleep disorder ( sl ) ov eczema ( ec ) chronic bronchitis ( ch ) cn osteoporosis ( os ) prostate ( pr ) cardiovascular ( ca ) ps glaucoma ( gl ) ec pr stroke ( st ) liver condition ( li ) ch psa test abnormal ( ps ) kidney ( ki ) endometriosis ( en ) emphysema ( em ) ha al ri sl gl figure 222 disease influence graph person least 20 year old nhane datum set collection datum object object within cluster similar one another dissimilar object cluster outlier analysis also employ clustering-based technique identify potential outlier object highly dissimilar other knowledge object similarity also used nearest-neighbor classification scheme give object ( eg patient ) assign class label ( relate say diagnosis ) base similarity toward object model 
24 measure datum similarity dissimilarity 67 section present similarity dissimilarity measure refer measure proximity similarity dissimilarity related similarity measure two object j typically return value 0 object unalike higher similarity value greater similarity object ( typically value 1 indicate complete similarity object identical ) dissimilarity measure work opposite way return value 0 object ( therefore far dissimilar ) higher dissimilarity value dissimilar two object section 241 present two datum structure commonly used type application datum matrix ( used store datum object ) dissimilarity matrix ( used store dissimilarity value pair object ) also switch different notation datum object previously used chapter since deal object describe one attribute discuss object dissimilarity compute object describe nominal attribute ( section 242 ) binary attribute ( section 243 ) numeric attribute ( section 244 ) ordinal attribute ( section 245 ) combination attribute type ( section 246 ) section 247 provide similarity measure long sparse datum vector term-frequency vector represent document information retrieval know compute dissimilarity useful study attribute also reference later topic cluster ( chapter 10 11 ) outlier analysis ( chapter 12 ) nearest-neighbor classification ( chapter 9 ) 241 datum matrix versus dissimilarity matrix section 22 look way study central tendency dispersion spread observed value attribute x object one-dimensional describe single attribute section talk object describe multiple attribute therefore need change notation suppose n object ( eg person item course ) describe p attribute ( also call measurement feature age height weight gender ) object x1 = ( x11 x12 x1p ) x2 = ( x21 x22 x2p ) xij value object xi jth attribute brevity hereafter refer object xi object i object may tuple relational database also refer datum sample feature vector main memory-based cluster nearest-neighbor algorithms typically operate either follow two datum structure datum matrix ( object-by-attribute structure ) structure store n datum object form relational table n-by-p matrix ( n object ×p attribute )   x11 · · · x1f · · · x1p ··· ··· ··· ··· ···     ( 28 )  xi1 · · · xif · · · xip    ··· ··· ··· ··· ··· xn1 · · · xnf · · · xnp 
68 chapter 2 get know datum row correspond object part notation may use f index p attribute dissimilarity matrix ( object-by-object structure ) structure store collection proximity available pair n object often represent n-by-n table   0  d ( 2 1 ) 0      d ( 3 1 ) ( 3 2 ) 0 ( 29 )       ( n 1 ) ( n 2 ) · · · · · · 0 ( j ) measure dissimilarity “ difference ” object j general ( j ) non-negative number close 0 object j highly similar “ near ” become larger differ note ( ) = 0 difference object furthermore ( j ) = ( j ) ( readability show ( j ) entry matrix symmetric ) measure dissimilarity discuss throughout remainder chapter measure similarity often expressed function measure dissimilarity example nominal datum sim ( j ) = 1 − ( j ) ( 210 ) sim ( j ) similarity object j throughout rest chapter also comment measure similarity datum matrix make two entity “ thing ” namely row ( object ) column ( attribute ) therefore datum matrix often call two-mode matrix dissimilarity matrix contain one kind entity ( dissimilarity ) call one-mode matrix many cluster nearest-neighbor algorithms operate dissimilarity matrix datum form datum matrix transform dissimilarity matrix apply algorithms 242 proximity measure nominal attribute nominal attribute take two state ( section 212 ) example map color nominal attribute may say five state red yellow green pink blue let number state nominal attribute m state denote letter symbol set integer 1 2 m notice integer used datum handle represent specific order 
24 measure datum similarity dissimilarity 69 “ dissimilarity compute object describe nominal attribute ” dissimilarity two object j compute base ratio mismatch ( j ) = p−m p ( 211 ) number match ( ie number attribute j state ) p total number attribute describe object weight assign increase effect assign greater weight match attribute larger number state example 217 dissimilarity nominal attribute suppose sample datum table 22 except object-identifier attribute test-1 available test-1 nominal ( use test-2 test-3 later example ) let ’ compute dissimilarity matrix ( eq 29 )   0  d ( 2 1 ) 0      d ( 3 1 ) ( 3 2 ) 0 ( 4 1 ) ( 4 2 ) ( 4 3 ) 0 since one nominal attribute test-1 set p = 1 eq ( 211 ) ( j ) evaluate 0 object j match 1 object differ thus get  0 1   1 0  0 1 1 0 1     0 see object dissimilar except object 1 4 ( ie ( 4 1 ) = 0 ) table 22 sample datum table contain attribute mixed type object identifier test-1 ( nominal ) test-2 ( ordinal ) test-3 ( numeric ) 1 2 3 4 code code b code c code excellent fair good excellent 45 22 64 28 
70 chapter 2 get know datum alternatively similarity compute sim ( j ) = 1 − ( j ) = p ( 212 ) proximity object describe nominal attribute compute used alternative encode scheme nominal attribute encode used asymmetric binary attribute create new binary attribute state object give state value binary attribute represent state set 1 remain binary attribute set example encode nominal attribute map color binary attribute create five color previously list object color yellow yellow attribute set 1 remain four attribute set proximity measure form encode calculate used method discuss next subsection 243 proximity measure binary attribute let ’ look dissimilarity similarity measure object describe either symmetric asymmetric binary attribute recall binary attribute one two state 0 1 0 mean attribute absent 1 mean present ( section 213 ) give attribute smoker describe patient instance 1 indicate patient smoke 0 indicate patient treat binary attribute numeric mislead therefore method specific binary datum necessary compute dissimilarity “ compute dissimilarity two binary attribute ” one approach involve compute dissimilarity matrix give binary datum binary attribute thought weight 2 × 2 contingency table table 23 q number attribute equal 1 object j r number attribute equal 1 object equal 0 object j number attribute equal 0 object equal 1 object j number attribute equal 0 object j total number attribute p p = q + r + + recall symmetric binary attribute state equally valuable dissimilarity base symmetric binary attribute call symmetric binary dissimilarity object j describe symmetric binary attribute table 23 contingency table binary attribute object j object 1 0 sum 1 q q+s 0 r r t sum q+r s+t p 
24 measure datum similarity dissimilarity 71 dissimilarity j ( j ) = r s q+r s+t ( 213 ) asymmetric binary attribute two state equally important positive ( 1 ) negative ( 0 ) outcome disease test give two asymmetric binary attribute agreement two 1s ( positive match ) consider significant two 0s ( negative match ) therefore binary attribute often consider “ monary ” ( one state ) dissimilarity base attribute call asymmetric binary dissimilarity number negative match consider unimportant thus ignore follow computation ( j ) = r s q+r s ( 214 ) complementarily measure difference two binary attribute base notion similarity instead dissimilarity example asymmetric binary similarity object j compute sim ( j ) = q = 1 − ( j ) q+r s ( 215 ) coefficient sim ( j ) eq ( 215 ) call jaccard coefficient popularly reference literature symmetric asymmetric binary attribute occur datum set mixed attribute approach describe section 246 apply example 218 dissimilarity binary attribute suppose patient record table ( table 24 ) contain attribute name gender fever cough test-1 test-2 test-3 test-4 name object identifier gender symmetric attribute remain attribute asymmetric binary asymmetric attribute value let value ( yes ) p ( positive ) set 1 value n ( negative ) set suppose distance object table 24 relational table patient describe binary attribute name gender fever cough test-1 test-2 test-3 test-4 jack jim mary f n n p n p n n n n n p n n n 
72 chapter 2 get know datum ( patient ) compute base asymmetric attribute accord eq ( 214 ) distance pair three patients—jack mary jim—is ( jack jim ) = 1+1 = 067 1+1+1 ( jack mary ) = 0+1 = 033 2+0+1 ( jim mary ) = 1+2 = 075 1+1+2 measurement suggest jim mary unlikely similar disease highest dissimilarity value among three pair three patient jack mary likely similar disease 244 dissimilarity numeric datum minkowski distance section describe distance measure commonly used compute dissimilarity object describe numeric attribute measure include euclidean manhattan minkowski distance case datum normalize apply distance calculation involve transform datum fall within smaller common range [ −1 1 ] [ 00 10 ] consider height attribute example can measure either meter inch general express attribute smaller unit lead larger range attribute thus tend give attribute greater effect “ weight ” normalize datum attempt give attribute equal weight may may useful particular application method normalize datum discuss detail chapter 3 datum preprocess popular distance measure euclidean distance ( ie straight line “ crow fly ” ) let = ( xi1 xi2 xip ) j = ( xj1 xj2 xjp ) two object describe p numeric attribute euclidean distance object j defined q ( j ) = ( xi1 − xj1 ) 2 + ( xi2 − xj2 ) 2 + · · · + ( xip − xjp ) 2 ( 216 ) another well-known measure manhattan ( city block ) distance name distance block two point city ( 2 block 3 block total 5 block ) defined ( j ) = xi1 − xj1 | + xi2 − xj2 | + · · · + xip − xjp | ( 217 ) euclidean manhattan distance satisfy follow mathematical property non-negativity ( j ) ≥ 0 distance non-negative number identity indiscernible ( ) = 0 distance object 0 
24 measure datum similarity dissimilarity 73 symmetry ( j ) = ( j ) distance symmetric function triangle inequality ( j ) ≤ ( k ) + ( k j ) go directly object object j space make detour object k measure satisfy condition know metric please note non-negativity property imply three property example 219 euclidean distance manhattan distance let x1 = ( 1 2 ) x2 = ( 3 5 ) represent √ two object show figure euclidean distance two 22 + 32 = manhattan distance two 2 + 3 = 5 minkowski distance generalization euclidean manhattan distance defined q ( j ) = h xi1 − xj1 h + xi2 − xj2 h + · · · + xip − xjp h ( 218 ) h real number h ≥ 1 ( distance also call lp norm literature symbol p refer notation h keep p number attribute consistent rest chapter ) represent manhattan distance h = 1 ( ie l1 norm ) euclidean distance h = 2 ( ie l2 norm ) supremum distance ( also refer lmax l∞ norm chebyshev distance ) generalization minkowski distance h → ∞ compute find attribute f give maximum difference value two object difference supremum distance defined formally  1 h p x p h  ( j ) = lim xif − xjf | = max xif − xjf | ( 219 ) h→∞ f 1 f l∞ norm also know uniform norm x2 = ( 3 5 ) 5 4 euclidean distance = ( 22 + 32 ) 2 = 361 3 3 2 x1 = ( 1 2 ) manhattan distance 2+3=5 supremum distance 5–2=3 2 1 1 2 3 figure 223 euclidean manhattan supremum distance two object 
74 chapter 2 get know datum example 220 supremum distance let ’ use two object x1 = ( 1 2 ) x2 = ( 3 5 ) figure second attribute give greatest difference value object 5 − 2 = supremum distance object attribute assign weight accord perceive importance weight euclidean distance compute q ( j ) = w1 xi1 − xj1 2 + w2 xi2 − xj2 2 + · · · + wm xip − xjp 2 ( 220 ) weighting also apply distance measure well 245 proximity measure ordinal attribute value ordinal attribute meaningful order ranking yet magnitude successive value unknown ( section 214 ) example include sequence small medium large size attribute ordinal attribute may also obtain discretization numeric attribute splitting value range finite number category category organized rank range numeric attribute map ordinal attribute f mf state example range interval-scaled attribute temperature ( celsius ) organized follow state −30 −10 −10 10 10 30 represent category cold temperature moderate temperature warm temperature respectively let represent number possible state ordinal attribute order state define ranking 1 mf “ ordinal attribute handled ” treatment ordinal attribute quite similar numeric attribute compute dissimilarity object suppose f attribute set ordinal attribute describe n object dissimilarity computation respect f involve follow step value f ith object xif f mf order state represent ranking 1 mf replace xif corresponding rank rif ∈ { 1 mf } since ordinal attribute different number state often necessary map range attribute onto [ 00 10 ] attribute equal weight perform datum normalization replace rank rif ith object f th attribute zif = rif − 1 mf − 1 ( 221 ) dissimilarity compute used distance measure describe section 244 numeric attribute used zif represent f value ith object 
24 measure datum similarity dissimilarity 75 example 221 dissimilarity ordinal attribute suppose sample datum show earlier table 22 except time object-identifier continuous ordinal attribute test-2 available three state test-2 fair good excellent mf = step 1 replace value test-2 rank four object assign rank 3 1 2 3 respectively step 2 normalizes ranking mapping rank 1 00 rank 2 05 rank 3 step 3 use say euclidean distance ( eq 216 ) result follow dissimilarity matrix  0 10 0   05 05 0 0 10 05      0 therefore object 1 2 dissimilar object 2 4 ( ie ( 2 1 ) = 10 ( 4 2 ) = 10 ) make intuitive sense since object 1 4 excellent object 2 fair opposite end range value test-2 similarity value ordinal attribute interpreted dissimilarity sim ( j ) = 1 − ( j ) 246 dissimilarity attribute mixed type section 242 245 discuss compute dissimilarity object describe attribute type type may either nominal symmetric binary asymmetric binary numeric ordinal however many real databasis object describe mixture attribute type general database contain attribute type “ compute dissimilarity object mixed attribute type ” one approach group type attribute together perform separate datum mining ( eg cluster ) analysis type feasible analysis derive compatible result however real application unlikely separate analysis per attribute type generate compatible result preferable approach process attribute type together perform single analysis one technique combine different attribute single dissimilarity matrix bring meaningful attribute onto common scale interval [ 00 10 ] suppose datum set contain p attribute mixed type dissimilarity ( j ) object j defined ( f ) ( f ) f 1 δij dij pp ( f ) f 1 δij pp ( j ) = ( 222 ) 
76 chapter 2 get know datum ( f ) indicator δij = 0 either ( 1 ) xif xjf miss ( ie measurement attribute f object object j ) ( 2 ) xif = xjf = 0 attribute ( f ) f asymmetric binary otherwise δij = contribution attribute f ( f ) dissimilarity j ( ie dij ) compute dependent type ( f ) f numeric dij = attribute f xif −xjf | maxh xhf −minh xhf h run nonmissing object ( f ) ( f ) f nominal binary dij = 0 xif = xjf otherwise dij = 1 f ordinal compute rank rif zif = rif −1 mf −1 treat zif numeric step identical already see individual attribute type difference numeric attribute normalize value map interval [ 00 10 ] thus dissimilarity object compute even attribute describe object different type example 222 dissimilarity attribute mixed type let ’ compute dissimilarity matrix object table consider attribute different type example 217 221 work dissimilarity matrix individual attribute procedure follow test-1 ( nominal ) test-2 ( ordinal ) outlined earlier process attribute mixed type therefore use dissimilarity matrix obtain test-1 test-2 later compute eq ( 222 ) first however need compute dissimilarity matrix third attribute test-3 ( numeric ) ( 3 ) must compute dij follow case numeric attribute let maxh xh = 64 minh xh = difference two used eq ( 222 ) normalize value dissimilarity matrix result dissimilarity matrix test-3  0 055   045 040  0 100 014 0 086     0 use dissimilarity matrix three attribute computation ( f ) eq ( 222 ) indicator δij = 1 three attribute f get example ( 3 1 ) = 1 ( 1 ) 1 ( 050 ) 1 ( 045 ) 3 = result dissimilarity matrix obtain 
24 measure datum similarity dissimilarity 77 datum describe three attribute mixed type  0 085   065 013  0 083 071 0 079     0 table 22 intuitively guess object 1 4 similar base value test-1 test-2 confirm dissimilarity matrix ( 4 1 ) lowest value pair different object similarly matrix indicate object 1 2 least similar 247 cosine similarity document represent thousand attribute record frequency particular word ( keyword ) phrase document thus document object represent call term-frequency vector example table 25 see document1 contain five instance word team hockey occur three time word coach absent entire document indicated count value datum highly asymmetric term-frequency vector typically long sparse ( ie many 0 value ) application used structure include information retrieval text document cluster biological taxonomy gene feature mapping traditional distance measure study chapter work well sparse numeric datum example two term-frequency vector may many 0 value common meaning corresponding document share many word make similar need measure focus word two document common occurrence frequency word word need measure numeric datum ignore zero-match cosine similarity measure similarity used compare document say give ranking document respect give vector query word let x two vector comparison used cosine measure table 25 document vector term-frequency vector document team coach hockey baseball soccer penalty score win loss season document1 document2 document3 document4 5 3 0 0 0 0 7 1 3 2 0 0 0 0 2 0 2 1 1 1 0 1 0 2 0 0 0 2 2 1 3 0 0 0 0 3 0 1 0 0 
78 chapter 2 get know datum similarity function sim ( x ) = x·y | ( 223 ) | euclidean norm vector x = ( x1 x2 xp ) defined q x12 + x22 + · · · + xp2 conceptually length vector similarly | euclidean norm vector y measure compute cosine angle vector x y cosine value 0 mean two vector 90 degree ( orthogonal ) match closer cosine value 1 smaller angle greater match vector note cosine similarity measure obey property section 244 define metric measure refer nonmetric measure example 223 cosine similarity two term-frequency vector suppose x first two term-frequency vector table x = ( 5 0 3 0 2 0 0 2 0 0 ) = ( 3 0 2 0 1 1 0 1 0 1 ) similar x used eq ( 223 ) compute cosine similarity two vector get xt · = 5 × 3 + 0 × 0 + 3 × 2 + 0 × 0 + 2 × 1 + 0 × 1 + 0 × 0 + 2 × 1 + 0 × 0 + 0 × 1 = 25 p | = 52 + 02 + 32 + 02 + 22 + 02 + 02 + 22 + 02 + 02 = 648 p | = 32 + 02 + 22 + 02 + 12 + 12 + 02 + 12 + 02 + 12 = 412 sim ( x ) = 094 therefore used cosine similarity measure compare document would consider quite similar attribute binary-valu cosine similarity function interpreted term share feature attribute suppose object x possess ith attribute xi = xt · number attribute possessed ( ie share ) x | geometric mean number attribute possessed x number possessed y thus sim ( x ) measure relative possession common attribute simple variation cosine similarity precede scenario sim ( x ) = x·y x·x+y·y−x·y ( 224 ) ratio number attribute share x number attribute possessed x y function know tanimoto coefficient tanimoto distance frequently used information retrieval biology taxonomy 
26 exercise 25 79 summary datum set make datum object datum object represent entity datum object describe attribute attribute nominal binary ordinal numeric value nominal ( categorical ) attribute symbol name thing value represent kind category code state binary attribute nominal attribute two possible state ( 1 0 true false ) two state equally important attribute symmetric otherwise asymmetric ordinal attribute attribute possible value meaningful order ranking among magnitude successive value know numeric attribute quantitative ( ie measurable quantity ) represent integer real value numeric attribute type interval-scaled ratioscale value interval-scaled attribute measure fix equal unit ratio-scale attribute numeric attribute inherent zero-point measurement ratio-scale speak value order magnitude larger unit measurement basic statistical description provide analytical foundation datum preprocess basic statistical measure datum summarization include mean weight mean median mode measure central tendency datum range quantile quartile interquartile range variance standard deviation measure dispersion datum graphical representation ( eg boxplot quantile plot quantile– quantile plot histogram scatter plot ) facilitate visual inspection datum thus useful datum preprocess mining datum visualization technique may pixel-oriented geometric-based icon-based hierarchical method apply multidimensional relational datum additional technique propose visualization complex datum text social network measure object similarity dissimilarity used datum mining application cluster outlier analysis nearest-neighbor classification measure proximity compute attribute type study chapter combination attribute example include jaccard coefficient asymmetric binary attribute euclidean manhattan minkowski supremum distance numeric attribute application involve sparse numeric datum vector term-frequency vector cosine measure tanimoto coefficient often used assessment similarity 26 exercise 21 give three additional commonly used statistical measure already illustrated chapter characterization datum dispersion discuss compute efficiently large databasis 
80 chapter 2 get know datum 22 suppose datum analysis include attribute age age value datum tuple ( increase order ) 13 15 16 16 19 20 20 21 22 22 25 25 25 25 30 33 33 35 35 35 35 36 40 45 46 52 70 ( ) mean datum median ( b ) mode datum comment datum ’ modality ( ie bimodal trimodal etc ) ( c ) midrange datum ( ) find ( roughly ) first quartile ( q1 ) third quartile ( q3 ) datum ( e ) give five-number summary datum ( f ) show boxplot datum ( g ) quantile–quantile plot different quantile plot 23 suppose value give set datum group interval interval corresponding frequency follow age 1–5 6–15 16–20 21–50 51–80 81–110 frequency 200 450 300 1500 700 44 compute approximate median value datum 24 suppose hospital test age body fat datum 18 randomly select adult follow result age % fat 23 95 23 265 27 78 27 178 39 314 41 259 47 274 49 272 50 312 age % fat 52 346 54 425 54 288 56 334 57 302 58 341 58 329 60 412 61 357 ( ) calculate mean median standard deviation age % fat ( b ) draw boxplot age % fat ( c ) draw scatter plot q-q plot base two variable 25 briefly outline compute dissimilarity object describe follow ( ) nominal attribute ( b ) asymmetric binary attribute 
27 bibliographic note 81 ( c ) numeric attribute ( ) term-frequency vector 26 give two object represent tuple ( 22 1 42 10 ) ( 20 0 36 8 ) ( ) ( b ) ( c ) ( ) compute euclidean distance two object compute manhattan distance two object compute minkowski distance two object used q = 3 compute supremum distance two object 27 median one important holistic measure datum analysis propose several method median approximation analyze respective complexity different parameter setting decide extent real value approximate moreover suggest heuristic strategy balance accuracy complexity apply method give 28 important define select similarity measure datum analysis however commonly accept subjective similarity measure result vary depend similarity measure used nonetheless seemingly different similarity measure may equivalent transformation suppose follow 2-d datum set x1 x2 x3 x4 x5 a1 15 2 16 12 15 a2 17 19 18 15 10 ( ) consider datum 2-d datum point give new datum point x = ( 14 16 ) query rank database point base similarity query used euclidean distance manhattan distance supremum distance cosine similarity ( b ) normalize datum set make norm datum point equal use euclidean distance transform datum rank datum point 27 bibliographic note method descriptive datum summarization study statistic literature long onset computer good summary statistical descriptive datum mining method include freedman pisani purf [ fpp07 ] devore [ dev95 ] 
82 chapter 2 get know datum statistics-based visualization datum used boxplot quantile plot quantile–quantile plot scatter plot loess curf see cleveland [ cle93 ] pioneer work datum visualization technique describe visual display quantitative information [ tuf83 ] envision information [ tuf90 ] visual explanation image quantity evidence narrative [ tuf97 ] tufte addition graphic graphic information process bertin [ ber81 ] visualize datum cleveland [ cle93 ] information visualization datum mining knowledge discovery edit fayyad grinstein wierse [ fgw01 ] major conference symposium visualization include acm human factor compute system ( chi ) visualization international symposium information visualization research visualization also publish transaction visualization computer graphic journal computational graphical statistic ieee computer graphic application many graphical user interface visualization tool develop find various datum mining product several book datum mining ( eg datum mining solution westphal blaxton [ wb98 ] ) present many good example visual snapshot survey visualization technique see “ visual technique explore databasis ” keim [ kei97 ] similarity distance measure among various variable introduce many textbook study cluster analysis include hartigan [ har75 ] jain dube [ jd88 ] kaufman rousseeuw [ kr90 ] arabie hubert de soete [ ahs96 ] method combine attribute different type single dissimilarity matrix introduce kaufman rousseeuw [ kr90 ] 
10 cluster analysis basic concept method imagine director customer relationship allelectronic five manager work would like organize company ’ customer five group group assign different manager strategically would like customer group similar possible moreover two give customer different business pattern place group intention behind business strategy develop customer relationship campaign specifically target group base common feature share customer per group kind datum mining technique help accomplish task unlike classification class label ( group id ) customer unknown need discover grouping give large number customer many attribute describe customer profile costly even infeasible human study datum manually come way partition customer strategic group need cluster tool help cluster process grouping set datum object multiple group cluster object within cluster high similarity dissimilar object cluster dissimilarity similarity assessed base attribute value describe object often involve distance measures1 cluster datum mining tool root many application area biology security business intelligence web search chapter present basic concept method cluster analysis section 101 introduce topic study requirement cluster method massive amount datum various application learn several basic cluster technique organized follow category partition method ( section 102 ) hierarchical method ( section 103 ) density-based method ( section 104 ) grid-based method ( section 105 ) section 106 briefly discuss evaluate 1 datum similarity dissimilarity discuss detail section may want refer section quick review datum mining concept technique doi b978-0-12-381479-100010-1 c 2012 elsevier right re-serve 443 
444 chapter 10 cluster analysis basic concept method cluster method discussion advanced method cluster re-serve chapter 11 101 cluster analysis section set groundwork study cluster analysis section 1011 define cluster analysis present example useful section 1012 learn aspect compare cluster method well requirement cluster overview basic cluster technique present section 1013 1011 cluster analysis cluster analysis simply cluster process partition set datum object ( observation ) subset subset cluster object cluster similar one another yet dissimilar object cluster set cluster result cluster analysis refer cluster context different cluster method may generate different clustering datum set partition perform human cluster algorithm hence cluster useful lead discovery previously unknown group within datum cluster analysis widely used many application business intelligence image pattern recognition web search biology security business intelligence cluster used organize large number customer group customer within group share strong similar characteristic facilitate development business strategy enhance customer relationship management moreover consider consultant company large number project improve project management cluster apply partition project category base similarity project audit diagnosis ( improve project delivery outcome ) conduct effectively image recognition cluster used discover cluster “ subclass ” handwritten character recognition system suppose datum set handwritten digit digit labele either 1 2 3 note large variance way person write digit take number 2 example person may write small circle left bottom part other may use cluster determine subclass “ 2 ” represent variation way 2 written used multiple model base subclass improve overall recognition accuracy cluster also find many application web search example keyword search may often return large number hit ( ie page relevant search ) due extremely large number web page cluster used organize search result group present result concise easily accessible way moreover cluster technique develop cluster document topic commonly used information retrieval practice 
101 cluster analysis 445 datum mining function cluster analysis used standalone tool gain insight distribution datum observe characteristic cluster focus particular set cluster analysis alternatively may serve preprocess step algorithms characterization attribute subset selection classification would operate detected cluster select attribute feature cluster collection datum object similar one another within cluster dissimilar object cluster cluster datum object treat implicit class sense cluster sometimes call automatic classification critical difference cluster automatically find grouping distinct advantage cluster analysis cluster also call datum segmentation application cluster partition large datum set group accord similarity cluster also used outlier detection outlier ( value “ far away ” cluster ) may interesting common case application outlier detection include detection credit card fraud monitoring criminal activity electronic commerce example exceptional case credit card transaction expensive infrequent purchase may interest possible fraudulent activity outlier detection subject chapter 12 datum cluster vigorous development contribute area research include datum mining statistic machine learn spatial database technology information retrieval web search biology marketing many application area owing huge amount datum collect databasis cluster analysis recently become highly active topic datum mining research branch statistic cluster analysis extensively study main focus distance-based cluster analysis cluster analysis tool base k-mean k-medoid several method also build many statistical analysis software package system s-plus spss sas machine learn recall classification know supervised learn class label information give learn algorithm supervised tell class membership training tuple cluster know unsupervised learn class label information present reason cluster form learn observation rather learn example datum mining effort focuse find method efficient effective cluster analysis large databasis active theme research focus scalability cluster method effectiveness method cluster complex shape ( eg nonconvex ) type datum ( eg text graph image ) high-dimensional cluster technique ( eg cluster object thousand feature ) method cluster mixed numerical nominal datum large databasis 1012 requirement cluster analysis cluster challenge research field section learn requirement cluster datum mining tool well aspect used compare cluster method 
446 chapter 10 cluster analysis basic concept method follow typical requirement cluster datum mining scalability many cluster algorithms work well small datum set contain fewer several hundred datum object however large database may contain million even billion object particularly web search scenario cluster sample give large datum set may lead bias result therefore highly scalable cluster algorithms need ability deal different type attribute many algorithms design cluster numeric ( interval-based ) datum however application may require cluster datum type binary nominal ( categorical ) ordinal datum mixture datum type recently application need cluster technique complex datum type graph sequence image document discovery cluster arbitrary shape many cluster algorithms determine cluster base euclidean manhattan distance measure ( chapter 2 ) algorithms base distance measure tend find spherical cluster similar size density however cluster can shape consider sensor example often deploy environment surveillance cluster analysis sensor reading detect interesting phenomena may want use cluster find frontier run forest fire often spherical important develop algorithms detect cluster arbitrary shape requirement domain knowledge determine input parameter many cluster algorithms require user provide domain knowledge form input parameter desire number cluster consequently cluster result may sensitive parameter parameter often hard determine especially high-dimensionality datum set user yet grasp deep understand datum require specification domain knowledge burden user also make quality cluster difficult control ability deal noisy datum real-world datum set contain outlier or miss unknown erroneous datum sensor reading example often noisy—some reading may inaccurate due sense mechanism reading may erroneous due interference surround transient object cluster algorithms sensitive noise may produce poor-quality cluster therefore need cluster method robust noise incremental cluster insensitivity input order many application incremental update ( represent newer datum ) may arrive time cluster algorithms incorporate incremental update exist cluster structure instead recompute new cluster scratch cluster algorithms may also sensitive input datum order give set datum object cluster algorithms may return dramatically different clustering depend order object present incremental cluster algorithms algorithms insensitive input order need 
101 cluster analysis 447 capability cluster high-dimensionality datum datum set contain numerous dimension attribute cluster document example keyword regard dimension often thousand keyword cluster algorithms good handle low-dimensional datum datum set involve two three dimension find cluster datum object highdimensional space challenge especially consider datum sparse highly skewer constraint-based cluster real-world application may need perform cluster various kind constraint suppose job choose location give number new automatic teller machine ( atms ) city decide upon may cluster household consider constraint city ’ river highway network type number customer per cluster challenge task find datum group good cluster behavior satisfy specify constraint interpretability usability user want cluster result interpretable comprehensible usable cluster may need tie specific semantic interpretation application important study application goal may influence selection cluster feature cluster method follow orthogonal aspect cluster method compare partition criterium method object partition hierarchy exist among cluster cluster level conceptually method useful example partition customer group group manager alternatively method partition datum object hierarchically cluster form different semantic level example text mining may want organize corpus document multiple general topic “ politic ” “ sport ” may subtopic instance “ football ” “ basketball ” “ baseball ” “ hockey ” exist subtopic “ ” latter four subtopic lower level hierarchy “ sport ” separation cluster method partition datum object mutually exclusive cluster cluster customer group group take care one manager customer may belong one group situation cluster may exclusive datum object may belong one cluster example cluster document topic document may related multiple topic thus topic cluster may exclusive similarity measure method determine similarity two object distance distance defined euclidean space 
448 chapter 10 cluster analysis basic concept method road network vector space space method similarity may defined connectivity base density contiguity may rely absolute distance two object similarity measure play fundamental role design cluster method distance-based method often take advantage optimization technique - continuity-based method often find cluster arbitrary shape cluster space many cluster method search cluster within entire give datum space method useful low-dimensionality datum set highdimensional datum however many irrelevant attribute make similarity measurement unreliable consequently cluster find full space often meaningless ’ often better instead search cluster within different subspace datum set subspace cluster discover cluster subspace ( often low dimensionality ) manifest object similarity conclude cluster algorithms several requirement factor include scalability ability deal different type attribute noisy datum incremental update cluster arbitrary shape constraint interpretability usability also important addition cluster method differ respect partition level whether cluster mutually exclusive similarity measure used whether subspace cluster perform 1013 overview basic cluster method many cluster algorithms literature difficult provide crisp categorization cluster method category may overlap method may feature several category nevertheless useful present relatively organized picture cluster method general major fundamental cluster method classify follow category discuss rest chapter partition method give set n object partition method construct k partition datum partition represent cluster k ≤ n divide datum k group group must contain least one object word partition method conduct one-level partition datum set basic partition method typically adopt exclusive cluster separation object must belong exactly one group requirement may relax example fuzzy partition technique reference technique give bibliographic note ( section 109 ) partition method distance-based give k number partition construct partition method create initial partition used iterative relocation technique attempt improve partition move object one group another general criterion good partition object cluster “ close ” related whereas object different cluster “ far apart ” different various kind 
101 cluster analysis 449 criterium judge quality partition traditional partition method extend subspace cluster rather search full datum space useful many attribute datum sparse achieve global optimality partitioning-based cluster often computationally prohibitive potentially require exhaustive enumeration possible partition instead application adopt popular heuristic method greedy approach like k-mean k-medoid algorithms progressively improve cluster quality approach local optimum heuristic cluster method work well find spherical-shap cluster - medium-size databasis find cluster complex shape large datum set partitioning-based method need extend partitioning-based cluster method study depth section 102 hierarchical method hierarchical method create hierarchical decomposition give set datum object hierarchical method classify either agglomerative divisive base hierarchical decomposition form agglomerative approach also call bottom-up approach start object form separate group successively merge object group close one another group merged one ( topmost level hierarchy ) termination condition hold divisive approach also call top-down approach start object cluster successive iteration cluster split smaller cluster eventually object one cluster termination condition hold hierarchical cluster method distance-based - continuitybased various extension hierarchical method consider cluster subspace well hierarchical method suffer fact step ( merge split ) do never undo rigidity useful lead smaller computation cost worry combinatorial number different choice technique correct erroneous decision however method improve quality hierarchical cluster propose hierarchical cluster method study section 103 density-based method partition method cluster object base distance object method find spherical-shap cluster encounter difficulty discover cluster arbitrary shape cluster method develop base notion density general idea continue grow give cluster long density ( number object datum point ) “ neighborhood ” exceed threshold example datum point within give cluster neighborhood give radius contain least minimum number point method used filter noise outlier discover cluster arbitrary shape density-based method divide set object multiple exclusive cluster hierarchy cluster typically density-based method consider exclusive cluster consider fuzzy cluster moreover density-based method extend full space subspace cluster density-based cluster method study section 104 
450 chapter 10 cluster analysis basic concept method grid-based method grid-based method quantize object space finite number cell form grid structure cluster operation perform grid structure ( ie quantized space ) main advantage approach fast process time typically independent number datum object dependent number cell dimension quantized space used grid often efficient approach many spatial datum mining problem include cluster therefore grid-based method integrate cluster method density-based method hierarchical method gridbase cluster study section 105 method briefly summarize figure cluster algorithms integrate idea several cluster method sometimes difficult classify give algorithm uniquely belong one cluster method category furthermore application may cluster criterium require integration several cluster technique follow section examine cluster method detail advanced cluster method related issue discuss chapter general notation used follow let datum set n object cluster object describe variable variable also call attribute dimension method partition method general characteristic – find mutually exclusive cluster spherical shape – distance-based – may use mean medoid ( etc ) represent cluster center – effective - medium-size datum set hierarchical method – cluster hierarchical decomposition ( ie multiple level ) – correct erroneous merge split – may incorporate technique like microcluster consider object “ linkage ” density-based method – find arbitrarily shape cluster – cluster dense region object space separated low-density region – cluster density point must minimum number point within “ neighborhood ” – may filter outlier grid-based method – use multiresolution grid datum structure – fast process time ( typically independent number datum object yet dependent grid size ) figure 101 overview cluster method discuss chapter note algorithms may combine various method 
102 partition method 451 therefore may also refer point d-dimensional object space object represent bold italic font ( eg p ) 102 partition method simplest fundamental version cluster analysis partition organize object set several exclusive group cluster keep problem specification concise assume number cluster give background knowledge parameter start point partition method formally give datum set n object k number cluster form partition algorithm organize object k partition ( k ≤ n ) partition represent cluster cluster form optimize objective partition criterion dissimilarity function base distance object within cluster “ similar ” one another “ dissimilar ” object cluster term datum set attribute section learn well-known commonly used partition methods—k-mean ( section 1021 ) k-medoid ( section 1022 ) also learn several variation classic partition method scale handle large datum set 1021 k-mean centroid-based technique suppose datum set contain n object euclidean space partition method distribute object k cluster c1 ck ci ⊂ ci ∩ cj = ∅ ( 1 ≤ j ≤ k ) objective function used assess partition quality object within cluster similar one another dissimilar object cluster objective function aim high intracluster similarity low intercluster similarity centroid-based partition technique used centroid cluster ci represent cluster conceptually centroid cluster center point centroid defined various way mean medoid object ( point ) assign cluster difference object p ∈ ci ci representative cluster measure dist ( p ci ) dist ( x ) euclidean distance two point x y quality cluster ci measure withincluster variation sum square error object ci centroid ci defined = k x x dist ( p ci ) 2 ( 101 ) i=1 p∈ci e sum square error object datum set p point space represent give object ci centroid cluster ci ( p ci multidimensional ) word object cluster distance 
452 chapter 10 cluster analysis basic concept method object cluster center square distance sum objective function try make result k cluster compact separate possible optimize within-cluster variation computationally challenge worst case would enumerate number possible partitioning exponential number cluster check within-cluster variation value show problem np-hard general euclidean space even two cluster ( ie k = 2 ) moreover problem np-hard general number cluster k even 2-d euclidean space number cluster k dimensionality space fix problem solve time ( ndk+1 log n ) n number object overcome prohibitive computational cost exact solution greedy approach often used practice prime example k-mean algorithm simple commonly used “ k-mean algorithm work ” k-mean algorithm define centroid cluster mean value point within cluster proceed follow first randomly select k object initially represent cluster mean center remain object object assign cluster similar base euclidean distance object cluster mean k-mean algorithm iteratively improve within-cluster variation cluster compute new mean used object assign cluster previous iteration object reassign used update mean new cluster center iteration continue assignment stable cluster form current round form previous round k-mean procedure summarize figure 102 algorithm k-mean k-mean algorithm partition cluster ’ center represent mean value object cluster input k number cluster datum set contain n object output set k cluster method ( 1 ) arbitrarily choose k object initial cluster center ( 2 ) repeat ( 3 ) ( ) assign object cluster object similar base mean value object cluster ( 4 ) update cluster mean calculate mean value object cluster ( 5 ) change figure 102 k-mean partition algorithm 
102 partition method 453 + + + + + ( ) initial cluster + ( b ) iterate + + + ( c ) final cluster figure 103 cluster set object used k-mean method ( b ) update cluster center reassign object accordingly ( mean cluster marked + ) example 101 cluster k-mean partition consider set object locate 2-d space depict figure 103 ( ) let k = 3 user would like object partition three cluster accord algorithm figure 102 arbitrarily choose three object three initial cluster center cluster center marked + object assign cluster base cluster center nearest distribution form silhouette encircle dot curf show figure 103 ( ) next cluster center update mean value cluster recalculate base current object cluster used new cluster center object redistribute cluster base cluster center nearest redistribution form new silhouette encircle dash curf show figure 103 ( b ) process iterate lead figure 103 ( c ) process iteratively reassigning object cluster improve partition refer iterative relocation eventually reassignment object cluster occur process terminate result cluster return cluster process k-mean method guarantee converge global optimum often terminate local optimum result may depend initial random selection cluster center ( ask give example show exercise ) obtain good result practice common run k-mean algorithm multiple time different initial cluster center time complexity k-mean algorithm ( nkt ) n total number object k number cluster number iteration normally k n n therefore method relatively scalable efficient process large datum set several variant k-mean method differ selection initial k-mean calculation dissimilarity strategy calculate cluster mean 
454 chapter 10 cluster analysis basic concept method k-mean method apply mean set object defined may case application datum nominal attribute involved k-mode method variant k-mean extend k-mean paradigm cluster nominal datum replace mean cluster mode used new dissimilarity measure deal nominal object frequency-based method update mode cluster k-mean k-mode method integrate cluster datum mixed numeric nominal value necessity user specify k number cluster advance see disadvantage study overcome difficulty however provide approximate range k value used analytical technique determine best k compare cluster result obtain different k value k-mean method suitable discover cluster nonconvex shape cluster different size moreover sensitive noise outlier datum point small number datum substantially influence mean value “ make k-mean algorithm scalable ” one approach make k-mean method efficient large datum set use good-sized set sample cluster another employ filter approach used spatial hierarchical datum index save cost compute mean third approach explore microcluster idea first group nearby object “ microcluster ” perform k-mean cluster microcluster microcluster discuss section 103 1022 k-medoid representative object-based technique k-mean algorithm sensitive outlier object far away majority datum thus assign cluster dramatically distort mean value cluster inadvertently affect assignment object cluster effect particularly exacerbate due use squared-error function eq ( 101 ) observed example 102 example 102 drawback k-mean consider six point 1-d space value 1 2 3 8 9 10 25 respectively intuitively visual inspection may imagine point partition cluster { 1 2 3 } { 8 9 10 } point 25 exclude appear outlier would k-mean partition value apply k-mean used k = 2 eq ( 101 ) partition { { 1 2 3 } { 8 9 10 25 } } within-cluster variation ( 1 − 2 ) 2 + ( 2 − 2 ) 2 + ( 3 − 2 ) 2 + ( 8 − 13 ) 2 + ( 9 − 13 ) 2 + ( 10 − 13 ) 2 + ( 25 − 13 ) 2 = 196 give mean cluster { 1 2 3 } 2 mean { 8 9 10 25 } compare partition { { 1 2 3 8 } { 9 10 25 } } k-mean compute withincluster variation ( 1 − 35 ) 2 + ( 2 − 35 ) 2 + ( 3 − 35 ) 2 + ( 8 − 35 ) 2 + ( 9 − 1467 ) 2 + ( 10 − 1467 ) 2 + ( 25 − 1467 ) 2 = 18967 
102 partition method 455 give 35 mean cluster { 1 2 3 8 } 1467 mean cluster { 9 10 25 } latter partition lowest within-cluster variation therefore k-mean method assign value 8 cluster different contain 9 10 due outlier point moreover center second cluster 1467 substantially far member cluster “ modify k-mean algorithm diminish sensitivity outlier ” instead take mean value object cluster reference point pick actual object represent cluster used one representative object per cluster remain object assign cluster representative object similar partition method perform base principle minimize sum dissimilarity object p corresponding representative object absolute-error criterion used defined = k x x dist ( p oi ) ( 102 ) i=1 p∈ci e sum absolute error object p datum set oi representative object ci basis k-medoid method group n object k cluster minimize absolute error ( eq 102 ) k = 1 find exact median ( n2 ) time however k general positive number k-medoid problem np-hard partition around medoid ( pam ) algorithm ( see figure 105 later ) popular realization k-medoid cluster tackle problem iterative greedy way like k-mean algorithm initial representative object ( call seed ) choose arbitrarily consider whether replace representative object nonrepresentative object would improve cluster quality possible replacement try iterative process replace representative object object continue quality result cluster improve replacement quality measure cost function average dissimilarity object representative object cluster specifically let o1 ok current set representative object ( ie medoid ) determine whether nonrepresentative object denote orandom good replacement current medoid oj ( 1 ≤ j ≤ k ) calculate distance every object p closest object set { o1 oj−1 orandom oj+1 ok } use distance update cost function reassignment object { o1 oj−1 orandom oj+1 ok } simple suppose object p currently assign cluster represent medoid oj ( figure 104a b ) need reassign p different cluster oj replace orandom object p need reassign either orandom cluster represent oi ( = j ) whichever closest example figure 104 ( ) p closest oi therefore reassign oi figure 104 ( b ) however p closest orandom reassign orandom instead p currently assign cluster represent object oi = j 
456 chapter 10 cluster analysis basic concept method oi p oj orandom ( ) reassign oi oi oj p oi oj oi oj p orandom ( b ) reassign orandom ( c ) change orandom p orandom datum object cluster center swap swap ( ) reassign orandom figure 104 four case cost function k-medoid cluster object remain assign cluster represent oi long still closer oi orandom ( figure 104c ) otherwise reassign orandom ( figure 104d ) time reassignment occur difference absolute error e contribute cost function therefore cost function calculate difference absolute-error value current representative object replace nonrepresentative object total cost swap sum cost incur nonrepresentative object total cost negative oj replace swap orandom actual absolute-error e reduce total cost positive current representative object oj consider acceptable nothing change iteration “ method robust—k-mean k-medoid ” k-medoid method robust k-mean presence noise outlier medoid less influenced outlier extreme value mean however complexity iteration k-medoid algorithm ( k ( n − k ) 2 ) large value n k computation become costly much costly k-mean method method require user specify k number cluster “ scale k-medoid method ” typical k-medoid partition algorithm like pam ( figure 105 ) work effectively small datum set scale well large datum set deal larger datum set sampling-based method call clara ( cluster large application ) used instead take whole datum set consideration clara used random sample datum set pam algorithm apply compute best medoid sample ideally sample closely represent original datum set many case large sample work well create object equal probability select sample representative object ( medoid ) choose likely similar would choose whole datum set clara build clustering multiple random sample return best cluster output complexity compute medoid random sample ( ks 2 + k ( n − k ) ) size sample k number cluster n total number object clara deal larger datum set pam effectiveness clara depend sample size notice pam search best k-medoid among give datum set whereas clara search best k-medoid among select sample datum set clara find good cluster best sample medoid far best k-medoid object 
103 hierarchical method 457 algorithm k-medoid pam k-medoid algorithm partition base medoid central object input k number cluster datum set contain n object output set k cluster method ( 1 ) arbitrarily choose k object initial representative object seed ( 2 ) repeat ( 3 ) assign remain object cluster nearest representative object ( 4 ) randomly select nonrepresentative object orandom ( 5 ) compute total cost swap representative object oj orandom ( 6 ) < 0 swap oj orandom form new set k representative object ( 7 ) change figure 105 pam k-medoid partition algorithm one best k-medoid select sampling clara never find best cluster ( ask provide example demonstrate exercise ) “ might improve quality scalability clara ” recall search better medoid pam examine every object datum set every current medoid whereas clara confine candidate medoid random sample datum set randomize algorithm call claran ( cluster large application base upon randomize search ) present trade-off cost effectiveness used sample obtain cluster first randomly select k object datum set current medoid randomly select current medoid x object one current medoid replace x improve absolute-error criterion yes replacement make claran conduct randomize search l time set current medoid l step consider local optimum claran repeat randomize process time return best local optimal final result 103 hierarchical method partition method meet basic cluster requirement organize set object number exclusive group situation may want partition datum group different level hierarchy hierarchical cluster method work grouping datum object hierarchy “ tree ” cluster represent datum object form hierarchy useful datum summarization visualization example manager human resource allelectronic 
458 chapter 10 cluster analysis basic concept method may organize employee major group executive manager staff partition group smaller subgroup instance general group staff divide subgroup senior officer officer trainee group form hierarchy easily summarize characterize datum organized hierarchy used find say average salary manager officer consider handwritten character recognition another example set handwriting sample may first partition general group group correspond unique character group partition subgroup since character may written multiple substantially different way necessary hierarchical partition continue recursively desire granularity reach previous example although partition datum hierarchically assume datum hierarchical structure ( eg manager level allelectronic hierarchy staff ) use hierarchy summarize represent underlie datum compress way hierarchy particularly useful datum visualization alternatively application may believe datum bear underlie hierarchical structure want discover example hierarchical cluster may uncover hierarchy allelectronic employee structure say salary study evolution hierarchical cluster may group animal accord biological feature uncover evolutionary path hierarchy species another example grouping configuration strategic game ( eg chess checker ) hierarchical way may help develop game strategy used train player section study hierarchical cluster method section 1031 begin discussion agglomerative versus divisive hierarchical cluster organize object hierarchy used bottom-up top-down strategy respectively agglomerative method start individual object cluster iteratively merged form larger cluster conversely divisive method initially let give object form one cluster iteratively split smaller cluster hierarchical cluster method encounter difficulty regard selection merge split point decision critical group object merged split process next step operate newly generate cluster neither undo do previously perform object swap cluster thus merge split decision well choose may lead low-quality cluster moreover method scale well decision merge split need examine evaluate many object cluster promising direction improve cluster quality hierarchical method integrate hierarchical cluster cluster technique result multiple-phase ( multiphase ) cluster introduce two method namely birch chameleon birch ( section 1033 ) begin partition object hierarchically used tree structure leaf low-level nonleaf node view “ microcluster ” depend resolution scale apply 
103 hierarchical method 459 cluster algorithms perform macrocluster microcluster chameleon ( section 1034 ) explore dynamic modele hierarchical cluster several orthogonal way categorize hierarchical cluster method instance may categorize algorithmic method probabilistic method bayesian method agglomerative divisive multiphase method algorithmic meaning consider datum object deterministic compute cluster accord deterministic distance object probabilistic method use probabilistic model capture cluster measure quality cluster fitness model discuss probabilistic hierarchical cluster section bayesian method compute distribution possible clustering instead output single deterministic cluster datum set return group cluster structure probability conditional give datum bayesian method consider advanced topic discuss book 1031 agglomerative versus divisive hierarchical cluster hierarchical cluster method either agglomerative divisive depend whether hierarchical decomposition form bottom-up ( merge ) topdown ( splitting ) fashion let ’ closer look strategy agglomerative hierarchical cluster method used bottom-up strategy typically start let object form cluster iteratively merge cluster larger larger cluster object single cluster certain termination condition satisfied single cluster become hierarchy ’ root merge step find two cluster closest ( accord similarity measure ) combine two form one cluster two cluster merged per iteration cluster contain least one object agglomerative method require n iteration divisive hierarchical cluster method employ top-down strategy start place object one cluster hierarchy ’ root divide root cluster several smaller subcluster recursively partition cluster smaller one partition process continue cluster lowest level coherent enough—either contain one object object within cluster sufficiently similar either agglomerative divisive hierarchical cluster user specify desire number cluster termination condition example 103 agglomerative versus divisive hierarchical cluster figure 106 show application agne ( agglomerative nest ) agglomerative hierarchical cluster method diana ( divisive analysis ) divisive hierarchical cluster method datum set five object { b c e } initially agne agglomerative method place object cluster cluster merged step-by-step accord criterion example cluster c1 c2 may merged object c1 object c2 form minimum euclidean distance two object 
chapter 10 cluster analysis basic concept method agglomerative ( agne ) step 0 step 1 step 2 step 3 step 4 ab b abcde c cde de e step 4 step 3 step 2 step 1 divisive ( diana ) step 0 figure 106 agglomerative divisive hierarchical cluster datum object { b c e } level l=0 b c e 10 l=1 l=2 06 l=3 04 l=4 02 08 similarity scale 460 00 figure 107 dendrogram representation hierarchical cluster datum object { b c e } different cluster single-linkage approach cluster represent object cluster similarity two cluster measure similarity closest pair datum point belong different cluster cluster-merge process repeat object eventually merged form one cluster diana divisive method proceed contrast way object used form one initial cluster cluster split accord principle maximum euclidean distance closest neighboring object cluster cluster-split process repeat eventually new cluster contain single object tree structure call dendrogram commonly used represent process hierarchical cluster show object group together ( agglomerative method ) partition ( divisive method ) step-by-step figure 107 show dendrogram five object present figure 106 l = 0 show five object singleton cluster level l = 1 object b group together form 
103 hierarchical method 461 first cluster stay together subsequent level also use vertical axis show similarity scale cluster example similarity two group object { b } { c e } roughly 016 merged together form single cluster challenge divisive method partition large cluster several smaller one example 2n−1 − 1 possible way partition set n object two exclusive subset n number object n large computationally prohibitive examine possibility consequently divisive method typically used heuristic partition lead inaccurate result sake efficiency divisive method typically backtrack partition decision make cluster partition alternative partition cluster consider due challenge divisive method many agglomerative method divisive method 1032 distance measure algorithmic method whether used agglomerative method divisive method core need measure distance two cluster cluster generally set object four widely used measure distance cluster follow p − p0 | distance two object point p p0 mi mean cluster ci ni number object ci also know linkage measure minimum distance distmin ( ci cj ) = maximum distance distmax ( ci cj ) = mean distance average distance min { p − p0 | } ( 103 ) max { p − p0 | } ( 104 ) p∈ci p0 ∈cj p∈ci p0 ∈cj distmean ( ci cj ) = mi − mj | distavg ( ci cj ) = 1 ni nj x ( 105 ) p − p0 | ( 106 ) p∈ci p0 ∈cj algorithm used minimum distance dmin ( ci cj ) measure distance cluster sometimes call nearest-neighbor cluster algorithm moreover cluster process terminate distance nearest cluster exceed user-defined threshold call single-linkage algorithm view datum point node graph edge form path node cluster merge two cluster ci cj correspond add edge nearest pair node ci cj edge link cluster always go distinct cluster result graph generate tree thus agglomerative hierarchical cluster algorithm used minimum distance measure also call 
462 chapter 10 cluster analysis basic concept method minimal span tree algorithm span tree graph tree connect vertex minimal span tree one least sum edge weight algorithm used maximum distance dmax ( ci cj ) measure distance cluster sometimes call farthest-neighbor cluster algorithm cluster process terminate maximum distance nearest cluster exceed user-defined threshold call complete-linkage algorithm view datum point node graph edge link node think cluster complete subgraph edge connect node cluster distance two cluster determine distant node two cluster farthest-neighbor algorithms tend minimize increase diameter cluster iteration true cluster rather compact approximately equal size method produce high-quality cluster otherwise cluster produce meaningless previous minimum maximum measure represent two extreme measure distance cluster tend overly sensitive outlier noisy datum use mean average distance compromise minimum maximum distance overcome outlier sensitivity problem whereas mean distance simplest compute average distance advantageous handle categoric well numeric datum computation mean vector categoric datum difficult impossible define example 104 single versus complete linkage let us apply hierarchical cluster datum set figure 108 ( ) figure 108 ( b ) show dendrogram used single linkage figure 108 ( c ) show case used complete linkage edge cluster { b j h } { c g f e } omitted ease presentation example show used single linkage find hierarchical cluster defined local proximity whereas complete linkage tend find cluster opt global closeness variation four essential linkage measure discuss example measure distance two cluster distance centroid ( ie central object ) cluster 1033 birch multiphase hierarchical cluster used cluster feature tree balanced iterative reduce cluster used hierarchy ( birch ) design cluster large amount numeric datum integrate hierarchical cluster ( initial microcluster stage ) cluster method iterative partition ( later macrocluster stage ) overcome two difficulty agglomerative cluster method ( 1 ) scalability ( 2 ) inability undo do previous step birch used notion cluster feature summarize cluster cluster feature tree ( cf-tree ) represent cluster hierarchy structure help 
103 hierarchical method b c 463 e j h g f ( ) datum set b c e j h g f b c e f g h j c ( b ) cluster used single linkage b c e j h g f b h j e f g ( c ) cluster used complete linkage figure 108 hierarchical cluster used single complete linkage cluster method achieve good speed scalability large even stream databasis also make effective incremental dynamic cluster incoming object consider cluster n d-dimensional datum object point cluster feature ( cf ) cluster 3-d vector summarize information cluster object defined cf = hn ls ssi ( 107 ) p ls linear n point ( ie ni=1 xi ) ss square sum pn sum datum point ( ie i=1 xi 2 ) cluster feature essentially summary statistic give cluster used cluster feature easily derive many useful statistic cluster example cluster ’ centroid x0 radius r diameter n p x0 = i=1 n xi = ls n ( 108 ) 
464 chapter 10 cluster analysis basic concept method = = v u n ux u ( xi − x0 ) 2 u i=1 n = v ux n x n u u ( xi − xj ) 2 u i=1 j=1 n ( n − 1 ) nss − 2ls2 + nls n2 = 2nss − 2ls2 n ( n − 1 ) ( 109 ) ( 1010 ) r average distance member object centroid average pairwise distance within cluster r reflect tightness cluster around centroid summarize cluster used cluster feature avoid store detailed information individual object point instead need constant size space store cluster feature key birch efficiency space moreover cluster feature additive two disjoint cluster c1 c2 cluster feature cf1 = hn1 ls1 ss1 cf2 = hn2 ls2 ss2 respectively cluster feature cluster form merge c1 c2 simply cf1 + cf2 = hn1 + n2 ls1 + ls2 ss1 + ss2 ( 1011 ) example 105 cluster feature suppose three point ( 2 5 ) ( 3 2 ) ( 4 3 ) cluster c1 cluster feature c1 cf1 = h3 ( 2 + 3 + 4 5 + 2 + 3 ) ( 22 + 32 + 42 52 + 22 + 32 ) = h3 ( 9 10 ) ( 29 38 ) suppose c1 disjoint second cluster c2 cf2 = h3 ( 35 36 ) ( 417 440 ) cluster feature new cluster c3 form merge c1 c2 derive add cf1 cf2 cf3 = h3 + 3 ( 9 + 35 10 + 36 ) ( 29 + 417 38 + 440 ) = h6 ( 44 46 ) ( 446 478 ) cf-tree height-balanced tree store cluster feature hierarchical cluster example show figure definition nonleaf node tree descendant “ ” nonleaf node store sum cfs child thus summarize cluster information child cf-tree two parameter branch factor b threshold t branch factor specify maximum number child per nonleaf node threshold parameter specify maximum diameter subcluster store leaf node tree two parameter implicitly control result tree ’ size give limit amount main memory important consideration birch minimize time require output ( o ) birch apply multiphase cluster technique single scan datum set yield basic good cluster 
103 hierarchical method cf1 cf11 cf12 cf2 cf1k cfk 465 root level first level figure 109 cf-tree structure one additional scan optionally used improve quality primary phase phase 1 birch scan database build initial in-memory cf-tree view multilevel compression datum try preserve datum ’ inherent cluster structure phase 2 birch apply ( select ) cluster algorithm cluster leaf node cf-tree remove sparse cluster outlier group dense cluster larger one phase 1 cf-tree build dynamically object insert thus method incremental object insert closest leaf entry ( subcluster ) diameter subcluster store leaf node insertion larger threshold value leaf node possibly node split insertion new object information object pass toward root tree size cf-tree change modify threshold size memory need store cf-tree larger size main memory larger threshold value specify cf-tree rebuild rebuild process perform build new tree leaf node old tree thus process rebuild tree do without necessity reread object point similar insertion node split construction b+-tree therefore build tree datum read heuristic method introduce deal outlier improve quality cf-tree additional scan datum cf-tree build cluster algorithm typical partition algorithm used cf-tree phase 2 “ effective birch ” time complexity algorithm ( n ) n number object cluster experiment show linear scalability algorithm respect number object good quality cluster datum however since node cf-tree hold limit number entry due size cf-tree node always correspond user may consider natural cluster moreover cluster spherical shape birch perform well used notion radius diameter control boundary cluster 
466 chapter 10 cluster analysis basic concept method idea cluster feature cf-tree apply beyond birch idea borrow many other tackle problem cluster stream dynamic datum 1034 chameleon multiphase hierarchical cluster used dynamic modele chameleon hierarchical cluster algorithm used dynamic modele determine similarity pair cluster chameleon cluster similarity assessed base ( 1 ) well connect object within cluster ( 2 ) proximity cluster two cluster merged interconnectivity high close together thus chameleon depend static user-supplied model automatically adapt internal characteristic cluster merged merge process facilitate discovery natural homogeneous cluster apply datum type long similarity function specify figure 1010 illustrate chameleon work chameleon used k-nearest-neighbor graph approach construct sparse graph vertex graph represent datum object exist edge two vertex ( object ) one object among k-most similar object edge weight reflect similarity object chameleon used graph partition algorithm partition k-nearest-neighbor graph large number relatively small subcluster minimize edge cut cluster c partition subcluster ci cj minimize weight edge would cut c bisect ci cj assess absolute interconnectivity cluster ci cj chameleon used agglomerative hierarchical cluster algorithm iteratively merge subcluster base similarity determine pair similar subcluster take account interconnectivity closeness cluster specifically chameleon determine similarity pair cluster ci cj accord relative interconnectivity ri ( ci cj ) relative closeness rc ( ci cj ) relative interconnectivity ri ( ci cj ) two cluster ci cj defined absolute interconnectivity ci cj normalize respect k-nearest-neighbor graph datum set construct sparse graph partition graph final cluster merge partition figure 1010 chameleon hierarchical cluster base k-nearest neighbor dynamic modele source base karypis han kumar [ khk99 ] 
103 hierarchical method 467 internal interconnectivity two cluster ci cj ri ( ci cj ) = ec { ci cj } | 1 2 ( ecci | + eccj | ) ( 1012 ) ec { ci cj } edge cut previously defined cluster contain ci cj similarly ecci ( eccj ) minimum sum cut edge partition ci ( cj ) two roughly equal part relative closeness rc ( ci cj ) pair cluster ci cj absolute closeness ci cj normalize respect internal closeness two cluster ci cj defined rc ( ci cj ) = sec { ci cj } ci | ci cj | ec ci c | j + ci c sec cj | ( 1013 ) sec { ci cj } average weight edge connect vertex ci vertex cj sec ci ( sec cj ) average weight edge belong mincut bisector cluster ci ( cj ) chameleon show greater power discover arbitrarily shape cluster high quality several well-known algorithms birch densitybased dbscan ( section 1041 ) however process cost high-dimensional datum may require ( n2 ) time n object worst case 1035 probabilistic hierarchical cluster algorithmic hierarchical cluster method used linkage measure tend easy understand often efficient cluster commonly used many cluster analysis application however algorithmic hierarchical cluster method suffer several drawback first choose good distance measure hierarchical cluster often far trivial second apply algorithmic method datum object miss attribute value case datum partially observed ( ie attribute value object miss ) easy apply algorithmic hierarchical cluster method distance computation conduct third algorithmic hierarchical cluster method heuristic step locally search good splitting decision consequently optimization goal result cluster hierarchy unclear probabilistic hierarchical cluster aim overcome disadvantage used probabilistic model measure distance cluster one way look cluster problem regard set datum object cluster sample underlie datum generation mechanism analyze formally generative model example conduct cluster analysis set marketing survey assume survey collect sample opinion possible customer datum generation mechanism probability 
468 chapter 10 cluster analysis basic concept method distribution opinion respect different customer obtain directly completely task cluster estimate generative model accurately possible used observed datum object cluster practice assume datum generative model adopt common distribution function gaussian distribution bernoulli distribution govern parameter task learn generative model reduce find parameter value model best fit observed datum set example 106 generative model suppose give set 1-d point x = { x1 xn } cluster analysis let us assume datum point generate gaussian distribution n ( µ σ 2 ) = √ 2 1 2π σ 2 e − ( x−µ ) 2 2σ ( 1014 ) parameter µ ( mean ) σ 2 ( variance ) probability point xi ∈ x generate model ( x −µ ) 2 1 − e 2σ 2 p ( xi µ σ 2 ) = √ 2π σ 2 ( 1015 ) consequently likelihood x generate model l ( n ( µ σ 2 ) x ) = p ( x|µ σ 2 ) = n i=1 √ 1 2π σ 2 e − ( xi −µ ) 2 2σ 2 ( 1016 ) task learn generative model find parameter µ σ 2 likelihood l ( n ( µ σ 2 ) x ) maximize find n ( µ0 σ02 ) = arg max { l ( n ( µ σ 2 ) x ) } ( 1017 ) max { l ( n ( µ σ 2 ) x ) } call maximum likelihood give set object quality cluster form object measure maximum likelihood set object partition cluster c1 cm quality measure q ( { c1 cm } ) = i=1 p ( ci ) ( 1018 ) 
103 hierarchical method 469 p ( ) maximum likelihood merge two cluster cj1 cj2 cluster cj1 ∪ cj2 change quality overall cluster q ( ( { c1 cm } − { cj1 cj2 } ) ∪ { cj1 ∪ cj2 } ) − q ( { c1 cm } ) qm p ( ci ) · p ( cj1 ∪ cj2 ) = i=1 − p ( ci ) p ( cj1 ) p ( cj2 ) i=1 = i=1  p ( cj1 ∪ cj2 ) −1 p ( ci ) p ( cj1 ) p ( cj2 )  ( 1019 ) q choose merge two cluster hierarchical cluster i=1 p ( ci ) constant pair cluster therefore give cluster c1 c2 distance measure dist ( ci cj ) = − log p ( c1 ∪ c2 ) p ( c1 ) p ( c2 ) ( 1020 ) probabilistic hierarchical cluster method adopt agglomerative cluster framework use probabilistic model ( eq 1020 ) measure distance cluster upon close observation eq ( 1019 ) see merge two cluster may p ( c ∪c ) always lead improvement cluster quality p ( cj j1 ) p ( cj2j ) may less 1 2 example assume gaussian distribution function used model figure although merge cluster c1 c2 result cluster better fit gaussian distribution merge cluster c3 c4 lower cluster quality gaussian function fit merged cluster well base observation probabilistic hierarchical cluster scheme start one cluster per object merge two cluster ci cj distance negative iteration try find ci cj maximize p ( c ∪c ) p ( c ∪c ) j j log p ( ci ) p ( c iteration continue long log p ( ci ) p ( c > 0 long j ) j ) improvement cluster quality pseudocode give figure 1012 probabilistic hierarchical cluster method easy understand generally efficiency algorithmic agglomerative hierarchical cluster method fact share framework probabilistic model interpretable sometimes less flexible distance metric probabilistic model handle partially observed datum example give multidimensional datum set object miss value dimension learn gaussian model dimension independently used observed value dimension result cluster hierarchy accomplish optimization goal fitting datum select probabilistic model drawback used probabilistic hierarchical cluster output one hierarchy respect choose probabilistic model handle uncertainty cluster hierarchy give datum set may exist multiple hierarchy 
470 chapter 10 cluster analysis basic concept method c1 c2 ( ) c3 c4 ( b ) ( c ) figure 1011 merge cluster probabilistic hierarchical cluster ( ) merge cluster c1 c2 lead increase overall cluster quality merge cluster ( b ) c3 ( c ) c4 algorithm probabilistic hierarchical cluster algorithm input = { o1 } datum set contain n object output hierarchy cluster method ( 1 ) create cluster object ci = { oi } 1 ≤ ≤ n ( 2 ) = 1 n p ( c ∪c ) ( 3 ) j find pair cluster ci cj ci cj = arg maxi6=j log p ( c ) p ( c ) ( 4 ) j log p ( c ) p ( c ) > 0 merge ci cj ( 5 ) else stop p ( c ∪c ) j j figure 1012 probabilistic hierarchical cluster algorithm fit observed datum neither algorithmic approach probabilistic approach find distribution hierarchy recently bayesian tree-structure model develop handle problem bayesian sophisticated probabilistic cluster method consider advanced topic cover book 
104 density-based method 104 471 density-based method partition hierarchical method design find spherical-shap cluster difficulty find cluster arbitrary shape “ ” shape oval cluster figure give datum would likely inaccurately identify convex region noise outlier include cluster find cluster arbitrary shape alternatively model cluster dense region datum space separated sparse region main strategy behind density-based cluster method discover cluster nonspherical shape section learn basic technique density-based cluster study three representative method namely dbscan ( section 1041 ) optic ( section 1042 ) denclue ( section 1043 ) 1041 dbscan density-based cluster base connect region high density “ find dense region density-based cluster ” density object measure number object close o dbscan ( density-based spatial cluster application noise ) find core object object dense neighborhood connect core object neighborhood form dense region cluster “ dbscan quantify neighborhood object ” user-specified parameter  > 0 used specify radius neighborhood consider every object -neighborhood object space within radius  center due fix neighborhood size parameterized  density neighborhood measure simply number object neighborhood determine whether neighborhood dense dbscan used another user-specified figure 1013 cluster arbitrary shape 
472 chapter 10 cluster analysis basic concept method parameter minpt specify density threshold dense region object core object -neighborhood object contain least minpt object core object pillar dense region give set object identify core object respect give parameter  minpt cluster task therein reduce used core object neighborhood form dense region dense region cluster core object q object p say p directly density-reachable q ( respect  minpt ) p within -neighborhood q clearly object p directly density-reachable another object q q core object p -neighborhood q used directly density-reachable relation core object “ bring ” object -neighborhood dense region “ assemble large dense region used small dense region center core object ” dbscan p density-reachable q ( respect  minpt ) chain object p1 pn p1 = q pn = p pi+1 directly density-reachable pi respect  minpt 1 ≤ ≤ n pi ∈ d note density-reachability equivalence relation symmetric o1 o2 core object o1 density-reachable o2 o2 density-reachable o1 however o2 core object o1 o1 may density-reachable o2 vice versa connect core object well neighbor dense region dbscan used notion density-connectedness two object p1 p2 ∈ density-connect respect  minpt object q ∈ p1 p2 densityreachable q respect  minpt unlike density-reachability densityconnectedness equivalence relation easy show object o1 o2 o3 o1 o2 density-connect o2 o3 density-connect o1 o3 example 107 density-reachability density-connectivity consider figure 1014 give  represent radius circle say let minpt = 3 labele point p r core object -neighborhood contain least three point object q directly density-reachable m object directly density-reachable p vice versa object q ( indirectly ) density-reachable p q directly densityreachable directly density-reachable p however p densityreachable q q core object similarly r density-reachable density-reachable r thus r density-connect use closure density-connectedness find connect dense region cluster close set density-based cluster subset c ⊆ cluster ( 1 ) two object o1 o2 ∈ c o1 o2 density-connect ( 2 ) exist object ∈ c another object o0 ∈ ( − c ) o0 densityconnect 
104 density-based method 473 q p r figure 1014 density-reachability density-connectivity density-based cluster source base ester kriegel sander xu [ eksx96 ] “ dbscan find cluster ” initially object give datum set marked “ ” dbscan randomly select unvisite object p mark p “ visit ” check whether -neighborhood p contain least minpt object p marked noise point otherwise new cluster c create p object -neighborhood p add candidate set n dbscan iteratively add c object n belong cluster process object p0 n carry label “ unvisite ” dbscan mark “ visit ” check -neighborhood -neighborhood p0 least minpt object object -neighborhood p0 add n dbscan continue add object c c longer expand n empty time cluster c complete thus output find next cluster dbscan randomly select unvisite object remain one cluster process continue object visit pseudocode dbscan algorithm give figure 1015 spatial index used computational complexity dbscan ( n log n ) n number database object otherwise complexity ( n2 ) appropriate setting user-defined parameter  minpt algorithm effective find arbitrary-shap cluster 1042 optic order point identify cluster structure although dbscan cluster object give input parameter  ( maximum radius neighborhood ) minpt ( minimum number point require neighborhood core object ) encumber user responsibility select parameter value lead discovery acceptable cluster problem associate many cluster algorithms parameter setting 
474 chapter 10 cluster analysis basic concept method algorithm dbscan density-based cluster algorithm input datum set contain n object  radius parameter minpt neighborhood density threshold output set density-based cluster method ( 1 ) mark object unvisite ( 2 ) ( 3 ) randomly select unvisite object p ( 4 ) mark p visit ( 5 ) -neighborhood p least minpt object ( 6 ) create new cluster c add p c ( 7 ) let n set object -neighborhood p ( 8 ) point p0 n ( 9 ) p0 unvisite ( 10 ) mark p0 visit ( 11 ) -neighborhood p0 least minpt point add point n ( 12 ) p0 yet member cluster add p0 c ( 13 ) end ( 14 ) output c ( 15 ) else mark p noise ( 16 ) object unvisite figure 1015 dbscan algorithm usually empirically set difficult determine especially real-world highdimensional datum set algorithms sensitive parameter value slightly different setting may lead different clustering datum moreover real-world high-dimensional datum set often skewer distribution intrinsic cluster structure may well characterize single set global density parameter note density-based cluster monotonic respect neighborhood threshold dbscan fix minpt value two neighborhood threshold 1 < 2 cluster c respect 1 minpt must subset cluster c 0 respect 2 minpt mean two object density-based cluster must also cluster lower density requirement overcome difficulty used one set global parameter cluster analysis cluster analysis method call optic propose optic explicitly produce datum set cluster instead output cluster order linear list 
104 density-based method 475 object analysis represent density-based cluster structure datum object denser cluster list closer cluster order order equivalent density-based cluster obtain wide range parameter setting thus optic require user provide specific density threshold cluster order used extract basic cluster information ( eg cluster center arbitrary-shap cluster ) derive intrinsic cluster structure well provide visualization cluster construct different clustering simultaneously object processed specific order order select object density-reachable respect lowest  value cluster higher density ( lower  ) finished first base idea optic need two important piece information per object core-distance object p smallest value  0  0 neighborhood p least minpt object  0 minimum distance threshold make p core object p core object respect  minpt core-distance p undefined reachability-distance object p q minimum radius value make p density-reachable q accord definition density-reachability q core object p must neighborhood q therefore reachability-distance q p max { core-distance ( q ) dist ( p q ) } q core object respect  minpt reachability-distance p q undefined object p may directly reachable multiple core object therefore p may multiple reachability-distance respect different core object smallest reachability-distance p particular interest give shortest path p connect dense cluster example 108 core-distance reachability-distance figure 1016 illustrate concept coredistance reachability-distance suppose  = 6 mm minpt = coredistance p distance  0 p fourth closest datum object p reachability-distance q1 p core-distance p ( ie  0 = 3 mm ) greater euclidean distance p q1 reachability-distance q2 respect p euclidean distance p q2 greater core-distance p optic compute order object give database object database store core-distance suitable reachability-distance optic maintain list call orderseed generate output order object orderseed sort reachability-distance respective closest core object smallest reachability-distance object optic begin arbitrary object input database current object p retrieve -neighborhood p determine core-distance set reachability-distance undefined current object p written output 
476 chapter 10 cluster analysis basic concept method = 6 mm p = 3 mm = 6 mm  p q1 q2 core-distance p reachability-distance ( p q1 ) = = 3 mm reachability-distance ( p q2 ) = dist ( p q2 ) figure 1016 optic terminology source base ankerst breunig kriegel sander [ abks99 ] p core object optic simply move next object orderseed list ( input database orderseed empty ) p core object object q -neighborhood p optic update reachability-distance p insert q orderseed q yet processed iteration continue input fully consume orderseed empty datum set ’ cluster order represent graphically help visualize understand cluster structure datum set example figure 1017 reachability plot simple 2-d datum set present general overview datum structure cluster datum object plot cluster order ( horizontal axis ) together respective reachability-distance ( vertical axis ) three gaussian “ bump ” plot reflect three cluster datum set method also develop view cluster structure high-dimensional datum various level detail structure optic algorithm similar dbscan consequently two algorithms time complexity complexity ( n log n ) spatial index used ( n2 ) otherwise n number object 1043 denclue cluster base density distribution function density estimation core issue density-based cluster method denclue ( density-based cluster ) cluster method base set density distribution function first give background density estimation describe denclue algorithm probability statistic density estimation estimation unobservable underlie probability density function base set observed datum context density-based cluster unobservable underlie probability density function true distribution population possible object analyze observed datum set regard random sample population 
104 density-based method 477 reachability-distance undefined cluster order object figure 1017 cluster order optic source adapt ankerst breunig kriegel sander [ abks99 ] 1 2 figure 1018 subtlety density estimation dbscan optic increase neighborhood radius slightly 1 2 result much higher density dbscan optic density calculate count number object neighborhood defined radius parameter  density estimate highly sensitive radius value used example figure 1018 density change significantly radius increase small amount overcome problem kernel density estimation used nonparametric density estimation approach statistic general idea behind kernel density estimation simple treat observed object indicator 
478 chapter 10 cluster analysis basic concept method high-probability density surround region probability density point depend distance point observed object formally let x1 xn independent identically distribute sample random variable f kernel density approximation probability density function   n x − xi 1 x ( 1021 ) k fˆh ( x ) = nh h i=1 k ( ) kernel h bandwidth serve smooth parameter kernel regard function modele influence sample point within neighborhood technically kernel k ( ) isra non-negative real-valu integrable func+∞ tion satisfy two requirement −∞ k ( u ) du = 1 k ( −u ) = k ( u ) value u frequently used kernel standard gaussian function mean 0 variance 1   x − xi 1 − ( x − 2xi ) 2 2h k ( 1022 ) √ e h 2π denclue used gaussian kernel estimate density base give set object cluster point x∗ call density attractor local maximum estimate density function avoid trivial local maximum point denclue used noise threshold ξ consider density attractor x∗ fˆ ( x∗ ) ≥ ξ nontrivial density attractor center cluster object analysis assign cluster density attractor used stepwise hill-climb procedure object x hill-climb procedure start x guide gradient estimate density function density attractor x compute x0 = x xj+1 = xj + δ ∇ fˆ ( xj ) ∇ fˆ ( xj ) | ( 1023 ) δ parameter control speed convergence ∇ fˆ ( x ) = hd+2 n 1   x − xi ( x − x ) k i=1 h pn ( 1024 ) hill-climb procedure stop step k > 0 fˆ ( xk+1 ) < fˆ ( xk ) assign x density attractor x∗ = xk object x outlier noise converge hillclimb procedure local maximum x∗ fˆ ( x∗ ) < ξ cluster denclue set density attractor x set input object c object c assign density attractor x exist path every pair density attractor density ξ used multiple density attractor connect path denclue find cluster arbitrary shape 
105 grid-based method 479 denclue several advantage regard generalization several well-known cluster method single-linkage approach dbscan moreover denclue invariant noise kernel density estimation effectively reduce influence noise uniformly distribute noise input datum 105 grid-based method cluster method discuss far data-driven—they partition set object adapt distribution object embedding space alternatively grid-based cluster method take space-driven approach partition embedding space cell independent distribution input object grid-based cluster approach used multiresolution grid datum structure quantize object space finite number cell form grid structure operation cluster perform main advantage approach fast process time typically independent number datum object yet dependent number cell dimension quantized space section illustrate grid-based cluster used two typical example sting ( section 1051 ) explore statistical information store grid cell clique ( section 1052 ) represent - density-based approach subspace cluster high-dimensional datum space 1051 sting statistical information grid sting grid-based multiresolution cluster technique embedding spatial area input object divide rectangular cell space divide hierarchical recursive way several level rectangular cell correspond different level resolution form hierarchical structure cell high level partition form number cell next lower level statistical information regard attribute grid cell mean maximum minimum value precompute store statistical parameter statistical parameter useful query process datum analysis task figure 1019 show hierarchical structure sting cluster statistical parameter higher-level cell easily compute parameter lower-level cell parameter include follow attribute-independent parameter count attribute-dependent parameter mean stdev ( standard deviation ) min ( minimum ) max ( maximum ) type distribution attribute value cell follow normal uniform exponential none ( distribution unknown ) attribute select measure analysis price house object datum load database parameter count mean stdev min max bottom-level cell calculate directly datum value distribution may either assign user distribution type know 
480 chapter 10 cluster analysis basic concept method first layer ( – 1 ) st layer ith layer figure 1019 hierarchical structure sting cluster beforehand obtain hypothesis test χ 2 test type distribution higher-level cell compute base majority distribution type corresponding lower-level cell conjunction threshold filter process distribution lower-level cell disagree fail threshold test distribution type high-level cell set none “ statistical information useful query answer ” statistical parameter used top-down grid-based manner follow first layer within hierarchical structure determine query-answer process start layer typically contain small number cell cell current layer compute confidence interval ( estimate probability range ) reflect cell ’ relevancy give query irrelevant cell remove consideration process next lower level examine remain relevant cell process repeat bottom layer reach time query specification meet region relevant cell satisfy query return otherwise datum fall relevant cell retrieve processed meet query ’ requirement interesting property sting approach cluster result dbscan granularity approach 0 ( ie toward low-level datum ) word used count cell size information dense cluster identify approximately used sting therefore sting also regard density-based cluster method “ advantage sting offer cluster method ” sting offer several advantage ( 1 ) grid-based computation query-independent statistical information store cell represent summary information datum grid cell independent query ( 2 ) grid structure facilitate parallel process incremental update ( 3 ) method ’ efficiency major advantage sting go database compute statistical parameter cell hence time complexity generate cluster ( n ) n total number object generate hierarchical structure query process time 
105 grid-based method 481 ( g ) g total number grid cell lowest level usually much smaller n sting used multiresolution approach cluster analysis quality sting cluster depend granularity lowest level grid structure granularity fine cost process increase substantially however bottom level grid structure coarse may reduce quality cluster analysis moreover sting consider spatial relationship child neighboring cell construction parent cell result shape result cluster isothetic cluster boundary either horizontal vertical diagonal boundary detected may lower quality accuracy cluster despite fast process time technique 1052 clique apriori-like subspace cluster method datum object often ten attribute many may irrelevant value attribute may vary considerably factor make difficult locate cluster span entire datum space may meaningful instead search cluster within different subspace datum example consider healthinformatic application patient record contain extensive attribute describe personal information numerous symptom condition family history find nontrivial group patient even attribute strongly agree unlikely bird flu patient instance age gender job attribute may vary dramatically within wide range value thus difficult find cluster within entire datum space instead search subspace may find cluster similar patient lower-dimensional space ( eg patient similar one respect symptom like high fever cough runny nose age 3 16 ) clique ( cluster quest ) simple grid-based method find densitybased cluster subspace clique partition dimension nonoverlapping interval thereby partition entire embedding space datum object cell used density threshold identify dense cell sparse one cell dense number object map exceed density threshold main strategy behind clique identify candidate search space used monotonicity dense cell respect dimensionality base apriori property used frequent pattern association rule mining ( chapter 6 ) context cluster subspace monotonicity say follow k-dimensional cell c ( k > 1 ) least l point every ( k − 1 ) dimensional projection c cell ( k − 1 ) dimensional subspace least l point consider figure 1020 embedding datum space contain three dimension age salary vacation 2-d cell say subspace form age salary contain l point projection cell every dimension age salary respectively contain least l point clique perform cluster two step first step clique partition d-dimensional datum space nonoverlapping rectangular unit identify dense unit among clique find dense cell subspace 
482 chapter 10 cluster analysis basic concept method 7 salary ( $ 10000 ) 6 5 4 3 2 1 0 20 30 40 50 60 age 30 40 50 60 age 7 vacation ( week ) 6 5 4 3 2 1 vacation 0 20 50 age sa la ry 30 figure 1020 dense unit find respect age dimension salary vacation intersected provide candidate search space dense unit higher dimensionality 
106 evaluation cluster 483 clique partition every dimension interval identify interval contain least l point l density threshold clique iteratively join two k-dimensional dense cell c1 c2 subspace ( di1 dik ) ( dj1 djk ) respectively di1 = dj1 dik−1 = djk−1 c1 c2 share interval dimension join operation generate new ( k + 1 ) dimensional candidate cell c space ( di1 dik−1 dik djk ) clique check whether number point c pass density threshold iteration terminate candidate generate candidate cell dense second step clique used dense cell subspace assemble cluster arbitrary shape idea apply minimum description length ( mdl ) principle ( chapter 8 ) use maximal region cover connect dense cell maximal region hyperrectangle every cell fall region dense region extend dimension subspace find best description cluster general np-hard thus clique adopt simple greedy approach start arbitrary dense cell find maximal region cover cell work remain dense cell yet cover greedy method terminate dense cell cover “ effective clique ” clique automatically find subspace highest dimensionality high-density cluster exist subspace insensitive order input object presume canonical datum distribution scale linearly size input good scalability number dimension datum increase however obtain meaningful cluster dependent proper tune grid size ( stable structure ) density threshold difficult practice grid size density threshold used across combination dimension datum set thus accuracy cluster result may degraded expense method ’ simplicity moreover give dense region projection region onto lower-dimensionality subspace also dense result large overlap among report dense region furthermore difficult find cluster rather different density within different dimensional subspace several extension approach follow similar philosophy example think grid set fix bin instead used fix bin dimension use adaptive data-driven strategy dynamically determine bin dimension base datum distribution statistic alternatively instead used density threshold may use entropy ( chapter 8 ) measure quality subspace cluster 106 evaluation cluster learn cluster know several popular cluster method may ask “ try cluster method datum set evaluate whether cluster result good ” general cluster evaluation assess 
484 chapter 10 cluster analysis basic concept method feasibility cluster analysis datum set quality result generate cluster method major task cluster evaluation include follow assess cluster tendency task give datum set assess whether nonrandom structure exist datum blindly apply cluster method datum set return cluster however cluster mine may mislead cluster analysis datum set meaningful nonrandom structure datum determine number cluster datum set algorithms k-mean require number cluster datum set parameter moreover number cluster regard interesting important summary statistic datum set therefore desirable estimate number even cluster algorithm used derive detailed cluster measure cluster quality apply cluster method datum set want assess good result cluster number measure used method measure well cluster fit datum set other measure well cluster match ground truth truth available also measure score clustering thus compare two set cluster result datum set rest section discuss three topic 1061 assess cluster tendency cluster tendency assessment determine whether give datum set non-random structure may lead meaningful cluster consider datum set non-random structure set uniformly distribute point datum space even though cluster algorithm may return cluster datum cluster random meaningful example 109 cluster require nonuniform distribution datum figure 1021 show datum set uniformly distribute 2-d datum space although cluster algorithm may still artificially partition point group group unlikely mean anything significant application due uniform distribution datum “ assess cluster tendency datum set ” intuitively try measure probability datum set generate uniform datum distribution achieve used statistical test spatial randomness illustrate idea let ’ look simple yet effective statistic call hopkin statistic hopkin statistic spatial statistic test spatial randomness variable distribute space give datum set regard sample 
106 evaluation cluster 485 figure 1021 datum set uniformly distribute datum space random variable want determine far away uniformly distribute datum space calculate hopkin statistic follow sample n point p1 pn uniformly d point probability include sample point pi find nearest neighbor pi ( 1 ≤ ≤ n ) let xi distance pi nearest neighbor d xi = min { dist ( pi v ) } v∈d ( 1025 ) sample n point q1 qn uniformly d qi ( 1 ≤ ≤ n ) find nearest neighbor qi − { qi } let yi distance qi nearest neighbor − { qi } yi = min { dist ( qi v ) } v∈d v6=qi ( 1026 ) calculate hopkin statistic h pn h = pn i=1 xi i=1 yi + pn i=1 yi ( 1027 ) “ hopkin statistic tell us likely datum set follow pn uniform distribution datum space ” uniformly distribute i=1 yi pn x would close thus h would 05 however i=1 p p highly skewer ni=1 yi would substantially smaller ni=1 xi expectation thus h would close 0 
486 chapter 10 cluster analysis basic concept method null hypothesis homogeneous hypothesis—that uniformly distribute thus contain meaningful cluster nonhomogeneous hypothesis ( ie uniformly distribute thus contain cluster ) alternative hypothesis conduct hopkin statistic test iteratively used 05 threshold reject alternative hypothesis h > 05 unlikely statistically significant cluster 1062 determine number cluster determine “ right ” number cluster datum set important cluster algorithms like k-mean require parameter also appropriate number cluster control proper granularity cluster analysis regard find good balance compressibility accuracy cluster analysis consider two extreme case treat entire datum set cluster would maximize compression datum cluster analysis value hand treat object datum set cluster give finest cluster resolution ( ie accurate due zero distance object corresponding cluster center ) method like k-mean even achieve best cost however one object per cluster enable datum summarization determine number cluster far easy often “ right ” number ambiguous figure right number cluster often depend distribution ’ shape scale datum set well cluster resolution require user many possible way estimate number cluster briefly introduce simple yet popular effective method q simple method set number cluster n2 datum set n √ point expectation cluster 2n point elbow method base observation increase number cluster help reduce sum within-cluster variance cluster cluster allow one capture finer group datum object similar however marginal effect reduce sum within-cluster variance may drop many cluster form splitting cohesive cluster two give small reduction consequently heuristic select right number cluster use turn point curve sum within-cluster variance respect number cluster technically give number k > 0 form k cluster datum set question used cluster algorithm like k-mean calculate sum within-cluster variance var ( k ) plot curve var respect k first ( significant ) turn point curve suggest “ right ” number advanced method determine number cluster used information criterium information theoretic approach please refer bibliographic note information ( section 109 ) 
106 evaluation cluster 487 “ right ” number cluster datum set also determine crossvalidation technique often used classification ( chapter 8 ) first divide give datum set part next use − 1 part build cluster model use remain part test quality cluster example point test set find closest centroid consequently use sum square distance point test set closest centroid measure well cluster model fit test set integer k > 0 repeat process time derive clustering k cluster used part turn test set average quality measure take overall quality measure compare overall quality measure respect different value k find number cluster best fit datum 1063 measure cluster quality suppose assessed cluster tendency give datum set may also try predetermine number cluster set apply one multiple cluster method obtain clustering datum set “ good cluster generate method compare clustering generate different method ” method choose measure quality cluster general method categorize two group accord whether ground truth available ground truth ideal cluster often build used human expert ground truth available used extrinsic method compare cluster group truth measure ground truth unavailable use intrinsic method evaluate goodness cluster consider well cluster separated ground truth consider supervision form “ cluster ” hence extrinsic method also know supervised method intrinsic method unsupervised method let ’ look simple method category extrinsic method ground truth available compare cluster assess cluster thus core task extrinsic method assign score q ( c cg ) cluster c give ground truth cg whether extrinsic method effective largely depend measure q used general measure q cluster quality effective satisfy follow four essential criterium cluster homogeneity require pure cluster cluster better cluster suppose ground truth say object datum set belong category l1 ln consider cluster c1 wherein cluster c ∈ c1 contain object two category li lj ( 1 ≤ < j ≤ n ) also 
488 chapter 10 cluster analysis basic concept method consider cluster c2 identical c1 except c2 split two cluster contain object li lj respectively cluster quality measure q respect cluster homogeneity give higher score c2 c1 q ( c2 cg ) > q ( c1 cg ) cluster completeness counterpart cluster homogeneity cluster completeness require cluster two object belong category accord ground truth assign cluster cluster completeness require cluster assign object belong category ( accord ground truth ) cluster consider cluster c1 contain cluster c1 c2 member belong category accord ground truth let cluster c2 identical c1 except c1 c2 merged one cluster c2 cluster quality measure q respect cluster completeness give higher score c2 q ( c2 cg ) > q ( c1 cg ) rag bag many practical scenario often “ rag bag ” category contain object merged object category often call “ miscellaneous ” “ ” rag bag criterion state putt heterogeneous object pure cluster penalize putt rag bag consider cluster c1 cluster c ∈ c1 object c except one denote belong category accord ground truth consider cluster c2 identical c1 except assign cluster c 0 = c c2 c 0 contain object various category accord ground truth thus noisy word c 0 c2 rag bag cluster quality measure q respect rag bag criterion give higher score c2 q ( c2 cg ) > q ( c1 cg ) small cluster preservation small category split small piece cluster small piece may likely become noise thus small category discover cluster small cluster preservation criterion state splitting small category piece harmful splitting large category piece consider extreme case let datum set n + 2 object accord ground truth n object denote o1 belong one category two object denote on+1 on+2 belong another category suppose cluster c1 three cluster c1 = { o1 } c2 = { on+1 } c3 = { on+2 } let cluster c2 three cluster namely c1 = { o1 on−1 } c2 = { } c3 = { on+1 on+2 } word c1 split small category c2 split big category cluster quality measure q preserve small cluster give higher score c2 q ( c2 cg ) > q ( c1 cg ) many cluster quality measure satisfy four criterium introduce bcube precision recall metric satisfy four criterium bcube evaluate precision recall every object cluster give datum set accord ground truth precision object indicate many object cluster belong category object recall 
106 evaluation cluster 489 object reflect many object category assign cluster formally let = { o1 } set object c cluster d let l ( oi ) ( 1 ≤ ≤ n ) category oi give ground truth c ( oi ) cluster id oi c two object oi oj ( 1 ≤ j ≤ n = j ) correctness relation oi oj cluster c give ( 1 l ( oi ) = l ( oj ) ⇔ c ( oi ) = c ( oj ) correctness ( oi oj ) = 0 otherwise ( 1028 ) bcube precision defined x n x oj i6=j c ( oi ) c ( oj ) precision bcube = correctness ( oi oj ) k { oj i = j c ( oi ) = c ( oj ) } k i=1 n ( 1029 ) bcube recall defined x n x oj i6=j l ( oi ) l ( oj ) recall bcube = i=1 correctness ( oi oj ) k { oj i = j l ( oi ) = l ( oj ) } k n ( 1030 ) intrinsic method ground truth datum set available use intrinsic method assess cluster quality general intrinsic method evaluate cluster examine well cluster separated compact cluster many intrinsic method advantage similarity metric object datum set silhouette coefficient measure datum set n object suppose partition k cluster c1 ck object ∈ calculate ( ) average distance object cluster belong similarly b ( ) minimum average distance cluster belong formally suppose ∈ ci ( 1 ≤ ≤ k ) p ( ) = o0 ∈ci o6=o0 dist ( ) ci | − 1 0 ( 1031 ) 
490 chapter 10 cluster analysis basic concept method ( p b ( ) = min cj 1≤j≤k j6=i 0 ) o0 ∈cj dist ( ) cj | ( 1032 ) silhouette coefficient defined ( ) = b ( ) − ( ) max { ( ) b ( ) } ( 1033 ) value silhouette coefficient −1 value ( ) reflect compactness cluster belong smaller value compact cluster value b ( ) capture degree separated cluster larger b ( ) separated cluster therefore silhouette coefficient value approach 1 cluster contain compact far away cluster preferable case however silhouette coefficient value negative ( ie b ( ) < ( ) ) mean expectation closer object another cluster object cluster many case bad situation avoid measure cluster ’ fitness within cluster compute average silhouette coefficient value object cluster measure quality cluster use average silhouette coefficient value object datum set silhouette coefficient intrinsic measure also used elbow method heuristically derive number cluster datum set replace sum within-cluster variance 107 summary cluster collection datum object similar one another within cluster dissimilar object cluster process grouping set physical abstract object class similar object call cluster cluster analysis extensive application include business intelligence image pattern recognition web search biology security cluster analysis used standalone datum mining tool gain insight datum distribution preprocess step datum mining algorithms operate detected cluster cluster dynamic field research datum mining related unsupervised learn machine learn cluster challenge field typical requirement include scalability ability deal different type datum attribute discovery cluster arbitrary shape minimal requirement domain knowledge determine input parameter ability deal noisy datum incremental cluster 
108 exercise 491 insensitivity input order capability cluster high-dimensionality datum constraint-based cluster well interpretability usability many cluster algorithms develop categorize several orthogonal aspect regard partition criterium separation cluster similarity measure used cluster space chapter discuss major fundamental cluster method follow category partition method hierarchical method density-based method grid-based method algorithms may belong one category partition method first create initial set k partition parameter k number partition construct used iterative relocation technique attempt improve partition move object one group another typical partition method include k-mean k-medoid claran hierarchical method create hierarchical decomposition give set datum object method classify either agglomerative ( bottom-up ) divisive ( top-down ) base hierarchical decomposition form compensate rigidity merge split quality hierarchical agglomeration improve analyze object linkage hierarchical partition ( eg chameleon ) first perform microcluster ( grouping object “ microcluster ” ) operate microcluster cluster technique iterative relocation ( birch ) density-based method cluster object base notion density grow cluster either accord density neighborhood object ( eg dbscan ) accord density function ( eg denclue ) optic density-based method generate augment order datum ’ cluster structure grid-based method first quantize object space finite number cell form grid structure perform cluster grid structure sting typical example grid-based method base statistical information store grid cell clique grid-based subspace cluster algorithm cluster evaluation assess feasibility cluster analysis datum set quality result generate cluster method task include assess cluster tendency determine number cluster measure cluster quality 108 exercise 101 briefly describe give example follow approach cluster partition method hierarchical method density-based method grid-based method 
492 chapter 10 cluster analysis basic concept method 102 suppose datum mining task cluster point ( ( x ) represent location ) three cluster point a1 ( 2 10 ) a2 ( 2 5 ) a3 ( 8 4 ) b1 ( 5 8 ) b2 ( 7 5 ) b3 ( 6 4 ) c1 ( 1 2 ) c2 ( 4 9 ) distance function euclidean distance suppose initially assign a1 b1 c1 center cluster respectively use k-mean algorithm show ( ) three cluster center first round execution ( b ) final three cluster 103 use example show k-mean algorithm may find global optimum optimize within-cluster variation 104 k-mean algorithm interesting note choose initial cluster center carefully may able speed algorithm ’ convergence also guarantee quality final cluster + algorithm variant k-mean choose initial center follow first select one center uniformly random object datum set iteratively object p choose center choose object new center object choose random probability proportional dist ( p ) 2 dist ( p ) distance p closest center already choose iteration continue k center select explain method speed convergence k-mean algorithm also guarantee quality final cluster result 105 provide pseudocode object reassignment step pam algorithm 106 k-mean k-medoid algorithms perform effective cluster ( ) illustrate strength weakness k-mean comparison k-medoid ( b ) illustrate strength weakness scheme comparison hierarchical cluster scheme ( eg agne ) 107 prove dbscan density-connectedness equivalence relation 108 prove dbscan fix minpt value two neighborhood threshold 1 < 2 cluster c respect 1 minpt must subset cluster c 0 respect 2 minpt 109 provide pseudocode optic algorithm 1010 birch encounter difficulty find cluster arbitrary shape optic propose modification birch help find cluster arbitrary shape 1011 provide pseudocode step clique find dense cell subspace 
108 exercise 493 1012 present condition density-based cluster suitable partitioning-based cluster hierarchical cluster give application example support argument 1013 give example specific cluster method integrate example one cluster algorithm used preprocess step another addition provide reasoning integration two method may sometimes lead improve cluster quality efficiency 1014 cluster recognize important datum mining task broad application give one application example follow case ( ) application used cluster major datum mining function ( b ) application used cluster preprocess tool datum preparation datum mining task 1015 datum cube multidimensional databasis contain nominal ordinal numeric datum hierarchical aggregate form base learn cluster method design cluster method find cluster large datum cube effectively efficiently 1016 describe follow cluster algorithms term follow criterium ( 1 ) shape cluster determine ( 2 ) input parameter must specify ( 3 ) limitation ( ) ( b ) ( c ) ( ) ( e ) ( f ) k-mean k-medoid clara birch chameleon dbscan 1017 human eye fast effective judge quality cluster method 2-d datum design datum visualization method may help human visualize datum cluster judge cluster quality 3-d datum even higher-dimensional datum 1018 suppose allocate number automatic teller machine ( atms ) give region satisfy number constraint household workplace may cluster typically one atm assign per cluster cluster however may constrain two factor ( 1 ) obstacle object ( ie bridge river highway affect atm accessibility ) ( 2 ) additional user-specified constraint atm serve least 10000 household cluster algorithm k-mean modify quality cluster constraint 1019 constraint-based cluster aside minimum number customer cluster ( atm allocation ) constraint many kind 
494 chapter 10 cluster analysis basic concept method constraint example constraint can form maximum number customer per cluster average income customer per cluster maximum distance every two cluster categorize kind constraint impose cluster produce discuss perform cluster efficiently kind constraint 1020 design privacy-preserve cluster method datum owner would able ask third party mine datum quality cluster without worry potential inappropriate disclosure certain private sensitive information store datum 1021 show bcube metric satisfy four essential requirement extrinsic cluster evaluation method 109 bibliographic note cluster extensively study 40 year across many discipline due broad application book pattern classification machine learn contain chapter cluster analysis unsupervised learn several textbook dedicate method cluster analysis include hartigan [ har75 ] jain dube [ jd88 ] kaufman rousseeuw [ kr90 ] arabie hubert de sorte [ ahs96 ] also many survey article different aspect cluster method recent one include jain murty flynn [ jmf99 ] parson haque liu [ phl04 ] jain [ jai10 ] partition method k-mean algorithm first introduce lloyd [ llo57 ] macqueen [ mac67 ] arthur vassilvitskii [ av07 ] present + algorithm filter algorithm used spatial hierarchical datum index speed computation cluster mean give kanungo mount netanyahu et al [ + 02 ] k-medoid algorithms pam clara propose kaufman rousseeuw [ kr90 ] k-mode ( cluster nominal datum ) k-prototype ( cluster hybrid datum ) algorithms propose huang [ hua98 ] k-mode cluster algorithm also propose independently chaturvedi green carroll [ cgc94 cgc01 ] claran algorithm propose ng han [ nh94 ] ester kriegel xu [ ekx95 ] propose technique improvement performance claran used efficient spatial access method r∗-tree focuse technique k-means-based scalable cluster algorithm propose bradley fayyad reina [ bfr98 ] early survey agglomerative hierarchical cluster algorithms conduct day edelsbrunner [ de84 ] agglomerative hierarchical cluster agne divisive hierarchical cluster diana introduce kaufman rousseeuw [ kr90 ] interesting direction improve cluster quality hierarchical cluster method integrate hierarchical cluster distance-based iterative relocation nonhierarchical cluster method example birch zhang ramakrishnan livny [ zrl96 ] first perform hierarchical cluster 
109 bibliographic note 495 cf-tree apply technique hierarchical cluster also perform sophisticated linkage analysis transformation nearest-neighbor analysis cure guha rastogi shim [ grs98 ] rock ( cluster nominal attribute ) guha rastogi shim [ grs99 ] chameleon karypis han kumar [ khk99 ] probabilistic hierarchical cluster framework follow normal linkage algorithms used probabilistic model define cluster similarity develop friedman [ fri03 ] heller ghahramani [ hg05 ] density-based cluster method dbscan propose ester kriegel sander xu [ eksx96 ] ankerst breunig kriegel sander [ abks99 ] develop optic cluster-order method facilitate density-based cluster without worry parameter specification denclue algorithm base set density distribution function propose hinneburg keim [ hk98 ] hinneburg gabriel [ hg07 ] develop denclue 20 include new hill-climb procedure gaussian kernel adjust step size automatically sting grid-based multiresolution approach collect statistical information grid cell propose wang yang muntz [ wym97 ] wavecluster develop sheikholeslami chatterjee zhang [ scz98 ] multiresolution cluster approach transform original feature space wavelet transform scalable method cluster nominal datum study gibson kleinberg raghavan [ gkr98 ] guha rastogi shim [ grs99 ] ganti gehrke ramakrishnan [ ggr99 ] also many cluster paradigm example fuzzy cluster method discuss kaufman rousseeuw [ kr90 ] bezdek [ bez81 ] bezdek pal [ bp92 ] high-dimensional cluster apriori-based dimension-growth subspace cluster algorithm call clique propose agrawal gehrke gunopulos raghavan [ aggr98 ] integrate density-based grid-based cluster method recent study proceed cluster stream datum babcock badu datar et al [ + 02 ] k-median-based datum stream cluster algorithm propose guha mishra motwani ’ callaghan [ gmmo00 ] ’ callaghan et al [ + 02 ] method cluster evolve datum stream propose aggarwal han wang yu [ ahwy03 ] framework project cluster high-dimensional datum stream propose aggarwal han wang yu [ ahwy04a ] cluster evaluation discuss monograph survey article jain dube [ jd88 ] halkidi batistakis vazirgiannis [ hbv01 ] extrinsic method cluster quality evaluation extensively explore recent study include meilǎ [ mei03 mei05 ] amigó gonzalo artile verdejo [ agav09 ] four essential criterium introduce chapter formulate amigó gonzalo artile verdejo [ agav09 ] individual criterium also mentioned earlier example meilǎ [ mei03 ] rosenberg hirschberg [ rh07 ] bagga baldwin [ bb98 ] introduce bcube metric silhouette coefficient describe kaufman rousseeuw [ kr90 ] 
11 advanced cluster analysis learn fundamental cluster analysis chapter chapter discuss advanced topic cluster analysis specifically investigate four major perspective probabilistic model-based cluster section 111 introduce general framework method derive cluster object assign probability belong cluster probabilistic model-based cluster widely used many datum mining application text mining cluster high-dimensional datum dimensionality high conventional distance measure dominate noise section 112 introduce fundamental method cluster analysis high-dimensional datum cluster graph network datum graph network datum increasingly popular application online social network world wide web digital library section 113 study key issue cluster graph network datum include similarity measurement cluster method cluster constraint discussion far assume constraint cluster application however various constraint may exist constraint may rise background knowledge spatial distribution object learn conduct cluster analysis different kind constraint section 114 end chapter good grasp issue technique regard advanced cluster analysis 111 probabilistic model-based cluster cluster analysis method discuss far datum object assign one number cluster cluster assignment rule require application assign customer marketing manager however datum mining concept technique doi b978-0-12-381479-100011-3 c 2012 elsevier right re-serve 497 
498 chapter 11 advanced cluster analysis application rigid requirement may desirable section demonstrate need fuzzy flexible cluster assignment application introduce general method compute probabilistic cluster assignment “ situation may datum object belong one cluster ” consider example 111 example 111 cluster product reviews allelectronic online store customer purchase online also create reviews product every product receive reviews instead product may many reviews many other none moreover review may involve multiple product thus review editor allelectronic task cluster reviews ideally cluster topic example group product service issue highly related assign review one cluster exclusively would work well task suppose cluster “ camera camcorder ” another “ ” review talk compatibility camcorder computer review relate cluster however exclusively belong either cluster would like use cluster method allow review belong one cluster review indeed involve one topic reflect strength review belong cluster want assignment review cluster carry weight represent partial membership scenario object may belong multiple cluster occur often many application illustrated example 112 example 112 cluster study user search intent allelectronic online store record customer browse purchasing behavior log important datum mining task use log datum categorize understand user search intent example consider user session ( short period user interact online store ) user search product make comparison among different product look customer support information cluster analysis help difficult predefine user behavior pattern thoroughly cluster contain similar user browse trajectory may represent similar user behavior however every session belong one cluster example suppose user session involve purchase digital camera form one cluster user session compare laptop computer form another cluster user one session make order digital camera time compare several laptop computer session belong cluster extent section systematically study theme cluster allow object belong one cluster start notion fuzzy cluster section generalize concept probabilistic model-based cluster section section 1113 introduce expectation-maximization algorithm general framework mining cluster 
111 probabilistic model-based cluster 499 1111 fuzzy cluster give set object x = { x1 xn } fuzzy set subset x allow object x membership degree 0 formally fuzzy set modeled function fs x → [ 0 1 ] example 113 fuzzy set digital camera unit sell popular camera allelectronic use follow formula compute degree popularity digital camera give sale pop ( ) = ( 1 1000 1000 unit sell ( < 1000 ) unit sell ( 111 ) function pop ( ) define fuzzy set popular digital camera example suppose sale digital camera allelectronic show table fuzzy set popular digital camera { ( 005 ) b ( 1 ) c ( 086 ) ( 027 ) } degree membership written parenthesis apply fuzzy set idea cluster give set object cluster fuzzy set object cluster call fuzzy cluster consequently cluster contain multiple fuzzy cluster formally give set object o1 fuzzy cluster k fuzzy cluster c1 ck represent used partition matrix = [ wij ] ( 1 ≤ ≤ n 1 ≤ j ≤ k ) wij membership degree oi fuzzy cluster cj partition matrix satisfy follow three requirement object oi cluster cj 0 ≤ wij ≤ requirement enforce fuzzy cluster fuzzy set object oi k x wij = requirement ensure every object - j=1 pate cluster equivalently table 111 set digital camera sale allelectronic camera sale ( unit ) b c 50 1320 860 270 
500 chapter 11 advanced cluster analysis cluster cj 0 < n x wij < n requirement ensure every cluster i=1 least one object membership value nonzero example 114 fuzzy cluster suppose allelectronic online store six reviews keyword contain reviews list table 112 group reviews two fuzzy cluster c1 c2 c1 “ digital camera ” “ lens ” c2 “ ” partition matrix  1 1  1  = 2 3  0 0  0 0  0  1  3 1 1 use keyword “ digital camera ” “ lens ” feature cluster c1 “ computer ” feature cluster c2 review ri cluster cj ( 1 ≤ ≤ 6 1 ≤ j ≤ 2 ) wij defined wij = ri ∩ cj | ri ∩ cj | = ri ∩ ( c1 ∪ c2 ) | ri ∩ { digital camera lens computer } | fuzzy cluster review r4 belong cluster c1 c2 membership degree 23 31 respectively “ evaluate well fuzzy cluster describe datum set ” consider set object o1 fuzzy cluster c k cluster c1 ck let = [ wij ] ( 1 ≤ ≤ n 1 ≤ j ≤ k ) partition matrix let c1 ck center cluster c1 ck respectively center defined either mean medoid way specific application discuss chapter 10 distance similarity object center cluster object assign used measure well table 112 set reviews keyword used review id keyword r1 r2 r3 r4 r5 r6 digital camera lens digital camera lens digital camera lens computer computer cpu computer computer game 
111 probabilistic model-based cluster 501 object belong cluster idea extend fuzzy cluster object oi cluster cj wij > 0 dist ( oi cj ) measure well oi represent cj thus belong cluster cj object participate one cluster sum distance corresponding cluster center weight degree membership capture well object fit cluster formally object oi sum square error ( sse ) give sse ( oi ) = k x p wij dist ( oi cj ) 2 ( 112 ) j=1 parameter p ( p ≥ 1 ) control influence degree membership larger value p larger influence degree membership orthogonally sse cluster cj sse ( cj ) = n x p wij dist ( oi cj ) 2 ( 113 ) i=1 finally sse cluster defined sse ( c ) = n x k x p wij dist ( oi cj ) 2 ( 114 ) i=1 j=1 sse used measure well fuzzy cluster fit datum set fuzzy cluster also call soft cluster allow object belong one cluster easy see traditional ( rigid ) cluster enforce object belong one cluster exclusively special case fuzzy cluster defer discussion compute fuzzy cluster section 1113 1112 probabilistic model-based cluster “ fuzzy cluster ( section 1111 ) provide flexibility allow object participate multiple cluster general framework specify clustering object may participate multiple cluster probabilistic way ” section introduce general notion probabilistic model-based cluster answer question discuss chapter 10 conduct cluster analysis datum set assume object datum set fact belong different inherent category recall cluster tendency analysis ( section 1061 ) used examine whether datum set contain object may lead meaningful cluster inherent category hide datum latent mean directly observed instead infer used datum observed example topic hide set reviews allelectronic online store latent one read topic directly however topic infer reviews review one multiple topic 
502 chapter 11 advanced cluster analysis therefore goal cluster analysis find hide category datum set subject cluster analysis regard sample possible instance hide category without category label cluster derive cluster analysis infer used datum set design approach hide category statistically assume hide category distribution datum space mathematically represent used probability density function ( distribution function ) call hide category probabilistic cluster probabilistic cluster c probability density function f point datum space f ( ) relative likelihood instance c appear example 115 probabilistic cluster suppose digital camera sell allelectronic divide two category c1 consumer line ( eg point-and-shoot camera ) c2 professional line ( eg single-len reflex camera ) respective probability density function f1 f2 show figure 111 respect attribute price price value say $ 1000 f1 ( 1000 ) relative likelihood price consumer-line camera $ 1000 similarly f2 ( 1000 ) relative likelihood price professional-line camera $ 1000 probability density function f1 f2 observed directly instead allelectronic infer distribution analyze price digital camera sell moreover camera often come well-determine category ( eg “ consumer line ” “ professional line ” ) instead category typically base user background knowledge vary example camera prosumer segment may regard high end consumer line customer low end professional line other analyst allelectronic consider category probabilistic cluster conduct cluster analysis price camera approach category probability consumer line professional line price 1000 figure 111 probability density function two probabilistic cluster 
111 probabilistic model-based cluster 503 suppose want find k probabilistic cluster c1 ck cluster analysis datum set n object regard finite sample possible instance cluster conceptually assume form follow cluster cj ( 1 ≤ j ≤ k ) associate probability ωj instance sample cluster often assume ω1 ωk give part problem set p kj=1 ωj = 1 ensure object generate k cluster parameter ωj capture background knowledge relative population cluster cj run follow two step generate object d step execute n time total generate n object o1 choose cluster cj accord probability ω1 ωk choose instance cj accord probability density function fj datum generation process basic assumption mixture model formally mixture model assume set observed object mixture instance multiple probabilistic cluster conceptually observed object generate independently two step first choose probabilistic cluster accord probability cluster choose sample accord probability density function choose cluster give datum set k number cluster require task probabilistic model-based cluster analysis infer set k probabilistic cluster likely generate used datum generation process important question remain measure likelihood set k probabilistic cluster probability generate observed datum set consider set c k probabilistic cluster c1 ck probability density function f1 fk respectively probability ω1 ωk object probability generate cluster cj ( 1 ≤ j ≤ k ) give p ( o|cj ) = ωj fj ( ) therefore probability generate set c cluster p ( o|c ) = k x ωj fj ( ) ( 115 ) j=1 since object assume generate independently datum set = { o1 } n object p ( d|c ) = n i=1 p ( oi c ) = k n x ωj fj ( oi ) ( 116 ) i=1 j=1 clear task probabilistic model-based cluster analysis datum set find set c k probabilistic cluster p ( d|c ) maximize maximize p ( d|c ) often intractable general probability density function 
504 chapter 11 advanced cluster analysis cluster take arbitrarily complicate form make probabilistic model-based cluster computationally feasible often compromise assume probability density function parameterized distribution formally let o1 n observed object 21 2k parameter k distribution denote = { o1 } 2 = { 21 2k } respectively object oi ∈ ( 1 ≤ ≤ n ) eq ( 115 ) rewrite p ( oi 2 ) = k x ωj pj ( oi 2j ) ( 117 ) j=1 pj ( oi 2j ) probability oi generate jth distribution used parameter 2j consequently eq ( 116 ) rewrite p ( o|2 ) = n x k ωj pj ( oi 2j ) ( 118 ) i=1 j=1 used parameterized probability distribution model task probabilistic model-based cluster analysis infer set parameter 2 maximize eq ( 118 ) example 116 univariate gaussian mixture model let ’ use univariate gaussian distribution example assume probability density function cluster follow 1-d gaussian distribution suppose k cluster two parameter probability density function cluster center µj standard deviation σj ( 1 ≤ j ≤ k ) denote parameter 2j = ( µj σj ) 2 = { 21 2k } let datum set = { o1 } oi ( 1 ≤ ≤ n ) real number point oi ∈ 1 e p ( oi 2j ) = √ 2π σj − ( oi −µj ) 2 2σ 2 ( 119 ) assume cluster probability ω1 = ω2 = · · · = ωk = k1 plug eq ( 119 ) eq ( 117 ) k 2 ( oi −µj ) 1x 1 − p ( oi 2 ) = e 2σ 2 √ k 2π σj ( 1110 ) j=1 apply eq ( 118 ) n p ( o|2 ) = k 2 ( oi −µj ) 1 yx 1 − e 2σ 2 √ k 2π σj ( 1111 ) i=1 j=1 task probabilistic model-based cluster analysis used univariate gaussian mixture model infer 2 eq ( 1111 ) maximize 
111 probabilistic model-based cluster 505 1113 expectation-maximization algorithm “ compute fuzzy clustering probabilistic model-based clustering ” section introduce principled approach let ’ start review k-mean cluster problem k-mean algorithm study chapter 10 easily show k-mean cluster special case fuzzy cluster ( exercise 111 ) k-mean algorithm iterate cluster improve iteration consist two step expectation step ( e-step ) give current cluster center object assign cluster center closest object object expect belong closest cluster maximization step ( m-step ) give cluster assignment cluster algorithm adjust center sum distance object assign cluster new center minimize similarity object assign cluster maximize generalize two-step method tackle fuzzy cluster probabilistic model-based cluster general expectation-maximization ( em ) algorithm framework approach maximum likelihood maximum posteriori estimate parameter statistical model context fuzzy probabilistic model-based cluster em algorithm start initial set parameter iterate cluster improve cluster converge change sufficiently small ( less preset threshold ) iteration also consist two step expectation step assign object cluster accord current fuzzy cluster parameter probabilistic cluster maximization step find new cluster parameter maximize sse fuzzy cluster ( eq 114 ) expect likelihood probabilistic model-based cluster example 117 fuzzy cluster used em algorithm consider six point figure 112 coordinate point also show let ’ compute two fuzzy cluster used em algorithm randomly select two point say c1 = c2 = b initial center two cluster first iteration conduct expectation step maximization step follow e-step point calculate membership degree cluster point assign c1 c2 membership weight 1 dist ( c1 ) 2 1 1 + 2 dist ( c1 ) dist ( c2 ) 2 = dist ( c2 ) 2 dist ( c1 ) 2 dist ( c1 ) 2 + dist ( c2 ) 2 dist ( c1 ) 2 + dist ( c2 ) 2 
506 chapter 11 advanced cluster analysis e ( 18 11 ) b ( 4 10 ) ( 14 8 ) c ( 9 6 ) f ( 21 7 ) ( 3 3 ) x figure 112 datum set fuzzy cluster table 113 intermediate result first three iteration example 117 ’ em algorithm iteration 1 2 3 e-step ` 1 0 = 0 1 ` 073 mt = 027 ` 080 mt = 020 048 052 042 058 m-step 041 059 # 047 053 049 051 091 009 026 074 033 067 076 024 099 001 002 098 014 086 c1 = ( 847 512 ) c2 = ( 1042 899 ) # 042 058 # 023 077 c1 = ( 851 611 ) c2 = ( 1442 869 ) c1 = ( 640 624 ) c2 = ( 1655 864 ) respectively dist ( ) euclidean distance rationale close c1 dist ( c1 ) small membership degree respect c1 high also normalize membership degree sum degree object equal 1 point wa c1 = 1 wa c2 = exclusively belong c1 41 = 048 point b wb c1 = 0 wb c2 = point c wc c1 = 45+41 45 wc c2 = 45+41 = degree membership point show partition matrix table 113 m-step recalculate centroid accord partition matrix minimize sse give eq ( 114 ) new centroid adjust x 2 wo c j point cj = ( 1112 ) x 2 wo c j point j = 1 2 
111 probabilistic model-based cluster 507 example 12 × 3 + 02 × 4 + 0482 × 9 + 0422 × 14 + 0412 × 18 + 0472 × 21 12 + 02 + 0482 + 0422 + 0412 + 0472  12 × 3 + 02 × 10 + 0482 × 6 + 0422 × 8 + 0412 × 11 + 0472 × 7 12 + 02 + 0482 + 0422 + 0412 + 0472  c1 = = ( 847 512 )  c2 = 02 × 3 + 12 × 4 + 0522 × 9 + 0582 × 14 + 0592 × 18 + 0532 × 21 02 + 12 + 0522 + 0582 + 0592 + 0532  02 × 3 + 12 × 10 + 0522 × 6 + 0582 × 8 + 0592 × 11 + 0532 × 7 02 + 12 + 0522 + 0582 + 0592 + 0532 = ( 1042 899 ) repeat iteration iteration contain e-step m-step table 113 show result first three iteration algorithm stop cluster center converge change small enough “ apply em algorithm compute probabilistic model-based cluster ” let ’ use univariate gaussian mixture model ( example 116 ) illustrate example 118 used em algorithm mixture model give set object = { o1 } want mine set parameter 2 = { 21 2k } p ( o|2 ) eq ( 1111 ) maximize 2j = ( µj σj ) mean standard deviation respectively jth univariate gaussian distribution ( 1 ≤ j ≤ k ) apply em algorithm assign random value parameter 2 initial value iteratively conduct e-step m-step follow parameter converge change sufficiently small e-step object oi ∈ ( 1 ≤ ≤ n ) calculate probability oi belong distribution p ( oi 2j ) p ( 2j oi 2 ) = pk l=1 p ( oi 2l ) ( 1113 ) m-step adjust parameter 2 expect likelihood p ( o|2 ) eq ( 1111 ) maximize achieve set pn n p ( 2j oi 2 ) 1x 1 i=1 oi p ( 2j oi 2 ) µj = oi pn = pn k k l=1 p ( 2j ol 2 ) i=1 p ( 2j oi 2 ) i=1 ( 1114 ) 
508 chapter 11 advanced cluster analysis σj = sp n 2 i=1 p ( 2j oi 2 ) ( oi − uj ) pn i=1 p ( 2j oi 2 ) ( 1115 ) many application probabilistic model-based cluster show effective general partition method fuzzy cluster method distinct advantage appropriate statistical model used capture latent cluster em algorithm commonly used handle many learn problem datum mining statistic due simplicity note general em algorithm may converge optimal solution may instead converge local maximum many heuristic explore avoid example can run em process multiple time used different random initial value furthermore em algorithm costly number distribution large datum set contain observed datum point 112 cluster high-dimensional datum cluster method study far work well dimensionality high less 10 attribute however important application high dimensionality “ conduct cluster analysis high-dimensional datum ” section study approach cluster high-dimensional datum section 1121 start overview major challenge approach used method high-dimensional datum cluster divide two category subspace cluster method ( section 1122 ) dimensionality reduction method ( section 1123 ) 1121 cluster high-dimensional datum problem challenge major methodology present specific method cluster high-dimensional datum let ’ first demonstrate need cluster analysis high-dimensional datum used example examine challenge call new method categorize major method accord whether search cluster subspace original space whether create new lower-dimensionality space search cluster application datum object may describe 10 attribute object refer high-dimensional datum space example 119 high-dimensional datum cluster allelectronic keep track product purchase every customer customer-relationship manager want cluster customer group accord purchase allelectronic 
112 cluster high-dimensional datum 509 table 114 customer purchase datum customer p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 ada bob cathy 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 customer purchase datum high dimensionality allelectronic carry ten thousand product therefore customer ’ purchase profile vector product carry company ten thousand dimension “ traditional distance measure frequently used low-dimensional cluster analysis also effective high-dimensional datum ” consider customer table 114 10 product p1 p10 used demonstration customer purchase product 1 set corresponding bit otherwise 0 appear let ’ calculate euclidean distance ( eq 216 ) among ada bob cathy easy see dist ( ada bob ) = dist ( bob cathy ) = dist ( ada cathy ) = √ 2 accord euclidean distance three customer equivalently similar ( dissimilar ) however close look tell us ada similar cathy bob ada cathy share one common purchase item p1 show example 119 traditional distance measure ineffective high-dimensional datum distance measure may dominate noise many dimension therefore cluster full high-dimensional space unreliable find cluster may meaningful “ kind cluster meaningful high-dimensional datum ” cluster analysis high-dimensional datum still want group similar object together however datum space often big messy additional challenge need find cluster cluster set attribute manifest cluster word cluster high-dimensional datum often defined used small set attribute instead full datum space essentially cluster high-dimensional datum return group object cluster ( conventional cluster analysis ) addition cluster set attribute characterize cluster example table 114 characterize similarity ada cathy p1 may return attribute ada cathy purchase p1 cluster high-dimensional datum search cluster space exist thus two major kind method subspace cluster approach search cluster exist subspace give high-dimensional datum space subspace defined used subset attribute full space subspace cluster approach discuss section 1122 
510 chapter 11 advanced cluster analysis dimensionality reduction approach try construct much lower-dimensional space search cluster space often method may construct new dimension combine dimension original datum dimensionality reduction method topic section 1124 general cluster high-dimensional datum raise several new challenge addition conventional cluster major issue create appropriate model cluster high-dimensional datum unlike conventional cluster low-dimensional space cluster hide high-dimensional datum often significantly smaller example cluster customer-purchase datum would expect many user similar purchase pattern search small meaningful cluster like find needle haystack show conventional distance measure ineffective instead often consider various sophisticated technique model correlation consistency among object subspace typically exponential number possible subspace dimensionality reduction option thus optimal solution often computationally prohibitive example original datum space 1000 dimension want 1000 find cluster dimensionality 10 = 263 × 1023 possible 10 subspace 1122 subspace cluster method “ find subspace cluster high-dimensional datum ” many method propose generally categorize three major group subspace search method correlation-based cluster method bicluster method subspace search method subspace search method search various subspace cluster cluster subset object similar subspace similarity often capture conventional measure distance density example clique algorithm introduce section 1052 subspace cluster method enumerate subspace cluster subspace dimensionality-increas order apply antimonotonicity prune subspace cluster may exist major challenge subspace search method face search series subspace effectively efficiently generally two kind strategy bottom-up approach start low-dimensional subspace search higherdimensional subspace may cluster higher-dimensional 
112 cluster high-dimensional datum 511 subspace various prune technique explore reduce number higherdimensional subspace need search clique example bottom-up approach top-down approach start full space search smaller smaller subspace recursively top-down approach effective locality assumption hold require subspace cluster determine local neighborhood example 1110 proclus top-down subspace approach proclus k-medoid-like method first generate k potential cluster center high-dimensional datum set used sample datum set refine subspace cluster iteratively iteration current k-medoid proclus consider local neighborhood medoid whole datum set identify subspace cluster minimize standard deviation distance point neighborhood medoid dimension subspace medoid determine point datum set assign closest medoid accord corresponding subspace cluster possible outlier identify next iteration new medoid replace exist one improve cluster quality correlation-based cluster method subspace search method search cluster similarity measure used conventional metric like distance density correlation-based approach discover cluster defined advanced correlation model example 1111 correlation-based approach used pca example pca-based approach first apply pca ( principal component analysis see chapter 3 ) derive set new uncorrelated dimension mine cluster new space subspace addition pca space transformation may used hough transform fractal dimension additional detail subspace search method correlation-based cluster method please refer bibliographic note ( section 117 ) bicluster method application want cluster object attribute simultaneously result cluster know bicluster meet four requirement ( 1 ) small set object participate cluster ( 2 ) cluster involve small number attribute ( 3 ) object may participate multiple cluster participate cluster ( 4 ) attribute may involved multiple cluster involved cluster section 1123 discuss bicluster detail 
512 chapter 11 advanced cluster analysis 1123 bicluster cluster analysis discuss far cluster object accord attribute value object attribute treat way however application object attribute defined symmetric way datum analysis involve search datum matrix submatrix show unique pattern cluster kind cluster technique belong category bicluster section first introduce two motivate application example biclustering— gene expression recommender system learn different type bicluster last present bicluster method application example bicluster technique first propose address need analyze gene expression datum gene unit passing-on trait live organism offspr typically gene reside segment dna gene critical live thing specify protein functional rna chain hold information build maintain live organism ’ cell pass genetic trait offspr synthesis functional gene product either rna protein rely process gene expression genotype genetic makeup cell organism individual phenotype observable characteristic organism gene expression fundamental level genetic genotype cause phenotype used dna chip ( also know dna microarray ) biological engineering technique measure expression level large number ( possibly ) organism ’ gene number different experimental condition condition may correspond different time point experiment sample different organ roughly speaking gene expression datum dna microarray datum conceptually condition matrix row correspond one gene column correspond one sample condition element matrix real number record expression level gene specific condition figure 113 show illustration cluster viewpoint interesting issue gene expression datum matrix analyze two dimensions—the gene dimension condition dimension analyze gene dimension treat gene object treat condition attribute mining gene dimension may find pattern share multiple gene cluster gene group example may find group gene express similarly highly interesting bioinformatic find pathway analyze condition dimension treat condition object treat gene attribute way may find pattern condition cluster condition group example may find difference gene expression compare group tumor sample nontumor sample 
112 cluster high-dimensional datum 513 condition gene w11 w12 w1m w21 w22 w2m w31 w32 w3m wn1 wn2 wnm figure 113 microarrary datum matrix example 1112 gene expression gene expression matrix popular bioinformatic research development example important task classify new gene used expression datum gene gene know class symmetrically may classify new sample ( eg new patient ) used expression datum sample sample know class ( eg tumor nontumor ) task invaluable understand mechanism disease clinical treatment see many gene expression datum mining problem highly related cluster analysis however challenge instead cluster one dimension ( eg gene condition ) many case need cluster two dimension simultaneously ( eg gene condition ) moreover unlike cluster model discuss far cluster gene expression datum matrix submatrix usually follow characteristic small set gene participate cluster cluster involve small subset condition gene may participate multiple cluster may participate cluster condition may involved multiple cluster may involved cluster find cluster condition matrix need new cluster technique meet follow requirement bicluster cluster gene defined used subset condition cluster condition defined used subset gene 
514 chapter 11 advanced cluster analysis cluster neither exclusive ( eg one gene participate multiple cluster ) exhaustive ( eg gene may participate cluster ) bicluster useful bioinformatic also application well consider recommender system example example 1113 used bicluster recommender system allelectronic collect datum customer ’ evaluation product used datum recommend product customer datum modeled customer-product matrix row represent customer column represent product element matrix represent customer ’ evaluation product may score ( eg like like somewhat like ) purchase behavior ( eg buy ) figure 114 illustrate structure customer-product matrix analyze two dimension customer dimension product dimension treat customer object product attribute allelectronic find customer group similar preference purchase pattern used product object customer attribute allelectronic mine product group similar customer interest moreover allelectronic mine cluster customer product simultaneously cluster contain subset customer involve subset product example allelectronic highly interested find group customer like group product cluster submatrix customer-product matrix element high value used cluster allelectronic make recommendation two direction first company recommend product new customer similar customer cluster second company recommend customer new product similar involved cluster bicluster gene expression datum matrix bicluster customerproduct matrix usually follow characteristic small set customer participate cluster cluster involve small subset product customer participate multiple cluster may participate cluster customer w11 w21 ··· wn1 product w12 · · · w22 · · · ··· ··· wn2 · · · figure 114 customer–product matrix w1m w2m ··· wnm 
112 cluster high-dimensional datum 515 product may involved multiple cluster may involved cluster bicluster apply customer-product matrix mine cluster satisfying requirement type bicluster “ model bicluster mine ” let ’ start basic notation sake simplicity use “ gene ” “ condition ” refer two dimension discussion discussion easily extend application example simply replace “ gene ” “ condition ” “ customer ” “ product ” tackle customer-product bicluster problem let = { a1 } set gene b = { b1 bm } set condition let e = [ eij ] gene expression datum matrix gene-condition matrix 1 ≤ ≤ n 1 ≤ j ≤ m submatrix × j defined subset ⊆ gene subset j ⊆ b condition example matrix show figure 115 { a1 a33 a86 } × { b6 b12 b36 b99 } submatrix bicluster submatrix gene condition follow consistent pattern define different type bicluster base pattern simplest case submatrix × j ( ⊆ j ⊆ b ) bicluster constant value ∈ j ∈ j eij = c c constant example submatrix { a1 a33 a86 } × { b6 b12 b36 b99 } figure 115 bicluster constant value bicluster interesting row constant value though different row may different value bicluster constant value row submatrix × j ∈ j ∈ j eij = c + αi αi adjustment row i example figure 116 show bicluster constant value row symmetrically bicluster constant value column submatrix × j ∈ j ∈ j eij = c + βj βj adjustment column j a1 ··· a33 ··· a86 ··· ··· ··· ··· ··· ··· ··· ··· b6 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b12 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b36 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b99 · · · 60 · · · ··· ··· 60 · · · ··· ··· 60 · · · ··· ··· figure 115 gene-condition matrix submatrix bicluster 
516 chapter 11 advanced cluster analysis generally bicluster interesting row change synchronize way respect column vice versa mathematically bicluster coherent value ( also know pattern-based cluster ) submatrix × j ∈ j ∈ j eij = c + αi + βj αi βj adjustment row column j respectively example figure 117 show bicluster coherent value show × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 moreover instead used addition define bicluster coherent value used multiplication eij = c · αi · βj clearly bicluster constant value row column special case bicluster coherent value application may interested - down-regulate change across gene condition without constrain exact value bicluster coherent evolution row submatrix × j i1 i2 ∈ j1 j2 ∈ j ( ei1 j1 − ei1 j2 ) ( ei2 j1 − ei2 j2 ) ≥ example figure 118 show bicluster coherent evolution row symmetrically define bicluster coherent evolution column next study mine bicluster 10 20 50 0 10 20 50 0 10 20 50 0 10 20 50 0 10 20 50 0 figure 116 bicluster constant value row 10 20 50 0 50 60 90 40 30 40 70 20 70 80 110 60 20 30 60 10 figure 117 bicluster coherent value 10 20 50 0 50 100 100 80 30 50 90 20 70 1000 120 100 20 30 80 10 figure 118 bicluster coherent evolution row 
112 cluster high-dimensional datum 517 bicluster method previous specification type bicluster consider ideal case real datum set perfect bicluster rarely exist exist usually small instead random noise affect reading eij thus prevent bicluster nature appear perfect shape two major type method discover bicluster datum may come noise optimization-based method conduct iterative search iteration submatrix highest significance score identify bicluster process terminate user-specified condition meet due cost concern computation greedy search often employ find local optimal bicluster enumeration method use tolerance threshold specify degree noise allow bicluster mine try enumerate submatrix bicluster satisfy requirement use δ-cluster maple algorithms example illustrate idea optimization used δ-cluster algorithm submatrix × j mean ith row 1 x eij = eij | ( 1116 ) j∈j symmetrically mean jth column 1 x eij = eij | ( 1117 ) i∈i mean element submatrix 1 x 1 x 1 x eij = eij = eij eij = | | | i∈i j∈j i∈i ( 1118 ) j∈j quality submatrix bicluster measure mean-squared residue value 1 x h ( × j ) = ( eij − eij − eij + eij ) 2 ( 1119 ) | i∈i j∈j submatrix × j δ-bicluster h ( × j ) ≤ δ δ ≥ 0 threshold δ = 0 × j perfect bicluster coherent value set δ > 0 user specify tolerance average noise per element perfect bicluster eq ( 1119 ) residue element residue ( eij ) = eij − eij − eij + eij ( 1120 ) maximal δ-bicluster δ-bicluster × j exist another δ-bicluster 0 × j 0 ⊆ 0 j ⊆ j 0 least one inequality hold find 
518 chapter 11 advanced cluster analysis maximal δ-bicluster largest size computationally costly therefore use heuristic greedy search method obtain local optimal cluster algorithm work two phase deletion phase start whole matrix mean-squared residue matrix δ iteratively remove row column iteration row compute mean-squared residue 1 x ( ) = ( eij − eij − eij + eij ) 2 ( 1121 ) | j∈j moreover column j compute mean-squared residue 1 x ( eij − eij − eij + eij ) 2 ( j ) = | ( 1122 ) i∈i remove row column largest mean-squared residue end phase obtain submatrix × j δ-bicluster however submatrix may maximal addition phase iteratively expand δ-bicluster × j obtain deletion phase long δ-bicluster requirement maintain iteration consider row column involved current bicluster × j calculate mean-squared residue row column smallest mean-squared residue add current δ-bicluster greedy algorithm find one δ-bicluster find multiple bicluster heavy overlap run algorithm multiple time execution δ-bicluster output replace element output bicluster random number although greedy algorithm may find neither optimal bicluster bicluster fast even large matrix enumerate bicluster used maple mentioned submatrix × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 2 × 2 submatrix × j define p-score ei1 j1 ei1 j2 p-score = | ( ei1 j1 − ei2 j1 ) − ( ei1 j2 − ei2 j2 ) | ( 1123 ) ei2 j1 ei2 j2 submatrix × j δ-pcluster ( pattern-based cluster ) p-score every 2 × 2 submatrix × j δ δ ≥ 0 threshold specify user ’ tolerance noise perfect bicluster p-score control noise every element bicluster mean-squared residue capture average noise interesting property δ-pcluster × j δ-pcluster every x × ( x ≥ 2 ) submatrix × j also δ-pcluster monotonicity enable 
112 cluster high-dimensional datum 519 us obtain succinct representation nonredundant δ-pcluster δ-pcluster maximal row column add cluster maintain δ-pcluster property avoid redundancy instead find δ-pcluster need compute maximal δ-pcluster maple algorithm enumerate maximal δ-pcluster systematically enumerate every combination condition used set enumeration tree depthfirst search enumeration framework pattern-growth method frequent pattern mining ( chapter 6 ) consider gene expression datum condition combination j maple find maximal subset gene × j δ-pcluster × j submatrix another δ-pcluster × j maximal δ-pcluster may huge number condition combination maple prune many unfruitful combination used monotonicity δ-pcluster condition combination j exist set gene × j δ-pcluster need consider superset j moreover consider × j candidate δ-pcluster every ( | − 1 ) subset j 0 j × j 0 δ-pcluster maple also employ several prune technique speed search retain completeness return maximal δ-pcluster example examine current δ-pcluster × j maple collect gene condition may add expand cluster candidate gene condition together j form submatrix δ-pcluster already find search × j superset j prune interested reader may refer bibliographic note additional information maple algorithm ( section 117 ) interesting observation search maximal δ-pcluster maple somewhat similar mining frequent close itemset consequently maple borrow depth-first search framework idea prune technique pattern-growth method frequent pattern mining example frequent pattern mining cluster analysis may share similar technique idea advantage maple algorithms enumerate bicluster guarantee completeness result miss overlapping bicluster however challenge enumeration algorithms may become time consume matrix become large customer-purchase matrix hundred thousand customer million product 1124 dimensionality reduction method spectral cluster subspace cluster method try find cluster subspace original datum space situation effective construct new space instead used subspace original datum motivation behind dimensionality reduction method cluster high-dimensional datum example 1114 cluster derive space consider three cluster point figure possible cluster point subspace original space x × 
520 chapter 11 advanced cluster analysis − 0707x + 0707y x figure 119 cluster derive space may effective three cluster would end project onto overlapping area x √ √ 2 2 axe instead construct new dimension − 2 x + 2 ( show dash line figure ) project point onto new dimension three cluster become apparent although example 1114 involve two dimension idea construct new space ( cluster structure hide datum become well manifest ) extend high-dimensional datum preferably newly construct space low dimensionality many dimensionality reduction method straightforward approach apply feature selection extraction method datum set discuss chapter however method may able detect cluster structure therefore method combine feature extraction cluster prefer section introduce spectral cluster group method effective highdimensional datum application figure 1110 show general framework spectral cluster approach ng-jordan-weiss algorithm spectral cluster method let ’ look step framework also note special condition apply ng-jordan-weiss algorithm example give set object o1 distance pair object dist ( oi oj ) ( 1 ≤ j ≤ n ) desire number k cluster spectral cluster approach work follow used distance measure calculate affinity matrix w wij = e − dist ( oi oj ) σ2 σ scaling parameter control fast affinity wij decrease dist ( oi oj ) increase ng-jordan-weiss algorithm wii set 0 
112 cluster high-dimensional datum datum affinity matrix [ wij ] compute lead k eigenvector cluster new space 521 project back cluster original datum av = λv = f ( w ) figure 1110 framework spectral cluster approach source adapt slide 8 http micued08 azran used affinity matrix w derive matrix = f ( w ) way do vary ng-jordan-weiss algorithm define matrix diagonal matrix dii sum ith row w dii = n x wij ( 1124 ) j=1 set 1 1 = d− 2 wd− 2 ( 1125 ) find k lead eigenvector a recall eigenvector square matrix nonzero vector remain proportional original vector multiply matrix mathematically vector v eigenvector matrix av = λv λ call corresponding eigenvalue step derive k new dimension base affinity matrix w typically k much smaller dimensionality original datum ng-jordan-weiss algorithm compute k eigenvector largest eigenvalue x1 xk used k lead eigenvector project original datum new space defined k lead eigenvector run cluster algorithm k-mean find k cluster ng-jordan-weiss algorithm stack k largest eigenvector column form matrix x = [ x1 x2 · · · xk ] ∈ rn×k algorithm form matrix renormalize row x unit length xij yij = qp k 2 j=1 xij ( 1126 ) algorithm treat row point k-dimensional space rk run k-mean ( algorithm serve partition purpose ) cluster point k cluster 
522 chapter 11 advanced cluster analysis v = [ v1 v2 v3 ] w 05 u = [ u1 u2 u3 ] 0 −05 0 10 20 30 40 50 60 1 05 0 −05 −1 0 10 20 30 40 50 60 1 05 05 0 0 0 04 02 0 −02 0 10 10 20 20 30 30 40 40 50 50 60 60 0 1 05 0 −05 0 10 10 20 20 30 30 40 40 50 50 60 60 figure 1111 new dimension cluster result ng-jordan-weiss algorithm source adapt slide 9 http micued08 azran assign original datum point cluster accord transform point assign cluster obtain step 4 ng-jordan-weiss algorithm original object oi assign jth cluster matrix ’ row assign jth cluster result step 4 spectral cluster method dimensionality new space set desire number cluster set expect new dimension able manifest cluster example 1115 ng-jordan-weiss algorithm consider set point figure datum set affinity matrix three largest eigenvector normalize vector show note three new dimension ( form three largest eigenvector ) cluster easily detected spectral cluster effective high-dimensional application image process theoretically work well certain condition apply scalability however challenge compute eigenvector large matrix costly spectral cluster combine cluster method bicluster additional information dimensionality reduction cluster method kernel pca find bibliographic note ( section 117 ) 113 cluster graph network datum cluster analysis graph network datum extract valuable knowledge information datum increasingly popular many application discuss application challenge cluster graph network datum section similarity measure form cluster give section learn graph cluster method section 1133 general term graph network used interchangeably rest section mainly use term graph 
113 cluster graph network datum 523 1131 application challenge customer relationship manager allelectronic notice lot datum relate customer purchase behavior preferably modeled used graph example 1116 bipartite graph customer purchase behavior allelectronic represent bipartite graph bipartite graph vertex divide two disjoint set edge connect vertex one set vertex set allelectronic customer purchase datum one set vertex represent customer one customer per vertex set represent product one product per vertex edge connect customer product represent purchase product customer figure 1112 show illustration “ kind knowledge obtain cluster analysis customer-product bipartite graph ” cluster customer customer buy similar set product place one group customer relationship manager make product recommendation example suppose ada belong customer cluster customer purchase digital camera last 12 month ada yet purchase one manager decide recommend digital camera alternatively cluster product product purchase similar set customer group together cluster information also used product recommendation example digital camera high-speed flash memory card belong product cluster customer purchase digital camera recommend high-speed flash memory card bipartite graph widely used many application consider another example example 1117 web search engine web search engine search log archive record user query corresponding click-through information ( click-through information tell us page give result search user click ) query click-through information represent used bipartite graph two set customer product figure 1112 bipartite graph represent customer-purchase datum 
524 chapter 11 advanced cluster analysis vertex correspond query web page respectively edge link query web page user click web page ask query valuable information obtain cluster analysis query–web page bipartite graph instance may identify query pose different language mean thing click-through information query similar another example web page web form direct graph also know web graph web page vertex hyperlink edge point source page destination page cluster analysis web graph disclose community find hub authoritative web page detect web spam addition bipartite graph cluster analysis also apply type graph include general graph elaborate example 1118 example 1118 social network social network social structure represent graph vertex individual organization link interdependency vertex represent friendship common interest collaborative activity allelectronic ’ customer form social network customer vertex edge link two customer know customer relationship manager interested find useful information derive allelectronic ’ social network cluster analysis obtain cluster network customer cluster know friend common customer within cluster may influence one another regard purchase decision make moreover communication channel design inform “ head ” cluster ( ie “ best ” connect person cluster ) promotional information spread quickly thus may use customer cluster promote sale allelectronic another example author scientific publication form social network author vertex two author connect edge coauthor publication network general weight graph edge two author carry weight represent strength collaboration many publication two author ( end vertex ) coauthor cluster coauthor network provide insight community author pattern collaboration “ challenge specific cluster analysis graph network datum ” cluster method discuss far object represent used set attribute unique feature graph network datum object ( vertex ) relationship ( edge ) give dimension attribute explicitly defined conduct cluster analysis graph network datum two major new challenge “ measure similarity two object graph accordingly ” typically use conventional distance measure euclidean distance instead need develop new measure quantify similarity 
113 cluster graph network datum 525 measure often metric thus raise new challenge regard development efficient cluster method similarity measure graph discuss section 1132 “ design cluster model method effective graph network datum ” graph network datum often complicate carry topological structure sophisticated traditional cluster analysis application many graph datum set large web graph contain least ten billion web page publicly indexable web graph also sparse average vertex connect small number vertex graph discover accurate useful knowledge hide deep datum good cluster method accommodate factor cluster method graph network datum introduce section 1133 1132 similarity measure “ measure similarity distance two vertex graph ” discussion examine two type measure geodesic distance distance base random walk geodesic distance simple measure distance two vertex graph shortest path vertex formally geodesic distance two vertex length term number edge shortest path vertex two vertex connect graph geodesic distance defined infinite used geodesic distance define several useful measurement graph analysis cluster give graph g = ( v e ) v set vertex e set edge define follow vertext v ∈ v eccentricity v denote eccen ( v ) largest geodesic distance v vertex u ∈ v − { v } eccentricity v capture far away v remotest vertex graph radius graph g minimum eccentricity vertex r = min eccen ( v ) v∈v ( 1127 ) radius capture distance “ central point ” “ farthest border ” graph diameter graph g maximum eccentricity vertex = max eccen ( v ) v∈v diameter represent largest distance pair vertex peripheral vertex vertex achieve diameter ( 1128 ) 
526 chapter 11 advanced cluster analysis b c e figure 1113 graph g vertex c e peripheral example 1119 measurement base geodesic distance consider graph g figure eccentricity 2 eccen ( ) = 2 eccen ( b ) = 2 eccen ( c ) = eccen ( ) = eccen ( e ) = thus radius g 2 diameter note necessary = 2 × r vertex c e peripheral vertex simrank similarity base random walk structural context application geodesic distance may inappropriate measure similarity vertex graph introduce simrank similarity measure base random walk structural context graph mathematics random walk trajectory consist take successive random step example 1120 similarity person social network let ’ consider measure similarity two vertex allelectronic customer social network example 1118 similarity explain closeness two participant network close two person term relationship represent social network “ well geodesic distance measure similarity closeness network ” suppose ada bob two customer network network undirected geodesic distance ( ie length shortest path ada bob ) shortest path message pass ada bob vice versa however information useful allelectronic ’ customer relationship management company typically want send specific message one customer another therefore geodesic distance suit application “ similarity mean social network ” consider two way define similarity two customer consider similar one another similar neighbor social network heuristic intuitive practice two person receive recommendation good number common friend often make similar decision kind similarity base local structure ( ie neighborhood ) vertex thus call structural context–based similarity 
113 cluster graph network datum 527 suppose allelectronic send promotional information ada bob social network ada bob may randomly forward information friend ( neighbor ) network closeness ada bob measure likelihood customer simultaneously receive promotional information originally send ada bob kind similarity base random walk reachability network thus refer similarity base random walk let ’ closer look meant similarity base structural context similarity base random walk intuition behind similarity base structural context two vertex graph similar connect similar vertex measure similarity need define notion individual neighborhood direct graph g = ( v e ) v set vertex e ⊆ v × v set edge vertex v ∈ v individual in-neighborhood v defined ( 1129 ) ( v ) = { | ( u v ) ∈ e } symmetrically define individual out-neighborhood v ( 1130 ) ( v ) = { | ( v w ) ∈ e } follow intuition illustrated example 1120 define simrank structural-context similarity value 0 1 pair vertex vertex v ∈ v similarity vertex ( v v ) = 1 neighborhood identical vertex u v ∈ v u = v define x x c ( u v ) = ( x ) ( 1131 ) i ( u ) i ( v ) | x∈i ( u ) y∈i ( v ) c constant 0 vertex may in-neighbor thus define eq ( 1131 ) 0 either ( u ) ( v ) ∅ parameter c specify rate decay similarity propagate across edge “ compute simrank ” straightforward method iteratively evaluate eq ( 1131 ) fix point reach let si ( u v ) simrank score calculate ith round begin set ( 0 u = v s0 ( u v ) = ( 1132 ) 1 u = v use eq ( 1131 ) compute si+1 si si+1 ( u v ) = x c i ( u ) i ( v ) | x x∈i ( u ) y∈i ( v ) si ( x ) ( 1133 ) 
528 chapter 11 advanced cluster analysis show lim si ( u v ) = ( u v ) additional method approximate i→∞ simrank give bibliographic note ( section 117 ) let ’ consider similarity base random walk direct graph strongly connect two node u v path u v another path v u strongly connect graph g = ( v e ) two vertex u v ∈ v define expect distance u v ( u v ) = x u p [ ] l ( ) ( 1134 ) v u v path start u end v may contain cycle reach v end travele tour = w1 → w2 → · · · → wk length l ( ) = k − probability tour defined ( q k−1 1 i=1 o ( wi ) | l ( ) > 0 ( 1135 ) p [ ] = 0 l ( ) = 0 measure probability vertex w receive message originated simultaneously u v extend expect distance notion expect meeting distance x ( u v ) = p [ ] l ( ) ( 1136 ) ( x x ) ( u v ) ( u v ) ( x x ) pair tour u x v x length used constant c 0 1 define expect meeting probability p ( u v ) = x ( u v ) p [ ] c l ( ) ( 1137 ) ( x x ) similarity measure base random walk parameter c specify probability continue walk step trajectory show ( u v ) = p ( u v ) two vertex u v simrank base structural context random walk 1133 graph cluster method let ’ consider conduct cluster graph first describe intuition behind graph cluster discuss two general category graph cluster method find cluster graph imagine cut graph piece piece cluster vertex within cluster well connect vertex different cluster connect much weaker way formally graph g = ( v e ) 
113 cluster graph network datum 529 cut c = ( ) partition set vertex v g v = ∪ ∩ = ∅ cut set cut set edge { ( u v ) ∈ e|u ∈ v ∈ } size cut number edge cut set weight graph size cut sum weight edge cut set “ kind cut good derive cluster graph ” graph theory network application minimum cut importance cut minimum cut ’ size greater cut ’ size polynomial time algorithms compute minimum cut graph use algorithms graph cluster example 1121 cut cluster consider graph g figure graph two cluster { b c e f } { g h j k } one outlier vertex l consider cut c1 = ( { b c e f g h j k } { l } ) one edge namely ( e l ) cross two partition create c1 therefore cut set c1 { ( e l ) } size c1 1 ( note size cut connect graph smaller 1 ) minimum cut c1 lead good cluster separate outlier vertex l rest graph cut c2 = ( { b c e f l } { g h j k } ) lead much better cluster c1 edge cut set c2 connect two “ natural cluster ” graph specifically edge ( h ) ( e k ) cut set edge connect h e k belong one cluster example 1121 indicate used minimum cut unlikely lead good cluster better choose cut vertex u involved edge cut set edge connect u belong one cluster formally let deg ( u ) degree u number edge connect u sparsity cut c = ( ) defined = cut size min { | | } ( 1138 ) sparsest cut c2 b c g f h e k minimum cut c1 l figure 1114 graph g two cut j 
530 chapter 11 advanced cluster analysis cut sparsest sparsity greater sparsity cut may one sparsest cut example 1121 figure 1114 c2 sparsest cut used sparsity objective function sparsest cut try minimize number edge cross partition balance partition size consider cluster graph g = ( v e ) partition graph k cluster modularity cluster assess quality cluster defined = k x i=1   di 2 li − | | ( 1139 ) li number edge vertex ith cluster di sum degree vertex ith cluster modularity cluster graph difference fraction edge fall individual cluster fraction would graph vertex randomly connect optimal cluster graph maximize modularity theoretically many graph cluster problem regard find good cut sparsest cut graph practice however number challenge exist high computational cost many graph cut problem computationally expensive sparsest cut problem example np-hard therefore find optimal solution large graph often impossible good trade-off scalability quality achieve sophisticated graph graph sophisticated one describe involve weight or cycle high dimensionality graph many vertex similarity matrix vertex represent vector ( row matrix ) dimensionality number vertex graph therefore graph cluster method must handle high dimensionality sparsity large graph often sparse meaning vertex average connect small number vertex similarity matrix large sparse graph also sparse two kind method cluster graph datum address challenge one used cluster method high-dimensional datum design specifically cluster graph first group method base generic cluster method highdimensional datum extract similarity matrix graph used similarity measure discuss section generic cluster method apply similarity matrix discover cluster cluster method 
113 cluster graph network datum 531 high-dimensional datum typically employ example many scenario similarity matrix obtain spectral cluster method ( section 1124 ) apply spectral cluster approximate optimal graph cut solution additional information please refer bibliographic note ( section 117 ) second group method specific graph search graph find well-connected component cluster let ’ look method call scan ( structural cluster algorithm network ) example give undirected graph g = ( v e ) vertex u ∈ v neighborhood u 0 ( u ) = { | ( u v ) ∈ e } ∪ { u } used idea structural-context similarity scan measure similarity two vertex u v ∈ v normalize common neighborhood size 0 ( u ) ∩ 0 ( v ) | σ ( u v ) = √ 0 ( u ) 0 ( v ) | ( 1140 ) larger value compute similar two vertex scan used similarity threshold ε define cluster membership vertex u ∈ v ε-neighborhood u defined nε ( u ) = { v ∈ 0 ( u ) σ ( u v ) ≥ ε } ε-neighborhood u contain neighbor u structural-context similarity u least ε scan core vertex vertex inside cluster u ∈ v core vertex nε ( u ) | ≥ µ µ popularity threshold scan grow cluster core vertex vertex v ε-neighborhood core u v assign cluster u process grow cluster continue cluster grow process similar density-based cluster method dbscan ( chapter 10 ) formally vertex v directly reach core u v ∈ nε ( u ) transitively vertex v reach core u exist vertex w1 wn w1 reach u wi reach wi−1 1 < ≤ n v reach wn moreover two vertex u v ∈ v may may core say connect exist core w u v reach w vertex cluster connect cluster maximum set vertex every pair set connect vertex may belong cluster vertex u hub neighborhood 0 ( u ) u contain vertex one cluster vertex belong cluster hub outlier scan algorithm show figure search framework closely resemble cluster-find process dbscan scan find cut graph cluster set vertex connect base transitive similarity structural context advantage scan time complexity linear respect number edge large sparse graph number edge scale number vertex therefore scan expect good scalability cluster large graph 
532 chapter 11 advanced cluster analysis algorithm scan cluster graph datum input graph g = ( v e ) similarity threshold ε population threshold µ output set cluster method set vertex v unlabeled unlabeled vertex u u core generate new cluster-id c insert v ∈ nε ( u ) queue q q = w ← first vertex q r ← set vertex directly reach w ∈ r unlabeled labele nonmember assign current cluster-id c endif unlabeled insert queue q endif endfor remove w q end else label u nonmember endif endfor vertex u labele nonmember ∃x ∈ 0 ( u ) x different cluster-id label u hub else label u outlier endif endfor figure 1115 scan algorithm cluster analysis graph datum 114 cluster constraint user often background knowledge want integrate cluster analysis may also application-specific requirement information modeled cluster constraint approach topic cluster constraint two step section 1141 categorize type constraint cluster graph datum method cluster constraint introduce section 1142 
114 cluster constraint 533 1141 categorization constraint section study categorize constraint used cluster analysis specifically categorize constraint accord subject set strongly constraint enforce discuss chapter 10 cluster analysis involve three essential aspect object instance cluster cluster group object similarity among object therefore first method discuss categorize constraint accord apply thus three type constraint instance constraint cluster constraint similarity measurement constraint instance constraint instance specify pair set instance group cluster analysis two common type constraint category include must-link constraint must-link constraint specify two object x x group one cluster output cluster analysis must-link constraint transitive must-link ( x ) must-link ( z ) must-link ( x z ) link constraint link constraint opposite must-link constraint link constraint specify two object x output cluster analysis x belong different cluster link constraint entail link ( x ) must-link ( x x 0 ) must-link ( 0 ) link ( x 0 0 ) constraint instance defined used specific instance alternatively also defined used instance variable attribute instance example constraint constraint ( x ) must-link ( x ) dist ( x ) ≤  used distance object specify must-link constraint constraint cluster constraint cluster specify requirement cluster possibly used attribute cluster example constraint may specify minimum number object cluster maximum diameter cluster shape cluster ( eg convex ) number cluster specify partition cluster method regard constraint cluster constraint similarity measurement often similarity measure euclidean distance used measure similarity object cluster analysis application exception apply constraint similarity measurement specify requirement similarity calculation must respect example cluster person move object plaza euclidean distance used give 
534 chapter 11 advanced cluster analysis walking distance two point constraint similarity measurement trajectory implement shortest distance cross wall one way express constraint depend category example specify constraint cluster constraint1 diameter cluster larger requirement also expressed used constraint instance constraint10 link ( x ) dist ( x ) > ( 1141 ) example 1122 constraint instance cluster similarity measurement allelectronic cluster customer group customer assign customer relationship manager suppose want specify customer address place group would allow comprehensive service family expressed used must-link constraint instance constraintfamily ( x ) must-link ( x ) xaddress = yaddress allelectronic eight customer relationship manager ensure similar workload place constraint cluster eight cluster cluster least 10 % customer 15 % customer calculate spatial distance two customer used drive distance two however two customer live different country use flight distance instead constraint similarity measurement another way categorize cluster constraint consider firmly constraint respect constraint hard cluster violate constraint unacceptable constraint soft cluster violate constraint preferable acceptable better solution find soft constraint also call preference example 1123 hard soft constraint allelectronic constraintfamily example 1122 hard constraint splitting family different cluster can prevent company provide comprehensive service family lead poor customer satisfaction constraint number cluster ( correspond number customer relationship manager company ) also hard example 1122 also constraint balance size cluster satisfying constraint strongly prefer company flexible willing assign senior capable customer relationship manager oversee larger cluster therefore constraint soft ideally specific datum set set constraint clustering satisfy constraint however possible may cluster datum set 
114 cluster constraint 535 satisfy constraint trivially two constraint set conflict cluster satisfy time example 1124 conflict constraint consider constraint must-link ( x ) dist ( x ) < 5 link ( x ) dist ( x ) > 3 datum set two object x dist ( x ) = 4 cluster satisfy constraint simultaneously consider two constraint must-link ( x ) dist ( x ) < 5 must-link ( x ) dist ( x ) < 3 second constraint redundant give first moreover datum set distance two object least 5 every possible cluster object satisfy constraint “ measure quality usefulness set constraint ” general consider either informativeness coherence informativeness amount information carry constraint beyond cluster model give datum set cluster method set constraint c informativeness c respect measure fraction constraint c unsatisfied cluster compute d higher informativeness specific requirement background knowledge constraint carry coherence set constraint degree agreement among constraint measure redundancy among constraint 1142 method cluster constraint although categorize cluster constraint application may different constraint specific form consequently various technique need handle specific constraint section discuss general principle handle hard soft constraint handle hard constraint general strategy handle hard constraint strictly respect constraint cluster assignment process illustrate idea use partition cluster example 
536 chapter 11 advanced cluster analysis give datum set set constraint instance ( ie must-link link constraint ) extend k-mean method satisfy constraint cop-k-mean algorithm work follow generate superinstance must-link constraint compute transitive closure must-link constraint must-link constraint treat equivalence relation closure give one multiple subset object object subset must assign one cluster represent subset replace object subset mean superinstance also carry weight number object represent step must-link constraint always satisfied conduct modify k-mean cluster recall k-mean object assign closest center nearest-center assignment violate link constraint respect link constraint modify center assignment process k-mean nearest feasible center assignment object assign center sequence step make sure assignment far violate link constraint object assign nearest center assignment respect link constraint cop-k-mean ensure constraint violate every step require backtracking greedy algorithm generate cluster satisfy constraint provide conflict exist among constraint handle soft constraint cluster soft constraint optimization problem cluster violate soft constraint penalty impose cluster therefore optimization goal cluster contain two part optimize cluster quality minimize constraint violation penalty overall objective function combination cluster quality score penalty score illustrate use partition cluster example give datum set set soft constraint instance cvqe ( constrain vector quantization error ) algorithm conduct k-mean cluster enforce constraint violation penalty objective function used cvqe sum distance used k-mean adjust constraint violation penalty calculate follow penalty must-link violation must-link constraint object x assign two different center c1 c2 respectively constraint violate result dist ( c1 c2 ) distance c1 c2 add objective function penalty penalty link violation link constraint object x assign common center c constraint violate 
114 cluster constraint 537 distance dist ( c c 0 ) c c 0 add objective function penalty speeding constrain cluster constraint similarity measurement lead heavy cost cluster consider follow cluster obstacle problem cluster person move object plaza euclidean distance used measure walking distance two point however constraint similarity measurement trajectory implement shortest distance cross wall ( section 1141 ) obstacle may occur object distance two object may derive geometric computation ( eg involve triangulation ) computational cost high large number object obstacle involved cluster obstacle problem represent used graphical notation first point p visible another point q region r straight line join p q intersect obstacle visibility graph graph vg = ( v e ) vertex obstacle corresponding node v two node v1 v2 v joined edge e corresponding vertex represent visible let vg 0 = ( v 0 e 0 ) visibility graph create vg add two additional point p q v 0 e 0 contain edge join two point v 0 two point mutually visible shortest path two point p q subpath vg 0 show figure 1116 ( ) see begin edge p either v1 v2 v3 go path vg end edge either v4 v5 q reduce cost distance computation two pair object point several preprocess optimization technique used one method group point close together microcluster do first triangulating region r triangle grouping nearby point triangle microcluster used method similar birch dbscan show figure 1116 ( b ) process microcluster rather individual point overall computation reduce precomputation perform build two v4 v1 p v2 o1 v3 o2 vg q v5 vg ( ) ( b ) figure 1116 cluster obstacle object ( o1 o2 ) ( ) visibility graph ( b ) triangulation region microcluster source adapt tung hou han [ thh01 ] 
538 chapter 11 advanced cluster analysis kind join index base computation shortest path ( 1 ) vv index pair obstacle vertex ( 2 ) mv index pair microcluster obstacle vertex use index help optimize overall performance used precomputation optimization strategy distance two point ( granularity level microcluster ) compute efficiently thus cluster process perform manner similar typical efficient k-medoid algorithm claran achieve good cluster quality large datum set 115 summary conventional cluster analysis object assign one cluster exclusively however application need assign object one cluster fuzzy probabilistic way fuzzy cluster probabilistic model-based cluster allow object belong one cluster partition matrix record membership degree object belong cluster probabilistic model-based cluster assume cluster parameterized distribution used datum cluster observed sample estimate parameter cluster mixture model assume set observed object mixture instance multiple probabilistic cluster conceptually observed object generate independently first choose probabilistic cluster accord probability cluster choose sample accord probability density function choose cluster expectation-maximization algorithm framework approach maximum likelihood maximum posteriori estimate parameter statistical model expectation-maximization algorithms used compute fuzzy cluster probabilistic model-based cluster high-dimensional datum pose several challenge cluster analysis include model high-dimensional cluster search cluster two major category cluster method high-dimensional datum subspace cluster method dimensionality reduction method subspace cluster method search cluster subspace original space example include subspace search method correlation-based cluster method bicluster method dimensionality reduction method create new space lower dimensionality search cluster bicluster method cluster object attribute simultaneously type bicluster include bicluster constant value constant value column coherent value coherent evolution column two major type bicluster method optimization-based method enumeration method 
116 exercise 539 spectral cluster dimensionality reduction method general idea construct new dimension used affinity matrix cluster graph network datum many application social network analysis challenge include measure similarity object graph design cluster model method graph network datum geodesic distance number edge two vertex graph used measure similarity alternatively similarity graph social network measure used structural context random walk simrank similarity measure base structural context random walk graph cluster modeled compute graph cut sparsest cut may lead good cluster modularity used measure cluster quality scan graph cluster algorithm search graph identify well-connected component cluster constraint used express application-specific requirement background knowledge cluster analysis constraint cluster categorize constraint instance cluster similarity measurement constraint instance include must-link link constraint constraint hard soft hard constraint cluster enforce strictly respect constraint cluster assignment process cluster soft constraint consider optimization problem heuristic used speed constrain cluster 116 exercise 111 traditional cluster method rigid require object belong exclusively one cluster explain special case fuzzy cluster may use k-mean example 112 allelectronic carry 1000 product p1 p1000 consider customer ada bob cathy ada bob purchase three product common p1 p2 p3 997 product ada bob independently purchase seven randomly cathy purchase 10 product randomly select 1000 product euclidean distance probability dist ( ada bob ) > dist ( ada cathy ) jaccard similarity ( chapter 2 ) used learn example 113 show × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 114 compare maple algorithm ( section 1123 ) frequent close itemset mining algorithm closet ( pei han mao [ phm00 ] ) major similarity difference 
540 chapter 11 advanced cluster analysis 115 simrank similarity measure cluster graph network datum ( ) prove lim si ( u v ) = ( u v ) simrank computation i→∞ ( b ) show ( u v ) = p ( u v ) simrank 116 large sparse graph average node low degree similarity matrix used simrank still sparse sense deliberate answer 117 compare scan algorithm ( section 1133 ) dbscan ( section 1041 ) similarity difference 118 consider partition cluster follow constraint cluster number object cluster must nk ( 1 − δ ) nk ( 1 + δ ) n total number object datum set k number cluster desire δ [ 0 1 ) parameter extend k-mean method handle constraint discuss situation constraint hard soft 117 bibliographic note höppner klawonn kruse runkler [ hkkr99 ] provide thorough discussion fuzzy cluster fuzzy c-mean algorithm ( example 117 base ) propose bezdek [ bez81 ] fraley raftery [ fr02 ] give comprehensive overview model-based cluster analysis probabilistic model mclachlan basford [ mb88 ] present systematic introduction mixture model application cluster analysis dempster laird rubin [ dlr77 ] recognize first introduce em algorithm give name however idea em algorithm “ propose many time special circumstance ” admit dempster laird rubin [ dlr77 ] wu [ wu83 ] give correct analysis em algorithm mixture model em algorithms used extensively many datum mining application introduction model-based cluster mixture model em algorithms find recent textbook machine learn statistical learning—for example bishop [ bis06 ] marsland [ mar09 ] alpaydin [ alp11 ] increase dimensionality severe effect distance function indicated beyer et al [ bgrs99 ] also dramatic impact various technique classification cluster semisupervised learn ( radovanović nanopoulos ivanović [ rni09 ] ) kriegel kröger zimek [ kkz09 ] present comprehensive survey method cluster high-dimensional datum clique algorithm develop agrawal gehrke gunopulos raghavan [ aggr98 ] proclus algorithm propose aggawal procopiuc wolf et al [ + 99 ] technique bicluster initially propose hartigan [ har72 ] term bicluster coin mirkin [ mir98 ] cheng church [ cc00 ] introduce 
117 bibliographic note 541 bicluster gene expression datum analysis many study bicluster model method notion δ-pcluster introduce wang wang yang yu [ wwyy02 ] informative survey see madeira oliveira [ mo04 ] tanay sharan shamir [ tss04 ] chapter introduce δ-cluster algorithm cheng church [ cc00 ] maple pei zhang cho et al [ + 03 ] example optimization-based method enumeration method bicluster respectively donath hoffman [ dh73 ] fiedler [ fie73 ] pioneer spectral cluster chapter use algorithm propose ng jordan weis [ njw01 ] example thorough tutorial spectral cluster see luxburg [ lux07 ] cluster graph network datum important fast-growing topic schaeffer [ sch07 ] provide survey simrank measure similarity develop jeh widom [ jw02a ] xu et al [ xyfs07 ] propose scan algorithm arora rao vazirani [ arv09 ] discuss sparsest cut approximation algorithms cluster constraint extensively study davidson wagstaff basu [ dwb06 ] propose measure informativeness coherence copk-mean algorithm give wagstaff et al [ wcrs01 ] cvqe algorithm propose davidson ravi [ dr05 ] tung han lakshmanan ng [ thln01 ] present framework constraint-based cluster base user-specified constraint efficient method constraint-based spatial cluster existence physical obstacle constraint propose tung hou han [ thh01 ] 
13 datum mining trend research frontier young research field datum mining make significant progress cover broad spectrum application since 1980s today datum mining used vast array area numerous commercial datum mining system service available many challenge however still remain final chapter introduce mining complex datum type prelude in-depth study reader may choose addition focus trend research frontier datum mining section 131 present overview methodology mining complex datum type extend concept task introduce book mining include mining time-series sequential pattern biological sequence graph network spatiotemporal datum include geospatial datum moving-object datum cyber-physical system datum multimedium datum text datum web datum datum stream section 132 briefly introduce approach datum mining include statistical method theoretical foundation visual audio datum mining section 133 learn datum mining application business science include financial retail telecommunication industry science engineering recommender system social impact datum mining discuss section 134 include ubiquitous invisible datum mining privacy-preserve datum mining finally section 135 speculate current expect datum mining trend arise response new challenge field 131 mining complex datum type section outline major development research effort mining complex datum type complex datum type summarize figure section 1311 cover mining sequence datum time-series symbolic sequence biological sequence section 1312 discuss mining graph social information network section 1313 address mining kind datum include spatial datum spatiotemporal datum moving-object datum cyber-physical system datum multimedium datum text datum datum mining concept technique doi b978-0-12-381479-100013-7 c 2012 elsevier right re-serve 585 
586 chapter 13 datum mining trend research frontier c p l e x p e f sequence datum graph network mining kind datum time-series datum ( eg stock market datum ) symbolic sequence ( eg customer shopping sequence web click stream ) biological sequence ( eg dna protein sequence ) homogeneous ( link type ) heterogeneous ( link different type ) example graph social information network etc spatial datum spatiotemporal datum cyber-physical system datum multimedium datum text datum web datum datum stream figure 131 complex datum type mining web datum datum stream due broad scope theme section present high-level overview topic discuss in-depth book 1311 mining sequence datum time-series symbolic sequence biological sequence sequence order list event sequence may categorize three group base characteristic event describe ( 1 ) time-series datum ( 2 ) symbolic sequence datum ( 3 ) biological sequence let ’ consider type time-series datum sequence datum consist long sequence numeric datum record equal time interval ( eg per minute per hour per day ) time-series datum generate many natural economic process stock market scientific medical natural observation symbolic sequence datum consist long sequence event nominal datum typically observed equal time interval many sequence gap ( ie lapse record event ) matter much example include customer shopping sequence web click stream well sequence event science engineering natural social development biological sequence include dna protein sequence sequence typically long carry important complicate hide semantic meaning gap usually important let ’ look datum mining sequence datum type 
131 mining complex datum type 587 similarity search time-series datum time-series datum set consist sequence numeric value obtain repeat measurement time value typically measure equal time interval ( eg every minute hour day ) time-series databasis popular many application stock market analysis economic sale forecasting budgetary analysis utility study inventory study yield projection workload projection process quality control also useful study natural phenomena ( eg atmosphere temperature wind earthquake ) scientific engineering experiment medical treatment unlike normal database query find datum match give query exactly similarity search find datum sequence differ slightly give query sequence many time-series similarity query require subsequence match find set sequence contain subsequence similar give query sequence similarity search often necessary first perform datum dimensionality reduction transformation time-series datum typical dimensionality reduction technique include ( 1 ) discrete fourier transform ( dft ) ( 2 ) discrete wavelet transform ( dwt ) ( 3 ) singular value decomposition ( svd ) base principle component analysis ( pca ) touch concept chapter 3 thorough explanation beyond scope book go great detail technique datum signal map signal transform space small subset “ strongest ” transform coefficient save feature feature form feature space projection transform space index construct original transform time-series datum speed search query-based similarity search technique include normalization transformation atomic match ( ie find pair gap-free window small length similar ) window stitching ( ie stitching similar window form pair large similar subsequence allow gap atomic match ) subsequence order ( ie linearly order subsequence match determine whether enough similar piece exist ) numerous software package exist similarity search time-series datum recently researcher propose transform time-series datum piecewise aggregate approximation datum view sequence symbolic representation problem similarity search transform one match subsequence symbolic sequence datum identify motif ( ie frequently occur sequential pattern ) build index hashing mechanism efficient search base motif experiment show approach fast simple comparable search quality dft dwt dimensionality reduction method regression trend analysis time-series datum regression analysis time-series datum study substantially field statistic signal analysis however one may often need go beyond pure regression 
chapter 13 datum mining trend research frontier price 588 allelectronic stock 10-day move average time figure 132 time-series datum stock price allelectronic time trend show dash curve calculate move average analysis perform trend analysis many practical application trend analysis build integrate model used follow four major component movement characterize time-series datum trend long-term movement indicate general direction time-series graph move time example used weight move average least square method find trend curf dash curve indicated figure 132 cyclic movement long-term oscillation trend line curve seasonal variation nearly identical pattern time series appear follow corresponding season successive year holiday shopping season effective trend analysis datum often need “ deseasonalize ” base seasonal index compute autocorrelation random movement characterize sporadic change due chance event labor dispute announce personnel change within company trend analysis also used time-series forecasting find mathematical function approximately generate historic pattern time series used make long-term short-term prediction future value arima ( auto-regressive integrate move average ) long-memory time-series modele autoregression popular method analysis sequential pattern mining symbolic sequence symbolic sequence consist order set element event record without concrete notion time many application involve datum 
131 mining complex datum type 589 symbolic sequence customer shopping sequence web click stream program execution sequence biological sequence sequence event science engineering natural social development biological sequence carry complicate semantic meaning pose many challenge research issue investigation conduct field bioinformatic sequential pattern mining focuse extensively mining symbolic sequence sequential pattern frequent subsequence exist single sequence set sequence sequence α = ha1 a2 · · · subsequence another sequence β = hb1 b2 · · · bm exist integer 1 ≤ j1 < j2 < · · · < jn ≤ a1 ⊆ bj1 a2 ⊆ bj2 ⊆ bjn example α = h { ab } di β = h { abc } { } { de } ai b c e item α subsequence mining sequential pattern consist mining set subsequence frequent one sequence set sequence many scalable algorithms develop result extensive study area alternatively mine set close sequential pattern sequential pattern close exist sequential pattern 0 proper subsequence 0 0 ( frequency ) support s similar frequent pattern mining counterpart also study efficient mining multidimensional multilevel sequential pattern constraint-based frequent pattern mining user-specified constraint used reduce search space sequential pattern mining derive pattern interest user refer constraint-based sequential pattern mining moreover may relax constraint enforce additional constraint problem sequential pattern mining derive different kind pattern sequence datum example enforce gap constraint pattern derive contain consecutive subsequence subsequence small gap alternatively may derive periodic sequential pattern fold event proper-size window find recur subsequence window another approach derive partial order pattern relax requirement strict sequential order mining subsequence pattern besides mining partial order pattern sequential pattern mining methodology also extend mining tree lattice episode order pattern sequence classification classification method perform model construction base feature vector however sequence explicit feature even sophisticated feature selection technique dimensionality potential feature still high sequential nature feature difficult capture make sequence classification challenge task sequence classification method organized three category ( 1 ) featurebased classification transform sequence feature vector apply conventional classification method ( 2 ) sequence distance–based classification distance function measure similarity sequence determine 
590 chapter 13 datum mining trend research frontier quality classification significantly ( 3 ) model-based classification used hide markov model ( hmm ) statistical model classify sequence time-series numeric-valu datum feature selection technique symbolic sequence easily apply time-series datum without discretization however discretization cause information loss recently propose time-series shapelet method used time-series subsequence maximally represent class feature achieve quality classification result alignment biological sequence biological sequence generally refer sequence nucleotide amino acid biological sequence analysis compare align index analyze biological sequence thus play crucial role bioinformatic modern biology sequence alignment base fact live organism related evolution imply nucleotide ( dna rna ) protein sequence species closer evolution exhibit similarity alignment process line sequence achieve maximal identity level also express degree similarity sequence two sequence homologous share common ancestor degree similarity obtain sequence alignment useful determine possibility homology two sequence alignment also help determine relative position multiple species evolution tree call phylogenetic tree problem alignment biological sequence describe follow give two input biological sequence identify similar sequence long conserve subsequence number sequence align exactly two problem know pairwise sequence alignment otherwise multiple sequence alignment sequence compare align either nucleotide ( rna ) amino acid ( protein ) nucleotide two symbol align identical however amino acid two symbol align identical one derive substitution likely occur nature two kind alignment local alignment global alignment former mean portion sequence align whereas latter require alignment entire length sequence either nucleotide amino acid insertion deletion substitution occur nature different probability substitution matrix used represent probability substitution nucleotide amino acid probability insertion deletion usually use gap character − indicate position preferable align two symbol evaluate quality alignment score mechanism typically defined usually count identical similar symbol positive score gap negative one algebraic sum score take alignment measure goal alignment achieve maximal score among possible alignment however expensive ( exactly np-hard problem ) find optimal alignment therefore various heuristic method develop find suboptimal alignment 
131 mining complex datum type 591 dynamic programming approach commonly used sequence alignment among many available analysis package blast ( basic local alignment search tool ) one popular tool biosequence analysis hide markov model biological sequence analysis give biological sequence biologist would like analyze sequence represent represent structure statistical regularity sequence class biologist construct various probabilistic model markov chain hide markov model model probability state depend previous state therefore particularly useful analysis biological sequence datum common method construct hide markov model forward algorithm viterbi algorithm baum-welch algorithm give sequence symbol x forward algorithm find probability obtain x model viterbi algorithm find probable path ( corresponding x ) model whereas baum-welch algorithm learn adjust model parameter best explain set training sequence 1312 mining graph network graph represent general class structure set sequence lattice tree broad range graph application web social network information network biological network bioinformatic chemical informatic computer vision multimedium text retrieval hence graph network mining become increasingly important heavily research overview follow major theme ( 1 ) graph pattern mining ( 2 ) statistical modele network ( 3 ) datum clean integration validation network analysis ( 4 ) cluster classification graph homogeneous network ( 5 ) cluster ranking classification heterogeneous network ( 6 ) role discovery link prediction information network ( 7 ) similarity search olap information network ( 8 ) evolution information network graph pattern mining graph pattern mining mining frequent subgraph ( also call ( sub ) graph pattern ) one set graph method mining graph pattern categorize apriori-based pattern growth–base approach alternatively mine set close graph graph g close exist proper supergraph g 0 carry support count g moreover many variant graph pattern include approximate frequent graph coherent graph dense graph user-specified constraint push deep graph pattern mining process improve mining efficiency graph pattern mining many interesting application example used generate compact effective graph index structure base concept 
592 chapter 13 datum mining trend research frontier frequent discriminative graph pattern approximate structure similarity search achieve explore graph index structure multiple graph feature moreover classification graph also perform effectively used frequent discriminative subgraph feature statistical modele network network consist set node corresponding object associate set property set edge ( link ) connect node represent relationship object network homogeneous node link type friend network coauthor network web page network network heterogeneous node link different type publication network ( link together author conference paper content ) health-care network ( link together doctor nurse patient disease treatment ) researcher propose multiple statistical model modele homogeneous network well-known generative model random graph model ( ie erdös-rényi model ) watts-strogatz model scale-free model scalefree model assume network follow power law distribution ( also know pareto distribution heavy-tailed distribution ) large-scale social network small-world phenomenon observed network characterize high degree local cluster small fraction node ( ie node interconnect one another ) degree separation remain node social network exhibit certain evolutionary characteristic tend follow densification power law state network become increasingly dense time shrink diameter another characteristic effective diameter often decrease network grow node out-degree in-degree typically follow heavytailed distribution datum clean integration validation information network analysis real-world datum often incomplete noisy uncertain unreliable information redundancy may exist among multiple piece datum interconnect large network information redundancy explore network perform quality datum clean datum integration information validation trustability analysis network analysis example distinguish author share name examine networked connection heterogeneous object coauthor publication venue term addition identify inaccurate author information present bookseller explore network build base author information provide multiple bookseller sophisticated information network analysis method develop direction many case portion datum serve “ training ” relatively clean reliable datum consensus datum multiple information 
131 mining complex datum type 593 provider used help consolidate remain unreliable portion datum reduce costly effort labele datum hand training massive dynamic real-world datum set cluster classification graph homogeneous network large graph network cohesive structure often hide among massive interconnect node link cluster analysis method develop large network uncover network structure discover hide community hub outlier base network topological structure associate property various kind network cluster method develop categorize either partition hierarchical density-based algorithms moreover give human-labele training datum discovery network structure guide human-specify heuristic constraint supervised classification semi-supervised classification network recent hot topic datum mining research community cluster ranking classification heterogeneous network heterogeneous network contain interconnect node link different type interconnect structure contain rich information used mutually enhance node link propagate knowledge one type another cluster ranking heterogeneous network perform hand-inhand context highly rank link cluster may contribute lower-rank counterpart evaluation cohesiveness cluster cluster may help consolidate high ranking link dedicate cluster mutual enhancement ranking cluster prompt development algorithm call rankclus moreover user may specify different ranking rule present labele link certain datum type knowledge one type propagate type propagation reach link type via heterogeneous-type connection algorithms develop supervised learn semi-supervised learn heterogeneous network role discovery link prediction information network exist many hide role relationship among different link heterogeneous network example include advisor–advisee leader–follower relationship research publication network discover hide role relationship expert specify constraint base background knowledge enforce constraint may help crosscheck validation large interconnect network information redundancy network often used help weed link follow constraint 
594 chapter 13 datum mining trend research frontier similarly link prediction perform base assessment ranking expect relationship among candidate link example may predict paper author may write read cite base author ’ recent publication history trend research similar topic study often require analyze proximity network link trend connection similar neighbor roughly speaking person refer link prediction link mining however link mining cover additional task include link-based object classification object type prediction link type prediction link existence prediction link cardinality estimation object reconciliation ( predict whether two object fact ) also include group detection ( cluster object ) well subgraph identification ( find characteristic subgraph within network ) metadata mining ( uncover schema-type information regard unstructured datum ) similarity search olap information network similarity search primitive operation database web search engine heterogeneous information network consist multityped interconnect object example include bibliographic network social medium network two object consider similar link similar way multityped object general object similarity within network determine base network structure object property similarity measure moreover network cluster hierarchical network structure help organize object network identify subcommunity well facilitate similarity search furthermore similarity defined differently per user consider different linkage path derive various similarity semantic network know path-based similarity organize network base notion similarity cluster generate multiple hierarchy within network online analytical process ( olap ) perform example drill dice information network base different level abstraction different angle view olap operation may generate multiple interrelate network relationship among network may disclose interesting hide semantic evolution social information network network dynamic constantly evolve detect evolve community evolve regularity anomaly homogeneous heterogeneous network help person better understand structural evolution network predict trend irregularity evolve network homogeneous network evolve community discover subnetwork consist object type set friend coauthor however heterogeneous network community discover subnetwork consist object different type connect set paper author venue term also derive set evolve object type like evolve author theme 
131 mining complex datum type 595 1313 mining kind datum addition sequence graph many kind semi-structure unstructured datum spatiotemporal multimedium hypertext datum interesting application datum carry various kind semantic either store dynamically stream system call specialize datum mining methodology thus mining multiple kind datum include spatial datum spatiotemporal datum cyber-physical system datum multimedium datum text datum web datum datum stream increasingly important task datum mining subsection overview methodology mining kind datum mining spatial datum spatial datum mining discover pattern knowledge spatial datum spatial datum many case refer geospace-related datum store geospatial datum repository datum “ vector ” “ raster ” format form imagery geo-reference multimedium recently large geographic datum warehouse construct integrate thematic geographically reference datum multiple source construct spatial datum cube contain spatial dimension measure support spatial olap multidimensional spatial datum analysis spatial datum mining perform spatial datum warehouse spatial databasis geospatial datum repository popular topic geographic knowledge discovery spatial datum mining include mining spatial association co-location pattern spatial cluster spatial classification spatial modele spatial trend outlier analysis mining spatiotemporal datum move object spatiotemporal datum datum relate space time spatiotemporal datum mining refer process discover pattern knowledge spatiotemporal datum typical example spatiotemporal datum mining include discover evolutionary history city land uncover weather pattern predict earthquake hurricane determine global warm trend spatiotemporal datum mining become increasingly important far-reaching implication give popularity mobile phone gps device internet-based map service weather service digital earth well satellite rfid sensor wireless video technology among many kind spatiotemporal datum moving-object datum ( ie datum move object ) especially important example animal scientist attach telemetry equipment wildlife analyze ecological behavior mobility manager emb gps car better monitor guide vehicle meteorologist use weather satellite radar observe hurricane massive-scale moving-object datum become rich complex ubiquitous example moving-object datum mining include mining movement pattern multiple move object ( ie discovery relationship among multiple move object move cluster leader follower merge convoy swarm pincer well collective movement pattern ) example 
596 chapter 13 datum mining trend research frontier moving-object datum mining include mining periodic pattern one set move object mining trajectory pattern cluster model outlier mining cyber-physical system datum cyber-physical system ( cp ) typically consist large number interact physical information component cp system may interconnect form large heterogeneous cyber-physical network example cyber-physical network include patient care system link patient monitoring system network medical information emergency handle system transportation system link transportation monitoring network consist many sensor video camera traffic information control system battlefield commander system link reconnaissance network battlefield information analysis system clearly cyber-physical system network ubiquitous form critical component modern information infrastructure datum generate cyber-physical system dynamic volatile noisy inconsistent interdependent contain rich spatiotemporal information critically important real-time decision make comparison typical spatiotemporal datum mining mining cyber-physical datum require link current situation large information base perform real-time calculation return prompt response research area include rare-event detection anomaly analysis cyber-physical datum stream reliability trustworthiness cyber-physical datum analysis effective spatiotemporal datum analysis cyber-physical network integration stream datum mining real-time automate control process mining multimedium datum multimedium datum mining discovery interesting pattern multimedium databasis store manage large collection multimedium object include image datum video datum audio datum well sequence datum hypertext datum contain text text markup linkage multimedium datum mining interdisciplinary field integrate image process understand computer vision datum mining pattern recognition issue multimedium datum mining include content-based retrieval similarity search generalization multidimensional analysis multimedium datum cube contain additional dimension measure multimedium information topic multimedium mining include classification prediction analysis mining association video audio datum mining ( section 1323 ) mining text datum text mining interdisciplinary field draw information retrieval datum mining machine learn statistic computational linguistic substantial portion information store text news article technical paper book digital library email message blog web page hence research text mining active important goal derive high-quality information text 
131 mining complex datum type 597 typically do discovery pattern trend mean statistical pattern learn topic modele statistical language modele text mining usually require structuring input text ( eg parse along addition derive linguistic feature removal other subsequent insertion database ) follow derive pattern within structure datum evaluation interpretation output “ high quality ” text mining usually refer combination relevance novelty interestingness typical text mining task include text categorization text cluster entity extraction production granular taxonomy sentiment analysis document summarization entity-relation modele ( ie learn relation name entity ) example include multilingual datum mining multidimensional text analysis contextual text mining trust evolution analysis text datum well text mining application security biomedical literature analysis online medium analysis analytical customer relationship management various kind text mining analysis software tool available academic institution open-source forum industry text mining often also used wordnet sematic web wikipedia information source enhance understand mining text datum mining web datum world wide web serve huge widely distribute global information center news advertisement consumer information financial management education government e-commerce contain rich dynamic collection information web page content hypertext structure multimedium hyperlink information access usage information provide fertile source datum mining web mining application datum mining technique discover pattern structure knowledge web accord analysis target web mining organized three main area web content mining web structure mining web usage mining web content mining analyze web content text multimedium datum structure datum ( within web page link across web page ) do understand content web page provide scalable informative keyword-based page indexing concept resolution web page relevance ranking web page content summary valuable information related web search analysis web page reside either surface web deep web surface web portion web index typical search engine deep web ( hide web ) refer web content part surface web content provide underlie database engine web content mining study extensively researcher search engine web service company web content mining build link across multiple web page individual therefore potential inappropriately disclose personal information study privacy-preserve datum mining address concern development technique protect personal privacy web web structure mining process used graph network mining theory method analyze node connection structure web extract pattern hyperlink hyperlink structural component connect 
598 chapter 13 datum mining trend research frontier web page another location also mine document structure within page ( eg analyze treelike structure page structure describe html xml tag usage ) kind web structure mining help us understand web content may also help transform web content relatively structure datum set web usage mining process extract useful information ( eg user click stream ) server log find pattern related general particular group user understand user ’ search pattern trend association predict user look internet help improve search efficiency effectiveness well promote product related information different group user right time web search company routinely conduct web usage mining improve quality service mining datum stream stream datum refer datum flow system vast volume change dynamically possibly infinite contain multidimensional feature datum store traditional database system moreover system may able read stream sequential order pose great challenge effective mining stream datum substantial research lead progress development efficient method mining datum stream area mining frequent sequential pattern multidimensional analysis ( eg construction stream cube ) classification cluster outlier analysis online detection rare event datum stream general philosophy develop single-scan a-few-scan algorithms used limit compute storage capability include collect information stream datum slide window tilt time window ( recent datum register finest granularity distant datum register coarser granularity ) explore technique like microcluster limit aggregation approximation many application stream datum mining explored—for example real-time detection anomaly computer network traffic botnet text stream video stream power-grid flow web search sensor network cyber-physical system 132 methodology datum mining due broad scope datum mining large variety datum mining methodology methodology datum mining thoroughly cover book section briefly discuss several interesting methodology fully address previous chapter methodology list figure 133 1321 statistical datum mining datum mining technique describe book primarily draw computer science discipline include datum mining machine learn datum warehousing algorithms design efficient handle huge amount datum 
132 methodology datum mining h e r n n g e h l g e statistical datum mining foundation datum mining visual audio datum mining 599 regression generalized linear model analysis variance mixed-effect model factor analysis discriminant analysis survival analysis datum reduction datum compression probability statistical theory microeconomic view pattern discovery inductive database datum visualization datum mining result visualization datum mining process visualization interactive visual datum mining audio datum mining figure 133 datum mining methodology typically multidimensional possibly various complex type however many well-established statistical technique datum analysis particularly numeric datum technique apply extensively scientific datum ( eg datum experiment physics engineering manufacturing psychology medicine ) well datum economic social science technique principal component analysis ( chapter 3 ) cluster ( chapter 10 11 ) already address book thorough discussion major statistical method datum analysis beyond scope book however several method mentioned sake completeness pointer technique provide bibliographic note ( section 138 ) regression general method used predict value response ( dependent ) variable one predictor ( independent ) variable variable numeric various form regression linear multiple weight polynomial nonparametric robust ( robust method useful error fail satisfy normalcy condition datum contain significant outlier ) generalized linear model model generalization ( generalized additive model ) allow categorical ( nominal ) response variable ( transformation 
600 chapter 13 datum mining trend research frontier ) related set predictor variable manner similar modele numeric response variable used linear regression generalized linear model include logistic regression poisson regression analysis variance technique analyze experimental datum two population describe numeric response variable one categorical variable ( factor ) general anova ( single-factor analysis variance ) problem involve comparison k population treatment mean determine least two mean different complex anova problem also exist mixed-effect model model analyze group data—data classify accord one grouping variable typically describe relationship response variable covariate datum group accord one factor common area application include multilevel datum repeat measure datum block design longitudinal datum factor analysis method used determine variable combine generate give factor example many psychiatric datum possible measure certain factor interest directly ( eg intelligence ) however often possible measure quantity ( eg student test score ) reflect factor interest none variable designate dependent discriminant analysis technique used predict categorical response variable unlike generalized linear model assume independent variable follow multivariate normal distribution procedure attempt determine several discriminant function ( linear combination independent variable ) discriminate among group defined response variable discriminant analysis commonly used social science survival analysis several well-established statistical technique exist survival analysis technique originally design predict probability patient undergo medical treatment would survive least time t method survival analysis however also commonly apply manufacturing setting estimate life span industrial equipment popular method include kaplanmeier estimate survival cox proportional hazard regression model extension quality control various statistic used prepare chart quality control shewhart chart cusum chart ( display group summary statistic ) statistic include mean standard deviation range count move average move standard deviation move range 1322 view datum mining foundation research theoretical foundation datum mining yet mature solid systematic theoretical foundation important help provide coherent 
132 methodology datum mining 601 framework development evaluation practice datum mining technology several theory basis datum mining include follow datum reduction theory basis datum mining reduce datum representation datum reduction trade accuracy speed response need obtain quick approximate answer query large databasis datum reduction technique include singular value decomposition ( drive element behind principal component analysis ) wavelet regression log-linear model histogram cluster sampling construction index tree datum compression accord theory basis datum mining compress give datum encode term bit association rule decision tree cluster encode base minimum description length principle state “ best ” theory infer datum set one minimize length theory datum encode used theory predictor datum encode typically bit probability statistical theory accord theory basis datum mining discover joint probability distribution random variable example bayesian belief network hierarchical bayesian model microeconomic view microeconomic view consider datum mining task find pattern interesting extent used decision-make process enterprise ( eg regard marketing strategy production plan ) view one utility pattern consider interesting act enterprise regard face optimization problem object maximize utility value decision theory datum mining become nonlinear optimization problem pattern discovery inductive databasis theory basis datum mining discover pattern occur datum association classification model sequential pattern area machine learn neural network association mining sequential pattern mining cluster several subfield contribute theory knowledge base view database consist datum pattern user interact system query datum theory ( ie pattern ) knowledge base knowledge base actually inductive database theory mutually exclusive example pattern discovery also see form datum reduction datum compression ideally theoretical framework able model typical datum mining task ( eg association classification cluster ) probabilistic nature able handle different form datum consider iterative interactive essence datum mining effort require establish well-defined framework datum mining satisfy requirement 
602 chapter 13 datum mining trend research frontier 1323 visual audio datum mining visual datum mining discover implicit useful knowledge large datum set used datum or knowledge visualization technique human visual system controlled eye brain latter thought powerful highly parallel process reasoning engine contain large knowledge base visual datum mining essentially combine power component make highly attractive effective tool comprehension datum distribution pattern cluster outlier datum visual datum mining view integration two discipline datum visualization datum mining also closely related computer graphic multimedium system human–computer interaction pattern recognition high-performance compute general datum visualization datum mining integrate follow way datum visualization datum database datum warehouse view different granularity abstraction level different combination attribute dimension datum present various visual form boxplot 3-d cube datum distribution chart curf surface link graph show datum visualization section chapter figure 134 135 statsoft show figure 134 boxplot show multiple variable combination statsoft source wwwstatsoftcom 
132 methodology datum mining 603 figure 135 multidimensional datum distribution analysis statsoft source wwwstatsoftcom datum distribution multidimensional space visual display help give user clear impression overview datum characteristic large datum set datum mining result visualization visualization datum mining result presentation result knowledge obtain datum mining visual form form may include scatter plot boxplot ( chapter 2 ) well decision tree association rule cluster outlier generalized rule example scatter plot show figure 136 sas enterprise miner figure 137 mineset used plane associate set pillar describe set association rule mine database figure 138 also mineset present decision tree figure 139 ibm intelligent miner present set cluster property associate datum mining process visualization type visualization present various process datum mining visual form user see datum extract database datum warehouse extract well select datum clean integrate preprocessed mine moreover may also show method select datum mining result store may view figure 1310 show visual presentation datum mining process clementine datum mining system 
604 chapter 13 datum mining trend research frontier figure 136 visualization datum mining result sas enterprise miner interactive visual datum mining ( interactive ) visual datum mining visualization tool used datum mining process help user make smart datum mining decision example datum distribution set attribute display used colored sector ( whole space represent circle ) display help user determine sector first select classification good split point sector may example show figure 1311 output perception-based classification ( pbc ) system develop university munich audio datum mining used audio signal indicate pattern datum feature datum mining result although visual datum mining may disclose interesting pattern used graphical display require user concentrate watch pattern identify interesting novel feature within sometimes quite tiresome pattern transform sound music instead watch picture listen pitch rhythm tune melody identify anything interesting unusual may relieve burden visual concentration 
132 methodology datum mining figure 137 visualization association rule mineset figure 138 visualization decision tree mineset 605 
606 chapter 13 datum mining trend research frontier figure 139 visualization cluster grouping ibm intelligent miner figure 1310 visualization datum mining process clementine 
133 datum mining application 607 figure 1311 perception-based classification interactive visual mining approach relax visual mining therefore audio datum mining interesting complement visual mining 133 datum mining application book study principle method mining relational datum datum warehouse complex datum type datum mining relatively young discipline wide diverse application still nontrivial gap general principle datum mining application-specific effective datum mining tool section examine several application domain list figure discuss customize datum mining method tool develop application 1331 datum mining financial datum analysis bank financial institution offer wide variety banking investment credit service ( latter include business mortgage automobile loan credit card ) also offer insurance stock investment service 
608 chapter 13 datum mining trend research frontier financial datum analysis retail telecommunication industry science engineering datum mining application intrusion detection prevention recommender system figure 1312 common datum mining application domain financial datum collect banking financial industry often relatively complete reliable high quality facilitate systematic datum analysis datum mining present typical case design construction datum warehouse multidimensional datum analysis datum mining like many application datum warehouse need construct banking financial datum multidimensional datum analysis method used analyze general property datum example company ’ financial officer may want view debt revenue change month region sector factor along maximum minimum total average trend deviation statistical information datum warehouse datum cube ( include advanced datum cube concept multifeature discovery-driven regression prediction datum cube ) characterization class comparison cluster outlier analysis play important role financial datum analysis mining loan payment prediction customer credit policy analysis loan payment prediction customer credit analysis critical business bank many factor strongly weakly influence loan payment performance customer credit rating datum mining method attribute selection attribute relevance ranking may help identify important factor eliminate irrelevant one example factor related risk loan payment include loan-to-value ratio term loan debt ratio ( total amount monthly debt versus total monthly income ) payment-to-income ratio customer income level education level residence region credit history analysis customer payment history may find say payment-to-income ratio dominant factor education level debt ratio bank may decide adjust loan-grant policy 
133 datum mining application 609 grant loan customer whose application previously deny whose profile show relatively low risk accord critical factor analysis classification cluster customer target marketing classification cluster method used customer group identification target marketing example use classification identify crucial factor may influence customer ’ decision regard banking customer similar behavior regard loan payment may identify multidimensional cluster technique help identify customer group associate new customer appropriate customer group facilitate target marketing detection money launder financial crime detect money launder financial crime important integrate information multiple heterogeneous databasis ( eg bank transaction databasis federal state crime history databasis ) long potentially related study multiple datum analysis tool used detect unusual pattern large amount cash flow certain period certain group customer useful tool include datum visualization tool ( display transaction activity used graph time group customer ) linkage information network analysis tool ( identify link among different customer activity ) classification tool ( filter unrelated attribute rank highly related one ) cluster tool ( group different case ) outlier analysis tool ( detect unusual amount fund transfer activity ) sequential pattern analysis tool ( characterize unusual access sequence ) tool may identify important relationship pattern activity help investigator focus suspicious case detailed examination 1332 datum mining retail telecommunication industry retail industry well-fit application area datum mining since collect huge amount datum sale customer shopping history good transportation consumption service quantity datum collect continue expand rapidly especially due increase availability ease popularity business conduct web e-commerce today major chain store also web site customer make purchase online business amazoncom ( wwwamazoncom ) exist solely online without brick-and-mortar ( ie physical ) store location retail datum provide rich source datum mining retail datum mining help identify customer buy behavior discover customer shopping pattern trend improve quality customer service achieve better customer retention satisfaction enhance good consumption ratio design effective good transportation distribution policy reduce cost business example datum mining retail industry outlined follow design construction datum warehouse retail datum cover wide spectrum ( include sale customer employee good transportation consumption 
610 chapter 13 datum mining trend research frontier service ) many way design datum warehouse industry level detail include vary substantially outcome preliminary datum mining exercise used help guide design development datum warehouse structure involve decide dimension level include preprocess perform facilitate effective datum mining multidimensional analysis sale customer product time region retail industry require timely information regard customer need product sale trend fashion well quality cost profit service commodity therefore important provide powerful multidimensional analysis visualization tool include construction sophisticated datum cube accord need datum analysis advanced datum cube structure introduce chapter 5 useful retail datum analysis facilitate analysis multidimensional aggregate complex condition analysis effectiveness sale campaign retail industry conduct sale campaign used advertisement coupon various kind discount bonuse promote product attract customer careful analysis effectiveness sale campaign help improve company profit multidimensional analysis used purpose compare amount sale number transaction contain sale item sale period versus contain item sale campaign moreover association analysis may disclose item likely purchase together item sale especially comparison sale campaign customer retention—analysis customer loyalty use customer loyalty card information register sequence purchase particular customer customer loyalty purchase trend analyze systematically good purchase different period customer group sequence sequential pattern mining used investigate change customer consumption loyalty suggest adjustment pricing variety good help retain customer attract new one product recommendation cross-referencing item mining association sale record may discover customer buy digital camera likely buy another set item information used form product recommendation collaborative recommender system ( section 1335 ) use datum mining technique make personalize product recommendation live customer transaction base opinion customer product recommendation also advertised sale receipt weekly flyer web help improve customer service aid customer select item increase sale similarly information “ hot item week ” attractive deal display together associative information promote sale fraudulent analysis identification unusual pattern fraudulent activity cost retail industry million dollar per year important ( 1 ) identify potentially fraudulent user atypical usage pattern ( 2 ) detect attempt gain fraudulent entry unauthorized access individual organizational 
133 datum mining application 611 account ( 3 ) discover unusual pattern may need special attention many pattern discover multidimensional analysis cluster analysis outlier analysis another industry handle huge amount datum telecommunication industry quickly evolved offer local long-distance telephone service provide many comprehensive communication service include cellular phone smart phone internet access email text message image computer web datum transmission datum traffic integration telecommunication computer network internet numerous mean communication compute way change face telecommunication compute create great demand datum mining help understand business dynamic identify telecommunication pattern catch fraudulent activity make better use resource improve service quality datum mining task telecommunication share many similarity retail industry common task include construct large-scale datum warehouse perform multidimensional visualization olap in-depth analysis trend customer pattern sequential pattern task contribute business improvement cost reduction customer retention fraud analysis sharpen edge competition many datum mining task customize datum mining tool telecommunication flourishing expect play increasingly important role business datum mining popularly used many industry insurance manufacturing health care well analysis governmental institutional administration datum although industry characteristic datum set application demand share many common principle methodology therefore effective mining one industry may gain experience methodology transfer industrial application 1333 datum mining science engineering past many scientific datum analysis task tend handle relatively small homogeneous datum set datum typically analyze used “ formulate hypothesis build model evaluate result ” paradigm case statistical technique typically employ analysis ( see section 1321 ) massive datum collection storage technology recently change landscape scientific datum analysis today scientific datum amassed much higher speed lower cost result accumulation huge volume high-dimensional datum stream datum heterogenous datum contain rich spatial temporal information consequently scientific application shift “ hypothesize-and-test ” paradigm toward “ collect store datum mine new hypothesis confirm datum experimentation ” process shift bring new challenge datum mining vast amount datum collect scientific domain ( include geoscience astronomy meteorology geology biological science ) used sophisticated 
612 chapter 13 datum mining trend research frontier telescope multispectral high-resolution remote satellite sensor global position system new generation biological datum collection analysis technology large datum set also generate due fast numeric simulation various field climate ecosystem modele chemical engineering fluid dynamic structural mechanic look challenge bring emerge scientific application datum mining datum warehouse datum preprocess datum preprocess datum warehouse critical information exchange datum mining create warehouse often require find mean resolve inconsistent incompatible datum collect multiple environment different time period require reconcile semantic reference system geometry measurement accuracy precision method need integrate datum heterogeneous source identify event instance consider climate ecosystem datum spatial temporal require cross-referencing geospatial datum major problem analyze datum many event spatial domain temporal domain example el nino event occur every four seven year previous datum might collect systematically today method also need efficient computation sophisticated spatial aggregate handle spatial-related datum stream mining complex datum type scientific datum set heterogeneous nature typically involve semi-structure unstructured datum multimedium datum georeference stream datum well datum sophisticated deeply hide semantic ( eg genomic proteomic datum ) robust dedicate analysis method need handle spatiotemporal datum biological datum related concept hierarchy complex semantic relationship example bioinformatic research problem identify regulatory influence gene gene regulation refer gene cell switch ( ) determine cell ’ function different biological process involve different set gene act together precisely regulate pattern thus understand biological process need identify participate gene regulator require development sophisticated datum mining method analyze large biological datum set clue regulatory influence specific gene find dna segment ( “ regulatory sequence ” ) mediate influence graph-based network-based mining often difficult impossible model several physical phenomena process due limitation exist modele approach alternatively labele graph network may used capture many spatial topological geometric biological relational characteristic present scientific datum set graph network modele object mine represent vertex graph edge vertex represent relationship object example graph used model chemical structure biological pathway datum generate numeric 
133 datum mining application 613 simulation fluid-flow simulation success graph network modele however depend improvement scalability efficiency many graph-based datum mining task classification frequent pattern mining cluster visualization tool domain-specific knowledge high-level graphical user interface visualization tool require scientific datum mining system integrate exist domain-specific datum information system guide researcher general user search pattern interpret visualize discover pattern used discover knowledge decision make datum mining engineering share many similarity datum mining science practice often collect massive amount datum require datum preprocess datum warehousing scalable mining complex type datum typically use visualization make good use graph network moreover many engineering process need real-time response mining datum stream real time often become critical component massive amount human communication datum pour daily life communication exist many form include news blog article web page online discussion product reviews twitter message advertisement communication web various kind social network hence datum mining social science social study become increasingly popular moreover user reader feedback regard product speech article analyze deduce general opinion sentiment view society analysis result used predict trend improve work help decision make computer science generate unique kind datum example computer program long execution often generate huge-size trace computer network complex structure network flow dynamic massive sensor network may generate large amount datum varied reliability computer system databasis suffer various kind attack data access may raise security privacy concern unique kind datum provide fertile land datum mining datum mining computer science used help monitor system status improve system performance isolate software bug detect software plagiarism analyze computer system fault uncover network intrusion recognize system malfunction datum mining software system engineering operate static dynamic ( ie stream-based ) datum depend whether system dump trace beforehand postanalysis must react real time handle online datum various method develop domain integrate extend method machine learn datum mining system engineering pattern recognition statistic datum mining computer science active rich domain datum miner unique challenge require development sophisticated scalable real-time datum mining system engineering method 
614 chapter 13 datum mining trend research frontier 1334 datum mining intrusion detection prevention security computer system datum continual risk extensive growth internet increase availability tool trick intrude attack network prompt intrusion detection prevention become critical component networked system intrusion defined set action threaten integrity confidentiality availability network resource ( eg user account file system system kernel ) intrusion detection system intrusion prevention system monitor network traffic or system execution malicious activity however former produce report whereas latter place in-line able actively block intrusion detected main function intrusion prevention system identify malicious activity log information say activity attempt stop activity report activity majority intrusion detection prevention system use either signaturebased detection anomaly-based detection signature-based detection method detection utilize signature attack pattern preconfigured predetermine domain expert signature-based intrusion prevention system monitor network traffic match signature match find intrusion detection system report anomaly intrusion prevention system take additional appropriate action note since system usually quite dynamic signature need update laboriously whenever new software version arrive change network configuration situation occur another drawback detection mechanism identify case match signature unable detect new previously unknown intrusion trick anomaly-based detection method build model normal network behavior ( call profile ) used detect new pattern significantly deviate profile deviation may represent actual intrusion simply new behavior need add profile main advantage anomaly detection may detect novel intrusion yet observed typically human analyst must sort deviation ascertain represent real intrusion limit factor anomaly detection high percentage false positive new pattern intrusion add set signature enhance signature-based detection datum mining method help intrusion detection prevention system enhance performance various way follow new datum mining algorithms intrusion detection datum mining algorithms used signature-based anomaly-based detection signature-based detection training datum labele either “ normal ” “ ” classifier derive detect know intrusion research area 
133 datum mining application 615 include application classification algorithms association rule mining cost-sensitive modele anomaly-based detection build model normal behavior automatically detect significant deviation method include application cluster outlier analysis classification algorithms statistical approach technique used must efficient scalable capable handle network datum high volume dimensionality heterogeneity association correlation discriminative pattern analysis help select build discriminative classifier association correlation discriminative pattern mining apply find relationship system attribute describe network datum information provide insight regard selection useful attribute intrusion detection new attribute derive aggregate datum may also helpful summary count traffic match particular pattern analysis stream datum due transient dynamic nature intrusion malicious attack crucial perform intrusion detection datum stream environment moreover event may normal consider malicious view part sequence event thus necessary study sequence event frequently encounter together find sequential pattern identify outlier datum mining method find evolve cluster build dynamic classification model datum stream also necessary real-time intrusion detection distribute datum mining intrusion launch several different location target many different destination distribute datum mining method may used analyze network datum several network location detect distribute attack visualization query tool visualization tool available view anomalous pattern detected tool may include feature view association discriminative pattern cluster outlier intrusion detection system also graphical user interface allow security analyst pose query regard network datum intrusion detection result summary computer system continual risk break security datum mining technology used develop strong intrusion detection prevention system may employ signature-based anomaly-based detection 1335 datum mining recommender system today ’ consumer face million good service shopping online recommender system help consumer make product recommendation likely interest user book cds movie restaurant online news article service recommender system may use either contentbased approach collaborative approach hybrid approach combine content-based collaborative method 
616 chapter 13 datum mining trend research frontier content-based approach recommend item similar item user prefer query past rely product feature textual item description collaborative approach ( collaborative filter approach ) may consider user ’ social environment recommend item base opinion customer similar taste preference user recommender system use broad range technique information retrieval statistic machine learn datum mining search similarity among item customer preference consider example 131 example 131 scenario used recommender system suppose visit web site online bookstore ( eg amazon ) intention purchasing book want read type name book first time visit web site browse even make purchase last christmas web store remember previous visit store click stream information information regard past purchase system display description price book specify compare interest customer similar interest recommend additional book title say “ customer buy book specify also buy title ” survey list see another title spark interest decide purchase one well suppose go another online store intention purchasing digital camera system suggest additional item consider base previously mine sequential pattern “ customer buy kind digital camera likely buy particular brand printer memory card photo editing software within three ” decide buy camera without additional item week later receive coupon store regard additional item advantage recommender system provide personalization customer e-commerce promote one-to-one marketing amazon pioneer use collaborative recommender system offer “ personalize store every customer ” part marketing strategy personalization benefit consumer company involved accurate model customer company gain better understand customer need serve need result greater success regard cross-selling related product upsel product affinity one-to-one promotion larger basket customer retention recommendation problem consider set c user set item let u utility function measure usefulness item user c utility commonly represent rating initially defined item previously rate user example join movie recommendation system user typically ask rate several movie space c × possible user item huge recommendation system able extrapolate know unknown rating predict item–user combination item highest predict utility user recommend user 
133 datum mining application 617 “ utility item estimate user ” content-based method estimate base utility assign user item similar many system focus recommend item contain textual information web site article news message look commonality among item movie may look similar genre director actor article may look similar term content-based method root information theory make use keyword ( describe item ) user profile contain information user ’ taste need profile may obtain explicitly ( eg questionnaire ) learn user ’ transactional behavior time collaborative recommender system try predict utility item user u base item previously rate user similar u example recommend book collaborative recommender system try find user history agree u ( eg tend buy similar book give similar rating book ) collaborative recommender system either memory ( heuristic ) base model base memory-based method essentially use heuristic make rating prediction base entire collection item previously rate user unknown rating item–user combination estimate aggregate rating similar user item typically k-nearest-neighbor approach used find k user ( neighbor ) similar target user u various approach used compute similarity user popular approach use either pearson ’ correlation coefficient ( section 332 ) cosine similarity ( section 247 ) weight aggregate used adjust fact different user may use rating scale differently model-based collaborative recommender system use collection rating learn model used make rating prediction example probabilistic model cluster ( find cluster like-minded customer ) bayesian network machine learn technique used recommender system face major challenge scalability ensure quality recommendation consumer example regard scalability collaborative recommender system must able search million potential neighbor real time site used browse pattern indication product preference may thousand datum point customer ensure quality recommendation essential gain consumer ’ trust consumer follow system recommendation end liking product less likely use recommender system classification system recommender system make two type error false negative false positive false negative product system fail recommend although consumer would like false positive product recommend consumer like false positive less desirable annoy anger consumer content-based recommender system limit feature used describe item recommend 
618 chapter 13 datum mining trend research frontier another challenge content-based collaborative recommender system deal new user buy history yet available hybrid approach integrate content-based collaborative method achieve improve recommendation netflix prize open competition hold online dvd-rental service payout $ 1000000 best recommender algorithm predict user rating film base previous rating competition study show predictive accuracy recommender system substantially improve blending multiple predictor especially used ensemble many substantially different method rather refine single technique collaborative recommender system form intelligent query answer consist analyze intent query provide generalized neighborhood associate information relevant query example rather simply return book description price response customer ’ query return additional information related query explicitly ask ( eg book evaluation comment recommendation book sale statistic ) provide intelligent answer query 134 datum mining society us datum mining part daily life although may often unaware presence section 1341 look several example “ ubiquitous invisible ” datum mining affect everyday thing product stock local supermarket ad see surfing internet crime prevention datum mining offer individual many benefit improve customer service satisfaction well lifestyle general however also serious implication regard one ’ right privacy datum security issue topic section 1342 1341 ubiquitous invisible datum mining datum mining present many aspect daily life whether realize affect shop work search information even influence leisure time health well-being section look example ubiquitous ( ever-present ) datum mining several example also represent invisible datum mining “ smart ” software search engine customer-adaptive web service ( eg used recommender algorithms ) “ intelligent ” database system email manager ticket master incorporate datum mining functional component often unbeknownst user grocery store print personalize coupon customer receipt online store recommend additional item base customer interest datum mining innovatively influenced buy way shop experience shopping one example wal-mart hundred million customer visit ten thousand store every week wal-mart allow supplier access datum 
134 datum mining society 619 product perform analysis used datum mining software allow supplier identify customer buy pattern different store control inventory product placement identify new merchandize opportunity affect item ( many ) end store ’ shelves—something think next time wander aisle wal-mart datum mining shape online shopping experience many shopper routinely turn online store purchase book music movie toy recommender system discuss section 1335 offer personalize product recommendation base opinion customer amazoncom forefront used personalize datum mining–based approach marketing strategy observed traditional brick-and-mortar store hardest part get customer store customer likely buy something since cost go another store high therefore marketing brick-and-mortar store tend emphasize draw customer rather actual in-store customer experience contrast online store customer “ walk ” enter another online store click mouse amazoncom capitalize difference offer “ personalize store every ” use several datum mining technique identify customer ’ like make reliable recommendation topic shopping suppose lot buy credit card nowadays unusual receive phone call one ’ credit card company regard suspicious unusual pattern spending credit card company use datum mining detect fraudulent usage save billion dollar year many company increasingly use datum mining customer relationship management ( crm ) help provide customize personal service address individual customer ’ need lieu mass marketing study browse purchasing pattern web store company tailor advertisement promotion customer profile customer less likely annoyed unwanted mass mailing junk mail action result substantial cost saving company customer benefit likely notified offer actually interest result less waste personal time greater satisfaction datum mining greatly influenced way person use computer search information work get internet example decide check email unbeknownst several annoying email already delete thank spam filter used classification algorithms recognize spam process email go google ( wwwgooglecom ) provide access information billion web page index server google one popular widely used internet search engine used google search information become way life many person google popular even become new verb english language meaning “ search ( something ) internet used google search engine extension comprehensive search ” 1 decide type keyword 1 http open-dictionarycom 
620 chapter 13 datum mining trend research frontier topic interest google return list web site topic mine index organized set datum mining algorithms include pagerank moreover type “ boston new york ” google show bus train schedule boston new york however minor change “ boston paris ” lead flight schedule boston paris smart offering information service likely base frequent pattern mine click stream many previous query view result google query various ad pop relate query google ’ strategy tailor advertising match user ’ interest one typical service explore every internet search provider also make happier less likely pester irrelevant ad datum mining omnipresent see daily-encounter example can go scenario many case datum mining invisible user may unaware examine result return datum mining click actually fed new datum datum mining function datum mining become improve accept technology continue research development need many area mentioned challenge throughout book include efficiency scalability increase user interaction incorporation background knowledge visualization technique effective method find interesting pattern improve handle complex datum type stream datum realtime datum mining web mining addition integration datum mining exist business scientific technology provide domain-specific datum mining tool contribute advancement technology success datum mining solution tailor e-commerce application opposed generic datum mining system example 1342 privacy security social impact datum mining information accessible electronic form available web increasingly powerful datum mining tool develop put use increase concern datum mining may pose threat privacy datum security however important note many datum mining application even touch personal datum prominent example include application involve natural resource prediction flood drought meteorology astronomy geography geology biology scientific engineering datum furthermore study datum mining research focus development scalable algorithms involve personal datum focus datum mining technology discovery general statistically significant pattern specific information regard individual sense believe real privacy concern unconstrained access individual record especially access privacy-sensitive information credit card transaction record health-care record personal financial record biological trait justice investigation ethnicity datum mining application involve personal datum many case simple method remove sensitive id datum may protect privacy individual nevertheless privacy concern exist wherever 
134 datum mining society 621 personally identifiable information collect store digital form datum mining program able access datum even datum preparation improper nonexistent disclosure control root cause privacy issue handle concern numerous datum security-enhancing technique develop addition great deal recent effort develop privacypreserve datum mining method section look advance protect privacy datum security datum mining “ secure privacy individual collect mining datum ” many datum security–enhancing technique develop help protect datum databasis employ multilevel security model classify restrict datum accord various security level user permit access authorize level show however user execute specific query authorize security level still infer sensitive information similar possibility occur datum mining encryption another technique individual datum item may encode may involve blind signature ( build public key encryption ) biometric encryption ( eg image person ’ iris fingerprint used encode personal information ) anonymous databasis ( permit consolidation various databasis limit access personal information need know personal information encrypt store different location ) intrusion detection another active area research help protect privacy personal datum privacy-preserve datum mining area datum mining research response privacy protection datum mining also know privacy-enhance privacysensitive datum mining deal obtain valid datum mining result without disclose underlie sensitive datum value privacy-preserve datum mining method use form transformation datum perform privacy preservation typically method reduce granularity representation preserve privacy example may generalize datum individual customer customer group reduction granularity cause loss information possibly usefulness datum mining result natural trade-off information loss privacy privacy-preserve datum mining method classify follow category randomization method method add noise datum mask attribute value record noise add sufficiently large individual record value especially sensitive one re-cover however add skillfully final result datum mining basically preserve technique design derive aggregate distribution perturbed datum subsequently datum mining technique develop work aggregate distribution k-anonymity l-diversity method method alter individual record uniquely identify k-anonymity method granularity datum representation reduce sufficiently give record map onto least k record datum used technique like generalization suppression k-anonymity method weak homogeneity 
622 chapter 13 datum mining trend research frontier sensitive value within group value may infer alter record l-diversity model design handle weakness enforce intragroup diversity sensitive value ensure anonymization goal make sufficiently difficult adversary use combination record attribute exactly identify individual record distribute privacy preservation large datum set can partition distribute either horizontally ( ie datum set partition different subset record distribute across multiple site ) vertically ( ie datum set partition distribute attribute ) even combination individual site may want share entire datum set may consent limit information sharing use variety protocol overall effect method maintain privacy individual object derive aggregate result datum downgrading effectiveness datum mining result many case even though datum may available output datum mining ( eg association rule classification model ) may result violation privacy solution can downgrade effectiveness datum mining either modify datum mining result hiding association rule slightly distort classification model recently researcher propose new idea privacy-preserve datum mining notion differential privacy general idea two datum set close one another ( ie differ tiny datum set single element ) give differentially private algorithm behave approximately datum set definition give strong guarantee presence absence tiny datum set ( eg represent individual ) affect final output query significantly base notion set differential privacy-preserve datum mining algorithms develop research direction ongoing expect powerful privacy-preserve datum publish datum mining algorithms near future like technology datum mining misuse however must lose sight benefit datum mining research bring range insight gain medical scientific application increase customer satisfaction help company better suit client ’ need expect computer scientist policy expert counterterrorism expert continue work social scientist lawyer company consumer take responsibility build solution ensure datum privacy protection security way may continue reap benefit datum mining term time money saving discovery new knowledge 135 datum mining trend diversity datum datum mining task datum mining approach pose many challenge research issue datum mining development efficient effective datum 
135 datum mining trend 623 mining method system service interactive integrate datum mining environment key area study use datum mining technique solve large sophisticated application problem important task datum mining researcher datum mining system application developer section describe trend datum mining reflect pursuit challenge application exploration early datum mining application put lot effort help business gain competitive edge exploration datum mining business continue expand e-commerce e-marketing become mainstream retail industry datum mining increasingly used exploration application area web text analysis financial analysis industry government biomedicine science emerge application area include datum mining counterterrorism mobile ( wireless ) datum mining generic datum mining system may limitation deal application-specific problem may see trend toward development application-specific datum mining system tool well invisible datum mining function embed various kind service scalable interactive datum mining method contrast traditional datum analysis method datum mining must able handle huge amount datum efficiently possible interactively amount datum collect continue increase rapidly scalable algorithms individual integrate datum mining function become essential one important direction toward improve overall efficiency mining process increase user interaction constraint-based mining provide user add control allow specification use constraint guide datum mining system search interesting pattern knowledge integration datum mining search engine database system datum warehouse system cloud compute system search engine database system datum warehouse system cloud compute system mainstream information process compute system important ensure datum mining serve essential datum analysis component smoothly integrate information process environment datum mining service tightly couple system seamless unify framework invisible function ensure datum availability datum mining portability scalability high performance integrate information process environment multidimensional datum analysis exploration mining social information network mining social information network link analysis critical task network ubiquitous complex development scalable effective knowledge discovery method application large number network datum essential outlined section 1312 mining spatiotemporal moving-object cyber-physical system cyberphysical system well spatiotemporal datum mount rapidly due 
624 chapter 13 datum mining trend research frontier popular use cellular phone gps sensor wireless equipment outlined section 1313 many challenge research issue realize real-time effective knowledge discovery datum mining multimedium text web datum outlined section 1313 mining kind datum recent focus datum mining research great progress make yet still many open issue solve mining biological biomedical datum unique combination complexity richness size importance biological biomedical datum warrant special attention datum mining mining dna protein sequence mining highdimensional microarray datum biological pathway network analysis topic field area biological datum mining research include mining biomedical literature link analysis across heterogeneous biological datum information integration biological datum datum mining datum mining software engineering system engineering software program large computer system become increasingly bulky size sophisticated complexity tend originate integration multiple component develop different implementation team trend make increasingly challenge task ensure software robustness reliability analysis execution buggy software program essentially datum mining process—trace datum generate program execution may disclose important pattern outlier can lead eventual automate discovery software bug expect development datum mining methodology system debug enhance software robustness bring new vigor system engineering visual audio datum mining visual audio datum mining effective way integrate human ’ visual audio system discover knowledge huge amount datum systematic development technique facilitate promotion human participation effective efficient datum analysis distribute datum mining real-time datum stream mining traditional datum mining method design work centralize location work well many distribute compute environment present today ( eg internet intranet local area network high-speed wireless network sensor network cloud compute ) advance distribute datum mining method expect moreover many application involve stream datum ( eg e-commerce web mining stock analysis intrusion detection mobile datum mining datum mining counterterrorism ) require dynamic datum mining model build real time additional research need direction privacy protection information security datum mining abundance personal confidential information available electronic form couple increasingly powerful datum mining tool pose threat datum privacy security grow interest datum mining counterterrorism also add concern 
136 summary 625 development privacy-preserve datum mining method foresee collaboration technologist social scientist law expert government company need produce rigorous privacy security protection mechanism datum publish datum mining confidence look forward next generation datum mining technology benefit bring 136 summary mining complex datum type pose challenge issue many dedicate line research development chapter present high-level overview mining complex datum type include mining sequence datum time series symbolic sequence biological sequence mining graph network mining kind datum include spatiotemporal cyber-physical system datum multimedium text web datum datum stream several well-established statistical method propose datum analysis regression generalized linear model analysis variance mixed-effect model factor analysis discriminant analysis survival analysis quality control full coverage statistical datum analysis method beyond scope book interested reader refer statistical literature cite bibliographic note ( section 138 ) researcher strive build theoretical foundation datum mining several interesting proposal appear base datum reduction datum compression probability statistic theory microeconomic theory pattern discovery–based inductive databasis visual datum mining integrate datum mining datum visualization discover implicit useful knowledge large datum set visual datum mining include datum visualization datum mining result visualization datum mining process visualization interactive visual datum mining audio datum mining used audio signal indicate datum pattern feature datum mining result many customize datum mining tool develop domain-specific application include finance retail telecommunication industry science engineering intrusion detection prevention recommender system application domain-based study integrate domain-specific knowledge datum analysis technique provide mission-specific datum mining solution ubiquitous datum mining constant presence datum mining many aspect daily life influence shop work search information use computer well leisure time health well-being invisible datum mining “ smart ” software search engine customer-adaptive web service 
626 chapter 13 datum mining trend research frontier ( eg used recommender algorithms ) email manager incorporate datum mining functional component often unbeknownst user major social concern datum mining issue privacy datum security privacy-preserve datum mining deal obtain valid datum mining result without disclose underlie sensitive value goal ensure privacy protection security preserve overall quality datum mining result datum mining trend include effort toward exploration new application area improve scalable interactive constraint-based mining method integration datum mining web service database warehousing cloud compute system mining social information network trend include mining spatiotemporal cyber-physical system datum biological datum system engineering datum multimedium text datum addition web mining distribute real-time datum stream mining visual audio mining privacy security datum mining 137 exercise 131 sequence datum ubiquitous diverse application chapter present general overview sequential pattern mining sequence classification sequence similarity search trend analysis biological sequence alignment modele however cover sequence cluster present overview method sequence cluster 132 chapter present overview sequence pattern mining graph pattern mining method mining tree pattern partial order pattern also study research summarize method mining structure pattern include sequence tree graph partial order relationship examine kind structural pattern mining cover research propose application create new mining problem 133 many study analyze homogeneous information network ( eg social network consist friend link friend ) however many application involve heterogeneous information network ( ie network link multiple type object research paper conference author topic ) major difference methodology mining heterogeneous information network method homogeneous counterpart 134 research describe datum mining application present chapter discuss different form datum mining used application 135 establishment theoretical foundation important datum mining name describe main theoretical foundation propose datum mining comment satisfy ( fail satisfy ) requirement ideal theoretical framework datum mining 
137 exercise 627 136 ( research project ) build theory datum mining require set theoretical framework major datum mining function explain framework take one theory example ( eg datum compression theory ) examine major datum mining function fit framework function fit well current theoretical framework propose way extend framework explain function 137 strong linkage statistical datum analysis datum mining person think datum mining automate scalable method statistical datum analysis agree disagree perception present one statistical analysis method automate or scale nicely integration current datum mining methodology 138 difference visual datum mining datum visualization datum visualization may suffer datum abundance problem example easy visually discover interesting property network connection social network huge complex dense connection propose visualization method may help person see network topology interesting feature social network 139 propose implementation method audio datum mining integrate audio visual datum mining bring fun power datum mining possible develop video datum mining method state scenario solution make integrate audiovisual mining effective 1310 general-purpose computer domain-independent relational database system become large market last several decade however many person feel generic datum mining system prevail datum mining market think datum mining focus effort develop domain-independent datum mining tool develop domain-specific datum mining solution present reasoning 1311 recommender system way differ customer productbased cluster system differ typical classification predictive modele system outline one method collaborative filter discuss work limitation practice 1312 suppose local bank datum mining system bank study debit card usage pattern notice make many transaction home renovation store bank decide contact offer information regard special loan home improvement ( ) discuss may conflict right privacy ( b ) describe another situation feel datum mining infringe privacy ( c ) describe privacy-preserve datum mining method may allow bank perform customer pattern analysis without infringe customer ’ right privacy ( ) example datum mining can used help society think way can used may detrimental society 
628 chapter 13 datum mining trend research frontier 1313 major challenge face bring datum mining research market illustrate one datum mining research issue view may strong impact market society discuss approach research issue 1314 base view challenge research problem datum mining give number year good number researcher implementor would plan make good progress toward effective solution problem 1315 base experience knowledge suggest new frontier datum mining mentioned chapter 138 bibliographic note mining complex datum type many research paper book cover various theme list recent book well-cite survey research article reference time-series analysis study statistic computer science community decade many textbook box jenkin reinsel [ bjr08 ] brockwell davis [ bd02 ] chatfield [ cha03b ] hamilton [ ham94 ] shumway stoffer [ ss05 ] fast subsequence match method time-series databasis present faloutsos ranganathan manolopoulos [ frm94 ] agrawal lin sawhney shim [ alss95 ] develop method fast similarity search presence noise scaling translation time-series databasis shasha zhu present overview method high-performance discovery time series [ sz04 ] sequential pattern mining method study many researcher include agrawal srikant [ as95 ] zaki [ zak01 ] pei han mortazavi-asl et al [ + 04 ] yan han afshar [ yha03 ] study sequence classification include ji bailey dong [ jbd05 ] ye keogh [ yk09 ] survey xing pei keogh [ xpk10 ] dong pei [ dp07 ] provide overview sequence datum mining method method analysis biological sequence include markov chain hide markov model introduce many book tutorial waterman [ wat95 ] setubal meidanis [ sm97 ] durbin eddy krogh mitchison [ dekm98 ] baldi brunak [ bb01 ] krane raymer [ kr03 ] rabiner [ rab89 ] jone pevzner [ jp04 ] baxevanis ouellette [ bo04 ] information blast ( see also korf yandell bedell [ kyb03 ] ) find ncbi web site graph pattern mining study extensively include holder cook djoko [ hcd94 ] inokuchi washio motoda [ iwm98 ] kuramochi karypis [ kk01 ] yan han [ yh02 yh03a ] borgelt berthold [ bb02 ] huan wang bandyopadhyay et al [ + 04 ] gaston tool nijssen kok [ nk04 ] 
138 bibliographic note 629 great deal research social information network analysis include newman [ new10 ] easley kleinberg [ ek10 ] yu han faloutsos [ yhf10 ] wasserman faust [ wf94 ] watt [ wat03 ] newman barabasi watt [ nbw06 ] statistical modele network study popularly albert barbasi [ ab99 ] watt [ wat03 ] faloutsos faloutsos faloutsos [ fff99 ] kumar raghavan rajagopalan et al [ + 00 ] leskovec kleinberg faloutsos [ lkf05 ] datum clean integration validation information network analysis study many include bhattacharya getoor [ bg04 ] yin han yu [ yhy07 yhy08 ] cluster ranking classification network study extensively include brin page [ bp98 ] chakrabarti dom indyk [ cdi98 ] kleinberg [ kle99 ] getoor friedman koller taskar [ gfkt01 ] newman m girvan [ ng04 ] yin han yang yu [ yhyy04 ] yin han yu [ yhy05 ] xu yuruk feng schweiger [ xyfs07 ] kuli basu dhillon mooney [ kbdm09 ] sun han zhao et al [ + 09 ] neville gallaher eliassi-rad [ nge-r09 ] ji sun danilevsky et al [ + 10 ] role discovery link prediction information network study extensively well krebs [ kre02 ] kubica moore schneider [ kms03 ] liben-nowell kleinberg [ l-nk03 ] wang han jia et al [ + 10 ] similarity search olap information network study many include tian hankin patel [ thp08 ] chen yan zhu et al [ + 08 ] evolution social information network study many researcher chakrabarti kumar tomkin [ ckt06 ] chi song zhou et al [ + 07 ] tang liu zhang nazeri [ tlzn08 ] xu zhang yu long [ xzyl08 ] kim han [ kh09 ] sun tang han [ + 10 ] spatial spatiotemporal datum mining study extensively collection paper miller han [ mh09 ] introduce textbook shekhar chawla [ sc03 ] hsu lee wang [ hlw07 ] spatial cluster algorithms study extensively chapter 10 11 book research conduct spatial warehouse olap stefanovic han koperski [ shk00 ] spatial spatiotemporal datum mining koperski han [ kh95 ] mamouli cao kollio hadjieleftheriou et al [ + 04 ] tsoukatos gunopulos [ tg01 ] hadjieleftheriou kollio gunopulos tsotra [ hkgt03 ] mining moving-object datum study many vlachos gunopulos kollio [ vgk02 ] tao faloutsos papadia liu [ tfpl04 ] li han kim gonzalez [ lhkg07 ] lee han whang [ lhw07 ] li ding han et al [ + 10 ] bibliography temporal spatial spatiotemporal datum mining research see collection roddick hornsby spiliopoulou [ rhs01 ] multimedium datum mining deep root image process pattern recognition study extensively many textbook include gonzalez wood [ gw07 ] russ [ rus06 ] duda hart stork [ dhs01 ] z zhang r zhang [ zz09 ] search mining multimedium datum study many ( see eg fayyad smyth [ fs93 ] faloutsos lin [ fl95 ] natsev rastogi 
630 chapter 13 datum mining trend research frontier shim [ nrs99 ] zaı̈ane han zhu [ zhz00 ] ) overview image mining method give hsu lee zhang [ hlz02 ] text datum analysis study extensively information retrieval many textbook survey article croft metzler strohman [ cms09 ] s buttcher c clarke g cormack [ bcc10 ] man raghavan schutze [ mrs08 ] grossman frieder [ gr04 ] baeza-yate riberio-neto [ byrn11 ] zhai [ zha08 ] feldman sanger [ fs06 ] berry [ ber03 ] weis indurkhya zhang damerau [ wizd04 ] text mining fast-developing field numerous paper publish recent year cover many topic topic model ( eg blei lafferty [ bl09 ] ) sentiment analysis ( eg pang lee [ pl07 ] ) contextual text mining ( eg mei zhai [ mz06 ] ) web mining another focuse theme book like chakrabarti [ cha03a ] liu [ liu06 ] berry [ ber03 ] web mining substantially improve search engine influential milestone work brin page [ bp98 ] kleinberg [ kle99 ] chakrabarti dom kumar et al [ + 99 ] kleinberg tomkin [ kt99 ] numerous result generate since search log mining ( eg silvestri [ sil10 ] ) blog mining ( eg mei liu su zhai [ mlsz06 ] ) mining online forum ( eg cong wang lin et al [ + 08 ] ) book survey stream datum system stream datum process include babu widom [ bw01 ] babcock babu datar et al [ + 02 ] muthukrishnan [ mut05 ] aggarwal [ agg06 ] stream datum mining research cover stream cube model ( eg chen dong han et al [ + 02 ] ) stream frequent pattern mining ( eg manku motwani [ mm02 ] karp papadimitriou shenker [ kps03 ] ) stream classification ( eg domingo hulten [ dh00 ] wang fan yu han [ wfyh03 ] aggarwal han wang yu [ ahwy04b ] ) stream cluster ( eg guha mishra motwani ’ callaghan [ gmmo00 ] aggarwal han wang yu [ ahwy03 ] ) many book discuss datum mining application financial datum analysis financial modele see example benninga [ ben08 ] higgin [ hig08 ] retail datum mining customer relationship management see example book berry linoff [ bl04 ] berson smith thearle [ bst99 ] telecommunication-related datum mining see example horak [ hor08 ] also book scientific datum analysis grossman kamath kegelmeyer et al [ + 01 ] kamath [ kam09 ] issue theoretical foundation datum mining address many researcher example mannila present summary study foundation datum mining [ man00 ] datum reduction view datum mining summarize new jersey datum reduction report barbará dumouchel faloutos et al [ + 97 ] datum compression view find study minimum description length principle grunwald rissanen [ gr07 ] pattern discovery point view datum mining address numerous machine learn datum mining study range association mining decision tree induction sequential pattern mining cluster probability theory point view popular statistic machine learn literature 
138 bibliographic note 631 bayesian network hierarchical bayesian model chapter 9 probabilistic graph model ( eg koller friedman [ kf09 ] ) kleinberg papadimitriou raghavan [ kpr98 ] present microeconomic view treat datum mining optimization problem study inductive database view include imielinski mannila [ im96 ] de raedt gun nijssen [ rgn10 ] statistical method datum analysis describe many book hastie tibshirani friedman [ htf09 ] freedman pisani purf [ fpp07 ] devore [ dev03 ] kutner nachtsheim neter li [ knnl04 ] dobson [ dob01 ] breiman friedman olshen stone [ bfos84 ] pinheiro bate [ pb00 ] johnson wichern [ jw02b ] huberty [ hub94 ] shumway stoffer [ ss05 ] miller [ mil98 ] visual datum mining popular book visual display datum information include tufte [ tuf90 tuf97 tuf01 ] summary technique visualize datum present cleveland [ cle93 ] dedicate visual datum mining book visual datum mining technique tool datum visualization mining soukup davidson [ sd02 ] book information visualization datum mining knowledge discovery edit fayyad grinstein wierse [ fgw01 ] contain collection article visual datum mining method ubiquitous invisible datum mining discuss many text include john [ joh99 ] article book edit kargupta joshi sivakumar yesha [ kjsy04 ] book business @ speed thought succeed digital economy gate [ gat00 ] discuss e-commerce customer relationship management provide interesting perspective datum mining future mena [ men03 ] informative book use datum mining detect prevent crime cover many form criminal activity range fraud detection money launder insurance crime identity crime intrusion detection datum mining issue regard privacy datum security address popularly literature book privacy security datum mining include thuraisingham [ thu04 ] aggarwal yu [ ay08 ] vaidya clifton zhu [ vcz10 ] fung wang fu yu [ fwfy10 ] research article include agrawal srikant [ as00 ] evfimievski srikant agrawal gehrke [ esag02 ] vaidya clifton [ vc03 ] differential privacy introduce dwork [ dwo06 ] study many hay rastogi miklau suciu [ hrms10 ] many discussion trend research direction datum mining various forum several book collection article issue kargupta han yu et al [ + 08 ] 

basic concept densitybased cluster 
density-based cluster method cluster base density ( local cluster criterion ) density-connect point  major feature  discover cluster arbitrary shape  handle noise  one scan ( examine local region justify density )  need density parameter termination condition  several interesting study cover lecture  dbscan ester et al ( kdd ’ 96 ) cover lecture  optic ankerst et al ( sigmod ’ 99 )  denclue hinneburg & d keim ( kdd ’ 98 )  clique agrawal et al ( sigmod ’ 98 ) ( also grid-based ) cover lecture  2 

cluster algorithm 
dbscan density-based spatial cluster algorithm  dbscan ( m ester kriegel j sander x xu kdd ’ 96 ) discover cluster arbitrary shape density-based spatial cluster application noise  density-based notion cluster  cluster defined maximal set density-connect point  two parameter  ep ( ε ) maximum radius neighborhood border  minpt minimum number point eps-neighborhood point core  ep ( ε ) neighborhood point q  neps ( q ) { p belong | dist ( p q ) ≤ ep } p  2 minpt = 5 q ep = 1 cm outlier noise cluster core point dense neighborhood border point cluster neighborhood dense 
dbscan density-reachable density-connect  directly density-reachable point p directly density-reachable point q wrt ep ( ε ) minpt  p belong neps ( q )  core point condition neps ( q ) | ≥ minpt  density-reachable  point p density-reachable point q wrt ep minpt chain point p1 … pn p1 = q pn = p pi+1 directly density-reachable pi  density-connect  point p density-connect point q wrt ep minpt point p q density-reachable wrt ep minpt p  3 minpt = 5 q ep = 1 cm p q p2 p q 
dbscan algorithm algorithm outlier  arbitrarily select point p noise cluster border  retrieve point density-reachable core point dense p wrt ep minpt neighborhood core  p core point cluster form border point cluster  p border point point density-reachable neighborhood dense p dbscan visit next point database  continue process point processed  computational complexity  spatial index used computational complexity dbscan ( nlogn ) n number database object  otherwise complexity ( n2 )  4 
dbscan sensitive set parameter 5 ack figure g karypis han v kumar computer 32 ( 8 ) 1999 

identify cluster structure 
optic order point identify cluster structure optic ( ankerst breunig kriegel sander sigmod ’ 99 )  dbscan sensitive parameter set  extension find cluster structure  observation give minpt density-based cluster wrt higher density completely contain cluster wrt lower density  idea higher density point processed first—find high-density cluster first  optic store cluster order used two piece information  core distance reachability distance  2 reachability plot dataset reachability-distance undefined ε ε ’ cluster-order object since point belong cluster low reachability distance nearest neighbor valley correspond cluster  deeper valley denser cluster  
optic extension dbscan  core distance object p smallest value ε ε-neighborhood p least minpt object let nε ( p ) ε-neighborhood p ε distance value core-distanceε minpt ( p ) = undefined card ( nε ( p ) ) < minpt minpts-distance ( p ) otherwise   3 reachability distance reachability distance object p core object q min radius value make p density-reachable q undefined ε reachability-distanceε minpt ( p q ) = undefined q core object ε ’ max ( core-distance ( q ) distance ( q p ) ) otherwise complexity ( n logn ) ( index-based ) cluster-order object n # point 
optic find hierarchically nest cluster structure optic produce special cluster-order datum point respect density-based cluster structure  cluster-order contain information equivalent density-based clustering corresponding broad range parameter setting  good automatic interactive cluster analysis—find intrinsic even hierarchically nest cluster structure  4 find nest cluster structure different parameter setting 

method start session go introduce grid-based cluster method 
grid-based cluster method grid-based cluster explore multi-resolution grid datum structure cluster  partition datum space finite number cell form grid structure  find cluster ( dense region ) cell grid structure  feature challenge typical grid-based algorithm  efficiency scalability # cell < < # datum point  uniformity uniform hard handle highly irregular datum distribution  locality limit predefined cell size border density threshold  curse dimensionality hard cluster high-dimensional datum  method introduce  sting ( statistical information grid approach ) ( wang yang muntz vldb ’ 97 )  clique ( agrawal gehrke gunopulos raghavan sigmod ’ 98 )  grid-based subspace cluster  2 grid-based cluster method essentially consider whole space partition multi-resolution grid structure work cell grid structure perform multi-resolution cluster mean partition datum space finite number cell form grid structure example plane may able partition plane 10 10 100 100 kind grid structure may find cluster dense region cell grid structure mean higher lower resolution refine resolution cross resolution cluster typical cluster algorithm follow feature challenge first thing obvious efficient scalable sense partition number cell usually number cell even say 10 10 100 cell much smaller number datum point can minute second one uniformity mean uniform get ten ten s 3d uniform structure s hard highly irregular datum distribution third feature locality mean s limit predefined cell size border predefined density threshold first one curse dimensionality mean s hard cluster high dimensional datum be go introduce two method lecture one call sting call statistic information grid approach develop wei wang joe yang dick muntz ucla publish 1997 vldb conference another one clique grid-based subspace cluster interesting methodology subpace cluster develop rakesh agrawal johanne gehrke gunopulos raghavan ibm publish 1998 sigmod conference [ music ] 

information grid approach 
sting statistical information grid approach sting ( statistical information grid ) ( wang yang muntz vldb ’ 97 )  spatial area divide rectangular cell different level resolution cell form tree structure  cell high level contain number smaller cell next lower level  statistical information cell calculate store beforehand used answer query  parameter higher level cell easily calculate lower level cell include  count mean ( standard deviation ) min max  type distribution—normal uniform etc  2 
query process sting analysis process region query  start root proceed next lower level used sting index  calculate likelihood cell relevant query confidence level used statistical information cell  child likely relevant cell recursively explore  repeat process bottom layer reach  advantage  query-independent easy parallelize incremental update  efficiency complexity ( k )  k # grid cell lowest level k < < n ( ie # datum point )  disadvantage  probabilistic nature may imply loss accuracy query process  3 

subspace cluster [ sound ] session go introduce clique grid-based subspace cluster algorithm algorithm clique actually abbreviation cluster quest quest ibm datum mining system develop group researcher ibm publish sigmod 1998 conference 
clique grid-based subspace cluster  clique ( cluster quest ) ( agrawal gehrke gunopulos raghavan sigmod ’ 98 )  clique density-based grid-based subspace cluster algorithm  grid-based discretize datum space grid estimate density count number point grid cell  density-based cluster maximal set connect dense unit subspace    2 unit dense fraction total datum point contain unit exceed input model parameter subspace cluster subspace cluster set neighboring dense cell arbitrary subspace also discover minimal description cluster automatically identify subspace high dimensional datum space allow better cluster original space used apriori principle clique density-based grid-based subspace cluster algorithm grid-based discretize datum space grid structure estimate density count number point grid cell density-based cluster actually maximal set connect dense unit subspace mean unit dense fraction total datum point contain unit exceed certain parameter try connect dense unit structure cluster mean density-based subspace cluster start low dimension like single dimension particular subspace try find neighboring dense cell arbitrary subspace grow 2-d 3-d find maximum number dimension subspace contain cluster also discover minimum description cluster mean particular algorithm automatically identify subspace high dimensional datum space allow better cluster original space used apriori principle 
clique subspace cluster aprori prune     3 start 1-d space discretize numerical interval axis grid find dense region ( cluster ) subspace generate minimal description use dense region find promising candidate 2-d space base apriori principle repeat level-wise manner higher dimensional subspace let s look example suppose want find cluster base salary age number vacation week first start one dimension find dense point example look salary may find salary dense point 20k 50k part pretty dense especially around 40k find age pretty dense 30 50 probably lower one base number vacation probably see vacation cluster around two four week mean find dense region subspace generate minimum description use dense region find promising candidate 2-d space base apriori principle simply say find s dense 1-d salary 1-d h probably find combine one try find cluster within candidate s space similarly find cluster within candidate space salary vacation okay repeat process level-wise manner higher dimensional subspace used apriori principle simply say find dense 2-d part 2-d part may find candidate 3-d region base intersection 
major step clique algorithm identify subspace contain cluster  partition datum space find number point lie inside cell partition   identify subspace contain cluster used apriori principle identify cluster  determine dense unit subspace interest   determine connect dense unit subspace interest generate minimal description cluster  determine maximal region cover cluster connect dense unit cluster   4 determine minimal cover cluster 
additional comment clique  strength  automatically find subspace highest dimensionality long high density cluster exist subspace  insensitive order record input presume canonical datum distribution  scale linearly size input good scalability number dimension datum increase  weakness  5 grid-based cluster approach quality result crucially depend appropriate choice number width partition grid cell major step clique algorithm first try identify subspace contain cluster mean partition database space grid structure find number point lie inside cell grid partition try identify subspace contain cluster used apriori principle try identify cluster second step determine dense unit subspace interest determine connect dense unit subspace interest generate minimal description cluster mean determine maximal region cover cluster connect dense unit determine minimal cover cluster master interesting point automatically find subspace highest dimensionality long high density cluster exist subspace find 2-d will try find intersection 2-d form cluster intersection 3-d space likely may able find cluster insensitive order record input also assume particular datum distribution scale linearly size input good scalability number dimension datum increase quite efficient algorithm weakness method essentially use grid-based cluster approach quality result depend choose number width partition grid cell nevertheless s interesting subspace cluster method 
recommend reading  m ester kriegel j sander x xu density-based algorithm discover 6 cluster large spatial databasis kdd96  w wang j yang r muntz sting statistical information grid approach spatial datum mining vldb ’ 97  r agrawal j gehrke d gunopulos p raghavan automatic subspace cluster high dimensional datum datum mining application sigmod ’ 98  a hinneburg d a keim efficient approach cluster large multimedium databasis noise kdd ’ 98  m ankerst m m breunig kriegel j sander optic order point identify cluster structure sigmod ’ 99  m ester density-based cluster ( chapter 5 ) aggarwal reddy ( ed ) datum cluster algorithms application crc press 2014  w cheng w wang s batista grid-based cluster ( chapter 6 ) aggarwal reddy ( ed ) datum cluster algorithms application crc press 2014 finally say original paper density-based grid-based cluster list also sheryl aggarwal reddy s book two chapter one call density-based cluster martin ester another call grid-based cluster cheng wang batista good summary also introduce many method density-based cluster grid-based cluster method [ music ] 

10 9 8 7 6 5 4 3 2 1 
author jiawei han bliss professor engineering department computer science university illinois urbana-champaign receive numerous award contribution research knowledge discovery datum mining include acm sigkdd innovation award ( 2004 ) ieee computer society technical achievement award ( 2005 ) ieee w wallace mcdowell award ( 2009 ) fellow acm ieee serve founding editor-in-chief acm transaction knowledge discovery datum ( 2006–2011 ) editorial board member several journal include ieee transaction knowledge datum engineering datum mining knowledge discovery micheline kamber master ’ degree computer science ( specialize artificial intelligence ) concordium university montreal quebec nserc scholar work researcher mcgill university simon fraser university switzerland background datum mining passion writing easyto-understand term help make text favorite professional instructor student jian pei currently associate professor school compute science simon fraser university british columbia receive degree compute science simon fraser university 2002 dr jiawei han ’ supervision publish prolifically premier academic forum datum mining databasis web search information retrieval actively serve academic community publication receive thousand citation several prestigious award associate editor several datum mining datum analytic journal xxxv 
2 get know datum ’ tempting jump straight mining first need get datum ready involve closer look attribute datum value real-world datum typically noisy enormous volume ( often several gigabyte ) may originate hodgepodge heterogenous source chapter get familiar datum knowledge datum useful datum preprocess ( see chapter 3 ) first major task datum mining process want know follow type attribute field make datum kind value attribute attribute discrete continuous-valu datum look like value distribute way visualize datum get better sense spot outlier measure similarity datum object respect other gain insight datum help subsequent analysis “ learn datum ’ helpful datum preprocess ” begin section 21 study various attribute type include nominal attribute binary attribute ordinal attribute numeric attribute basic statistical description used learn attribute ’ value describe section 22 give temperature attribute example determine mean ( average value ) median ( middle value ) mode ( common value ) measure central tendency give us idea “ middle ” center distribution know basic statistic regard attribute make easier fill miss value smooth noisy value spot outlier datum preprocess knowledge attribute attribute value also help fix inconsistency incur datum integration plot measure central tendency show us datum symmetric skewer quantile plot histogram scatter plot graphic display basic statistical description useful datum preprocess provide insight area mining field datum visualization provide many additional technique view datum graphical mean help identify relation trend biase “ hide ” unstructured datum set technique may simple scatter-plot matrix ( datum mining concept technique doi b978-0-12-381479-100002-2 c 2012 elsevier right re-serve 39 
40 chapter 2 get know datum two attribute map onto 2-d grid ) sophisticated method treemaps ( hierarchical partition screen display base attribute value ) datum visualization technique describe section 23 finally may want examine similar ( dissimilar ) datum object example suppose database datum object patient describe symptom may want find similarity dissimilarity individual patient information allow us find cluster like patient within datum set dissimilarity object may also used detect outlier datum perform nearest-neighbor classification ( cluster topic chapter 10 11 nearest-neighbor classification discuss chapter 9 ) many measure assess similarity dissimilarity general measure refer proximity measure think proximity two object function distance attribute value although proximity also calculate base probability rather actual distance measure datum proximity describe section 24 summary end chapter know different attribute type basic statistical measure describe central tendency dispersion ( spread ) attribute datum also know technique visualize attribute distribution compute similarity dissimilarity object 21 datum object attribute type datum set make datum object datum object represent entity—in sale database object may customer store item sale medical database object may patient university database object may student professor course datum object typically describe attribute datum object also refer sample example instance datum point object datum object store database datum tuple row database correspond datum object column correspond attribute section define attribute look various attribute type 211 attribute attribute datum field represent characteristic feature datum object noun attribute dimension feature variable often used interchangeably literature term dimension commonly used datum warehousing machine learn literature tend use term feature statistician prefer term variable datum mining database professional commonly use term attribute well attribute describe customer object include example customer id name address observed value give attribute know observation set attribute used describe give object call attribute vector ( feature vector ) distribution datum involve one attribute ( variable ) call univariate bivariate distribution involve two attribute 
21 datum object attribute type 41 type attribute determine set possible values—nominal binary ordinal numeric—the attribute follow subsection introduce type 212 nominal attribute nominal mean “ relate ” value nominal attribute symbol name thing value represent kind category code state nominal attribute also refer categorical value meaningful order computer science value also know enumeration example 21 nominal attribute suppose hair color marital status two attribute describe person object application possible value hair color black brown blond red auburn gray white attribute marital status take value single married divorce widow hair color marital status nominal attribute another example nominal attribute occupation value teacher dentist programmer farmer although say value nominal attribute symbol “ name thing ” possible represent symbol “ name ” number hair color instance assign code 0 black 1 brown another example customor id possible value numeric however case number intend used quantitatively mathematical operation value nominal attribute meaningful make sense subtract one customer id number another unlike say subtract age value another ( age numeric attribute ) even though nominal attribute may integer value consider numeric attribute integer meant used quantitatively say numeric attribute section 215 nominal attribute value meaningful order quantitative make sense find mean ( average ) value median ( middle ) value attribute give set object one thing interest however attribute ’ commonly occur value value know mode one measure central tendency learn measure central tendency section 22 213 binary attribute binary attribute nominal attribute two category state 0 1 0 typically mean attribute absent 1 mean present binary attribute refer boolean two state correspond true false example 22 binary attribute give attribute smoker describe patient object 1 indicate patient smoke 0 indicate patient similarly suppose 
42 chapter 2 get know datum patient undergo medical test two possible outcome attribute medical test binary value 1 mean result test patient positive 0 mean result negative binary attribute symmetric state equally valuable carry weight preference outcome code 0 one example can attribute gender state male female binary attribute asymmetric outcome state equally important positive negative outcome medical test hiv convention code important outcome usually rarest one 1 ( eg hiv positive ) 0 ( eg hiv negative ) 214 ordinal attribute ordinal attribute attribute possible value meaningful order ranking among magnitude successive value know example 23 ordinal attribute suppose drink size correspond size drink available fast-food restaurant nominal attribute three possible value small medium large value meaningful sequence ( correspond increase drink size ) however tell value much bigger say medium large example ordinal attribute include grade ( eg + a− + ) professional rank professional rank enumerate sequential order example assistant associate full professor private private first class specialist corporal sergeant army rank ordinal attribute useful register subjective assessment quality measure objectively thus ordinal attribute often used survey rating one survey participant ask rate satisfied customer customer satisfaction follow ordinal category 0 dissatisfied 1 somewhat dissatisfied 2 neutral 3 satisfied 4 satisfied ordinal attribute may also obtain discretization numeric quantity splitting value range finite number order category describe chapter 3 datum reduction central tendency ordinal attribute represent mode median ( middle value order sequence ) mean defined note nominal binary ordinal attribute qualitative describe feature object without give actual size quantity value qualitative attribute typically word represent category integer used represent computer code category opposed measurable quantity ( eg 0 small drink size 1 medium 2 large ) follow subsection look numeric attribute provide quantitative measurement object 
21 datum object attribute type 215 43 numeric attribute numeric attribute quantitative measurable quantity represent integer real value numeric attribute interval-scaled ratio-scale interval-scaled attribute interval-scaled attribute measure scale equal-size unit value interval-scaled attribute order positive 0 negative thus addition provide ranking value attribute allow us compare quantify difference value example 24 interval-scaled attribute temperature attribute interval-scaled suppose outdoor temperature value number different day day object order value obtain ranking object respect temperature addition quantify difference value example temperature 20◦ c five degree higher temperature 15◦ c calendar date another example instance year 2002 2010 eight year apart temperature celsius fahrenheit true zero-point neither 0◦ c 0◦ f indicate “ ” ( celsius scale example unit measurement 100 difference melt temperature boil temperature water atmospheric pressure ) although compute difference temperature value talk one temperature value multiple another without true zero say instance 10◦ c twice warm 5◦ c speak value term ratio similarly true zero-point calendar date ( year 0 correspond begin time ) bring us ratio-scale attribute true zero-point exit interval-scaled attribute numeric compute mean value addition median mode measure central tendency ratio-scale attribute ratio-scale attribute numeric attribute inherent zero-point measurement ratio-scale speak value multiple ( ratio ) another value addition value order also compute difference value well mean median mode example 25 ratio-scale attribute unlike temperature celsius fahrenheit kelvin ( k ) temperature scale consider true zero-point ( 0◦ k = −27315◦ c ) point particle comprise matter zero kinetic energy example ratio-scale attribute include count attribute year experience ( eg object employee ) number word ( eg object document ) additional example include attribute measure weight height latitude longitude 
44 chapter 2 get know datum coordinate ( eg cluster house ) monetary quantity ( eg 100 time richer $ 100 $ 1 ) 216 discrete versus continuous attribute presentation organized attribute nominal binary ordinal numeric type many way organize attribute type type mutually exclusive classification algorithms develop field machine learn often talk attribute either discrete continuous type may processed differently discrete attribute finite countably infinite set value may may represent integer attribute hair color smoker medical test drink size finite number value discrete note discrete attribute may numeric value 0 1 binary attribute value 0 110 attribute age attribute countably infinite set possible value infinite value put one-to-one correspondence natural number example attribute customer id countably infinite number customer grow infinity reality actual set value countable ( value put one-to-one correspondence set integer ) zip code another example attribute discrete continuous term numeric attribute continuous attribute often used interchangeably literature ( confuse classic sense continuous value real number whereas numeric value either integer real number ) practice real value represent used finite number digit continuous attribute typically represent floating-point variable 22 basic statistical description datum datum preprocess successful essential overall picture datum basic statistical description used identify property datum highlight datum value treat noise outlier section discuss three area basic statistical description start measure central tendency ( section 221 ) measure location middle center datum distribution intuitively speaking give attribute value fall particular discuss mean median mode midrange addition assess central tendency datum set also would like idea dispersion datum datum spread common datum dispersion measure range quartile interquartile range five-number summary boxplot variance standard deviation datum measure useful identify outlier describe section 222 finally use many graphic display basic statistical description visually inspect datum ( section 223 ) statistical graphical datum presentation software 
22 basic statistical description datum 45 package include bar chart pie chart line graph popular display datum summary distribution include quantile plot quantile–quantile plot histogram scatter plot 221 measure central tendency mean median mode section look various way measure central tendency datum suppose attribute x like salary record set object let x1 x2 xn set n observed value observation x value may also refer datum set ( x ) plot observation salary would value fall give us idea central tendency datum measure central tendency include mean median mode midrange common effective numeric measure “ center ” set datum ( arithmetic ) mean let x1 x2 xn set n value observation numeric attribute x like salary mean set value n x x̄ = xi i=1 n = x1 + x2 + · · · + xn n ( 21 ) correspond built-in aggregate function average ( avg ( ) sql ) provide relational database system example 26 mean suppose follow value salary ( thousand dollar ) show increase order 30 36 47 50 52 52 56 60 63 70 70 used eq ( 21 ) 30 + 36 + 47 + 50 + 52 + 52 + 56 + 60 + 63 + 70 + 70 + 110 12 696 = = 58 12 x̄ = thus mean salary $ 58000 sometimes value xi set may associate weight wi = 1 n weight reflect significance importance occurrence frequency attach respective value case compute n x x̄ = wi xi i=1 n x = w1 x1 + w2 x2 + · · · + wn xn w1 + w2 + · · · + wn wi i=1 call weight arithmetic mean weight average ( 22 ) 
46 chapter 2 get know datum although mean singlemost useful quantity describe datum set always best way measure center datum major problem mean sensitivity extreme ( eg outlier ) value even small number extreme value corrupt mean example mean salary company may substantially push highly paid manager similarly mean score class exam can pull quite bit low score offset effect cause small number extreme value instead use trim mean mean obtain chop value high low extreme example sort value observed salary remove top bottom 2 % compute mean avoid trimming large portion ( 20 % ) end result loss valuable information skewer ( asymmetric ) datum better measure center datum median middle value set order datum value value separate higher half datum set lower half probability statistic median generally apply numeric datum however may extend concept ordinal datum suppose give datum set n value attribute x sort increase order n odd median middle value order set n even median unique two middlemost value value x numeric attribute case convention median take average two middlemost value example 27 median let ’ find median datum example datum already sort increase order even number observation ( ie 12 ) therefore median unique value within two middlemost value 52 56 ( within sixth seventh value list ) convention assign = 108 average two middlemost value median 52+56 2 2 = thus median $ 54000 suppose first 11 value list give odd number value median middlemost value sixth value list value $ 52000 median expensive compute large number observation numeric attribute however easily approximate value assume datum group interval accord xi datum value frequency ( ie number datum value ) interval know example employee may group accord annual salary interval $ 10–20000 $ 20–30000 let interval contain median frequency median interval approximate median entire datum set ( eg median salary ) interpolation used formula  p n 2 − freq l median = l1 + width ( 23 ) freqmedian l1 lower median interval n number value  pboundary entire datum set freq l sum frequency interval 
22 basic statistical description datum 47 lower median interval freqmedian frequency median interval width width median interval mode another measure central tendency mode set datum value occur frequently set therefore determine qualitative quantitative attribute possible greatest frequency correspond several different value result one mode datum set one two three mode respectively call unimodal bimodal trimodal general datum set two mode multimodal extreme datum value occur mode example 28 mode datum example 26 bimodal two mode $ 52000 $ 70000 unimodal numeric datum moderately skewer ( asymmetrical ) follow empirical relation mean − mode ≈ 3 × ( mean − median ) ( 24 ) imply mode unimodal frequency curf moderately skewer easily approximate mean median value know midrange also used assess central tendency numeric datum set average largest smallest value set measure easy compute used sql aggregate function max ( ) min ( ) example 29 midrange midrange datum example 26 30000+110000 2 = $ 70000 unimodal frequency curve perfect symmetric datum distribution mean median mode center value show figure 21 ( ) datum real application symmetric may instead either positively skewer mode occur value smaller median ( figure 21b ) negatively skewer mode occur value greater median ( figure 21c ) mean median mode mode mean median ( ) symmetric datum ( b ) positively skewer datum mean mode median ( c ) negatively skewer datum figure 21 mean median mode symmetric versus positively negatively skewer datum 
48 chapter 2 get know datum 222 measure dispersion datum range quartile variance standard deviation interquartile range look measure assess dispersion spread numeric datum measure include range quantile quartile percentile interquartile range five-number summary display boxplot useful identify outlier variance standard deviation also indicate spread datum distribution range quartile interquartile range start let ’ study range quantile quartile percentile interquartile range measure datum dispersion let x1 x2 xn set observation numeric attribute x range set difference largest ( max ( ) ) smallest ( min ( ) ) value suppose datum attribute x sort increase numeric order imagine pick certain datum point split datum distribution equal-size consecutive set figure datum point call quantile quantile point take regular interval datum distribution divide essentially equalsize consecutive set ( say “ essentially ” may datum value x divide datum exactly equal-sized subset readability refer equal ) kth q-quantile give datum distribution value x q datum value less x ( q − k ) q datum value x k integer 0 < k < q q − 1 q-quantile 2-quantile datum point divide lower upper half datum distribution correspond median 4-quantiles three datum point split datum distribution four equal part part represent one-fourth datum distribution commonly refer quartile 100-quantile commonly refer percentile divide datum distribution 100 equal-sized consecutive set median quartile percentile widely used form quantile 25 % q1 q2 q3 median 75th 25th percentile percentile figure 22 plot datum distribution attribute x quantile plot quartile three quartile divide distribution four equal-size consecutive subset second quartile correspond median 
22 basic statistical description datum 49 quartile give indication distribution ’ center spread shape first quartile denote q1 25th percentile cut lowest 25 % datum third quartile denote q3 75th percentile—it cut lowest 75 % ( highest 25 % ) datum second quartile 50th percentile median give center datum distribution distance first third quartile simple measure spread give range cover middle half datum distance call interquartile range ( iqr ) defined iqr = q3 − q1 ( 25 ) example 210 interquartile range quartile three value split sort datum set four equal part datum example 26 contain 12 observation already sort increase order thus quartile datum third sixth ninth value respectively sort list therefore q1 = $ 47000 q3 $ 63000 thus interquartile range iqr = 63 − 47 = $ 16000 ( note sixth value median $ 52000 although datum set two median since number datum value even ) five-number summary boxplot outlier single numeric measure spread ( eg iqr ) useful describe skewer distribution look symmetric skewer datum distribution figure 21 symmetric distribution median ( measure central tendency ) split datum equal-size half occur skewer distribution therefore informative also provide two quartile q1 q3 along median common rule thumb identify suspect outlier single value fall least 15 × iqr third quartile first quartile q1 median q3 together contain information endpoint ( eg tail ) datum fuller summary shape distribution obtain provide lowest highest datum value well know five-number summary five-number summary distribution consist median ( q2 ) quartile q1 q3 smallest largest individual observation written order minimum q1 median q3 maximum boxplot popular way visualize distribution boxplot incorporate five-number summary follow typically end box quartile box length interquartile range median marked line within box two line ( call whisker ) outside box extend smallest ( minimum ) largest ( maximum ) observation 
50 chapter 2 get know datum 220 200 180 160 unit price ( $ ) 140 120 100 80 60 40 20 branch 1 branch 2 branch 3 branch 4 figure 23 boxplot unit price datum item sell four branch allelectronic give time period deal moderate number observation worthwhile plot potential outlier individually boxplot whisker extend extreme low high observation value less 15 × iqr beyond quartile otherwise whisker terminate extreme observation occur within 15 × iqr quartile remain case plot individually boxplot used comparison several set compatible datum example 211 boxplot figure 23 show boxplot unit price datum item sell four branch allelectronic give time period branch 1 see median price item sell $ 80 q1 $ 60 q3 $ 100 notice two outlying observation branch plot individually value 175 202 15 time iqr 40 boxplot compute ( n log n ) time approximate boxplot compute linear sublinear time depend quality guarantee require variance standard deviation variance standard deviation measure datum dispersion indicate spread datum distribution low standard deviation mean datum observation tend close mean high standard deviation indicate datum spread large range value 
22 basic statistical description datum variance n observation x1 x2 xn numeric attribute x n n x x 1 1 σ2 = ( xi − x̄ ) 2 = xi2 − x̄ 2 n n i=1 51 ( 26 ) i=1 x̄ mean value observation defined eq ( 21 ) standard deviation σ observation square root variance σ 2 example 212 variance standard deviation example 26 find x̄ = $ 58000 used eq ( 21 ) mean determine variance standard deviation datum example set n = 12 use eq ( 26 ) obtain 1 ( 302 + 362 + 472 + 1102 ) − 582 12 ≈ 37917 √ σ ≈ 37917 ≈ 1947 σ2 = basic property standard deviation σ measure spread follow σ measure spread mean consider mean choose measure center σ = 0 spread observation value otherwise σ > 0 importantly observation unlikely several standard deviation away mathematically used chebyshev ’ inequality show  mean  least 1 − k12 × 100 % observation k standard deviation mean therefore standard deviation good indicator spread datum set computation variance standard deviation scalable large databasis 223 graphic display basic statistical description datum section study graphic display basic statistical description include quantile plot quantile–quantile plot histogram scatter plot graph helpful visual inspection datum useful datum preprocess first three show univariate distribution ( ie datum one attribute ) scatter plot show bivariate distribution ( ie involve two attribute ) quantile plot follow subsection cover common graphic display datum distribution quantile plot simple effective way first look univariate datum distribution first display datum give attribute ( allow user 
52 chapter 2 get know datum assess overall behavior unusual occurrence ) second plot quantile information ( see section 222 ) let xi = 1 n datum sort increase order x1 smallest observation xn largest ordinal numeric attribute x observation xi pair percentage fi indicate approximately fi × 100 % datum value xi say “ approximately ” may value exactly fraction fi datum xi note 025 percentile correspond quartile q1 050 percentile median 075 percentile q3 let fi = − 05 n ( 27 ) 1 number increase equal step n range 2n ( slightly 1 0 ) 1 − 2n ( slightly 1 ) quantile plot xi graph fi allow us compare different distribution base quantile example give quantile plot sale datum two different time period compare q1 median q3 fi value glance example 213 quantile plot figure 24 show quantile plot unit price datum table 21 quantile–quantile plot unit price ( $ ) quantile–quantile plot q-q plot graph quantile one univariate distribution corresponding quantile another powerful visualization tool allow user view whether shift go one distribution another suppose two set observation attribute variable unit price take two different branch location let x1 xn datum first branch y1 ym datum second datum set sort increase order = n ( ie number point set ) simply plot yi xi yi xi ( − 05 ) n quantile respective datum set < n ( ie second branch fewer observation first ) point q-q plot yi ( − 05 ) m quantile 140 120 100 80 60 40 20 0 000 q3 median q1 025 050 f-value 075 figure 24 quantile plot unit price datum table 21 100 
22 basic statistical description datum 53 table 21 set unit price datum item sell branch allelectronic unit price ( $ ) count item sell 40 43 47 − 74 75 78 − 115 117 120 275 300 250 − 360 515 540 − 320 270 350 branch 2 ( unit price $ ) 120 110 q3 100 median 90 80 70 q1 60 50 40 40 50 60 70 80 90 branch 1 ( unit price $ ) 100 110 120 figure 25 q-q plot unit price datum two allelectronic branch datum plot ( − 05 ) m quantile x datum computation typically involve interpolation example 214 quantile–quantile plot figure 25 show quantile–quantile plot unit price datum item sell two branch allelectronic give time period point correspond quantile datum set show unit price item sell branch 1 versus branch 2 quantile ( aid comparison straight line represent case give quantile unit price branch darker point correspond datum q1 median q3 respectively ) see example q1 unit price item sell branch 1 slightly less branch word 25 % item sell branch 1 less 
54 chapter 2 get know datum equal $ 60 25 % item sell branch 2 less equal $ 64 50th percentile ( marked median also q2 ) see 50 % item sell branch 1 less $ 78 50 % item branch 2 less $ 85 general note shift distribution branch 1 respect branch 2 unit price item sell branch 1 tend lower branch 2 histogram histogram ( frequency histogram ) least century old widely used “ histos ” mean pole mast “ gram ” mean chart histogram chart pole plot histogram graphical method summarize distribution give attribute x x nominal automobile model item type pole vertical bar draw know value x height bar indicate frequency ( ie count ) x value result graph commonly know bar chart x numeric term histogram prefer range value x partition disjoint consecutive subrange subrange refer bucket bin disjoint subset datum distribution x range bucket know width typically bucket equal width example price attribute value range $ 1 $ 200 ( round nearest dollar ) partition subrange 1 20 21 40 41 60 subrange bar draw height represent total count item observed within subrange histogram partition rule discuss chapter 3 datum reduction example 215 histogram figure 26 show histogram datum set table 21 bucket ( bin ) defined equal-width range represent $ 20 increment frequency count item sell although histogram widely used may effective quantile plot q-q plot boxplot method compare group univariate observation scatter plot datum correlation scatter plot one effective graphical method determine appear relationship pattern trend two numeric attribute construct scatter plot pair value treat pair coordinate algebraic sense plot point plane figure 27 show scatter plot set datum table 21 scatter plot useful method provide first look bivariate datum see cluster point outlier explore possibility correlation relationship two attribute x correlated one attribute imply correlation positive negative null ( uncorrelated ) figure 28 show example positive negative correlation two attribute plot point pattern slope 
22 basic statistical description datum 55 6000 count item sell 5000 4000 3000 2000 1000 0 40–59 60–79 80–99 unit price ( $ ) 100–119 120–139 figure 26 histogram table 21 datum set 700 item sell 600 500 400 300 200 100 0 0 20 40 60 80 unit price ( $ ) 100 120 140 figure 27 scatter plot table 21 datum set ( ) ( b ) figure 28 scatter plot used find ( ) positive ( b ) negative correlation attribute 
56 chapter 2 get know datum figure 29 three case observed correlation two plot attribute datum set lower left upper right mean value x increase value increase suggest positive correlation ( figure 28a ) pattern plot point slope upper left lower right value x increase value decrease suggest negative correlation ( figure 28b ) line best fit draw study correlation variable statistical test correlation give chapter 3 datum integration ( eq ( 33 ) ) figure 29 show three case correlation relationship two attribute give datum set section 232 show scatter plot extend n attribute result scatter-plot matrix conclusion basic datum description ( eg measure central tendency measure dispersion ) graphic statistical display ( eg quantile plot histogram scatter plot ) provide valuable insight overall behavior datum help identify noise outlier especially useful datum clean 23 datum visualization convey datum user effectively datum visualization aim communicate datum clearly effectively graphical representation datum visualization used extensively many applications—for example work report manage business operation tracking progress task popularly take advantage visualization technique discover datum relationship otherwise easily observable look raw datum nowadays person also use datum visualization create fun interesting graphic section briefly introduce basic concept datum visualization start multidimensional datum store relational databasis discuss several representative approach include pixel-oriented technique geometric projection technique icon-based technique hierarchical graph-based technique discuss visualization complex datum relation 
23 datum visualization 231 57 pixel-oriented visualization technique simple way visualize value dimension use pixel color pixel reflect dimension ’ value datum set dimension pixel-oriented technique create window screen one dimension dimension value record map pixel corresponding position window color pixel reflect corresponding value inside window datum value arrange global order share window global order may obtain sort datum record way ’ meaningful task hand example 216 pixel-oriented visualization allelectronic maintain customer information table consist four dimension income credit limit transaction volume age analyze correlation income attribute visualization sort customer income-ascending order use order lay customer datum four visualization window show figure pixel color choose smaller value lighter shading used pixelbased visualization easily observe follow credit limit increase income increase customer whose income middle range likely purchase allelectronic clear correlation income age pixel-oriented technique datum record also order query-dependent way example give point query sort record descend order similarity point query fill window layer datum record linear way may work well wide window first pixel row far away last pixel previous row though next global order moreover pixel next one window even though two next global order solve problem lay datum record space-filling curve ( ) income ( b ) credit_limit ( c ) transaction_volume ( ) age figure 210 pixel-oriented visualization four attribute sort customer income ascend order 
58 chapter 2 get know datum ( ) hilbert curve ( b ) gray code ( c ) z-curve figure 211 frequently used 2-d space-filling curf one datum record dim 6 dim 6 dim 5 dim 1 dim 4 dim 2 dim 3 ( ) dim 5 dim 1 dim 4 dim 2 dim 3 ( b ) figure 212 circle segment technique ( ) represent datum record circle segment ( b ) layer pixel circle segment fill window space-filling curve curve range cover entire n-dimensional unit hypercube since visualization window 2-d use 2-d space-filling curve figure 211 show frequently used 2-d space-filling curf note window rectangular example circle segment technique used window shape segment circle illustrated figure 212 technique ease comparison dimension dimension window locate side side form circle 232 geometric projection visualization technique drawback pixel-oriented visualization technique help us much understand distribution datum multidimensional space example show whether dense area multidimensional subspace geometric 
23 datum visualization 59 80 70 60 50 40 30 20 10 0 0 10 20 30 40 x 50 60 70 80 figure 213 visualization 2-d datum set used scatter plot source rareevent-geoinformatica06pdf projection technique help user find interesting projection multidimensional datum set central challenge geometric projection technique try address visualize high-dimensional space 2-d display scatter plot display 2-d datum point used cartesian coordinate third dimension add used different color shape represent different datum point figure 213 show example x two spatial attribute third dimension represent different shape visualization see point type “ + ” “ × ” tend colocate 3-d scatter plot used three axe cartesian coordinate system also used color display 4-d datum point ( figure 214 ) datum set four dimension scatter plot usually ineffective scatter-plot matrix technique useful extension scatter plot ndimensional datum set scatter-plot matrix n × n grid 2-d scatter plot provide visualization dimension every dimension figure 215 show example visualize iris datum set datum set consist 450 sample three species iris flower five dimension datum set length width sepal petal species scatter-plot matrix become less effective dimensionality increase another popular technique call parallel coordinate handle higher dimensionality visualize n-dimensional datum point parallel coordinate technique draw n equally space axe one dimension parallel one display axe 
60 chapter 2 get know datum figure 214 visualization 3-d datum set used scatter plot source http scatter plotjpg datum record represent polygonal line intersect axis point corresponding associate dimension value ( figure 216 ) major limitation parallel coordinate technique effectively show datum set many record even datum set several thousand record visual clutter overlap often reduce readability visualization make pattern hard find 233 icon-based visualization technique icon-based visualization technique use small icon represent multidimensional datum value look two popular icon-based technique chernoff face stick figure chernoff face introduce 1973 statistician herman chernoff display multidimensional datum 18 variable ( dimension ) cartoon human face ( figure 217 ) chernoff face help reveal trend datum component face eye ears mouth nose represent value dimension shape size placement orientation example dimension map follow facial characteristic eye size eye spacing nose length nose width mouth curvature mouth width mouth openness pupil size eyebrow slant eye eccentricity head eccentricity chernoff face make use ability human mind recognize small difference facial characteristic assimilate many facial characteristic 
23 datum visualization 10 30 50 70 0 10 61 20 80 70 sepal length ( mm ) 60 50 40 70 50 petal length ( mm ) 30 10 45 40 35 30 25 20 sepal width ( mm ) 25 20 15 10 5 0 petal width ( mm ) 40 50 60 70 80 iris species 20 setosa 30 versicolor 40 virginica figure 215 visualization iris datum set used scatter-plot matrix source http gsgscmatgif view large table datum tedious condense datum chernoff face make datum easier user digest way facilitate visualization regularity irregularity present datum although power relate multiple relationship limit another limitation specific datum value show furthermore facial feature vary perceive importance mean similarity two face ( represent two multidimensional datum point ) vary depend order dimension assign facial characteristic therefore mapping carefully choose eye size eyebrow slant find important asymmetrical chernoff face propose extension original technique since face vertical symmetry ( along y-axis ) left right side face identical waste space asymmetrical chernoff face double number facial characteristic thus allow 36 dimension display stick figure visualization technique map multidimensional datum five-piece stick figure figure four limb body two dimension map display ( x ) axe remain dimension map angle 
62 chapter 2 get know datum 10 5 x 0 –5 –10 ⫻1 ⫻2 ⫻3 ⫻4 ⫻5 ⫻6 ⫻7 ⫻8 ⫻9 ⫻10 figure 216 visualization used parallel coordinate source parallel coordithml figure 217 chernoff face face represent n-dimensional datum point ( n ≤ 18 ) or length limb figure 218 show census datum age income map display axe remain dimension ( gender education ) map stick figure datum item relatively dense respect two display dimension result visualization show texture pattern reflect datum trend 
23 datum visualization 63 figure 218 census datum represent used stick figure source professor g grinstein department computer science university massachusett lowell 234 hierarchical visualization technique visualization technique discuss far focus visualize multiple dimension simultaneously however large datum set high dimensionality would difficult visualize dimension time hierarchical visualization technique partition dimension subset ( ie subspace ) subspace visualize hierarchical manner “ worlds-within-world ” also know n-vision representative hierarchical visualization method suppose want visualize 6-d datum set dimension f x1 x5 want observe dimension f change respect dimension first fix value dimension x3 x4 x5 select value say c3 c4 c5 visualize f x1 x2 used 3-d plot call world show figure position origin inner world locate point ( c3 c4 c5 ) outer world another 3-d plot used dimension x3 x4 x5 user interactively change outer world location origin inner world user view result change inner world moreover user vary dimension used inner world outer world give dimension level world used method call “ worlds-withinworld ” another example hierarchical visualization method tree-maps display hierarchical datum set nest rectangle example figure 220 show tree-map visualize google news story news story organized seven category show large rectangle unique color within category ( ie rectangle top level ) news story partition smaller subcategory 
64 chapter 2 get know datum figure 219 “ worlds-within-world ” ( also know n-vision ) source http 1dipstick5gif 235 visualize complex datum relation early day visualization technique mainly numeric datum recently non-numeric datum text social network become available visualize analyze datum attract lot interest many new visualization technique dedicate kind datum example many person web tag various object picture blog entry product reviews tag cloud visualization statistic user-generated tag often tag cloud tag list alphabetically user-preferred order importance tag indicated font size color figure 221 show tag cloud visualize popular tag used web site tag cloud often used two way first tag cloud single item use size tag represent number time tag apply item different user second visualize tag statistic multiple item use size tag represent number item tag apply popularity tag addition complex datum complex relation among datum entry also raise challenge visualization example figure 222 used disease influence graph visualize correlation disease node graph disease size node proportional prevalence corresponding disease two node link edge corresponding disease strong correlation width edge proportional strength correlation pattern two corresponding disease 
24 measure datum similarity dissimilarity 65 figure 220 newsmap use tree-maps visualize google news headline story source wwwcsumd newsmappng summary visualization provide effective tool explore datum introduce several popular method essential idea behind many exist tool method moreover visualization used datum mining various aspect addition visualize datum visualization used represent datum mining process pattern obtain mining method user interaction datum visual datum mining important research development direction 24 measure datum similarity dissimilarity datum mining application cluster outlier analysis nearest-neighbor classification need way assess alike unalike object comparison one another example store may want search cluster customer object result group customer similar characteristic ( eg similar income area residence age ) information used marketing cluster 
66 chapter 2 get know datum figure 221 used tag cloud visualize popular web site tag source snapshot january 23 2010 high blood pressure ( hb ) allergy ( al ) st li overweight ( ov ) en high cholesterol level ( hc ) ki arthritis ( ar ) trouble see ( tr ) li risk diabetes ( ri ) asthma ( ) ca th diabetes ( di ) hayfever ( ha ) hc thyroid problem ( th ) di heart disease ( ) em tr ar hb cancer ( cn ) os sleep disorder ( sl ) ov eczema ( ec ) chronic bronchitis ( ch ) cn osteoporosis ( os ) prostate ( pr ) cardiovascular ( ca ) ps glaucoma ( gl ) ec pr stroke ( st ) liver condition ( li ) ch psa test abnormal ( ps ) kidney ( ki ) endometriosis ( en ) emphysema ( em ) ha al ri sl gl figure 222 disease influence graph person least 20 year old nhane datum set collection datum object object within cluster similar one another dissimilar object cluster outlier analysis also employ clustering-based technique identify potential outlier object highly dissimilar other knowledge object similarity also used nearest-neighbor classification scheme give object ( eg patient ) assign class label ( relate say diagnosis ) base similarity toward object model 
24 measure datum similarity dissimilarity 67 section present similarity dissimilarity measure refer measure proximity similarity dissimilarity related similarity measure two object j typically return value 0 object unalike higher similarity value greater similarity object ( typically value 1 indicate complete similarity object identical ) dissimilarity measure work opposite way return value 0 object ( therefore far dissimilar ) higher dissimilarity value dissimilar two object section 241 present two datum structure commonly used type application datum matrix ( used store datum object ) dissimilarity matrix ( used store dissimilarity value pair object ) also switch different notation datum object previously used chapter since deal object describe one attribute discuss object dissimilarity compute object describe nominal attribute ( section 242 ) binary attribute ( section 243 ) numeric attribute ( section 244 ) ordinal attribute ( section 245 ) combination attribute type ( section 246 ) section 247 provide similarity measure long sparse datum vector term-frequency vector represent document information retrieval know compute dissimilarity useful study attribute also reference later topic cluster ( chapter 10 11 ) outlier analysis ( chapter 12 ) nearest-neighbor classification ( chapter 9 ) 241 datum matrix versus dissimilarity matrix section 22 look way study central tendency dispersion spread observed value attribute x object one-dimensional describe single attribute section talk object describe multiple attribute therefore need change notation suppose n object ( eg person item course ) describe p attribute ( also call measurement feature age height weight gender ) object x1 = ( x11 x12 x1p ) x2 = ( x21 x22 x2p ) xij value object xi jth attribute brevity hereafter refer object xi object i object may tuple relational database also refer datum sample feature vector main memory-based cluster nearest-neighbor algorithms typically operate either follow two datum structure datum matrix ( object-by-attribute structure ) structure store n datum object form relational table n-by-p matrix ( n object ×p attribute )   x11 · · · x1f · · · x1p ··· ··· ··· ··· ···     ( 28 )  xi1 · · · xif · · · xip    ··· ··· ··· ··· ··· xn1 · · · xnf · · · xnp 
68 chapter 2 get know datum row correspond object part notation may use f index p attribute dissimilarity matrix ( object-by-object structure ) structure store collection proximity available pair n object often represent n-by-n table   0  d ( 2 1 ) 0      d ( 3 1 ) ( 3 2 ) 0 ( 29 )       ( n 1 ) ( n 2 ) · · · · · · 0 ( j ) measure dissimilarity “ difference ” object j general ( j ) non-negative number close 0 object j highly similar “ near ” become larger differ note ( ) = 0 difference object furthermore ( j ) = ( j ) ( readability show ( j ) entry matrix symmetric ) measure dissimilarity discuss throughout remainder chapter measure similarity often expressed function measure dissimilarity example nominal datum sim ( j ) = 1 − ( j ) ( 210 ) sim ( j ) similarity object j throughout rest chapter also comment measure similarity datum matrix make two entity “ thing ” namely row ( object ) column ( attribute ) therefore datum matrix often call two-mode matrix dissimilarity matrix contain one kind entity ( dissimilarity ) call one-mode matrix many cluster nearest-neighbor algorithms operate dissimilarity matrix datum form datum matrix transform dissimilarity matrix apply algorithms 242 proximity measure nominal attribute nominal attribute take two state ( section 212 ) example map color nominal attribute may say five state red yellow green pink blue let number state nominal attribute m state denote letter symbol set integer 1 2 m notice integer used datum handle represent specific order 
24 measure datum similarity dissimilarity 69 “ dissimilarity compute object describe nominal attribute ” dissimilarity two object j compute base ratio mismatch ( j ) = p−m p ( 211 ) number match ( ie number attribute j state ) p total number attribute describe object weight assign increase effect assign greater weight match attribute larger number state example 217 dissimilarity nominal attribute suppose sample datum table 22 except object-identifier attribute test-1 available test-1 nominal ( use test-2 test-3 later example ) let ’ compute dissimilarity matrix ( eq 29 )   0  d ( 2 1 ) 0      d ( 3 1 ) ( 3 2 ) 0 ( 4 1 ) ( 4 2 ) ( 4 3 ) 0 since one nominal attribute test-1 set p = 1 eq ( 211 ) ( j ) evaluate 0 object j match 1 object differ thus get  0 1   1 0  0 1 1 0 1     0 see object dissimilar except object 1 4 ( ie ( 4 1 ) = 0 ) table 22 sample datum table contain attribute mixed type object identifier test-1 ( nominal ) test-2 ( ordinal ) test-3 ( numeric ) 1 2 3 4 code code b code c code excellent fair good excellent 45 22 64 28 
70 chapter 2 get know datum alternatively similarity compute sim ( j ) = 1 − ( j ) = p ( 212 ) proximity object describe nominal attribute compute used alternative encode scheme nominal attribute encode used asymmetric binary attribute create new binary attribute state object give state value binary attribute represent state set 1 remain binary attribute set example encode nominal attribute map color binary attribute create five color previously list object color yellow yellow attribute set 1 remain four attribute set proximity measure form encode calculate used method discuss next subsection 243 proximity measure binary attribute let ’ look dissimilarity similarity measure object describe either symmetric asymmetric binary attribute recall binary attribute one two state 0 1 0 mean attribute absent 1 mean present ( section 213 ) give attribute smoker describe patient instance 1 indicate patient smoke 0 indicate patient treat binary attribute numeric mislead therefore method specific binary datum necessary compute dissimilarity “ compute dissimilarity two binary attribute ” one approach involve compute dissimilarity matrix give binary datum binary attribute thought weight 2 × 2 contingency table table 23 q number attribute equal 1 object j r number attribute equal 1 object equal 0 object j number attribute equal 0 object equal 1 object j number attribute equal 0 object j total number attribute p p = q + r + + recall symmetric binary attribute state equally valuable dissimilarity base symmetric binary attribute call symmetric binary dissimilarity object j describe symmetric binary attribute table 23 contingency table binary attribute object j object 1 0 sum 1 q q+s 0 r r t sum q+r s+t p 
24 measure datum similarity dissimilarity 71 dissimilarity j ( j ) = r s q+r s+t ( 213 ) asymmetric binary attribute two state equally important positive ( 1 ) negative ( 0 ) outcome disease test give two asymmetric binary attribute agreement two 1s ( positive match ) consider significant two 0s ( negative match ) therefore binary attribute often consider “ monary ” ( one state ) dissimilarity base attribute call asymmetric binary dissimilarity number negative match consider unimportant thus ignore follow computation ( j ) = r s q+r s ( 214 ) complementarily measure difference two binary attribute base notion similarity instead dissimilarity example asymmetric binary similarity object j compute sim ( j ) = q = 1 − ( j ) q+r s ( 215 ) coefficient sim ( j ) eq ( 215 ) call jaccard coefficient popularly reference literature symmetric asymmetric binary attribute occur datum set mixed attribute approach describe section 246 apply example 218 dissimilarity binary attribute suppose patient record table ( table 24 ) contain attribute name gender fever cough test-1 test-2 test-3 test-4 name object identifier gender symmetric attribute remain attribute asymmetric binary asymmetric attribute value let value ( yes ) p ( positive ) set 1 value n ( negative ) set suppose distance object table 24 relational table patient describe binary attribute name gender fever cough test-1 test-2 test-3 test-4 jack jim mary f n n p n p n n n n n p n n n 
72 chapter 2 get know datum ( patient ) compute base asymmetric attribute accord eq ( 214 ) distance pair three patients—jack mary jim—is ( jack jim ) = 1+1 = 067 1+1+1 ( jack mary ) = 0+1 = 033 2+0+1 ( jim mary ) = 1+2 = 075 1+1+2 measurement suggest jim mary unlikely similar disease highest dissimilarity value among three pair three patient jack mary likely similar disease 244 dissimilarity numeric datum minkowski distance section describe distance measure commonly used compute dissimilarity object describe numeric attribute measure include euclidean manhattan minkowski distance case datum normalize apply distance calculation involve transform datum fall within smaller common range [ −1 1 ] [ 00 10 ] consider height attribute example can measure either meter inch general express attribute smaller unit lead larger range attribute thus tend give attribute greater effect “ weight ” normalize datum attempt give attribute equal weight may may useful particular application method normalize datum discuss detail chapter 3 datum preprocess popular distance measure euclidean distance ( ie straight line “ crow fly ” ) let = ( xi1 xi2 xip ) j = ( xj1 xj2 xjp ) two object describe p numeric attribute euclidean distance object j defined q ( j ) = ( xi1 − xj1 ) 2 + ( xi2 − xj2 ) 2 + · · · + ( xip − xjp ) 2 ( 216 ) another well-known measure manhattan ( city block ) distance name distance block two point city ( 2 block 3 block total 5 block ) defined ( j ) = xi1 − xj1 | + xi2 − xj2 | + · · · + xip − xjp | ( 217 ) euclidean manhattan distance satisfy follow mathematical property non-negativity ( j ) ≥ 0 distance non-negative number identity indiscernible ( ) = 0 distance object 0 
24 measure datum similarity dissimilarity 73 symmetry ( j ) = ( j ) distance symmetric function triangle inequality ( j ) ≤ ( k ) + ( k j ) go directly object object j space make detour object k measure satisfy condition know metric please note non-negativity property imply three property example 219 euclidean distance manhattan distance let x1 = ( 1 2 ) x2 = ( 3 5 ) represent √ two object show figure euclidean distance two 22 + 32 = manhattan distance two 2 + 3 = 5 minkowski distance generalization euclidean manhattan distance defined q ( j ) = h xi1 − xj1 h + xi2 − xj2 h + · · · + xip − xjp h ( 218 ) h real number h ≥ 1 ( distance also call lp norm literature symbol p refer notation h keep p number attribute consistent rest chapter ) represent manhattan distance h = 1 ( ie l1 norm ) euclidean distance h = 2 ( ie l2 norm ) supremum distance ( also refer lmax l∞ norm chebyshev distance ) generalization minkowski distance h → ∞ compute find attribute f give maximum difference value two object difference supremum distance defined formally  1 h p x p h  ( j ) = lim xif − xjf | = max xif − xjf | ( 219 ) h→∞ f 1 f l∞ norm also know uniform norm x2 = ( 3 5 ) 5 4 euclidean distance = ( 22 + 32 ) 2 = 361 3 3 2 x1 = ( 1 2 ) manhattan distance 2+3=5 supremum distance 5–2=3 2 1 1 2 3 figure 223 euclidean manhattan supremum distance two object 
74 chapter 2 get know datum example 220 supremum distance let ’ use two object x1 = ( 1 2 ) x2 = ( 3 5 ) figure second attribute give greatest difference value object 5 − 2 = supremum distance object attribute assign weight accord perceive importance weight euclidean distance compute q ( j ) = w1 xi1 − xj1 2 + w2 xi2 − xj2 2 + · · · + wm xip − xjp 2 ( 220 ) weighting also apply distance measure well 245 proximity measure ordinal attribute value ordinal attribute meaningful order ranking yet magnitude successive value unknown ( section 214 ) example include sequence small medium large size attribute ordinal attribute may also obtain discretization numeric attribute splitting value range finite number category category organized rank range numeric attribute map ordinal attribute f mf state example range interval-scaled attribute temperature ( celsius ) organized follow state −30 −10 −10 10 10 30 represent category cold temperature moderate temperature warm temperature respectively let represent number possible state ordinal attribute order state define ranking 1 mf “ ordinal attribute handled ” treatment ordinal attribute quite similar numeric attribute compute dissimilarity object suppose f attribute set ordinal attribute describe n object dissimilarity computation respect f involve follow step value f ith object xif f mf order state represent ranking 1 mf replace xif corresponding rank rif ∈ { 1 mf } since ordinal attribute different number state often necessary map range attribute onto [ 00 10 ] attribute equal weight perform datum normalization replace rank rif ith object f th attribute zif = rif − 1 mf − 1 ( 221 ) dissimilarity compute used distance measure describe section 244 numeric attribute used zif represent f value ith object 
24 measure datum similarity dissimilarity 75 example 221 dissimilarity ordinal attribute suppose sample datum show earlier table 22 except time object-identifier continuous ordinal attribute test-2 available three state test-2 fair good excellent mf = step 1 replace value test-2 rank four object assign rank 3 1 2 3 respectively step 2 normalizes ranking mapping rank 1 00 rank 2 05 rank 3 step 3 use say euclidean distance ( eq 216 ) result follow dissimilarity matrix  0 10 0   05 05 0 0 10 05      0 therefore object 1 2 dissimilar object 2 4 ( ie ( 2 1 ) = 10 ( 4 2 ) = 10 ) make intuitive sense since object 1 4 excellent object 2 fair opposite end range value test-2 similarity value ordinal attribute interpreted dissimilarity sim ( j ) = 1 − ( j ) 246 dissimilarity attribute mixed type section 242 245 discuss compute dissimilarity object describe attribute type type may either nominal symmetric binary asymmetric binary numeric ordinal however many real databasis object describe mixture attribute type general database contain attribute type “ compute dissimilarity object mixed attribute type ” one approach group type attribute together perform separate datum mining ( eg cluster ) analysis type feasible analysis derive compatible result however real application unlikely separate analysis per attribute type generate compatible result preferable approach process attribute type together perform single analysis one technique combine different attribute single dissimilarity matrix bring meaningful attribute onto common scale interval [ 00 10 ] suppose datum set contain p attribute mixed type dissimilarity ( j ) object j defined ( f ) ( f ) f 1 δij dij pp ( f ) f 1 δij pp ( j ) = ( 222 ) 
76 chapter 2 get know datum ( f ) indicator δij = 0 either ( 1 ) xif xjf miss ( ie measurement attribute f object object j ) ( 2 ) xif = xjf = 0 attribute ( f ) f asymmetric binary otherwise δij = contribution attribute f ( f ) dissimilarity j ( ie dij ) compute dependent type ( f ) f numeric dij = attribute f xif −xjf | maxh xhf −minh xhf h run nonmissing object ( f ) ( f ) f nominal binary dij = 0 xif = xjf otherwise dij = 1 f ordinal compute rank rif zif = rif −1 mf −1 treat zif numeric step identical already see individual attribute type difference numeric attribute normalize value map interval [ 00 10 ] thus dissimilarity object compute even attribute describe object different type example 222 dissimilarity attribute mixed type let ’ compute dissimilarity matrix object table consider attribute different type example 217 221 work dissimilarity matrix individual attribute procedure follow test-1 ( nominal ) test-2 ( ordinal ) outlined earlier process attribute mixed type therefore use dissimilarity matrix obtain test-1 test-2 later compute eq ( 222 ) first however need compute dissimilarity matrix third attribute test-3 ( numeric ) ( 3 ) must compute dij follow case numeric attribute let maxh xh = 64 minh xh = difference two used eq ( 222 ) normalize value dissimilarity matrix result dissimilarity matrix test-3  0 055   045 040  0 100 014 0 086     0 use dissimilarity matrix three attribute computation ( f ) eq ( 222 ) indicator δij = 1 three attribute f get example ( 3 1 ) = 1 ( 1 ) 1 ( 050 ) 1 ( 045 ) 3 = result dissimilarity matrix obtain 
24 measure datum similarity dissimilarity 77 datum describe three attribute mixed type  0 085   065 013  0 083 071 0 079     0 table 22 intuitively guess object 1 4 similar base value test-1 test-2 confirm dissimilarity matrix ( 4 1 ) lowest value pair different object similarly matrix indicate object 1 2 least similar 247 cosine similarity document represent thousand attribute record frequency particular word ( keyword ) phrase document thus document object represent call term-frequency vector example table 25 see document1 contain five instance word team hockey occur three time word coach absent entire document indicated count value datum highly asymmetric term-frequency vector typically long sparse ( ie many 0 value ) application used structure include information retrieval text document cluster biological taxonomy gene feature mapping traditional distance measure study chapter work well sparse numeric datum example two term-frequency vector may many 0 value common meaning corresponding document share many word make similar need measure focus word two document common occurrence frequency word word need measure numeric datum ignore zero-match cosine similarity measure similarity used compare document say give ranking document respect give vector query word let x two vector comparison used cosine measure table 25 document vector term-frequency vector document team coach hockey baseball soccer penalty score win loss season document1 document2 document3 document4 5 3 0 0 0 0 7 1 3 2 0 0 0 0 2 0 2 1 1 1 0 1 0 2 0 0 0 2 2 1 3 0 0 0 0 3 0 1 0 0 
78 chapter 2 get know datum similarity function sim ( x ) = x·y | ( 223 ) | euclidean norm vector x = ( x1 x2 xp ) defined q x12 + x22 + · · · + xp2 conceptually length vector similarly | euclidean norm vector y measure compute cosine angle vector x y cosine value 0 mean two vector 90 degree ( orthogonal ) match closer cosine value 1 smaller angle greater match vector note cosine similarity measure obey property section 244 define metric measure refer nonmetric measure example 223 cosine similarity two term-frequency vector suppose x first two term-frequency vector table x = ( 5 0 3 0 2 0 0 2 0 0 ) = ( 3 0 2 0 1 1 0 1 0 1 ) similar x used eq ( 223 ) compute cosine similarity two vector get xt · = 5 × 3 + 0 × 0 + 3 × 2 + 0 × 0 + 2 × 1 + 0 × 1 + 0 × 0 + 2 × 1 + 0 × 0 + 0 × 1 = 25 p | = 52 + 02 + 32 + 02 + 22 + 02 + 02 + 22 + 02 + 02 = 648 p | = 32 + 02 + 22 + 02 + 12 + 12 + 02 + 12 + 02 + 12 = 412 sim ( x ) = 094 therefore used cosine similarity measure compare document would consider quite similar attribute binary-valu cosine similarity function interpreted term share feature attribute suppose object x possess ith attribute xi = xt · number attribute possessed ( ie share ) x | geometric mean number attribute possessed x number possessed y thus sim ( x ) measure relative possession common attribute simple variation cosine similarity precede scenario sim ( x ) = x·y x·x+y·y−x·y ( 224 ) ratio number attribute share x number attribute possessed x y function know tanimoto coefficient tanimoto distance frequently used information retrieval biology taxonomy 
26 exercise 25 79 summary datum set make datum object datum object represent entity datum object describe attribute attribute nominal binary ordinal numeric value nominal ( categorical ) attribute symbol name thing value represent kind category code state binary attribute nominal attribute two possible state ( 1 0 true false ) two state equally important attribute symmetric otherwise asymmetric ordinal attribute attribute possible value meaningful order ranking among magnitude successive value know numeric attribute quantitative ( ie measurable quantity ) represent integer real value numeric attribute type interval-scaled ratioscale value interval-scaled attribute measure fix equal unit ratio-scale attribute numeric attribute inherent zero-point measurement ratio-scale speak value order magnitude larger unit measurement basic statistical description provide analytical foundation datum preprocess basic statistical measure datum summarization include mean weight mean median mode measure central tendency datum range quantile quartile interquartile range variance standard deviation measure dispersion datum graphical representation ( eg boxplot quantile plot quantile– quantile plot histogram scatter plot ) facilitate visual inspection datum thus useful datum preprocess mining datum visualization technique may pixel-oriented geometric-based icon-based hierarchical method apply multidimensional relational datum additional technique propose visualization complex datum text social network measure object similarity dissimilarity used datum mining application cluster outlier analysis nearest-neighbor classification measure proximity compute attribute type study chapter combination attribute example include jaccard coefficient asymmetric binary attribute euclidean manhattan minkowski supremum distance numeric attribute application involve sparse numeric datum vector term-frequency vector cosine measure tanimoto coefficient often used assessment similarity 26 exercise 21 give three additional commonly used statistical measure already illustrated chapter characterization datum dispersion discuss compute efficiently large databasis 
80 chapter 2 get know datum 22 suppose datum analysis include attribute age age value datum tuple ( increase order ) 13 15 16 16 19 20 20 21 22 22 25 25 25 25 30 33 33 35 35 35 35 36 40 45 46 52 70 ( ) mean datum median ( b ) mode datum comment datum ’ modality ( ie bimodal trimodal etc ) ( c ) midrange datum ( ) find ( roughly ) first quartile ( q1 ) third quartile ( q3 ) datum ( e ) give five-number summary datum ( f ) show boxplot datum ( g ) quantile–quantile plot different quantile plot 23 suppose value give set datum group interval interval corresponding frequency follow age 1–5 6–15 16–20 21–50 51–80 81–110 frequency 200 450 300 1500 700 44 compute approximate median value datum 24 suppose hospital test age body fat datum 18 randomly select adult follow result age % fat 23 95 23 265 27 78 27 178 39 314 41 259 47 274 49 272 50 312 age % fat 52 346 54 425 54 288 56 334 57 302 58 341 58 329 60 412 61 357 ( ) calculate mean median standard deviation age % fat ( b ) draw boxplot age % fat ( c ) draw scatter plot q-q plot base two variable 25 briefly outline compute dissimilarity object describe follow ( ) nominal attribute ( b ) asymmetric binary attribute 
27 bibliographic note 81 ( c ) numeric attribute ( ) term-frequency vector 26 give two object represent tuple ( 22 1 42 10 ) ( 20 0 36 8 ) ( ) ( b ) ( c ) ( ) compute euclidean distance two object compute manhattan distance two object compute minkowski distance two object used q = 3 compute supremum distance two object 27 median one important holistic measure datum analysis propose several method median approximation analyze respective complexity different parameter setting decide extent real value approximate moreover suggest heuristic strategy balance accuracy complexity apply method give 28 important define select similarity measure datum analysis however commonly accept subjective similarity measure result vary depend similarity measure used nonetheless seemingly different similarity measure may equivalent transformation suppose follow 2-d datum set x1 x2 x3 x4 x5 a1 15 2 16 12 15 a2 17 19 18 15 10 ( ) consider datum 2-d datum point give new datum point x = ( 14 16 ) query rank database point base similarity query used euclidean distance manhattan distance supremum distance cosine similarity ( b ) normalize datum set make norm datum point equal use euclidean distance transform datum rank datum point 27 bibliographic note method descriptive datum summarization study statistic literature long onset computer good summary statistical descriptive datum mining method include freedman pisani purf [ fpp07 ] devore [ dev95 ] 
82 chapter 2 get know datum statistics-based visualization datum used boxplot quantile plot quantile–quantile plot scatter plot loess curf see cleveland [ cle93 ] pioneer work datum visualization technique describe visual display quantitative information [ tuf83 ] envision information [ tuf90 ] visual explanation image quantity evidence narrative [ tuf97 ] tufte addition graphic graphic information process bertin [ ber81 ] visualize datum cleveland [ cle93 ] information visualization datum mining knowledge discovery edit fayyad grinstein wierse [ fgw01 ] major conference symposium visualization include acm human factor compute system ( chi ) visualization international symposium information visualization research visualization also publish transaction visualization computer graphic journal computational graphical statistic ieee computer graphic application many graphical user interface visualization tool develop find various datum mining product several book datum mining ( eg datum mining solution westphal blaxton [ wb98 ] ) present many good example visual snapshot survey visualization technique see “ visual technique explore databasis ” keim [ kei97 ] similarity distance measure among various variable introduce many textbook study cluster analysis include hartigan [ har75 ] jain dube [ jd88 ] kaufman rousseeuw [ kr90 ] arabie hubert de soete [ ahs96 ] method combine attribute different type single dissimilarity matrix introduce kaufman rousseeuw [ kr90 ] 
10 cluster analysis basic concept method imagine director customer relationship allelectronic five manager work would like organize company ’ customer five group group assign different manager strategically would like customer group similar possible moreover two give customer different business pattern place group intention behind business strategy develop customer relationship campaign specifically target group base common feature share customer per group kind datum mining technique help accomplish task unlike classification class label ( group id ) customer unknown need discover grouping give large number customer many attribute describe customer profile costly even infeasible human study datum manually come way partition customer strategic group need cluster tool help cluster process grouping set datum object multiple group cluster object within cluster high similarity dissimilar object cluster dissimilarity similarity assessed base attribute value describe object often involve distance measures1 cluster datum mining tool root many application area biology security business intelligence web search chapter present basic concept method cluster analysis section 101 introduce topic study requirement cluster method massive amount datum various application learn several basic cluster technique organized follow category partition method ( section 102 ) hierarchical method ( section 103 ) density-based method ( section 104 ) grid-based method ( section 105 ) section 106 briefly discuss evaluate 1 datum similarity dissimilarity discuss detail section may want refer section quick review datum mining concept technique doi b978-0-12-381479-100010-1 c 2012 elsevier right re-serve 443 
444 chapter 10 cluster analysis basic concept method cluster method discussion advanced method cluster re-serve chapter 11 101 cluster analysis section set groundwork study cluster analysis section 1011 define cluster analysis present example useful section 1012 learn aspect compare cluster method well requirement cluster overview basic cluster technique present section 1013 1011 cluster analysis cluster analysis simply cluster process partition set datum object ( observation ) subset subset cluster object cluster similar one another yet dissimilar object cluster set cluster result cluster analysis refer cluster context different cluster method may generate different clustering datum set partition perform human cluster algorithm hence cluster useful lead discovery previously unknown group within datum cluster analysis widely used many application business intelligence image pattern recognition web search biology security business intelligence cluster used organize large number customer group customer within group share strong similar characteristic facilitate development business strategy enhance customer relationship management moreover consider consultant company large number project improve project management cluster apply partition project category base similarity project audit diagnosis ( improve project delivery outcome ) conduct effectively image recognition cluster used discover cluster “ subclass ” handwritten character recognition system suppose datum set handwritten digit digit labele either 1 2 3 note large variance way person write digit take number 2 example person may write small circle left bottom part other may use cluster determine subclass “ 2 ” represent variation way 2 written used multiple model base subclass improve overall recognition accuracy cluster also find many application web search example keyword search may often return large number hit ( ie page relevant search ) due extremely large number web page cluster used organize search result group present result concise easily accessible way moreover cluster technique develop cluster document topic commonly used information retrieval practice 
101 cluster analysis 445 datum mining function cluster analysis used standalone tool gain insight distribution datum observe characteristic cluster focus particular set cluster analysis alternatively may serve preprocess step algorithms characterization attribute subset selection classification would operate detected cluster select attribute feature cluster collection datum object similar one another within cluster dissimilar object cluster cluster datum object treat implicit class sense cluster sometimes call automatic classification critical difference cluster automatically find grouping distinct advantage cluster analysis cluster also call datum segmentation application cluster partition large datum set group accord similarity cluster also used outlier detection outlier ( value “ far away ” cluster ) may interesting common case application outlier detection include detection credit card fraud monitoring criminal activity electronic commerce example exceptional case credit card transaction expensive infrequent purchase may interest possible fraudulent activity outlier detection subject chapter 12 datum cluster vigorous development contribute area research include datum mining statistic machine learn spatial database technology information retrieval web search biology marketing many application area owing huge amount datum collect databasis cluster analysis recently become highly active topic datum mining research branch statistic cluster analysis extensively study main focus distance-based cluster analysis cluster analysis tool base k-mean k-medoid several method also build many statistical analysis software package system s-plus spss sas machine learn recall classification know supervised learn class label information give learn algorithm supervised tell class membership training tuple cluster know unsupervised learn class label information present reason cluster form learn observation rather learn example datum mining effort focuse find method efficient effective cluster analysis large databasis active theme research focus scalability cluster method effectiveness method cluster complex shape ( eg nonconvex ) type datum ( eg text graph image ) high-dimensional cluster technique ( eg cluster object thousand feature ) method cluster mixed numerical nominal datum large databasis 1012 requirement cluster analysis cluster challenge research field section learn requirement cluster datum mining tool well aspect used compare cluster method 
446 chapter 10 cluster analysis basic concept method follow typical requirement cluster datum mining scalability many cluster algorithms work well small datum set contain fewer several hundred datum object however large database may contain million even billion object particularly web search scenario cluster sample give large datum set may lead bias result therefore highly scalable cluster algorithms need ability deal different type attribute many algorithms design cluster numeric ( interval-based ) datum however application may require cluster datum type binary nominal ( categorical ) ordinal datum mixture datum type recently application need cluster technique complex datum type graph sequence image document discovery cluster arbitrary shape many cluster algorithms determine cluster base euclidean manhattan distance measure ( chapter 2 ) algorithms base distance measure tend find spherical cluster similar size density however cluster can shape consider sensor example often deploy environment surveillance cluster analysis sensor reading detect interesting phenomena may want use cluster find frontier run forest fire often spherical important develop algorithms detect cluster arbitrary shape requirement domain knowledge determine input parameter many cluster algorithms require user provide domain knowledge form input parameter desire number cluster consequently cluster result may sensitive parameter parameter often hard determine especially high-dimensionality datum set user yet grasp deep understand datum require specification domain knowledge burden user also make quality cluster difficult control ability deal noisy datum real-world datum set contain outlier or miss unknown erroneous datum sensor reading example often noisy—some reading may inaccurate due sense mechanism reading may erroneous due interference surround transient object cluster algorithms sensitive noise may produce poor-quality cluster therefore need cluster method robust noise incremental cluster insensitivity input order many application incremental update ( represent newer datum ) may arrive time cluster algorithms incorporate incremental update exist cluster structure instead recompute new cluster scratch cluster algorithms may also sensitive input datum order give set datum object cluster algorithms may return dramatically different clustering depend order object present incremental cluster algorithms algorithms insensitive input order need 
101 cluster analysis 447 capability cluster high-dimensionality datum datum set contain numerous dimension attribute cluster document example keyword regard dimension often thousand keyword cluster algorithms good handle low-dimensional datum datum set involve two three dimension find cluster datum object highdimensional space challenge especially consider datum sparse highly skewer constraint-based cluster real-world application may need perform cluster various kind constraint suppose job choose location give number new automatic teller machine ( atms ) city decide upon may cluster household consider constraint city ’ river highway network type number customer per cluster challenge task find datum group good cluster behavior satisfy specify constraint interpretability usability user want cluster result interpretable comprehensible usable cluster may need tie specific semantic interpretation application important study application goal may influence selection cluster feature cluster method follow orthogonal aspect cluster method compare partition criterium method object partition hierarchy exist among cluster cluster level conceptually method useful example partition customer group group manager alternatively method partition datum object hierarchically cluster form different semantic level example text mining may want organize corpus document multiple general topic “ politic ” “ sport ” may subtopic instance “ football ” “ basketball ” “ baseball ” “ hockey ” exist subtopic “ ” latter four subtopic lower level hierarchy “ sport ” separation cluster method partition datum object mutually exclusive cluster cluster customer group group take care one manager customer may belong one group situation cluster may exclusive datum object may belong one cluster example cluster document topic document may related multiple topic thus topic cluster may exclusive similarity measure method determine similarity two object distance distance defined euclidean space 
448 chapter 10 cluster analysis basic concept method road network vector space space method similarity may defined connectivity base density contiguity may rely absolute distance two object similarity measure play fundamental role design cluster method distance-based method often take advantage optimization technique - continuity-based method often find cluster arbitrary shape cluster space many cluster method search cluster within entire give datum space method useful low-dimensionality datum set highdimensional datum however many irrelevant attribute make similarity measurement unreliable consequently cluster find full space often meaningless ’ often better instead search cluster within different subspace datum set subspace cluster discover cluster subspace ( often low dimensionality ) manifest object similarity conclude cluster algorithms several requirement factor include scalability ability deal different type attribute noisy datum incremental update cluster arbitrary shape constraint interpretability usability also important addition cluster method differ respect partition level whether cluster mutually exclusive similarity measure used whether subspace cluster perform 1013 overview basic cluster method many cluster algorithms literature difficult provide crisp categorization cluster method category may overlap method may feature several category nevertheless useful present relatively organized picture cluster method general major fundamental cluster method classify follow category discuss rest chapter partition method give set n object partition method construct k partition datum partition represent cluster k ≤ n divide datum k group group must contain least one object word partition method conduct one-level partition datum set basic partition method typically adopt exclusive cluster separation object must belong exactly one group requirement may relax example fuzzy partition technique reference technique give bibliographic note ( section 109 ) partition method distance-based give k number partition construct partition method create initial partition used iterative relocation technique attempt improve partition move object one group another general criterion good partition object cluster “ close ” related whereas object different cluster “ far apart ” different various kind 
101 cluster analysis 449 criterium judge quality partition traditional partition method extend subspace cluster rather search full datum space useful many attribute datum sparse achieve global optimality partitioning-based cluster often computationally prohibitive potentially require exhaustive enumeration possible partition instead application adopt popular heuristic method greedy approach like k-mean k-medoid algorithms progressively improve cluster quality approach local optimum heuristic cluster method work well find spherical-shap cluster - medium-size databasis find cluster complex shape large datum set partitioning-based method need extend partitioning-based cluster method study depth section 102 hierarchical method hierarchical method create hierarchical decomposition give set datum object hierarchical method classify either agglomerative divisive base hierarchical decomposition form agglomerative approach also call bottom-up approach start object form separate group successively merge object group close one another group merged one ( topmost level hierarchy ) termination condition hold divisive approach also call top-down approach start object cluster successive iteration cluster split smaller cluster eventually object one cluster termination condition hold hierarchical cluster method distance-based - continuitybased various extension hierarchical method consider cluster subspace well hierarchical method suffer fact step ( merge split ) do never undo rigidity useful lead smaller computation cost worry combinatorial number different choice technique correct erroneous decision however method improve quality hierarchical cluster propose hierarchical cluster method study section 103 density-based method partition method cluster object base distance object method find spherical-shap cluster encounter difficulty discover cluster arbitrary shape cluster method develop base notion density general idea continue grow give cluster long density ( number object datum point ) “ neighborhood ” exceed threshold example datum point within give cluster neighborhood give radius contain least minimum number point method used filter noise outlier discover cluster arbitrary shape density-based method divide set object multiple exclusive cluster hierarchy cluster typically density-based method consider exclusive cluster consider fuzzy cluster moreover density-based method extend full space subspace cluster density-based cluster method study section 104 
450 chapter 10 cluster analysis basic concept method grid-based method grid-based method quantize object space finite number cell form grid structure cluster operation perform grid structure ( ie quantized space ) main advantage approach fast process time typically independent number datum object dependent number cell dimension quantized space used grid often efficient approach many spatial datum mining problem include cluster therefore grid-based method integrate cluster method density-based method hierarchical method gridbase cluster study section 105 method briefly summarize figure cluster algorithms integrate idea several cluster method sometimes difficult classify give algorithm uniquely belong one cluster method category furthermore application may cluster criterium require integration several cluster technique follow section examine cluster method detail advanced cluster method related issue discuss chapter general notation used follow let datum set n object cluster object describe variable variable also call attribute dimension method partition method general characteristic – find mutually exclusive cluster spherical shape – distance-based – may use mean medoid ( etc ) represent cluster center – effective - medium-size datum set hierarchical method – cluster hierarchical decomposition ( ie multiple level ) – correct erroneous merge split – may incorporate technique like microcluster consider object “ linkage ” density-based method – find arbitrarily shape cluster – cluster dense region object space separated low-density region – cluster density point must minimum number point within “ neighborhood ” – may filter outlier grid-based method – use multiresolution grid datum structure – fast process time ( typically independent number datum object yet dependent grid size ) figure 101 overview cluster method discuss chapter note algorithms may combine various method 
102 partition method 451 therefore may also refer point d-dimensional object space object represent bold italic font ( eg p ) 102 partition method simplest fundamental version cluster analysis partition organize object set several exclusive group cluster keep problem specification concise assume number cluster give background knowledge parameter start point partition method formally give datum set n object k number cluster form partition algorithm organize object k partition ( k ≤ n ) partition represent cluster cluster form optimize objective partition criterion dissimilarity function base distance object within cluster “ similar ” one another “ dissimilar ” object cluster term datum set attribute section learn well-known commonly used partition methods—k-mean ( section 1021 ) k-medoid ( section 1022 ) also learn several variation classic partition method scale handle large datum set 1021 k-mean centroid-based technique suppose datum set contain n object euclidean space partition method distribute object k cluster c1 ck ci ⊂ ci ∩ cj = ∅ ( 1 ≤ j ≤ k ) objective function used assess partition quality object within cluster similar one another dissimilar object cluster objective function aim high intracluster similarity low intercluster similarity centroid-based partition technique used centroid cluster ci represent cluster conceptually centroid cluster center point centroid defined various way mean medoid object ( point ) assign cluster difference object p ∈ ci ci representative cluster measure dist ( p ci ) dist ( x ) euclidean distance two point x y quality cluster ci measure withincluster variation sum square error object ci centroid ci defined = k x x dist ( p ci ) 2 ( 101 ) i=1 p∈ci e sum square error object datum set p point space represent give object ci centroid cluster ci ( p ci multidimensional ) word object cluster distance 
452 chapter 10 cluster analysis basic concept method object cluster center square distance sum objective function try make result k cluster compact separate possible optimize within-cluster variation computationally challenge worst case would enumerate number possible partitioning exponential number cluster check within-cluster variation value show problem np-hard general euclidean space even two cluster ( ie k = 2 ) moreover problem np-hard general number cluster k even 2-d euclidean space number cluster k dimensionality space fix problem solve time ( ndk+1 log n ) n number object overcome prohibitive computational cost exact solution greedy approach often used practice prime example k-mean algorithm simple commonly used “ k-mean algorithm work ” k-mean algorithm define centroid cluster mean value point within cluster proceed follow first randomly select k object initially represent cluster mean center remain object object assign cluster similar base euclidean distance object cluster mean k-mean algorithm iteratively improve within-cluster variation cluster compute new mean used object assign cluster previous iteration object reassign used update mean new cluster center iteration continue assignment stable cluster form current round form previous round k-mean procedure summarize figure 102 algorithm k-mean k-mean algorithm partition cluster ’ center represent mean value object cluster input k number cluster datum set contain n object output set k cluster method ( 1 ) arbitrarily choose k object initial cluster center ( 2 ) repeat ( 3 ) ( ) assign object cluster object similar base mean value object cluster ( 4 ) update cluster mean calculate mean value object cluster ( 5 ) change figure 102 k-mean partition algorithm 
102 partition method 453 + + + + + ( ) initial cluster + ( b ) iterate + + + ( c ) final cluster figure 103 cluster set object used k-mean method ( b ) update cluster center reassign object accordingly ( mean cluster marked + ) example 101 cluster k-mean partition consider set object locate 2-d space depict figure 103 ( ) let k = 3 user would like object partition three cluster accord algorithm figure 102 arbitrarily choose three object three initial cluster center cluster center marked + object assign cluster base cluster center nearest distribution form silhouette encircle dot curf show figure 103 ( ) next cluster center update mean value cluster recalculate base current object cluster used new cluster center object redistribute cluster base cluster center nearest redistribution form new silhouette encircle dash curf show figure 103 ( b ) process iterate lead figure 103 ( c ) process iteratively reassigning object cluster improve partition refer iterative relocation eventually reassignment object cluster occur process terminate result cluster return cluster process k-mean method guarantee converge global optimum often terminate local optimum result may depend initial random selection cluster center ( ask give example show exercise ) obtain good result practice common run k-mean algorithm multiple time different initial cluster center time complexity k-mean algorithm ( nkt ) n total number object k number cluster number iteration normally k n n therefore method relatively scalable efficient process large datum set several variant k-mean method differ selection initial k-mean calculation dissimilarity strategy calculate cluster mean 
454 chapter 10 cluster analysis basic concept method k-mean method apply mean set object defined may case application datum nominal attribute involved k-mode method variant k-mean extend k-mean paradigm cluster nominal datum replace mean cluster mode used new dissimilarity measure deal nominal object frequency-based method update mode cluster k-mean k-mode method integrate cluster datum mixed numeric nominal value necessity user specify k number cluster advance see disadvantage study overcome difficulty however provide approximate range k value used analytical technique determine best k compare cluster result obtain different k value k-mean method suitable discover cluster nonconvex shape cluster different size moreover sensitive noise outlier datum point small number datum substantially influence mean value “ make k-mean algorithm scalable ” one approach make k-mean method efficient large datum set use good-sized set sample cluster another employ filter approach used spatial hierarchical datum index save cost compute mean third approach explore microcluster idea first group nearby object “ microcluster ” perform k-mean cluster microcluster microcluster discuss section 103 1022 k-medoid representative object-based technique k-mean algorithm sensitive outlier object far away majority datum thus assign cluster dramatically distort mean value cluster inadvertently affect assignment object cluster effect particularly exacerbate due use squared-error function eq ( 101 ) observed example 102 example 102 drawback k-mean consider six point 1-d space value 1 2 3 8 9 10 25 respectively intuitively visual inspection may imagine point partition cluster { 1 2 3 } { 8 9 10 } point 25 exclude appear outlier would k-mean partition value apply k-mean used k = 2 eq ( 101 ) partition { { 1 2 3 } { 8 9 10 25 } } within-cluster variation ( 1 − 2 ) 2 + ( 2 − 2 ) 2 + ( 3 − 2 ) 2 + ( 8 − 13 ) 2 + ( 9 − 13 ) 2 + ( 10 − 13 ) 2 + ( 25 − 13 ) 2 = 196 give mean cluster { 1 2 3 } 2 mean { 8 9 10 25 } compare partition { { 1 2 3 8 } { 9 10 25 } } k-mean compute withincluster variation ( 1 − 35 ) 2 + ( 2 − 35 ) 2 + ( 3 − 35 ) 2 + ( 8 − 35 ) 2 + ( 9 − 1467 ) 2 + ( 10 − 1467 ) 2 + ( 25 − 1467 ) 2 = 18967 
102 partition method 455 give 35 mean cluster { 1 2 3 8 } 1467 mean cluster { 9 10 25 } latter partition lowest within-cluster variation therefore k-mean method assign value 8 cluster different contain 9 10 due outlier point moreover center second cluster 1467 substantially far member cluster “ modify k-mean algorithm diminish sensitivity outlier ” instead take mean value object cluster reference point pick actual object represent cluster used one representative object per cluster remain object assign cluster representative object similar partition method perform base principle minimize sum dissimilarity object p corresponding representative object absolute-error criterion used defined = k x x dist ( p oi ) ( 102 ) i=1 p∈ci e sum absolute error object p datum set oi representative object ci basis k-medoid method group n object k cluster minimize absolute error ( eq 102 ) k = 1 find exact median ( n2 ) time however k general positive number k-medoid problem np-hard partition around medoid ( pam ) algorithm ( see figure 105 later ) popular realization k-medoid cluster tackle problem iterative greedy way like k-mean algorithm initial representative object ( call seed ) choose arbitrarily consider whether replace representative object nonrepresentative object would improve cluster quality possible replacement try iterative process replace representative object object continue quality result cluster improve replacement quality measure cost function average dissimilarity object representative object cluster specifically let o1 ok current set representative object ( ie medoid ) determine whether nonrepresentative object denote orandom good replacement current medoid oj ( 1 ≤ j ≤ k ) calculate distance every object p closest object set { o1 oj−1 orandom oj+1 ok } use distance update cost function reassignment object { o1 oj−1 orandom oj+1 ok } simple suppose object p currently assign cluster represent medoid oj ( figure 104a b ) need reassign p different cluster oj replace orandom object p need reassign either orandom cluster represent oi ( = j ) whichever closest example figure 104 ( ) p closest oi therefore reassign oi figure 104 ( b ) however p closest orandom reassign orandom instead p currently assign cluster represent object oi = j 
456 chapter 10 cluster analysis basic concept method oi p oj orandom ( ) reassign oi oi oj p oi oj oi oj p orandom ( b ) reassign orandom ( c ) change orandom p orandom datum object cluster center swap swap ( ) reassign orandom figure 104 four case cost function k-medoid cluster object remain assign cluster represent oi long still closer oi orandom ( figure 104c ) otherwise reassign orandom ( figure 104d ) time reassignment occur difference absolute error e contribute cost function therefore cost function calculate difference absolute-error value current representative object replace nonrepresentative object total cost swap sum cost incur nonrepresentative object total cost negative oj replace swap orandom actual absolute-error e reduce total cost positive current representative object oj consider acceptable nothing change iteration “ method robust—k-mean k-medoid ” k-medoid method robust k-mean presence noise outlier medoid less influenced outlier extreme value mean however complexity iteration k-medoid algorithm ( k ( n − k ) 2 ) large value n k computation become costly much costly k-mean method method require user specify k number cluster “ scale k-medoid method ” typical k-medoid partition algorithm like pam ( figure 105 ) work effectively small datum set scale well large datum set deal larger datum set sampling-based method call clara ( cluster large application ) used instead take whole datum set consideration clara used random sample datum set pam algorithm apply compute best medoid sample ideally sample closely represent original datum set many case large sample work well create object equal probability select sample representative object ( medoid ) choose likely similar would choose whole datum set clara build clustering multiple random sample return best cluster output complexity compute medoid random sample ( ks 2 + k ( n − k ) ) size sample k number cluster n total number object clara deal larger datum set pam effectiveness clara depend sample size notice pam search best k-medoid among give datum set whereas clara search best k-medoid among select sample datum set clara find good cluster best sample medoid far best k-medoid object 
103 hierarchical method 457 algorithm k-medoid pam k-medoid algorithm partition base medoid central object input k number cluster datum set contain n object output set k cluster method ( 1 ) arbitrarily choose k object initial representative object seed ( 2 ) repeat ( 3 ) assign remain object cluster nearest representative object ( 4 ) randomly select nonrepresentative object orandom ( 5 ) compute total cost swap representative object oj orandom ( 6 ) < 0 swap oj orandom form new set k representative object ( 7 ) change figure 105 pam k-medoid partition algorithm one best k-medoid select sampling clara never find best cluster ( ask provide example demonstrate exercise ) “ might improve quality scalability clara ” recall search better medoid pam examine every object datum set every current medoid whereas clara confine candidate medoid random sample datum set randomize algorithm call claran ( cluster large application base upon randomize search ) present trade-off cost effectiveness used sample obtain cluster first randomly select k object datum set current medoid randomly select current medoid x object one current medoid replace x improve absolute-error criterion yes replacement make claran conduct randomize search l time set current medoid l step consider local optimum claran repeat randomize process time return best local optimal final result 103 hierarchical method partition method meet basic cluster requirement organize set object number exclusive group situation may want partition datum group different level hierarchy hierarchical cluster method work grouping datum object hierarchy “ tree ” cluster represent datum object form hierarchy useful datum summarization visualization example manager human resource allelectronic 
458 chapter 10 cluster analysis basic concept method may organize employee major group executive manager staff partition group smaller subgroup instance general group staff divide subgroup senior officer officer trainee group form hierarchy easily summarize characterize datum organized hierarchy used find say average salary manager officer consider handwritten character recognition another example set handwriting sample may first partition general group group correspond unique character group partition subgroup since character may written multiple substantially different way necessary hierarchical partition continue recursively desire granularity reach previous example although partition datum hierarchically assume datum hierarchical structure ( eg manager level allelectronic hierarchy staff ) use hierarchy summarize represent underlie datum compress way hierarchy particularly useful datum visualization alternatively application may believe datum bear underlie hierarchical structure want discover example hierarchical cluster may uncover hierarchy allelectronic employee structure say salary study evolution hierarchical cluster may group animal accord biological feature uncover evolutionary path hierarchy species another example grouping configuration strategic game ( eg chess checker ) hierarchical way may help develop game strategy used train player section study hierarchical cluster method section 1031 begin discussion agglomerative versus divisive hierarchical cluster organize object hierarchy used bottom-up top-down strategy respectively agglomerative method start individual object cluster iteratively merged form larger cluster conversely divisive method initially let give object form one cluster iteratively split smaller cluster hierarchical cluster method encounter difficulty regard selection merge split point decision critical group object merged split process next step operate newly generate cluster neither undo do previously perform object swap cluster thus merge split decision well choose may lead low-quality cluster moreover method scale well decision merge split need examine evaluate many object cluster promising direction improve cluster quality hierarchical method integrate hierarchical cluster cluster technique result multiple-phase ( multiphase ) cluster introduce two method namely birch chameleon birch ( section 1033 ) begin partition object hierarchically used tree structure leaf low-level nonleaf node view “ microcluster ” depend resolution scale apply 
103 hierarchical method 459 cluster algorithms perform macrocluster microcluster chameleon ( section 1034 ) explore dynamic modele hierarchical cluster several orthogonal way categorize hierarchical cluster method instance may categorize algorithmic method probabilistic method bayesian method agglomerative divisive multiphase method algorithmic meaning consider datum object deterministic compute cluster accord deterministic distance object probabilistic method use probabilistic model capture cluster measure quality cluster fitness model discuss probabilistic hierarchical cluster section bayesian method compute distribution possible clustering instead output single deterministic cluster datum set return group cluster structure probability conditional give datum bayesian method consider advanced topic discuss book 1031 agglomerative versus divisive hierarchical cluster hierarchical cluster method either agglomerative divisive depend whether hierarchical decomposition form bottom-up ( merge ) topdown ( splitting ) fashion let ’ closer look strategy agglomerative hierarchical cluster method used bottom-up strategy typically start let object form cluster iteratively merge cluster larger larger cluster object single cluster certain termination condition satisfied single cluster become hierarchy ’ root merge step find two cluster closest ( accord similarity measure ) combine two form one cluster two cluster merged per iteration cluster contain least one object agglomerative method require n iteration divisive hierarchical cluster method employ top-down strategy start place object one cluster hierarchy ’ root divide root cluster several smaller subcluster recursively partition cluster smaller one partition process continue cluster lowest level coherent enough—either contain one object object within cluster sufficiently similar either agglomerative divisive hierarchical cluster user specify desire number cluster termination condition example 103 agglomerative versus divisive hierarchical cluster figure 106 show application agne ( agglomerative nest ) agglomerative hierarchical cluster method diana ( divisive analysis ) divisive hierarchical cluster method datum set five object { b c e } initially agne agglomerative method place object cluster cluster merged step-by-step accord criterion example cluster c1 c2 may merged object c1 object c2 form minimum euclidean distance two object 
chapter 10 cluster analysis basic concept method agglomerative ( agne ) step 0 step 1 step 2 step 3 step 4 ab b abcde c cde de e step 4 step 3 step 2 step 1 divisive ( diana ) step 0 figure 106 agglomerative divisive hierarchical cluster datum object { b c e } level l=0 b c e 10 l=1 l=2 06 l=3 04 l=4 02 08 similarity scale 460 00 figure 107 dendrogram representation hierarchical cluster datum object { b c e } different cluster single-linkage approach cluster represent object cluster similarity two cluster measure similarity closest pair datum point belong different cluster cluster-merge process repeat object eventually merged form one cluster diana divisive method proceed contrast way object used form one initial cluster cluster split accord principle maximum euclidean distance closest neighboring object cluster cluster-split process repeat eventually new cluster contain single object tree structure call dendrogram commonly used represent process hierarchical cluster show object group together ( agglomerative method ) partition ( divisive method ) step-by-step figure 107 show dendrogram five object present figure 106 l = 0 show five object singleton cluster level l = 1 object b group together form 
103 hierarchical method 461 first cluster stay together subsequent level also use vertical axis show similarity scale cluster example similarity two group object { b } { c e } roughly 016 merged together form single cluster challenge divisive method partition large cluster several smaller one example 2n−1 − 1 possible way partition set n object two exclusive subset n number object n large computationally prohibitive examine possibility consequently divisive method typically used heuristic partition lead inaccurate result sake efficiency divisive method typically backtrack partition decision make cluster partition alternative partition cluster consider due challenge divisive method many agglomerative method divisive method 1032 distance measure algorithmic method whether used agglomerative method divisive method core need measure distance two cluster cluster generally set object four widely used measure distance cluster follow p − p0 | distance two object point p p0 mi mean cluster ci ni number object ci also know linkage measure minimum distance distmin ( ci cj ) = maximum distance distmax ( ci cj ) = mean distance average distance min { p − p0 | } ( 103 ) max { p − p0 | } ( 104 ) p∈ci p0 ∈cj p∈ci p0 ∈cj distmean ( ci cj ) = mi − mj | distavg ( ci cj ) = 1 ni nj x ( 105 ) p − p0 | ( 106 ) p∈ci p0 ∈cj algorithm used minimum distance dmin ( ci cj ) measure distance cluster sometimes call nearest-neighbor cluster algorithm moreover cluster process terminate distance nearest cluster exceed user-defined threshold call single-linkage algorithm view datum point node graph edge form path node cluster merge two cluster ci cj correspond add edge nearest pair node ci cj edge link cluster always go distinct cluster result graph generate tree thus agglomerative hierarchical cluster algorithm used minimum distance measure also call 
462 chapter 10 cluster analysis basic concept method minimal span tree algorithm span tree graph tree connect vertex minimal span tree one least sum edge weight algorithm used maximum distance dmax ( ci cj ) measure distance cluster sometimes call farthest-neighbor cluster algorithm cluster process terminate maximum distance nearest cluster exceed user-defined threshold call complete-linkage algorithm view datum point node graph edge link node think cluster complete subgraph edge connect node cluster distance two cluster determine distant node two cluster farthest-neighbor algorithms tend minimize increase diameter cluster iteration true cluster rather compact approximately equal size method produce high-quality cluster otherwise cluster produce meaningless previous minimum maximum measure represent two extreme measure distance cluster tend overly sensitive outlier noisy datum use mean average distance compromise minimum maximum distance overcome outlier sensitivity problem whereas mean distance simplest compute average distance advantageous handle categoric well numeric datum computation mean vector categoric datum difficult impossible define example 104 single versus complete linkage let us apply hierarchical cluster datum set figure 108 ( ) figure 108 ( b ) show dendrogram used single linkage figure 108 ( c ) show case used complete linkage edge cluster { b j h } { c g f e } omitted ease presentation example show used single linkage find hierarchical cluster defined local proximity whereas complete linkage tend find cluster opt global closeness variation four essential linkage measure discuss example measure distance two cluster distance centroid ( ie central object ) cluster 1033 birch multiphase hierarchical cluster used cluster feature tree balanced iterative reduce cluster used hierarchy ( birch ) design cluster large amount numeric datum integrate hierarchical cluster ( initial microcluster stage ) cluster method iterative partition ( later macrocluster stage ) overcome two difficulty agglomerative cluster method ( 1 ) scalability ( 2 ) inability undo do previous step birch used notion cluster feature summarize cluster cluster feature tree ( cf-tree ) represent cluster hierarchy structure help 
103 hierarchical method b c 463 e j h g f ( ) datum set b c e j h g f b c e f g h j c ( b ) cluster used single linkage b c e j h g f b h j e f g ( c ) cluster used complete linkage figure 108 hierarchical cluster used single complete linkage cluster method achieve good speed scalability large even stream databasis also make effective incremental dynamic cluster incoming object consider cluster n d-dimensional datum object point cluster feature ( cf ) cluster 3-d vector summarize information cluster object defined cf = hn ls ssi ( 107 ) p ls linear n point ( ie ni=1 xi ) ss square sum pn sum datum point ( ie i=1 xi 2 ) cluster feature essentially summary statistic give cluster used cluster feature easily derive many useful statistic cluster example cluster ’ centroid x0 radius r diameter n p x0 = i=1 n xi = ls n ( 108 ) 
464 chapter 10 cluster analysis basic concept method = = v u n ux u ( xi − x0 ) 2 u i=1 n = v ux n x n u u ( xi − xj ) 2 u i=1 j=1 n ( n − 1 ) nss − 2ls2 + nls n2 = 2nss − 2ls2 n ( n − 1 ) ( 109 ) ( 1010 ) r average distance member object centroid average pairwise distance within cluster r reflect tightness cluster around centroid summarize cluster used cluster feature avoid store detailed information individual object point instead need constant size space store cluster feature key birch efficiency space moreover cluster feature additive two disjoint cluster c1 c2 cluster feature cf1 = hn1 ls1 ss1 cf2 = hn2 ls2 ss2 respectively cluster feature cluster form merge c1 c2 simply cf1 + cf2 = hn1 + n2 ls1 + ls2 ss1 + ss2 ( 1011 ) example 105 cluster feature suppose three point ( 2 5 ) ( 3 2 ) ( 4 3 ) cluster c1 cluster feature c1 cf1 = h3 ( 2 + 3 + 4 5 + 2 + 3 ) ( 22 + 32 + 42 52 + 22 + 32 ) = h3 ( 9 10 ) ( 29 38 ) suppose c1 disjoint second cluster c2 cf2 = h3 ( 35 36 ) ( 417 440 ) cluster feature new cluster c3 form merge c1 c2 derive add cf1 cf2 cf3 = h3 + 3 ( 9 + 35 10 + 36 ) ( 29 + 417 38 + 440 ) = h6 ( 44 46 ) ( 446 478 ) cf-tree height-balanced tree store cluster feature hierarchical cluster example show figure definition nonleaf node tree descendant “ ” nonleaf node store sum cfs child thus summarize cluster information child cf-tree two parameter branch factor b threshold t branch factor specify maximum number child per nonleaf node threshold parameter specify maximum diameter subcluster store leaf node tree two parameter implicitly control result tree ’ size give limit amount main memory important consideration birch minimize time require output ( o ) birch apply multiphase cluster technique single scan datum set yield basic good cluster 
103 hierarchical method cf1 cf11 cf12 cf2 cf1k cfk 465 root level first level figure 109 cf-tree structure one additional scan optionally used improve quality primary phase phase 1 birch scan database build initial in-memory cf-tree view multilevel compression datum try preserve datum ’ inherent cluster structure phase 2 birch apply ( select ) cluster algorithm cluster leaf node cf-tree remove sparse cluster outlier group dense cluster larger one phase 1 cf-tree build dynamically object insert thus method incremental object insert closest leaf entry ( subcluster ) diameter subcluster store leaf node insertion larger threshold value leaf node possibly node split insertion new object information object pass toward root tree size cf-tree change modify threshold size memory need store cf-tree larger size main memory larger threshold value specify cf-tree rebuild rebuild process perform build new tree leaf node old tree thus process rebuild tree do without necessity reread object point similar insertion node split construction b+-tree therefore build tree datum read heuristic method introduce deal outlier improve quality cf-tree additional scan datum cf-tree build cluster algorithm typical partition algorithm used cf-tree phase 2 “ effective birch ” time complexity algorithm ( n ) n number object cluster experiment show linear scalability algorithm respect number object good quality cluster datum however since node cf-tree hold limit number entry due size cf-tree node always correspond user may consider natural cluster moreover cluster spherical shape birch perform well used notion radius diameter control boundary cluster 
466 chapter 10 cluster analysis basic concept method idea cluster feature cf-tree apply beyond birch idea borrow many other tackle problem cluster stream dynamic datum 1034 chameleon multiphase hierarchical cluster used dynamic modele chameleon hierarchical cluster algorithm used dynamic modele determine similarity pair cluster chameleon cluster similarity assessed base ( 1 ) well connect object within cluster ( 2 ) proximity cluster two cluster merged interconnectivity high close together thus chameleon depend static user-supplied model automatically adapt internal characteristic cluster merged merge process facilitate discovery natural homogeneous cluster apply datum type long similarity function specify figure 1010 illustrate chameleon work chameleon used k-nearest-neighbor graph approach construct sparse graph vertex graph represent datum object exist edge two vertex ( object ) one object among k-most similar object edge weight reflect similarity object chameleon used graph partition algorithm partition k-nearest-neighbor graph large number relatively small subcluster minimize edge cut cluster c partition subcluster ci cj minimize weight edge would cut c bisect ci cj assess absolute interconnectivity cluster ci cj chameleon used agglomerative hierarchical cluster algorithm iteratively merge subcluster base similarity determine pair similar subcluster take account interconnectivity closeness cluster specifically chameleon determine similarity pair cluster ci cj accord relative interconnectivity ri ( ci cj ) relative closeness rc ( ci cj ) relative interconnectivity ri ( ci cj ) two cluster ci cj defined absolute interconnectivity ci cj normalize respect k-nearest-neighbor graph datum set construct sparse graph partition graph final cluster merge partition figure 1010 chameleon hierarchical cluster base k-nearest neighbor dynamic modele source base karypis han kumar [ khk99 ] 
103 hierarchical method 467 internal interconnectivity two cluster ci cj ri ( ci cj ) = ec { ci cj } | 1 2 ( ecci | + eccj | ) ( 1012 ) ec { ci cj } edge cut previously defined cluster contain ci cj similarly ecci ( eccj ) minimum sum cut edge partition ci ( cj ) two roughly equal part relative closeness rc ( ci cj ) pair cluster ci cj absolute closeness ci cj normalize respect internal closeness two cluster ci cj defined rc ( ci cj ) = sec { ci cj } ci | ci cj | ec ci c | j + ci c sec cj | ( 1013 ) sec { ci cj } average weight edge connect vertex ci vertex cj sec ci ( sec cj ) average weight edge belong mincut bisector cluster ci ( cj ) chameleon show greater power discover arbitrarily shape cluster high quality several well-known algorithms birch densitybased dbscan ( section 1041 ) however process cost high-dimensional datum may require ( n2 ) time n object worst case 1035 probabilistic hierarchical cluster algorithmic hierarchical cluster method used linkage measure tend easy understand often efficient cluster commonly used many cluster analysis application however algorithmic hierarchical cluster method suffer several drawback first choose good distance measure hierarchical cluster often far trivial second apply algorithmic method datum object miss attribute value case datum partially observed ( ie attribute value object miss ) easy apply algorithmic hierarchical cluster method distance computation conduct third algorithmic hierarchical cluster method heuristic step locally search good splitting decision consequently optimization goal result cluster hierarchy unclear probabilistic hierarchical cluster aim overcome disadvantage used probabilistic model measure distance cluster one way look cluster problem regard set datum object cluster sample underlie datum generation mechanism analyze formally generative model example conduct cluster analysis set marketing survey assume survey collect sample opinion possible customer datum generation mechanism probability 
468 chapter 10 cluster analysis basic concept method distribution opinion respect different customer obtain directly completely task cluster estimate generative model accurately possible used observed datum object cluster practice assume datum generative model adopt common distribution function gaussian distribution bernoulli distribution govern parameter task learn generative model reduce find parameter value model best fit observed datum set example 106 generative model suppose give set 1-d point x = { x1 xn } cluster analysis let us assume datum point generate gaussian distribution n ( µ σ 2 ) = √ 2 1 2π σ 2 e − ( x−µ ) 2 2σ ( 1014 ) parameter µ ( mean ) σ 2 ( variance ) probability point xi ∈ x generate model ( x −µ ) 2 1 − e 2σ 2 p ( xi µ σ 2 ) = √ 2π σ 2 ( 1015 ) consequently likelihood x generate model l ( n ( µ σ 2 ) x ) = p ( x|µ σ 2 ) = n i=1 √ 1 2π σ 2 e − ( xi −µ ) 2 2σ 2 ( 1016 ) task learn generative model find parameter µ σ 2 likelihood l ( n ( µ σ 2 ) x ) maximize find n ( µ0 σ02 ) = arg max { l ( n ( µ σ 2 ) x ) } ( 1017 ) max { l ( n ( µ σ 2 ) x ) } call maximum likelihood give set object quality cluster form object measure maximum likelihood set object partition cluster c1 cm quality measure q ( { c1 cm } ) = i=1 p ( ci ) ( 1018 ) 
103 hierarchical method 469 p ( ) maximum likelihood merge two cluster cj1 cj2 cluster cj1 ∪ cj2 change quality overall cluster q ( ( { c1 cm } − { cj1 cj2 } ) ∪ { cj1 ∪ cj2 } ) − q ( { c1 cm } ) qm p ( ci ) · p ( cj1 ∪ cj2 ) = i=1 − p ( ci ) p ( cj1 ) p ( cj2 ) i=1 = i=1  p ( cj1 ∪ cj2 ) −1 p ( ci ) p ( cj1 ) p ( cj2 )  ( 1019 ) q choose merge two cluster hierarchical cluster i=1 p ( ci ) constant pair cluster therefore give cluster c1 c2 distance measure dist ( ci cj ) = − log p ( c1 ∪ c2 ) p ( c1 ) p ( c2 ) ( 1020 ) probabilistic hierarchical cluster method adopt agglomerative cluster framework use probabilistic model ( eq 1020 ) measure distance cluster upon close observation eq ( 1019 ) see merge two cluster may p ( c ∪c ) always lead improvement cluster quality p ( cj j1 ) p ( cj2j ) may less 1 2 example assume gaussian distribution function used model figure although merge cluster c1 c2 result cluster better fit gaussian distribution merge cluster c3 c4 lower cluster quality gaussian function fit merged cluster well base observation probabilistic hierarchical cluster scheme start one cluster per object merge two cluster ci cj distance negative iteration try find ci cj maximize p ( c ∪c ) p ( c ∪c ) j j log p ( ci ) p ( c iteration continue long log p ( ci ) p ( c > 0 long j ) j ) improvement cluster quality pseudocode give figure 1012 probabilistic hierarchical cluster method easy understand generally efficiency algorithmic agglomerative hierarchical cluster method fact share framework probabilistic model interpretable sometimes less flexible distance metric probabilistic model handle partially observed datum example give multidimensional datum set object miss value dimension learn gaussian model dimension independently used observed value dimension result cluster hierarchy accomplish optimization goal fitting datum select probabilistic model drawback used probabilistic hierarchical cluster output one hierarchy respect choose probabilistic model handle uncertainty cluster hierarchy give datum set may exist multiple hierarchy 
470 chapter 10 cluster analysis basic concept method c1 c2 ( ) c3 c4 ( b ) ( c ) figure 1011 merge cluster probabilistic hierarchical cluster ( ) merge cluster c1 c2 lead increase overall cluster quality merge cluster ( b ) c3 ( c ) c4 algorithm probabilistic hierarchical cluster algorithm input = { o1 } datum set contain n object output hierarchy cluster method ( 1 ) create cluster object ci = { oi } 1 ≤ ≤ n ( 2 ) = 1 n p ( c ∪c ) ( 3 ) j find pair cluster ci cj ci cj = arg maxi6=j log p ( c ) p ( c ) ( 4 ) j log p ( c ) p ( c ) > 0 merge ci cj ( 5 ) else stop p ( c ∪c ) j j figure 1012 probabilistic hierarchical cluster algorithm fit observed datum neither algorithmic approach probabilistic approach find distribution hierarchy recently bayesian tree-structure model develop handle problem bayesian sophisticated probabilistic cluster method consider advanced topic cover book 
104 density-based method 104 471 density-based method partition hierarchical method design find spherical-shap cluster difficulty find cluster arbitrary shape “ ” shape oval cluster figure give datum would likely inaccurately identify convex region noise outlier include cluster find cluster arbitrary shape alternatively model cluster dense region datum space separated sparse region main strategy behind density-based cluster method discover cluster nonspherical shape section learn basic technique density-based cluster study three representative method namely dbscan ( section 1041 ) optic ( section 1042 ) denclue ( section 1043 ) 1041 dbscan density-based cluster base connect region high density “ find dense region density-based cluster ” density object measure number object close o dbscan ( density-based spatial cluster application noise ) find core object object dense neighborhood connect core object neighborhood form dense region cluster “ dbscan quantify neighborhood object ” user-specified parameter  > 0 used specify radius neighborhood consider every object -neighborhood object space within radius  center due fix neighborhood size parameterized  density neighborhood measure simply number object neighborhood determine whether neighborhood dense dbscan used another user-specified figure 1013 cluster arbitrary shape 
472 chapter 10 cluster analysis basic concept method parameter minpt specify density threshold dense region object core object -neighborhood object contain least minpt object core object pillar dense region give set object identify core object respect give parameter  minpt cluster task therein reduce used core object neighborhood form dense region dense region cluster core object q object p say p directly density-reachable q ( respect  minpt ) p within -neighborhood q clearly object p directly density-reachable another object q q core object p -neighborhood q used directly density-reachable relation core object “ bring ” object -neighborhood dense region “ assemble large dense region used small dense region center core object ” dbscan p density-reachable q ( respect  minpt ) chain object p1 pn p1 = q pn = p pi+1 directly density-reachable pi respect  minpt 1 ≤ ≤ n pi ∈ d note density-reachability equivalence relation symmetric o1 o2 core object o1 density-reachable o2 o2 density-reachable o1 however o2 core object o1 o1 may density-reachable o2 vice versa connect core object well neighbor dense region dbscan used notion density-connectedness two object p1 p2 ∈ density-connect respect  minpt object q ∈ p1 p2 densityreachable q respect  minpt unlike density-reachability densityconnectedness equivalence relation easy show object o1 o2 o3 o1 o2 density-connect o2 o3 density-connect o1 o3 example 107 density-reachability density-connectivity consider figure 1014 give  represent radius circle say let minpt = 3 labele point p r core object -neighborhood contain least three point object q directly density-reachable m object directly density-reachable p vice versa object q ( indirectly ) density-reachable p q directly densityreachable directly density-reachable p however p densityreachable q q core object similarly r density-reachable density-reachable r thus r density-connect use closure density-connectedness find connect dense region cluster close set density-based cluster subset c ⊆ cluster ( 1 ) two object o1 o2 ∈ c o1 o2 density-connect ( 2 ) exist object ∈ c another object o0 ∈ ( − c ) o0 densityconnect 
104 density-based method 473 q p r figure 1014 density-reachability density-connectivity density-based cluster source base ester kriegel sander xu [ eksx96 ] “ dbscan find cluster ” initially object give datum set marked “ ” dbscan randomly select unvisite object p mark p “ visit ” check whether -neighborhood p contain least minpt object p marked noise point otherwise new cluster c create p object -neighborhood p add candidate set n dbscan iteratively add c object n belong cluster process object p0 n carry label “ unvisite ” dbscan mark “ visit ” check -neighborhood -neighborhood p0 least minpt object object -neighborhood p0 add n dbscan continue add object c c longer expand n empty time cluster c complete thus output find next cluster dbscan randomly select unvisite object remain one cluster process continue object visit pseudocode dbscan algorithm give figure 1015 spatial index used computational complexity dbscan ( n log n ) n number database object otherwise complexity ( n2 ) appropriate setting user-defined parameter  minpt algorithm effective find arbitrary-shap cluster 1042 optic order point identify cluster structure although dbscan cluster object give input parameter  ( maximum radius neighborhood ) minpt ( minimum number point require neighborhood core object ) encumber user responsibility select parameter value lead discovery acceptable cluster problem associate many cluster algorithms parameter setting 
474 chapter 10 cluster analysis basic concept method algorithm dbscan density-based cluster algorithm input datum set contain n object  radius parameter minpt neighborhood density threshold output set density-based cluster method ( 1 ) mark object unvisite ( 2 ) ( 3 ) randomly select unvisite object p ( 4 ) mark p visit ( 5 ) -neighborhood p least minpt object ( 6 ) create new cluster c add p c ( 7 ) let n set object -neighborhood p ( 8 ) point p0 n ( 9 ) p0 unvisite ( 10 ) mark p0 visit ( 11 ) -neighborhood p0 least minpt point add point n ( 12 ) p0 yet member cluster add p0 c ( 13 ) end ( 14 ) output c ( 15 ) else mark p noise ( 16 ) object unvisite figure 1015 dbscan algorithm usually empirically set difficult determine especially real-world highdimensional datum set algorithms sensitive parameter value slightly different setting may lead different clustering datum moreover real-world high-dimensional datum set often skewer distribution intrinsic cluster structure may well characterize single set global density parameter note density-based cluster monotonic respect neighborhood threshold dbscan fix minpt value two neighborhood threshold 1 < 2 cluster c respect 1 minpt must subset cluster c 0 respect 2 minpt mean two object density-based cluster must also cluster lower density requirement overcome difficulty used one set global parameter cluster analysis cluster analysis method call optic propose optic explicitly produce datum set cluster instead output cluster order linear list 
104 density-based method 475 object analysis represent density-based cluster structure datum object denser cluster list closer cluster order order equivalent density-based cluster obtain wide range parameter setting thus optic require user provide specific density threshold cluster order used extract basic cluster information ( eg cluster center arbitrary-shap cluster ) derive intrinsic cluster structure well provide visualization cluster construct different clustering simultaneously object processed specific order order select object density-reachable respect lowest  value cluster higher density ( lower  ) finished first base idea optic need two important piece information per object core-distance object p smallest value  0  0 neighborhood p least minpt object  0 minimum distance threshold make p core object p core object respect  minpt core-distance p undefined reachability-distance object p q minimum radius value make p density-reachable q accord definition density-reachability q core object p must neighborhood q therefore reachability-distance q p max { core-distance ( q ) dist ( p q ) } q core object respect  minpt reachability-distance p q undefined object p may directly reachable multiple core object therefore p may multiple reachability-distance respect different core object smallest reachability-distance p particular interest give shortest path p connect dense cluster example 108 core-distance reachability-distance figure 1016 illustrate concept coredistance reachability-distance suppose  = 6 mm minpt = coredistance p distance  0 p fourth closest datum object p reachability-distance q1 p core-distance p ( ie  0 = 3 mm ) greater euclidean distance p q1 reachability-distance q2 respect p euclidean distance p q2 greater core-distance p optic compute order object give database object database store core-distance suitable reachability-distance optic maintain list call orderseed generate output order object orderseed sort reachability-distance respective closest core object smallest reachability-distance object optic begin arbitrary object input database current object p retrieve -neighborhood p determine core-distance set reachability-distance undefined current object p written output 
476 chapter 10 cluster analysis basic concept method = 6 mm p = 3 mm = 6 mm  p q1 q2 core-distance p reachability-distance ( p q1 ) = = 3 mm reachability-distance ( p q2 ) = dist ( p q2 ) figure 1016 optic terminology source base ankerst breunig kriegel sander [ abks99 ] p core object optic simply move next object orderseed list ( input database orderseed empty ) p core object object q -neighborhood p optic update reachability-distance p insert q orderseed q yet processed iteration continue input fully consume orderseed empty datum set ’ cluster order represent graphically help visualize understand cluster structure datum set example figure 1017 reachability plot simple 2-d datum set present general overview datum structure cluster datum object plot cluster order ( horizontal axis ) together respective reachability-distance ( vertical axis ) three gaussian “ bump ” plot reflect three cluster datum set method also develop view cluster structure high-dimensional datum various level detail structure optic algorithm similar dbscan consequently two algorithms time complexity complexity ( n log n ) spatial index used ( n2 ) otherwise n number object 1043 denclue cluster base density distribution function density estimation core issue density-based cluster method denclue ( density-based cluster ) cluster method base set density distribution function first give background density estimation describe denclue algorithm probability statistic density estimation estimation unobservable underlie probability density function base set observed datum context density-based cluster unobservable underlie probability density function true distribution population possible object analyze observed datum set regard random sample population 
104 density-based method 477 reachability-distance undefined cluster order object figure 1017 cluster order optic source adapt ankerst breunig kriegel sander [ abks99 ] 1 2 figure 1018 subtlety density estimation dbscan optic increase neighborhood radius slightly 1 2 result much higher density dbscan optic density calculate count number object neighborhood defined radius parameter  density estimate highly sensitive radius value used example figure 1018 density change significantly radius increase small amount overcome problem kernel density estimation used nonparametric density estimation approach statistic general idea behind kernel density estimation simple treat observed object indicator 
478 chapter 10 cluster analysis basic concept method high-probability density surround region probability density point depend distance point observed object formally let x1 xn independent identically distribute sample random variable f kernel density approximation probability density function   n x − xi 1 x ( 1021 ) k fˆh ( x ) = nh h i=1 k ( ) kernel h bandwidth serve smooth parameter kernel regard function modele influence sample point within neighborhood technically kernel k ( ) isra non-negative real-valu integrable func+∞ tion satisfy two requirement −∞ k ( u ) du = 1 k ( −u ) = k ( u ) value u frequently used kernel standard gaussian function mean 0 variance 1   x − xi 1 − ( x − 2xi ) 2 2h k ( 1022 ) √ e h 2π denclue used gaussian kernel estimate density base give set object cluster point x∗ call density attractor local maximum estimate density function avoid trivial local maximum point denclue used noise threshold ξ consider density attractor x∗ fˆ ( x∗ ) ≥ ξ nontrivial density attractor center cluster object analysis assign cluster density attractor used stepwise hill-climb procedure object x hill-climb procedure start x guide gradient estimate density function density attractor x compute x0 = x xj+1 = xj + δ ∇ fˆ ( xj ) ∇ fˆ ( xj ) | ( 1023 ) δ parameter control speed convergence ∇ fˆ ( x ) = hd+2 n 1   x − xi ( x − x ) k i=1 h pn ( 1024 ) hill-climb procedure stop step k > 0 fˆ ( xk+1 ) < fˆ ( xk ) assign x density attractor x∗ = xk object x outlier noise converge hillclimb procedure local maximum x∗ fˆ ( x∗ ) < ξ cluster denclue set density attractor x set input object c object c assign density attractor x exist path every pair density attractor density ξ used multiple density attractor connect path denclue find cluster arbitrary shape 
105 grid-based method 479 denclue several advantage regard generalization several well-known cluster method single-linkage approach dbscan moreover denclue invariant noise kernel density estimation effectively reduce influence noise uniformly distribute noise input datum 105 grid-based method cluster method discuss far data-driven—they partition set object adapt distribution object embedding space alternatively grid-based cluster method take space-driven approach partition embedding space cell independent distribution input object grid-based cluster approach used multiresolution grid datum structure quantize object space finite number cell form grid structure operation cluster perform main advantage approach fast process time typically independent number datum object yet dependent number cell dimension quantized space section illustrate grid-based cluster used two typical example sting ( section 1051 ) explore statistical information store grid cell clique ( section 1052 ) represent - density-based approach subspace cluster high-dimensional datum space 1051 sting statistical information grid sting grid-based multiresolution cluster technique embedding spatial area input object divide rectangular cell space divide hierarchical recursive way several level rectangular cell correspond different level resolution form hierarchical structure cell high level partition form number cell next lower level statistical information regard attribute grid cell mean maximum minimum value precompute store statistical parameter statistical parameter useful query process datum analysis task figure 1019 show hierarchical structure sting cluster statistical parameter higher-level cell easily compute parameter lower-level cell parameter include follow attribute-independent parameter count attribute-dependent parameter mean stdev ( standard deviation ) min ( minimum ) max ( maximum ) type distribution attribute value cell follow normal uniform exponential none ( distribution unknown ) attribute select measure analysis price house object datum load database parameter count mean stdev min max bottom-level cell calculate directly datum value distribution may either assign user distribution type know 
480 chapter 10 cluster analysis basic concept method first layer ( – 1 ) st layer ith layer figure 1019 hierarchical structure sting cluster beforehand obtain hypothesis test χ 2 test type distribution higher-level cell compute base majority distribution type corresponding lower-level cell conjunction threshold filter process distribution lower-level cell disagree fail threshold test distribution type high-level cell set none “ statistical information useful query answer ” statistical parameter used top-down grid-based manner follow first layer within hierarchical structure determine query-answer process start layer typically contain small number cell cell current layer compute confidence interval ( estimate probability range ) reflect cell ’ relevancy give query irrelevant cell remove consideration process next lower level examine remain relevant cell process repeat bottom layer reach time query specification meet region relevant cell satisfy query return otherwise datum fall relevant cell retrieve processed meet query ’ requirement interesting property sting approach cluster result dbscan granularity approach 0 ( ie toward low-level datum ) word used count cell size information dense cluster identify approximately used sting therefore sting also regard density-based cluster method “ advantage sting offer cluster method ” sting offer several advantage ( 1 ) grid-based computation query-independent statistical information store cell represent summary information datum grid cell independent query ( 2 ) grid structure facilitate parallel process incremental update ( 3 ) method ’ efficiency major advantage sting go database compute statistical parameter cell hence time complexity generate cluster ( n ) n total number object generate hierarchical structure query process time 
105 grid-based method 481 ( g ) g total number grid cell lowest level usually much smaller n sting used multiresolution approach cluster analysis quality sting cluster depend granularity lowest level grid structure granularity fine cost process increase substantially however bottom level grid structure coarse may reduce quality cluster analysis moreover sting consider spatial relationship child neighboring cell construction parent cell result shape result cluster isothetic cluster boundary either horizontal vertical diagonal boundary detected may lower quality accuracy cluster despite fast process time technique 1052 clique apriori-like subspace cluster method datum object often ten attribute many may irrelevant value attribute may vary considerably factor make difficult locate cluster span entire datum space may meaningful instead search cluster within different subspace datum example consider healthinformatic application patient record contain extensive attribute describe personal information numerous symptom condition family history find nontrivial group patient even attribute strongly agree unlikely bird flu patient instance age gender job attribute may vary dramatically within wide range value thus difficult find cluster within entire datum space instead search subspace may find cluster similar patient lower-dimensional space ( eg patient similar one respect symptom like high fever cough runny nose age 3 16 ) clique ( cluster quest ) simple grid-based method find densitybased cluster subspace clique partition dimension nonoverlapping interval thereby partition entire embedding space datum object cell used density threshold identify dense cell sparse one cell dense number object map exceed density threshold main strategy behind clique identify candidate search space used monotonicity dense cell respect dimensionality base apriori property used frequent pattern association rule mining ( chapter 6 ) context cluster subspace monotonicity say follow k-dimensional cell c ( k > 1 ) least l point every ( k − 1 ) dimensional projection c cell ( k − 1 ) dimensional subspace least l point consider figure 1020 embedding datum space contain three dimension age salary vacation 2-d cell say subspace form age salary contain l point projection cell every dimension age salary respectively contain least l point clique perform cluster two step first step clique partition d-dimensional datum space nonoverlapping rectangular unit identify dense unit among clique find dense cell subspace 
482 chapter 10 cluster analysis basic concept method 7 salary ( $ 10000 ) 6 5 4 3 2 1 0 20 30 40 50 60 age 30 40 50 60 age 7 vacation ( week ) 6 5 4 3 2 1 vacation 0 20 50 age sa la ry 30 figure 1020 dense unit find respect age dimension salary vacation intersected provide candidate search space dense unit higher dimensionality 
106 evaluation cluster 483 clique partition every dimension interval identify interval contain least l point l density threshold clique iteratively join two k-dimensional dense cell c1 c2 subspace ( di1 dik ) ( dj1 djk ) respectively di1 = dj1 dik−1 = djk−1 c1 c2 share interval dimension join operation generate new ( k + 1 ) dimensional candidate cell c space ( di1 dik−1 dik djk ) clique check whether number point c pass density threshold iteration terminate candidate generate candidate cell dense second step clique used dense cell subspace assemble cluster arbitrary shape idea apply minimum description length ( mdl ) principle ( chapter 8 ) use maximal region cover connect dense cell maximal region hyperrectangle every cell fall region dense region extend dimension subspace find best description cluster general np-hard thus clique adopt simple greedy approach start arbitrary dense cell find maximal region cover cell work remain dense cell yet cover greedy method terminate dense cell cover “ effective clique ” clique automatically find subspace highest dimensionality high-density cluster exist subspace insensitive order input object presume canonical datum distribution scale linearly size input good scalability number dimension datum increase however obtain meaningful cluster dependent proper tune grid size ( stable structure ) density threshold difficult practice grid size density threshold used across combination dimension datum set thus accuracy cluster result may degraded expense method ’ simplicity moreover give dense region projection region onto lower-dimensionality subspace also dense result large overlap among report dense region furthermore difficult find cluster rather different density within different dimensional subspace several extension approach follow similar philosophy example think grid set fix bin instead used fix bin dimension use adaptive data-driven strategy dynamically determine bin dimension base datum distribution statistic alternatively instead used density threshold may use entropy ( chapter 8 ) measure quality subspace cluster 106 evaluation cluster learn cluster know several popular cluster method may ask “ try cluster method datum set evaluate whether cluster result good ” general cluster evaluation assess 
484 chapter 10 cluster analysis basic concept method feasibility cluster analysis datum set quality result generate cluster method major task cluster evaluation include follow assess cluster tendency task give datum set assess whether nonrandom structure exist datum blindly apply cluster method datum set return cluster however cluster mine may mislead cluster analysis datum set meaningful nonrandom structure datum determine number cluster datum set algorithms k-mean require number cluster datum set parameter moreover number cluster regard interesting important summary statistic datum set therefore desirable estimate number even cluster algorithm used derive detailed cluster measure cluster quality apply cluster method datum set want assess good result cluster number measure used method measure well cluster fit datum set other measure well cluster match ground truth truth available also measure score clustering thus compare two set cluster result datum set rest section discuss three topic 1061 assess cluster tendency cluster tendency assessment determine whether give datum set non-random structure may lead meaningful cluster consider datum set non-random structure set uniformly distribute point datum space even though cluster algorithm may return cluster datum cluster random meaningful example 109 cluster require nonuniform distribution datum figure 1021 show datum set uniformly distribute 2-d datum space although cluster algorithm may still artificially partition point group group unlikely mean anything significant application due uniform distribution datum “ assess cluster tendency datum set ” intuitively try measure probability datum set generate uniform datum distribution achieve used statistical test spatial randomness illustrate idea let ’ look simple yet effective statistic call hopkin statistic hopkin statistic spatial statistic test spatial randomness variable distribute space give datum set regard sample 
106 evaluation cluster 485 figure 1021 datum set uniformly distribute datum space random variable want determine far away uniformly distribute datum space calculate hopkin statistic follow sample n point p1 pn uniformly d point probability include sample point pi find nearest neighbor pi ( 1 ≤ ≤ n ) let xi distance pi nearest neighbor d xi = min { dist ( pi v ) } v∈d ( 1025 ) sample n point q1 qn uniformly d qi ( 1 ≤ ≤ n ) find nearest neighbor qi − { qi } let yi distance qi nearest neighbor − { qi } yi = min { dist ( qi v ) } v∈d v6=qi ( 1026 ) calculate hopkin statistic h pn h = pn i=1 xi i=1 yi + pn i=1 yi ( 1027 ) “ hopkin statistic tell us likely datum set follow pn uniform distribution datum space ” uniformly distribute i=1 yi pn x would close thus h would 05 however i=1 p p highly skewer ni=1 yi would substantially smaller ni=1 xi expectation thus h would close 0 
486 chapter 10 cluster analysis basic concept method null hypothesis homogeneous hypothesis—that uniformly distribute thus contain meaningful cluster nonhomogeneous hypothesis ( ie uniformly distribute thus contain cluster ) alternative hypothesis conduct hopkin statistic test iteratively used 05 threshold reject alternative hypothesis h > 05 unlikely statistically significant cluster 1062 determine number cluster determine “ right ” number cluster datum set important cluster algorithms like k-mean require parameter also appropriate number cluster control proper granularity cluster analysis regard find good balance compressibility accuracy cluster analysis consider two extreme case treat entire datum set cluster would maximize compression datum cluster analysis value hand treat object datum set cluster give finest cluster resolution ( ie accurate due zero distance object corresponding cluster center ) method like k-mean even achieve best cost however one object per cluster enable datum summarization determine number cluster far easy often “ right ” number ambiguous figure right number cluster often depend distribution ’ shape scale datum set well cluster resolution require user many possible way estimate number cluster briefly introduce simple yet popular effective method q simple method set number cluster n2 datum set n √ point expectation cluster 2n point elbow method base observation increase number cluster help reduce sum within-cluster variance cluster cluster allow one capture finer group datum object similar however marginal effect reduce sum within-cluster variance may drop many cluster form splitting cohesive cluster two give small reduction consequently heuristic select right number cluster use turn point curve sum within-cluster variance respect number cluster technically give number k > 0 form k cluster datum set question used cluster algorithm like k-mean calculate sum within-cluster variance var ( k ) plot curve var respect k first ( significant ) turn point curve suggest “ right ” number advanced method determine number cluster used information criterium information theoretic approach please refer bibliographic note information ( section 109 ) 
106 evaluation cluster 487 “ right ” number cluster datum set also determine crossvalidation technique often used classification ( chapter 8 ) first divide give datum set part next use − 1 part build cluster model use remain part test quality cluster example point test set find closest centroid consequently use sum square distance point test set closest centroid measure well cluster model fit test set integer k > 0 repeat process time derive clustering k cluster used part turn test set average quality measure take overall quality measure compare overall quality measure respect different value k find number cluster best fit datum 1063 measure cluster quality suppose assessed cluster tendency give datum set may also try predetermine number cluster set apply one multiple cluster method obtain clustering datum set “ good cluster generate method compare clustering generate different method ” method choose measure quality cluster general method categorize two group accord whether ground truth available ground truth ideal cluster often build used human expert ground truth available used extrinsic method compare cluster group truth measure ground truth unavailable use intrinsic method evaluate goodness cluster consider well cluster separated ground truth consider supervision form “ cluster ” hence extrinsic method also know supervised method intrinsic method unsupervised method let ’ look simple method category extrinsic method ground truth available compare cluster assess cluster thus core task extrinsic method assign score q ( c cg ) cluster c give ground truth cg whether extrinsic method effective largely depend measure q used general measure q cluster quality effective satisfy follow four essential criterium cluster homogeneity require pure cluster cluster better cluster suppose ground truth say object datum set belong category l1 ln consider cluster c1 wherein cluster c ∈ c1 contain object two category li lj ( 1 ≤ < j ≤ n ) also 
488 chapter 10 cluster analysis basic concept method consider cluster c2 identical c1 except c2 split two cluster contain object li lj respectively cluster quality measure q respect cluster homogeneity give higher score c2 c1 q ( c2 cg ) > q ( c1 cg ) cluster completeness counterpart cluster homogeneity cluster completeness require cluster two object belong category accord ground truth assign cluster cluster completeness require cluster assign object belong category ( accord ground truth ) cluster consider cluster c1 contain cluster c1 c2 member belong category accord ground truth let cluster c2 identical c1 except c1 c2 merged one cluster c2 cluster quality measure q respect cluster completeness give higher score c2 q ( c2 cg ) > q ( c1 cg ) rag bag many practical scenario often “ rag bag ” category contain object merged object category often call “ miscellaneous ” “ ” rag bag criterion state putt heterogeneous object pure cluster penalize putt rag bag consider cluster c1 cluster c ∈ c1 object c except one denote belong category accord ground truth consider cluster c2 identical c1 except assign cluster c 0 = c c2 c 0 contain object various category accord ground truth thus noisy word c 0 c2 rag bag cluster quality measure q respect rag bag criterion give higher score c2 q ( c2 cg ) > q ( c1 cg ) small cluster preservation small category split small piece cluster small piece may likely become noise thus small category discover cluster small cluster preservation criterion state splitting small category piece harmful splitting large category piece consider extreme case let datum set n + 2 object accord ground truth n object denote o1 belong one category two object denote on+1 on+2 belong another category suppose cluster c1 three cluster c1 = { o1 } c2 = { on+1 } c3 = { on+2 } let cluster c2 three cluster namely c1 = { o1 on−1 } c2 = { } c3 = { on+1 on+2 } word c1 split small category c2 split big category cluster quality measure q preserve small cluster give higher score c2 q ( c2 cg ) > q ( c1 cg ) many cluster quality measure satisfy four criterium introduce bcube precision recall metric satisfy four criterium bcube evaluate precision recall every object cluster give datum set accord ground truth precision object indicate many object cluster belong category object recall 
106 evaluation cluster 489 object reflect many object category assign cluster formally let = { o1 } set object c cluster d let l ( oi ) ( 1 ≤ ≤ n ) category oi give ground truth c ( oi ) cluster id oi c two object oi oj ( 1 ≤ j ≤ n = j ) correctness relation oi oj cluster c give ( 1 l ( oi ) = l ( oj ) ⇔ c ( oi ) = c ( oj ) correctness ( oi oj ) = 0 otherwise ( 1028 ) bcube precision defined x n x oj i6=j c ( oi ) c ( oj ) precision bcube = correctness ( oi oj ) k { oj i = j c ( oi ) = c ( oj ) } k i=1 n ( 1029 ) bcube recall defined x n x oj i6=j l ( oi ) l ( oj ) recall bcube = i=1 correctness ( oi oj ) k { oj i = j l ( oi ) = l ( oj ) } k n ( 1030 ) intrinsic method ground truth datum set available use intrinsic method assess cluster quality general intrinsic method evaluate cluster examine well cluster separated compact cluster many intrinsic method advantage similarity metric object datum set silhouette coefficient measure datum set n object suppose partition k cluster c1 ck object ∈ calculate ( ) average distance object cluster belong similarly b ( ) minimum average distance cluster belong formally suppose ∈ ci ( 1 ≤ ≤ k ) p ( ) = o0 ∈ci o6=o0 dist ( ) ci | − 1 0 ( 1031 ) 
490 chapter 10 cluster analysis basic concept method ( p b ( ) = min cj 1≤j≤k j6=i 0 ) o0 ∈cj dist ( ) cj | ( 1032 ) silhouette coefficient defined ( ) = b ( ) − ( ) max { ( ) b ( ) } ( 1033 ) value silhouette coefficient −1 value ( ) reflect compactness cluster belong smaller value compact cluster value b ( ) capture degree separated cluster larger b ( ) separated cluster therefore silhouette coefficient value approach 1 cluster contain compact far away cluster preferable case however silhouette coefficient value negative ( ie b ( ) < ( ) ) mean expectation closer object another cluster object cluster many case bad situation avoid measure cluster ’ fitness within cluster compute average silhouette coefficient value object cluster measure quality cluster use average silhouette coefficient value object datum set silhouette coefficient intrinsic measure also used elbow method heuristically derive number cluster datum set replace sum within-cluster variance 107 summary cluster collection datum object similar one another within cluster dissimilar object cluster process grouping set physical abstract object class similar object call cluster cluster analysis extensive application include business intelligence image pattern recognition web search biology security cluster analysis used standalone datum mining tool gain insight datum distribution preprocess step datum mining algorithms operate detected cluster cluster dynamic field research datum mining related unsupervised learn machine learn cluster challenge field typical requirement include scalability ability deal different type datum attribute discovery cluster arbitrary shape minimal requirement domain knowledge determine input parameter ability deal noisy datum incremental cluster 
108 exercise 491 insensitivity input order capability cluster high-dimensionality datum constraint-based cluster well interpretability usability many cluster algorithms develop categorize several orthogonal aspect regard partition criterium separation cluster similarity measure used cluster space chapter discuss major fundamental cluster method follow category partition method hierarchical method density-based method grid-based method algorithms may belong one category partition method first create initial set k partition parameter k number partition construct used iterative relocation technique attempt improve partition move object one group another typical partition method include k-mean k-medoid claran hierarchical method create hierarchical decomposition give set datum object method classify either agglomerative ( bottom-up ) divisive ( top-down ) base hierarchical decomposition form compensate rigidity merge split quality hierarchical agglomeration improve analyze object linkage hierarchical partition ( eg chameleon ) first perform microcluster ( grouping object “ microcluster ” ) operate microcluster cluster technique iterative relocation ( birch ) density-based method cluster object base notion density grow cluster either accord density neighborhood object ( eg dbscan ) accord density function ( eg denclue ) optic density-based method generate augment order datum ’ cluster structure grid-based method first quantize object space finite number cell form grid structure perform cluster grid structure sting typical example grid-based method base statistical information store grid cell clique grid-based subspace cluster algorithm cluster evaluation assess feasibility cluster analysis datum set quality result generate cluster method task include assess cluster tendency determine number cluster measure cluster quality 108 exercise 101 briefly describe give example follow approach cluster partition method hierarchical method density-based method grid-based method 
492 chapter 10 cluster analysis basic concept method 102 suppose datum mining task cluster point ( ( x ) represent location ) three cluster point a1 ( 2 10 ) a2 ( 2 5 ) a3 ( 8 4 ) b1 ( 5 8 ) b2 ( 7 5 ) b3 ( 6 4 ) c1 ( 1 2 ) c2 ( 4 9 ) distance function euclidean distance suppose initially assign a1 b1 c1 center cluster respectively use k-mean algorithm show ( ) three cluster center first round execution ( b ) final three cluster 103 use example show k-mean algorithm may find global optimum optimize within-cluster variation 104 k-mean algorithm interesting note choose initial cluster center carefully may able speed algorithm ’ convergence also guarantee quality final cluster + algorithm variant k-mean choose initial center follow first select one center uniformly random object datum set iteratively object p choose center choose object new center object choose random probability proportional dist ( p ) 2 dist ( p ) distance p closest center already choose iteration continue k center select explain method speed convergence k-mean algorithm also guarantee quality final cluster result 105 provide pseudocode object reassignment step pam algorithm 106 k-mean k-medoid algorithms perform effective cluster ( ) illustrate strength weakness k-mean comparison k-medoid ( b ) illustrate strength weakness scheme comparison hierarchical cluster scheme ( eg agne ) 107 prove dbscan density-connectedness equivalence relation 108 prove dbscan fix minpt value two neighborhood threshold 1 < 2 cluster c respect 1 minpt must subset cluster c 0 respect 2 minpt 109 provide pseudocode optic algorithm 1010 birch encounter difficulty find cluster arbitrary shape optic propose modification birch help find cluster arbitrary shape 1011 provide pseudocode step clique find dense cell subspace 
108 exercise 493 1012 present condition density-based cluster suitable partitioning-based cluster hierarchical cluster give application example support argument 1013 give example specific cluster method integrate example one cluster algorithm used preprocess step another addition provide reasoning integration two method may sometimes lead improve cluster quality efficiency 1014 cluster recognize important datum mining task broad application give one application example follow case ( ) application used cluster major datum mining function ( b ) application used cluster preprocess tool datum preparation datum mining task 1015 datum cube multidimensional databasis contain nominal ordinal numeric datum hierarchical aggregate form base learn cluster method design cluster method find cluster large datum cube effectively efficiently 1016 describe follow cluster algorithms term follow criterium ( 1 ) shape cluster determine ( 2 ) input parameter must specify ( 3 ) limitation ( ) ( b ) ( c ) ( ) ( e ) ( f ) k-mean k-medoid clara birch chameleon dbscan 1017 human eye fast effective judge quality cluster method 2-d datum design datum visualization method may help human visualize datum cluster judge cluster quality 3-d datum even higher-dimensional datum 1018 suppose allocate number automatic teller machine ( atms ) give region satisfy number constraint household workplace may cluster typically one atm assign per cluster cluster however may constrain two factor ( 1 ) obstacle object ( ie bridge river highway affect atm accessibility ) ( 2 ) additional user-specified constraint atm serve least 10000 household cluster algorithm k-mean modify quality cluster constraint 1019 constraint-based cluster aside minimum number customer cluster ( atm allocation ) constraint many kind 
494 chapter 10 cluster analysis basic concept method constraint example constraint can form maximum number customer per cluster average income customer per cluster maximum distance every two cluster categorize kind constraint impose cluster produce discuss perform cluster efficiently kind constraint 1020 design privacy-preserve cluster method datum owner would able ask third party mine datum quality cluster without worry potential inappropriate disclosure certain private sensitive information store datum 1021 show bcube metric satisfy four essential requirement extrinsic cluster evaluation method 109 bibliographic note cluster extensively study 40 year across many discipline due broad application book pattern classification machine learn contain chapter cluster analysis unsupervised learn several textbook dedicate method cluster analysis include hartigan [ har75 ] jain dube [ jd88 ] kaufman rousseeuw [ kr90 ] arabie hubert de sorte [ ahs96 ] also many survey article different aspect cluster method recent one include jain murty flynn [ jmf99 ] parson haque liu [ phl04 ] jain [ jai10 ] partition method k-mean algorithm first introduce lloyd [ llo57 ] macqueen [ mac67 ] arthur vassilvitskii [ av07 ] present + algorithm filter algorithm used spatial hierarchical datum index speed computation cluster mean give kanungo mount netanyahu et al [ + 02 ] k-medoid algorithms pam clara propose kaufman rousseeuw [ kr90 ] k-mode ( cluster nominal datum ) k-prototype ( cluster hybrid datum ) algorithms propose huang [ hua98 ] k-mode cluster algorithm also propose independently chaturvedi green carroll [ cgc94 cgc01 ] claran algorithm propose ng han [ nh94 ] ester kriegel xu [ ekx95 ] propose technique improvement performance claran used efficient spatial access method r∗-tree focuse technique k-means-based scalable cluster algorithm propose bradley fayyad reina [ bfr98 ] early survey agglomerative hierarchical cluster algorithms conduct day edelsbrunner [ de84 ] agglomerative hierarchical cluster agne divisive hierarchical cluster diana introduce kaufman rousseeuw [ kr90 ] interesting direction improve cluster quality hierarchical cluster method integrate hierarchical cluster distance-based iterative relocation nonhierarchical cluster method example birch zhang ramakrishnan livny [ zrl96 ] first perform hierarchical cluster 
109 bibliographic note 495 cf-tree apply technique hierarchical cluster also perform sophisticated linkage analysis transformation nearest-neighbor analysis cure guha rastogi shim [ grs98 ] rock ( cluster nominal attribute ) guha rastogi shim [ grs99 ] chameleon karypis han kumar [ khk99 ] probabilistic hierarchical cluster framework follow normal linkage algorithms used probabilistic model define cluster similarity develop friedman [ fri03 ] heller ghahramani [ hg05 ] density-based cluster method dbscan propose ester kriegel sander xu [ eksx96 ] ankerst breunig kriegel sander [ abks99 ] develop optic cluster-order method facilitate density-based cluster without worry parameter specification denclue algorithm base set density distribution function propose hinneburg keim [ hk98 ] hinneburg gabriel [ hg07 ] develop denclue 20 include new hill-climb procedure gaussian kernel adjust step size automatically sting grid-based multiresolution approach collect statistical information grid cell propose wang yang muntz [ wym97 ] wavecluster develop sheikholeslami chatterjee zhang [ scz98 ] multiresolution cluster approach transform original feature space wavelet transform scalable method cluster nominal datum study gibson kleinberg raghavan [ gkr98 ] guha rastogi shim [ grs99 ] ganti gehrke ramakrishnan [ ggr99 ] also many cluster paradigm example fuzzy cluster method discuss kaufman rousseeuw [ kr90 ] bezdek [ bez81 ] bezdek pal [ bp92 ] high-dimensional cluster apriori-based dimension-growth subspace cluster algorithm call clique propose agrawal gehrke gunopulos raghavan [ aggr98 ] integrate density-based grid-based cluster method recent study proceed cluster stream datum babcock badu datar et al [ + 02 ] k-median-based datum stream cluster algorithm propose guha mishra motwani ’ callaghan [ gmmo00 ] ’ callaghan et al [ + 02 ] method cluster evolve datum stream propose aggarwal han wang yu [ ahwy03 ] framework project cluster high-dimensional datum stream propose aggarwal han wang yu [ ahwy04a ] cluster evaluation discuss monograph survey article jain dube [ jd88 ] halkidi batistakis vazirgiannis [ hbv01 ] extrinsic method cluster quality evaluation extensively explore recent study include meilǎ [ mei03 mei05 ] amigó gonzalo artile verdejo [ agav09 ] four essential criterium introduce chapter formulate amigó gonzalo artile verdejo [ agav09 ] individual criterium also mentioned earlier example meilǎ [ mei03 ] rosenberg hirschberg [ rh07 ] bagga baldwin [ bb98 ] introduce bcube metric silhouette coefficient describe kaufman rousseeuw [ kr90 ] 
11 advanced cluster analysis learn fundamental cluster analysis chapter chapter discuss advanced topic cluster analysis specifically investigate four major perspective probabilistic model-based cluster section 111 introduce general framework method derive cluster object assign probability belong cluster probabilistic model-based cluster widely used many datum mining application text mining cluster high-dimensional datum dimensionality high conventional distance measure dominate noise section 112 introduce fundamental method cluster analysis high-dimensional datum cluster graph network datum graph network datum increasingly popular application online social network world wide web digital library section 113 study key issue cluster graph network datum include similarity measurement cluster method cluster constraint discussion far assume constraint cluster application however various constraint may exist constraint may rise background knowledge spatial distribution object learn conduct cluster analysis different kind constraint section 114 end chapter good grasp issue technique regard advanced cluster analysis 111 probabilistic model-based cluster cluster analysis method discuss far datum object assign one number cluster cluster assignment rule require application assign customer marketing manager however datum mining concept technique doi b978-0-12-381479-100011-3 c 2012 elsevier right re-serve 497 
498 chapter 11 advanced cluster analysis application rigid requirement may desirable section demonstrate need fuzzy flexible cluster assignment application introduce general method compute probabilistic cluster assignment “ situation may datum object belong one cluster ” consider example 111 example 111 cluster product reviews allelectronic online store customer purchase online also create reviews product every product receive reviews instead product may many reviews many other none moreover review may involve multiple product thus review editor allelectronic task cluster reviews ideally cluster topic example group product service issue highly related assign review one cluster exclusively would work well task suppose cluster “ camera camcorder ” another “ ” review talk compatibility camcorder computer review relate cluster however exclusively belong either cluster would like use cluster method allow review belong one cluster review indeed involve one topic reflect strength review belong cluster want assignment review cluster carry weight represent partial membership scenario object may belong multiple cluster occur often many application illustrated example 112 example 112 cluster study user search intent allelectronic online store record customer browse purchasing behavior log important datum mining task use log datum categorize understand user search intent example consider user session ( short period user interact online store ) user search product make comparison among different product look customer support information cluster analysis help difficult predefine user behavior pattern thoroughly cluster contain similar user browse trajectory may represent similar user behavior however every session belong one cluster example suppose user session involve purchase digital camera form one cluster user session compare laptop computer form another cluster user one session make order digital camera time compare several laptop computer session belong cluster extent section systematically study theme cluster allow object belong one cluster start notion fuzzy cluster section generalize concept probabilistic model-based cluster section section 1113 introduce expectation-maximization algorithm general framework mining cluster 
111 probabilistic model-based cluster 499 1111 fuzzy cluster give set object x = { x1 xn } fuzzy set subset x allow object x membership degree 0 formally fuzzy set modeled function fs x → [ 0 1 ] example 113 fuzzy set digital camera unit sell popular camera allelectronic use follow formula compute degree popularity digital camera give sale pop ( ) = ( 1 1000 1000 unit sell ( < 1000 ) unit sell ( 111 ) function pop ( ) define fuzzy set popular digital camera example suppose sale digital camera allelectronic show table fuzzy set popular digital camera { ( 005 ) b ( 1 ) c ( 086 ) ( 027 ) } degree membership written parenthesis apply fuzzy set idea cluster give set object cluster fuzzy set object cluster call fuzzy cluster consequently cluster contain multiple fuzzy cluster formally give set object o1 fuzzy cluster k fuzzy cluster c1 ck represent used partition matrix = [ wij ] ( 1 ≤ ≤ n 1 ≤ j ≤ k ) wij membership degree oi fuzzy cluster cj partition matrix satisfy follow three requirement object oi cluster cj 0 ≤ wij ≤ requirement enforce fuzzy cluster fuzzy set object oi k x wij = requirement ensure every object - j=1 pate cluster equivalently table 111 set digital camera sale allelectronic camera sale ( unit ) b c 50 1320 860 270 
500 chapter 11 advanced cluster analysis cluster cj 0 < n x wij < n requirement ensure every cluster i=1 least one object membership value nonzero example 114 fuzzy cluster suppose allelectronic online store six reviews keyword contain reviews list table 112 group reviews two fuzzy cluster c1 c2 c1 “ digital camera ” “ lens ” c2 “ ” partition matrix  1 1  1  = 2 3  0 0  0 0  0  1  3 1 1 use keyword “ digital camera ” “ lens ” feature cluster c1 “ computer ” feature cluster c2 review ri cluster cj ( 1 ≤ ≤ 6 1 ≤ j ≤ 2 ) wij defined wij = ri ∩ cj | ri ∩ cj | = ri ∩ ( c1 ∪ c2 ) | ri ∩ { digital camera lens computer } | fuzzy cluster review r4 belong cluster c1 c2 membership degree 23 31 respectively “ evaluate well fuzzy cluster describe datum set ” consider set object o1 fuzzy cluster c k cluster c1 ck let = [ wij ] ( 1 ≤ ≤ n 1 ≤ j ≤ k ) partition matrix let c1 ck center cluster c1 ck respectively center defined either mean medoid way specific application discuss chapter 10 distance similarity object center cluster object assign used measure well table 112 set reviews keyword used review id keyword r1 r2 r3 r4 r5 r6 digital camera lens digital camera lens digital camera lens computer computer cpu computer computer game 
111 probabilistic model-based cluster 501 object belong cluster idea extend fuzzy cluster object oi cluster cj wij > 0 dist ( oi cj ) measure well oi represent cj thus belong cluster cj object participate one cluster sum distance corresponding cluster center weight degree membership capture well object fit cluster formally object oi sum square error ( sse ) give sse ( oi ) = k x p wij dist ( oi cj ) 2 ( 112 ) j=1 parameter p ( p ≥ 1 ) control influence degree membership larger value p larger influence degree membership orthogonally sse cluster cj sse ( cj ) = n x p wij dist ( oi cj ) 2 ( 113 ) i=1 finally sse cluster defined sse ( c ) = n x k x p wij dist ( oi cj ) 2 ( 114 ) i=1 j=1 sse used measure well fuzzy cluster fit datum set fuzzy cluster also call soft cluster allow object belong one cluster easy see traditional ( rigid ) cluster enforce object belong one cluster exclusively special case fuzzy cluster defer discussion compute fuzzy cluster section 1113 1112 probabilistic model-based cluster “ fuzzy cluster ( section 1111 ) provide flexibility allow object participate multiple cluster general framework specify clustering object may participate multiple cluster probabilistic way ” section introduce general notion probabilistic model-based cluster answer question discuss chapter 10 conduct cluster analysis datum set assume object datum set fact belong different inherent category recall cluster tendency analysis ( section 1061 ) used examine whether datum set contain object may lead meaningful cluster inherent category hide datum latent mean directly observed instead infer used datum observed example topic hide set reviews allelectronic online store latent one read topic directly however topic infer reviews review one multiple topic 
502 chapter 11 advanced cluster analysis therefore goal cluster analysis find hide category datum set subject cluster analysis regard sample possible instance hide category without category label cluster derive cluster analysis infer used datum set design approach hide category statistically assume hide category distribution datum space mathematically represent used probability density function ( distribution function ) call hide category probabilistic cluster probabilistic cluster c probability density function f point datum space f ( ) relative likelihood instance c appear example 115 probabilistic cluster suppose digital camera sell allelectronic divide two category c1 consumer line ( eg point-and-shoot camera ) c2 professional line ( eg single-len reflex camera ) respective probability density function f1 f2 show figure 111 respect attribute price price value say $ 1000 f1 ( 1000 ) relative likelihood price consumer-line camera $ 1000 similarly f2 ( 1000 ) relative likelihood price professional-line camera $ 1000 probability density function f1 f2 observed directly instead allelectronic infer distribution analyze price digital camera sell moreover camera often come well-determine category ( eg “ consumer line ” “ professional line ” ) instead category typically base user background knowledge vary example camera prosumer segment may regard high end consumer line customer low end professional line other analyst allelectronic consider category probabilistic cluster conduct cluster analysis price camera approach category probability consumer line professional line price 1000 figure 111 probability density function two probabilistic cluster 
111 probabilistic model-based cluster 503 suppose want find k probabilistic cluster c1 ck cluster analysis datum set n object regard finite sample possible instance cluster conceptually assume form follow cluster cj ( 1 ≤ j ≤ k ) associate probability ωj instance sample cluster often assume ω1 ωk give part problem set p kj=1 ωj = 1 ensure object generate k cluster parameter ωj capture background knowledge relative population cluster cj run follow two step generate object d step execute n time total generate n object o1 choose cluster cj accord probability ω1 ωk choose instance cj accord probability density function fj datum generation process basic assumption mixture model formally mixture model assume set observed object mixture instance multiple probabilistic cluster conceptually observed object generate independently two step first choose probabilistic cluster accord probability cluster choose sample accord probability density function choose cluster give datum set k number cluster require task probabilistic model-based cluster analysis infer set k probabilistic cluster likely generate used datum generation process important question remain measure likelihood set k probabilistic cluster probability generate observed datum set consider set c k probabilistic cluster c1 ck probability density function f1 fk respectively probability ω1 ωk object probability generate cluster cj ( 1 ≤ j ≤ k ) give p ( o|cj ) = ωj fj ( ) therefore probability generate set c cluster p ( o|c ) = k x ωj fj ( ) ( 115 ) j=1 since object assume generate independently datum set = { o1 } n object p ( d|c ) = n i=1 p ( oi c ) = k n x ωj fj ( oi ) ( 116 ) i=1 j=1 clear task probabilistic model-based cluster analysis datum set find set c k probabilistic cluster p ( d|c ) maximize maximize p ( d|c ) often intractable general probability density function 
504 chapter 11 advanced cluster analysis cluster take arbitrarily complicate form make probabilistic model-based cluster computationally feasible often compromise assume probability density function parameterized distribution formally let o1 n observed object 21 2k parameter k distribution denote = { o1 } 2 = { 21 2k } respectively object oi ∈ ( 1 ≤ ≤ n ) eq ( 115 ) rewrite p ( oi 2 ) = k x ωj pj ( oi 2j ) ( 117 ) j=1 pj ( oi 2j ) probability oi generate jth distribution used parameter 2j consequently eq ( 116 ) rewrite p ( o|2 ) = n x k ωj pj ( oi 2j ) ( 118 ) i=1 j=1 used parameterized probability distribution model task probabilistic model-based cluster analysis infer set parameter 2 maximize eq ( 118 ) example 116 univariate gaussian mixture model let ’ use univariate gaussian distribution example assume probability density function cluster follow 1-d gaussian distribution suppose k cluster two parameter probability density function cluster center µj standard deviation σj ( 1 ≤ j ≤ k ) denote parameter 2j = ( µj σj ) 2 = { 21 2k } let datum set = { o1 } oi ( 1 ≤ ≤ n ) real number point oi ∈ 1 e p ( oi 2j ) = √ 2π σj − ( oi −µj ) 2 2σ 2 ( 119 ) assume cluster probability ω1 = ω2 = · · · = ωk = k1 plug eq ( 119 ) eq ( 117 ) k 2 ( oi −µj ) 1x 1 − p ( oi 2 ) = e 2σ 2 √ k 2π σj ( 1110 ) j=1 apply eq ( 118 ) n p ( o|2 ) = k 2 ( oi −µj ) 1 yx 1 − e 2σ 2 √ k 2π σj ( 1111 ) i=1 j=1 task probabilistic model-based cluster analysis used univariate gaussian mixture model infer 2 eq ( 1111 ) maximize 
111 probabilistic model-based cluster 505 1113 expectation-maximization algorithm “ compute fuzzy clustering probabilistic model-based clustering ” section introduce principled approach let ’ start review k-mean cluster problem k-mean algorithm study chapter 10 easily show k-mean cluster special case fuzzy cluster ( exercise 111 ) k-mean algorithm iterate cluster improve iteration consist two step expectation step ( e-step ) give current cluster center object assign cluster center closest object object expect belong closest cluster maximization step ( m-step ) give cluster assignment cluster algorithm adjust center sum distance object assign cluster new center minimize similarity object assign cluster maximize generalize two-step method tackle fuzzy cluster probabilistic model-based cluster general expectation-maximization ( em ) algorithm framework approach maximum likelihood maximum posteriori estimate parameter statistical model context fuzzy probabilistic model-based cluster em algorithm start initial set parameter iterate cluster improve cluster converge change sufficiently small ( less preset threshold ) iteration also consist two step expectation step assign object cluster accord current fuzzy cluster parameter probabilistic cluster maximization step find new cluster parameter maximize sse fuzzy cluster ( eq 114 ) expect likelihood probabilistic model-based cluster example 117 fuzzy cluster used em algorithm consider six point figure 112 coordinate point also show let ’ compute two fuzzy cluster used em algorithm randomly select two point say c1 = c2 = b initial center two cluster first iteration conduct expectation step maximization step follow e-step point calculate membership degree cluster point assign c1 c2 membership weight 1 dist ( c1 ) 2 1 1 + 2 dist ( c1 ) dist ( c2 ) 2 = dist ( c2 ) 2 dist ( c1 ) 2 dist ( c1 ) 2 + dist ( c2 ) 2 dist ( c1 ) 2 + dist ( c2 ) 2 
506 chapter 11 advanced cluster analysis e ( 18 11 ) b ( 4 10 ) ( 14 8 ) c ( 9 6 ) f ( 21 7 ) ( 3 3 ) x figure 112 datum set fuzzy cluster table 113 intermediate result first three iteration example 117 ’ em algorithm iteration 1 2 3 e-step ` 1 0 = 0 1 ` 073 mt = 027 ` 080 mt = 020 048 052 042 058 m-step 041 059 # 047 053 049 051 091 009 026 074 033 067 076 024 099 001 002 098 014 086 c1 = ( 847 512 ) c2 = ( 1042 899 ) # 042 058 # 023 077 c1 = ( 851 611 ) c2 = ( 1442 869 ) c1 = ( 640 624 ) c2 = ( 1655 864 ) respectively dist ( ) euclidean distance rationale close c1 dist ( c1 ) small membership degree respect c1 high also normalize membership degree sum degree object equal 1 point wa c1 = 1 wa c2 = exclusively belong c1 41 = 048 point b wb c1 = 0 wb c2 = point c wc c1 = 45+41 45 wc c2 = 45+41 = degree membership point show partition matrix table 113 m-step recalculate centroid accord partition matrix minimize sse give eq ( 114 ) new centroid adjust x 2 wo c j point cj = ( 1112 ) x 2 wo c j point j = 1 2 
111 probabilistic model-based cluster 507 example 12 × 3 + 02 × 4 + 0482 × 9 + 0422 × 14 + 0412 × 18 + 0472 × 21 12 + 02 + 0482 + 0422 + 0412 + 0472  12 × 3 + 02 × 10 + 0482 × 6 + 0422 × 8 + 0412 × 11 + 0472 × 7 12 + 02 + 0482 + 0422 + 0412 + 0472  c1 = = ( 847 512 )  c2 = 02 × 3 + 12 × 4 + 0522 × 9 + 0582 × 14 + 0592 × 18 + 0532 × 21 02 + 12 + 0522 + 0582 + 0592 + 0532  02 × 3 + 12 × 10 + 0522 × 6 + 0582 × 8 + 0592 × 11 + 0532 × 7 02 + 12 + 0522 + 0582 + 0592 + 0532 = ( 1042 899 ) repeat iteration iteration contain e-step m-step table 113 show result first three iteration algorithm stop cluster center converge change small enough “ apply em algorithm compute probabilistic model-based cluster ” let ’ use univariate gaussian mixture model ( example 116 ) illustrate example 118 used em algorithm mixture model give set object = { o1 } want mine set parameter 2 = { 21 2k } p ( o|2 ) eq ( 1111 ) maximize 2j = ( µj σj ) mean standard deviation respectively jth univariate gaussian distribution ( 1 ≤ j ≤ k ) apply em algorithm assign random value parameter 2 initial value iteratively conduct e-step m-step follow parameter converge change sufficiently small e-step object oi ∈ ( 1 ≤ ≤ n ) calculate probability oi belong distribution p ( oi 2j ) p ( 2j oi 2 ) = pk l=1 p ( oi 2l ) ( 1113 ) m-step adjust parameter 2 expect likelihood p ( o|2 ) eq ( 1111 ) maximize achieve set pn n p ( 2j oi 2 ) 1x 1 i=1 oi p ( 2j oi 2 ) µj = oi pn = pn k k l=1 p ( 2j ol 2 ) i=1 p ( 2j oi 2 ) i=1 ( 1114 ) 
508 chapter 11 advanced cluster analysis σj = sp n 2 i=1 p ( 2j oi 2 ) ( oi − uj ) pn i=1 p ( 2j oi 2 ) ( 1115 ) many application probabilistic model-based cluster show effective general partition method fuzzy cluster method distinct advantage appropriate statistical model used capture latent cluster em algorithm commonly used handle many learn problem datum mining statistic due simplicity note general em algorithm may converge optimal solution may instead converge local maximum many heuristic explore avoid example can run em process multiple time used different random initial value furthermore em algorithm costly number distribution large datum set contain observed datum point 112 cluster high-dimensional datum cluster method study far work well dimensionality high less 10 attribute however important application high dimensionality “ conduct cluster analysis high-dimensional datum ” section study approach cluster high-dimensional datum section 1121 start overview major challenge approach used method high-dimensional datum cluster divide two category subspace cluster method ( section 1122 ) dimensionality reduction method ( section 1123 ) 1121 cluster high-dimensional datum problem challenge major methodology present specific method cluster high-dimensional datum let ’ first demonstrate need cluster analysis high-dimensional datum used example examine challenge call new method categorize major method accord whether search cluster subspace original space whether create new lower-dimensionality space search cluster application datum object may describe 10 attribute object refer high-dimensional datum space example 119 high-dimensional datum cluster allelectronic keep track product purchase every customer customer-relationship manager want cluster customer group accord purchase allelectronic 
112 cluster high-dimensional datum 509 table 114 customer purchase datum customer p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 ada bob cathy 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 customer purchase datum high dimensionality allelectronic carry ten thousand product therefore customer ’ purchase profile vector product carry company ten thousand dimension “ traditional distance measure frequently used low-dimensional cluster analysis also effective high-dimensional datum ” consider customer table 114 10 product p1 p10 used demonstration customer purchase product 1 set corresponding bit otherwise 0 appear let ’ calculate euclidean distance ( eq 216 ) among ada bob cathy easy see dist ( ada bob ) = dist ( bob cathy ) = dist ( ada cathy ) = √ 2 accord euclidean distance three customer equivalently similar ( dissimilar ) however close look tell us ada similar cathy bob ada cathy share one common purchase item p1 show example 119 traditional distance measure ineffective high-dimensional datum distance measure may dominate noise many dimension therefore cluster full high-dimensional space unreliable find cluster may meaningful “ kind cluster meaningful high-dimensional datum ” cluster analysis high-dimensional datum still want group similar object together however datum space often big messy additional challenge need find cluster cluster set attribute manifest cluster word cluster high-dimensional datum often defined used small set attribute instead full datum space essentially cluster high-dimensional datum return group object cluster ( conventional cluster analysis ) addition cluster set attribute characterize cluster example table 114 characterize similarity ada cathy p1 may return attribute ada cathy purchase p1 cluster high-dimensional datum search cluster space exist thus two major kind method subspace cluster approach search cluster exist subspace give high-dimensional datum space subspace defined used subset attribute full space subspace cluster approach discuss section 1122 
510 chapter 11 advanced cluster analysis dimensionality reduction approach try construct much lower-dimensional space search cluster space often method may construct new dimension combine dimension original datum dimensionality reduction method topic section 1124 general cluster high-dimensional datum raise several new challenge addition conventional cluster major issue create appropriate model cluster high-dimensional datum unlike conventional cluster low-dimensional space cluster hide high-dimensional datum often significantly smaller example cluster customer-purchase datum would expect many user similar purchase pattern search small meaningful cluster like find needle haystack show conventional distance measure ineffective instead often consider various sophisticated technique model correlation consistency among object subspace typically exponential number possible subspace dimensionality reduction option thus optimal solution often computationally prohibitive example original datum space 1000 dimension want 1000 find cluster dimensionality 10 = 263 × 1023 possible 10 subspace 1122 subspace cluster method “ find subspace cluster high-dimensional datum ” many method propose generally categorize three major group subspace search method correlation-based cluster method bicluster method subspace search method subspace search method search various subspace cluster cluster subset object similar subspace similarity often capture conventional measure distance density example clique algorithm introduce section 1052 subspace cluster method enumerate subspace cluster subspace dimensionality-increas order apply antimonotonicity prune subspace cluster may exist major challenge subspace search method face search series subspace effectively efficiently generally two kind strategy bottom-up approach start low-dimensional subspace search higherdimensional subspace may cluster higher-dimensional 
112 cluster high-dimensional datum 511 subspace various prune technique explore reduce number higherdimensional subspace need search clique example bottom-up approach top-down approach start full space search smaller smaller subspace recursively top-down approach effective locality assumption hold require subspace cluster determine local neighborhood example 1110 proclus top-down subspace approach proclus k-medoid-like method first generate k potential cluster center high-dimensional datum set used sample datum set refine subspace cluster iteratively iteration current k-medoid proclus consider local neighborhood medoid whole datum set identify subspace cluster minimize standard deviation distance point neighborhood medoid dimension subspace medoid determine point datum set assign closest medoid accord corresponding subspace cluster possible outlier identify next iteration new medoid replace exist one improve cluster quality correlation-based cluster method subspace search method search cluster similarity measure used conventional metric like distance density correlation-based approach discover cluster defined advanced correlation model example 1111 correlation-based approach used pca example pca-based approach first apply pca ( principal component analysis see chapter 3 ) derive set new uncorrelated dimension mine cluster new space subspace addition pca space transformation may used hough transform fractal dimension additional detail subspace search method correlation-based cluster method please refer bibliographic note ( section 117 ) bicluster method application want cluster object attribute simultaneously result cluster know bicluster meet four requirement ( 1 ) small set object participate cluster ( 2 ) cluster involve small number attribute ( 3 ) object may participate multiple cluster participate cluster ( 4 ) attribute may involved multiple cluster involved cluster section 1123 discuss bicluster detail 
512 chapter 11 advanced cluster analysis 1123 bicluster cluster analysis discuss far cluster object accord attribute value object attribute treat way however application object attribute defined symmetric way datum analysis involve search datum matrix submatrix show unique pattern cluster kind cluster technique belong category bicluster section first introduce two motivate application example biclustering— gene expression recommender system learn different type bicluster last present bicluster method application example bicluster technique first propose address need analyze gene expression datum gene unit passing-on trait live organism offspr typically gene reside segment dna gene critical live thing specify protein functional rna chain hold information build maintain live organism ’ cell pass genetic trait offspr synthesis functional gene product either rna protein rely process gene expression genotype genetic makeup cell organism individual phenotype observable characteristic organism gene expression fundamental level genetic genotype cause phenotype used dna chip ( also know dna microarray ) biological engineering technique measure expression level large number ( possibly ) organism ’ gene number different experimental condition condition may correspond different time point experiment sample different organ roughly speaking gene expression datum dna microarray datum conceptually condition matrix row correspond one gene column correspond one sample condition element matrix real number record expression level gene specific condition figure 113 show illustration cluster viewpoint interesting issue gene expression datum matrix analyze two dimensions—the gene dimension condition dimension analyze gene dimension treat gene object treat condition attribute mining gene dimension may find pattern share multiple gene cluster gene group example may find group gene express similarly highly interesting bioinformatic find pathway analyze condition dimension treat condition object treat gene attribute way may find pattern condition cluster condition group example may find difference gene expression compare group tumor sample nontumor sample 
112 cluster high-dimensional datum 513 condition gene w11 w12 w1m w21 w22 w2m w31 w32 w3m wn1 wn2 wnm figure 113 microarrary datum matrix example 1112 gene expression gene expression matrix popular bioinformatic research development example important task classify new gene used expression datum gene gene know class symmetrically may classify new sample ( eg new patient ) used expression datum sample sample know class ( eg tumor nontumor ) task invaluable understand mechanism disease clinical treatment see many gene expression datum mining problem highly related cluster analysis however challenge instead cluster one dimension ( eg gene condition ) many case need cluster two dimension simultaneously ( eg gene condition ) moreover unlike cluster model discuss far cluster gene expression datum matrix submatrix usually follow characteristic small set gene participate cluster cluster involve small subset condition gene may participate multiple cluster may participate cluster condition may involved multiple cluster may involved cluster find cluster condition matrix need new cluster technique meet follow requirement bicluster cluster gene defined used subset condition cluster condition defined used subset gene 
514 chapter 11 advanced cluster analysis cluster neither exclusive ( eg one gene participate multiple cluster ) exhaustive ( eg gene may participate cluster ) bicluster useful bioinformatic also application well consider recommender system example example 1113 used bicluster recommender system allelectronic collect datum customer ’ evaluation product used datum recommend product customer datum modeled customer-product matrix row represent customer column represent product element matrix represent customer ’ evaluation product may score ( eg like like somewhat like ) purchase behavior ( eg buy ) figure 114 illustrate structure customer-product matrix analyze two dimension customer dimension product dimension treat customer object product attribute allelectronic find customer group similar preference purchase pattern used product object customer attribute allelectronic mine product group similar customer interest moreover allelectronic mine cluster customer product simultaneously cluster contain subset customer involve subset product example allelectronic highly interested find group customer like group product cluster submatrix customer-product matrix element high value used cluster allelectronic make recommendation two direction first company recommend product new customer similar customer cluster second company recommend customer new product similar involved cluster bicluster gene expression datum matrix bicluster customerproduct matrix usually follow characteristic small set customer participate cluster cluster involve small subset product customer participate multiple cluster may participate cluster customer w11 w21 ··· wn1 product w12 · · · w22 · · · ··· ··· wn2 · · · figure 114 customer–product matrix w1m w2m ··· wnm 
112 cluster high-dimensional datum 515 product may involved multiple cluster may involved cluster bicluster apply customer-product matrix mine cluster satisfying requirement type bicluster “ model bicluster mine ” let ’ start basic notation sake simplicity use “ gene ” “ condition ” refer two dimension discussion discussion easily extend application example simply replace “ gene ” “ condition ” “ customer ” “ product ” tackle customer-product bicluster problem let = { a1 } set gene b = { b1 bm } set condition let e = [ eij ] gene expression datum matrix gene-condition matrix 1 ≤ ≤ n 1 ≤ j ≤ m submatrix × j defined subset ⊆ gene subset j ⊆ b condition example matrix show figure 115 { a1 a33 a86 } × { b6 b12 b36 b99 } submatrix bicluster submatrix gene condition follow consistent pattern define different type bicluster base pattern simplest case submatrix × j ( ⊆ j ⊆ b ) bicluster constant value ∈ j ∈ j eij = c c constant example submatrix { a1 a33 a86 } × { b6 b12 b36 b99 } figure 115 bicluster constant value bicluster interesting row constant value though different row may different value bicluster constant value row submatrix × j ∈ j ∈ j eij = c + αi αi adjustment row i example figure 116 show bicluster constant value row symmetrically bicluster constant value column submatrix × j ∈ j ∈ j eij = c + βj βj adjustment column j a1 ··· a33 ··· a86 ··· ··· ··· ··· ··· ··· ··· ··· b6 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b12 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b36 60 ··· 60 ··· 60 ··· ··· ··· ··· ··· ··· ··· ··· b99 · · · 60 · · · ··· ··· 60 · · · ··· ··· 60 · · · ··· ··· figure 115 gene-condition matrix submatrix bicluster 
516 chapter 11 advanced cluster analysis generally bicluster interesting row change synchronize way respect column vice versa mathematically bicluster coherent value ( also know pattern-based cluster ) submatrix × j ∈ j ∈ j eij = c + αi + βj αi βj adjustment row column j respectively example figure 117 show bicluster coherent value show × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 moreover instead used addition define bicluster coherent value used multiplication eij = c · αi · βj clearly bicluster constant value row column special case bicluster coherent value application may interested - down-regulate change across gene condition without constrain exact value bicluster coherent evolution row submatrix × j i1 i2 ∈ j1 j2 ∈ j ( ei1 j1 − ei1 j2 ) ( ei2 j1 − ei2 j2 ) ≥ example figure 118 show bicluster coherent evolution row symmetrically define bicluster coherent evolution column next study mine bicluster 10 20 50 0 10 20 50 0 10 20 50 0 10 20 50 0 10 20 50 0 figure 116 bicluster constant value row 10 20 50 0 50 60 90 40 30 40 70 20 70 80 110 60 20 30 60 10 figure 117 bicluster coherent value 10 20 50 0 50 100 100 80 30 50 90 20 70 1000 120 100 20 30 80 10 figure 118 bicluster coherent evolution row 
112 cluster high-dimensional datum 517 bicluster method previous specification type bicluster consider ideal case real datum set perfect bicluster rarely exist exist usually small instead random noise affect reading eij thus prevent bicluster nature appear perfect shape two major type method discover bicluster datum may come noise optimization-based method conduct iterative search iteration submatrix highest significance score identify bicluster process terminate user-specified condition meet due cost concern computation greedy search often employ find local optimal bicluster enumeration method use tolerance threshold specify degree noise allow bicluster mine try enumerate submatrix bicluster satisfy requirement use δ-cluster maple algorithms example illustrate idea optimization used δ-cluster algorithm submatrix × j mean ith row 1 x eij = eij | ( 1116 ) j∈j symmetrically mean jth column 1 x eij = eij | ( 1117 ) i∈i mean element submatrix 1 x 1 x 1 x eij = eij = eij eij = | | | i∈i j∈j i∈i ( 1118 ) j∈j quality submatrix bicluster measure mean-squared residue value 1 x h ( × j ) = ( eij − eij − eij + eij ) 2 ( 1119 ) | i∈i j∈j submatrix × j δ-bicluster h ( × j ) ≤ δ δ ≥ 0 threshold δ = 0 × j perfect bicluster coherent value set δ > 0 user specify tolerance average noise per element perfect bicluster eq ( 1119 ) residue element residue ( eij ) = eij − eij − eij + eij ( 1120 ) maximal δ-bicluster δ-bicluster × j exist another δ-bicluster 0 × j 0 ⊆ 0 j ⊆ j 0 least one inequality hold find 
518 chapter 11 advanced cluster analysis maximal δ-bicluster largest size computationally costly therefore use heuristic greedy search method obtain local optimal cluster algorithm work two phase deletion phase start whole matrix mean-squared residue matrix δ iteratively remove row column iteration row compute mean-squared residue 1 x ( ) = ( eij − eij − eij + eij ) 2 ( 1121 ) | j∈j moreover column j compute mean-squared residue 1 x ( eij − eij − eij + eij ) 2 ( j ) = | ( 1122 ) i∈i remove row column largest mean-squared residue end phase obtain submatrix × j δ-bicluster however submatrix may maximal addition phase iteratively expand δ-bicluster × j obtain deletion phase long δ-bicluster requirement maintain iteration consider row column involved current bicluster × j calculate mean-squared residue row column smallest mean-squared residue add current δ-bicluster greedy algorithm find one δ-bicluster find multiple bicluster heavy overlap run algorithm multiple time execution δ-bicluster output replace element output bicluster random number although greedy algorithm may find neither optimal bicluster bicluster fast even large matrix enumerate bicluster used maple mentioned submatrix × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 2 × 2 submatrix × j define p-score ei1 j1 ei1 j2 p-score = | ( ei1 j1 − ei2 j1 ) − ( ei1 j2 − ei2 j2 ) | ( 1123 ) ei2 j1 ei2 j2 submatrix × j δ-pcluster ( pattern-based cluster ) p-score every 2 × 2 submatrix × j δ δ ≥ 0 threshold specify user ’ tolerance noise perfect bicluster p-score control noise every element bicluster mean-squared residue capture average noise interesting property δ-pcluster × j δ-pcluster every x × ( x ≥ 2 ) submatrix × j also δ-pcluster monotonicity enable 
112 cluster high-dimensional datum 519 us obtain succinct representation nonredundant δ-pcluster δ-pcluster maximal row column add cluster maintain δ-pcluster property avoid redundancy instead find δ-pcluster need compute maximal δ-pcluster maple algorithm enumerate maximal δ-pcluster systematically enumerate every combination condition used set enumeration tree depthfirst search enumeration framework pattern-growth method frequent pattern mining ( chapter 6 ) consider gene expression datum condition combination j maple find maximal subset gene × j δ-pcluster × j submatrix another δ-pcluster × j maximal δ-pcluster may huge number condition combination maple prune many unfruitful combination used monotonicity δ-pcluster condition combination j exist set gene × j δ-pcluster need consider superset j moreover consider × j candidate δ-pcluster every ( | − 1 ) subset j 0 j × j 0 δ-pcluster maple also employ several prune technique speed search retain completeness return maximal δ-pcluster example examine current δ-pcluster × j maple collect gene condition may add expand cluster candidate gene condition together j form submatrix δ-pcluster already find search × j superset j prune interested reader may refer bibliographic note additional information maple algorithm ( section 117 ) interesting observation search maximal δ-pcluster maple somewhat similar mining frequent close itemset consequently maple borrow depth-first search framework idea prune technique pattern-growth method frequent pattern mining example frequent pattern mining cluster analysis may share similar technique idea advantage maple algorithms enumerate bicluster guarantee completeness result miss overlapping bicluster however challenge enumeration algorithms may become time consume matrix become large customer-purchase matrix hundred thousand customer million product 1124 dimensionality reduction method spectral cluster subspace cluster method try find cluster subspace original datum space situation effective construct new space instead used subspace original datum motivation behind dimensionality reduction method cluster high-dimensional datum example 1114 cluster derive space consider three cluster point figure possible cluster point subspace original space x × 
520 chapter 11 advanced cluster analysis − 0707x + 0707y x figure 119 cluster derive space may effective three cluster would end project onto overlapping area x √ √ 2 2 axe instead construct new dimension − 2 x + 2 ( show dash line figure ) project point onto new dimension three cluster become apparent although example 1114 involve two dimension idea construct new space ( cluster structure hide datum become well manifest ) extend high-dimensional datum preferably newly construct space low dimensionality many dimensionality reduction method straightforward approach apply feature selection extraction method datum set discuss chapter however method may able detect cluster structure therefore method combine feature extraction cluster prefer section introduce spectral cluster group method effective highdimensional datum application figure 1110 show general framework spectral cluster approach ng-jordan-weiss algorithm spectral cluster method let ’ look step framework also note special condition apply ng-jordan-weiss algorithm example give set object o1 distance pair object dist ( oi oj ) ( 1 ≤ j ≤ n ) desire number k cluster spectral cluster approach work follow used distance measure calculate affinity matrix w wij = e − dist ( oi oj ) σ2 σ scaling parameter control fast affinity wij decrease dist ( oi oj ) increase ng-jordan-weiss algorithm wii set 0 
112 cluster high-dimensional datum datum affinity matrix [ wij ] compute lead k eigenvector cluster new space 521 project back cluster original datum av = λv = f ( w ) figure 1110 framework spectral cluster approach source adapt slide 8 http micued08 azran used affinity matrix w derive matrix = f ( w ) way do vary ng-jordan-weiss algorithm define matrix diagonal matrix dii sum ith row w dii = n x wij ( 1124 ) j=1 set 1 1 = d− 2 wd− 2 ( 1125 ) find k lead eigenvector a recall eigenvector square matrix nonzero vector remain proportional original vector multiply matrix mathematically vector v eigenvector matrix av = λv λ call corresponding eigenvalue step derive k new dimension base affinity matrix w typically k much smaller dimensionality original datum ng-jordan-weiss algorithm compute k eigenvector largest eigenvalue x1 xk used k lead eigenvector project original datum new space defined k lead eigenvector run cluster algorithm k-mean find k cluster ng-jordan-weiss algorithm stack k largest eigenvector column form matrix x = [ x1 x2 · · · xk ] ∈ rn×k algorithm form matrix renormalize row x unit length xij yij = qp k 2 j=1 xij ( 1126 ) algorithm treat row point k-dimensional space rk run k-mean ( algorithm serve partition purpose ) cluster point k cluster 
522 chapter 11 advanced cluster analysis v = [ v1 v2 v3 ] w 05 u = [ u1 u2 u3 ] 0 −05 0 10 20 30 40 50 60 1 05 0 −05 −1 0 10 20 30 40 50 60 1 05 05 0 0 0 04 02 0 −02 0 10 10 20 20 30 30 40 40 50 50 60 60 0 1 05 0 −05 0 10 10 20 20 30 30 40 40 50 50 60 60 figure 1111 new dimension cluster result ng-jordan-weiss algorithm source adapt slide 9 http micued08 azran assign original datum point cluster accord transform point assign cluster obtain step 4 ng-jordan-weiss algorithm original object oi assign jth cluster matrix ’ row assign jth cluster result step 4 spectral cluster method dimensionality new space set desire number cluster set expect new dimension able manifest cluster example 1115 ng-jordan-weiss algorithm consider set point figure datum set affinity matrix three largest eigenvector normalize vector show note three new dimension ( form three largest eigenvector ) cluster easily detected spectral cluster effective high-dimensional application image process theoretically work well certain condition apply scalability however challenge compute eigenvector large matrix costly spectral cluster combine cluster method bicluster additional information dimensionality reduction cluster method kernel pca find bibliographic note ( section 117 ) 113 cluster graph network datum cluster analysis graph network datum extract valuable knowledge information datum increasingly popular many application discuss application challenge cluster graph network datum section similarity measure form cluster give section learn graph cluster method section 1133 general term graph network used interchangeably rest section mainly use term graph 
113 cluster graph network datum 523 1131 application challenge customer relationship manager allelectronic notice lot datum relate customer purchase behavior preferably modeled used graph example 1116 bipartite graph customer purchase behavior allelectronic represent bipartite graph bipartite graph vertex divide two disjoint set edge connect vertex one set vertex set allelectronic customer purchase datum one set vertex represent customer one customer per vertex set represent product one product per vertex edge connect customer product represent purchase product customer figure 1112 show illustration “ kind knowledge obtain cluster analysis customer-product bipartite graph ” cluster customer customer buy similar set product place one group customer relationship manager make product recommendation example suppose ada belong customer cluster customer purchase digital camera last 12 month ada yet purchase one manager decide recommend digital camera alternatively cluster product product purchase similar set customer group together cluster information also used product recommendation example digital camera high-speed flash memory card belong product cluster customer purchase digital camera recommend high-speed flash memory card bipartite graph widely used many application consider another example example 1117 web search engine web search engine search log archive record user query corresponding click-through information ( click-through information tell us page give result search user click ) query click-through information represent used bipartite graph two set customer product figure 1112 bipartite graph represent customer-purchase datum 
524 chapter 11 advanced cluster analysis vertex correspond query web page respectively edge link query web page user click web page ask query valuable information obtain cluster analysis query–web page bipartite graph instance may identify query pose different language mean thing click-through information query similar another example web page web form direct graph also know web graph web page vertex hyperlink edge point source page destination page cluster analysis web graph disclose community find hub authoritative web page detect web spam addition bipartite graph cluster analysis also apply type graph include general graph elaborate example 1118 example 1118 social network social network social structure represent graph vertex individual organization link interdependency vertex represent friendship common interest collaborative activity allelectronic ’ customer form social network customer vertex edge link two customer know customer relationship manager interested find useful information derive allelectronic ’ social network cluster analysis obtain cluster network customer cluster know friend common customer within cluster may influence one another regard purchase decision make moreover communication channel design inform “ head ” cluster ( ie “ best ” connect person cluster ) promotional information spread quickly thus may use customer cluster promote sale allelectronic another example author scientific publication form social network author vertex two author connect edge coauthor publication network general weight graph edge two author carry weight represent strength collaboration many publication two author ( end vertex ) coauthor cluster coauthor network provide insight community author pattern collaboration “ challenge specific cluster analysis graph network datum ” cluster method discuss far object represent used set attribute unique feature graph network datum object ( vertex ) relationship ( edge ) give dimension attribute explicitly defined conduct cluster analysis graph network datum two major new challenge “ measure similarity two object graph accordingly ” typically use conventional distance measure euclidean distance instead need develop new measure quantify similarity 
113 cluster graph network datum 525 measure often metric thus raise new challenge regard development efficient cluster method similarity measure graph discuss section 1132 “ design cluster model method effective graph network datum ” graph network datum often complicate carry topological structure sophisticated traditional cluster analysis application many graph datum set large web graph contain least ten billion web page publicly indexable web graph also sparse average vertex connect small number vertex graph discover accurate useful knowledge hide deep datum good cluster method accommodate factor cluster method graph network datum introduce section 1133 1132 similarity measure “ measure similarity distance two vertex graph ” discussion examine two type measure geodesic distance distance base random walk geodesic distance simple measure distance two vertex graph shortest path vertex formally geodesic distance two vertex length term number edge shortest path vertex two vertex connect graph geodesic distance defined infinite used geodesic distance define several useful measurement graph analysis cluster give graph g = ( v e ) v set vertex e set edge define follow vertext v ∈ v eccentricity v denote eccen ( v ) largest geodesic distance v vertex u ∈ v − { v } eccentricity v capture far away v remotest vertex graph radius graph g minimum eccentricity vertex r = min eccen ( v ) v∈v ( 1127 ) radius capture distance “ central point ” “ farthest border ” graph diameter graph g maximum eccentricity vertex = max eccen ( v ) v∈v diameter represent largest distance pair vertex peripheral vertex vertex achieve diameter ( 1128 ) 
526 chapter 11 advanced cluster analysis b c e figure 1113 graph g vertex c e peripheral example 1119 measurement base geodesic distance consider graph g figure eccentricity 2 eccen ( ) = 2 eccen ( b ) = 2 eccen ( c ) = eccen ( ) = eccen ( e ) = thus radius g 2 diameter note necessary = 2 × r vertex c e peripheral vertex simrank similarity base random walk structural context application geodesic distance may inappropriate measure similarity vertex graph introduce simrank similarity measure base random walk structural context graph mathematics random walk trajectory consist take successive random step example 1120 similarity person social network let ’ consider measure similarity two vertex allelectronic customer social network example 1118 similarity explain closeness two participant network close two person term relationship represent social network “ well geodesic distance measure similarity closeness network ” suppose ada bob two customer network network undirected geodesic distance ( ie length shortest path ada bob ) shortest path message pass ada bob vice versa however information useful allelectronic ’ customer relationship management company typically want send specific message one customer another therefore geodesic distance suit application “ similarity mean social network ” consider two way define similarity two customer consider similar one another similar neighbor social network heuristic intuitive practice two person receive recommendation good number common friend often make similar decision kind similarity base local structure ( ie neighborhood ) vertex thus call structural context–based similarity 
113 cluster graph network datum 527 suppose allelectronic send promotional information ada bob social network ada bob may randomly forward information friend ( neighbor ) network closeness ada bob measure likelihood customer simultaneously receive promotional information originally send ada bob kind similarity base random walk reachability network thus refer similarity base random walk let ’ closer look meant similarity base structural context similarity base random walk intuition behind similarity base structural context two vertex graph similar connect similar vertex measure similarity need define notion individual neighborhood direct graph g = ( v e ) v set vertex e ⊆ v × v set edge vertex v ∈ v individual in-neighborhood v defined ( 1129 ) ( v ) = { | ( u v ) ∈ e } symmetrically define individual out-neighborhood v ( 1130 ) ( v ) = { | ( v w ) ∈ e } follow intuition illustrated example 1120 define simrank structural-context similarity value 0 1 pair vertex vertex v ∈ v similarity vertex ( v v ) = 1 neighborhood identical vertex u v ∈ v u = v define x x c ( u v ) = ( x ) ( 1131 ) i ( u ) i ( v ) | x∈i ( u ) y∈i ( v ) c constant 0 vertex may in-neighbor thus define eq ( 1131 ) 0 either ( u ) ( v ) ∅ parameter c specify rate decay similarity propagate across edge “ compute simrank ” straightforward method iteratively evaluate eq ( 1131 ) fix point reach let si ( u v ) simrank score calculate ith round begin set ( 0 u = v s0 ( u v ) = ( 1132 ) 1 u = v use eq ( 1131 ) compute si+1 si si+1 ( u v ) = x c i ( u ) i ( v ) | x x∈i ( u ) y∈i ( v ) si ( x ) ( 1133 ) 
528 chapter 11 advanced cluster analysis show lim si ( u v ) = ( u v ) additional method approximate i→∞ simrank give bibliographic note ( section 117 ) let ’ consider similarity base random walk direct graph strongly connect two node u v path u v another path v u strongly connect graph g = ( v e ) two vertex u v ∈ v define expect distance u v ( u v ) = x u p [ ] l ( ) ( 1134 ) v u v path start u end v may contain cycle reach v end travele tour = w1 → w2 → · · · → wk length l ( ) = k − probability tour defined ( q k−1 1 i=1 o ( wi ) | l ( ) > 0 ( 1135 ) p [ ] = 0 l ( ) = 0 measure probability vertex w receive message originated simultaneously u v extend expect distance notion expect meeting distance x ( u v ) = p [ ] l ( ) ( 1136 ) ( x x ) ( u v ) ( u v ) ( x x ) pair tour u x v x length used constant c 0 1 define expect meeting probability p ( u v ) = x ( u v ) p [ ] c l ( ) ( 1137 ) ( x x ) similarity measure base random walk parameter c specify probability continue walk step trajectory show ( u v ) = p ( u v ) two vertex u v simrank base structural context random walk 1133 graph cluster method let ’ consider conduct cluster graph first describe intuition behind graph cluster discuss two general category graph cluster method find cluster graph imagine cut graph piece piece cluster vertex within cluster well connect vertex different cluster connect much weaker way formally graph g = ( v e ) 
113 cluster graph network datum 529 cut c = ( ) partition set vertex v g v = ∪ ∩ = ∅ cut set cut set edge { ( u v ) ∈ e|u ∈ v ∈ } size cut number edge cut set weight graph size cut sum weight edge cut set “ kind cut good derive cluster graph ” graph theory network application minimum cut importance cut minimum cut ’ size greater cut ’ size polynomial time algorithms compute minimum cut graph use algorithms graph cluster example 1121 cut cluster consider graph g figure graph two cluster { b c e f } { g h j k } one outlier vertex l consider cut c1 = ( { b c e f g h j k } { l } ) one edge namely ( e l ) cross two partition create c1 therefore cut set c1 { ( e l ) } size c1 1 ( note size cut connect graph smaller 1 ) minimum cut c1 lead good cluster separate outlier vertex l rest graph cut c2 = ( { b c e f l } { g h j k } ) lead much better cluster c1 edge cut set c2 connect two “ natural cluster ” graph specifically edge ( h ) ( e k ) cut set edge connect h e k belong one cluster example 1121 indicate used minimum cut unlikely lead good cluster better choose cut vertex u involved edge cut set edge connect u belong one cluster formally let deg ( u ) degree u number edge connect u sparsity cut c = ( ) defined = cut size min { | | } ( 1138 ) sparsest cut c2 b c g f h e k minimum cut c1 l figure 1114 graph g two cut j 
530 chapter 11 advanced cluster analysis cut sparsest sparsity greater sparsity cut may one sparsest cut example 1121 figure 1114 c2 sparsest cut used sparsity objective function sparsest cut try minimize number edge cross partition balance partition size consider cluster graph g = ( v e ) partition graph k cluster modularity cluster assess quality cluster defined = k x i=1   di 2 li − | | ( 1139 ) li number edge vertex ith cluster di sum degree vertex ith cluster modularity cluster graph difference fraction edge fall individual cluster fraction would graph vertex randomly connect optimal cluster graph maximize modularity theoretically many graph cluster problem regard find good cut sparsest cut graph practice however number challenge exist high computational cost many graph cut problem computationally expensive sparsest cut problem example np-hard therefore find optimal solution large graph often impossible good trade-off scalability quality achieve sophisticated graph graph sophisticated one describe involve weight or cycle high dimensionality graph many vertex similarity matrix vertex represent vector ( row matrix ) dimensionality number vertex graph therefore graph cluster method must handle high dimensionality sparsity large graph often sparse meaning vertex average connect small number vertex similarity matrix large sparse graph also sparse two kind method cluster graph datum address challenge one used cluster method high-dimensional datum design specifically cluster graph first group method base generic cluster method highdimensional datum extract similarity matrix graph used similarity measure discuss section generic cluster method apply similarity matrix discover cluster cluster method 
113 cluster graph network datum 531 high-dimensional datum typically employ example many scenario similarity matrix obtain spectral cluster method ( section 1124 ) apply spectral cluster approximate optimal graph cut solution additional information please refer bibliographic note ( section 117 ) second group method specific graph search graph find well-connected component cluster let ’ look method call scan ( structural cluster algorithm network ) example give undirected graph g = ( v e ) vertex u ∈ v neighborhood u 0 ( u ) = { | ( u v ) ∈ e } ∪ { u } used idea structural-context similarity scan measure similarity two vertex u v ∈ v normalize common neighborhood size 0 ( u ) ∩ 0 ( v ) | σ ( u v ) = √ 0 ( u ) 0 ( v ) | ( 1140 ) larger value compute similar two vertex scan used similarity threshold ε define cluster membership vertex u ∈ v ε-neighborhood u defined nε ( u ) = { v ∈ 0 ( u ) σ ( u v ) ≥ ε } ε-neighborhood u contain neighbor u structural-context similarity u least ε scan core vertex vertex inside cluster u ∈ v core vertex nε ( u ) | ≥ µ µ popularity threshold scan grow cluster core vertex vertex v ε-neighborhood core u v assign cluster u process grow cluster continue cluster grow process similar density-based cluster method dbscan ( chapter 10 ) formally vertex v directly reach core u v ∈ nε ( u ) transitively vertex v reach core u exist vertex w1 wn w1 reach u wi reach wi−1 1 < ≤ n v reach wn moreover two vertex u v ∈ v may may core say connect exist core w u v reach w vertex cluster connect cluster maximum set vertex every pair set connect vertex may belong cluster vertex u hub neighborhood 0 ( u ) u contain vertex one cluster vertex belong cluster hub outlier scan algorithm show figure search framework closely resemble cluster-find process dbscan scan find cut graph cluster set vertex connect base transitive similarity structural context advantage scan time complexity linear respect number edge large sparse graph number edge scale number vertex therefore scan expect good scalability cluster large graph 
532 chapter 11 advanced cluster analysis algorithm scan cluster graph datum input graph g = ( v e ) similarity threshold ε population threshold µ output set cluster method set vertex v unlabeled unlabeled vertex u u core generate new cluster-id c insert v ∈ nε ( u ) queue q q = w ← first vertex q r ← set vertex directly reach w ∈ r unlabeled labele nonmember assign current cluster-id c endif unlabeled insert queue q endif endfor remove w q end else label u nonmember endif endfor vertex u labele nonmember ∃x ∈ 0 ( u ) x different cluster-id label u hub else label u outlier endif endfor figure 1115 scan algorithm cluster analysis graph datum 114 cluster constraint user often background knowledge want integrate cluster analysis may also application-specific requirement information modeled cluster constraint approach topic cluster constraint two step section 1141 categorize type constraint cluster graph datum method cluster constraint introduce section 1142 
114 cluster constraint 533 1141 categorization constraint section study categorize constraint used cluster analysis specifically categorize constraint accord subject set strongly constraint enforce discuss chapter 10 cluster analysis involve three essential aspect object instance cluster cluster group object similarity among object therefore first method discuss categorize constraint accord apply thus three type constraint instance constraint cluster constraint similarity measurement constraint instance constraint instance specify pair set instance group cluster analysis two common type constraint category include must-link constraint must-link constraint specify two object x x group one cluster output cluster analysis must-link constraint transitive must-link ( x ) must-link ( z ) must-link ( x z ) link constraint link constraint opposite must-link constraint link constraint specify two object x output cluster analysis x belong different cluster link constraint entail link ( x ) must-link ( x x 0 ) must-link ( 0 ) link ( x 0 0 ) constraint instance defined used specific instance alternatively also defined used instance variable attribute instance example constraint constraint ( x ) must-link ( x ) dist ( x ) ≤  used distance object specify must-link constraint constraint cluster constraint cluster specify requirement cluster possibly used attribute cluster example constraint may specify minimum number object cluster maximum diameter cluster shape cluster ( eg convex ) number cluster specify partition cluster method regard constraint cluster constraint similarity measurement often similarity measure euclidean distance used measure similarity object cluster analysis application exception apply constraint similarity measurement specify requirement similarity calculation must respect example cluster person move object plaza euclidean distance used give 
534 chapter 11 advanced cluster analysis walking distance two point constraint similarity measurement trajectory implement shortest distance cross wall one way express constraint depend category example specify constraint cluster constraint1 diameter cluster larger requirement also expressed used constraint instance constraint10 link ( x ) dist ( x ) > ( 1141 ) example 1122 constraint instance cluster similarity measurement allelectronic cluster customer group customer assign customer relationship manager suppose want specify customer address place group would allow comprehensive service family expressed used must-link constraint instance constraintfamily ( x ) must-link ( x ) xaddress = yaddress allelectronic eight customer relationship manager ensure similar workload place constraint cluster eight cluster cluster least 10 % customer 15 % customer calculate spatial distance two customer used drive distance two however two customer live different country use flight distance instead constraint similarity measurement another way categorize cluster constraint consider firmly constraint respect constraint hard cluster violate constraint unacceptable constraint soft cluster violate constraint preferable acceptable better solution find soft constraint also call preference example 1123 hard soft constraint allelectronic constraintfamily example 1122 hard constraint splitting family different cluster can prevent company provide comprehensive service family lead poor customer satisfaction constraint number cluster ( correspond number customer relationship manager company ) also hard example 1122 also constraint balance size cluster satisfying constraint strongly prefer company flexible willing assign senior capable customer relationship manager oversee larger cluster therefore constraint soft ideally specific datum set set constraint clustering satisfy constraint however possible may cluster datum set 
114 cluster constraint 535 satisfy constraint trivially two constraint set conflict cluster satisfy time example 1124 conflict constraint consider constraint must-link ( x ) dist ( x ) < 5 link ( x ) dist ( x ) > 3 datum set two object x dist ( x ) = 4 cluster satisfy constraint simultaneously consider two constraint must-link ( x ) dist ( x ) < 5 must-link ( x ) dist ( x ) < 3 second constraint redundant give first moreover datum set distance two object least 5 every possible cluster object satisfy constraint “ measure quality usefulness set constraint ” general consider either informativeness coherence informativeness amount information carry constraint beyond cluster model give datum set cluster method set constraint c informativeness c respect measure fraction constraint c unsatisfied cluster compute d higher informativeness specific requirement background knowledge constraint carry coherence set constraint degree agreement among constraint measure redundancy among constraint 1142 method cluster constraint although categorize cluster constraint application may different constraint specific form consequently various technique need handle specific constraint section discuss general principle handle hard soft constraint handle hard constraint general strategy handle hard constraint strictly respect constraint cluster assignment process illustrate idea use partition cluster example 
536 chapter 11 advanced cluster analysis give datum set set constraint instance ( ie must-link link constraint ) extend k-mean method satisfy constraint cop-k-mean algorithm work follow generate superinstance must-link constraint compute transitive closure must-link constraint must-link constraint treat equivalence relation closure give one multiple subset object object subset must assign one cluster represent subset replace object subset mean superinstance also carry weight number object represent step must-link constraint always satisfied conduct modify k-mean cluster recall k-mean object assign closest center nearest-center assignment violate link constraint respect link constraint modify center assignment process k-mean nearest feasible center assignment object assign center sequence step make sure assignment far violate link constraint object assign nearest center assignment respect link constraint cop-k-mean ensure constraint violate every step require backtracking greedy algorithm generate cluster satisfy constraint provide conflict exist among constraint handle soft constraint cluster soft constraint optimization problem cluster violate soft constraint penalty impose cluster therefore optimization goal cluster contain two part optimize cluster quality minimize constraint violation penalty overall objective function combination cluster quality score penalty score illustrate use partition cluster example give datum set set soft constraint instance cvqe ( constrain vector quantization error ) algorithm conduct k-mean cluster enforce constraint violation penalty objective function used cvqe sum distance used k-mean adjust constraint violation penalty calculate follow penalty must-link violation must-link constraint object x assign two different center c1 c2 respectively constraint violate result dist ( c1 c2 ) distance c1 c2 add objective function penalty penalty link violation link constraint object x assign common center c constraint violate 
114 cluster constraint 537 distance dist ( c c 0 ) c c 0 add objective function penalty speeding constrain cluster constraint similarity measurement lead heavy cost cluster consider follow cluster obstacle problem cluster person move object plaza euclidean distance used measure walking distance two point however constraint similarity measurement trajectory implement shortest distance cross wall ( section 1141 ) obstacle may occur object distance two object may derive geometric computation ( eg involve triangulation ) computational cost high large number object obstacle involved cluster obstacle problem represent used graphical notation first point p visible another point q region r straight line join p q intersect obstacle visibility graph graph vg = ( v e ) vertex obstacle corresponding node v two node v1 v2 v joined edge e corresponding vertex represent visible let vg 0 = ( v 0 e 0 ) visibility graph create vg add two additional point p q v 0 e 0 contain edge join two point v 0 two point mutually visible shortest path two point p q subpath vg 0 show figure 1116 ( ) see begin edge p either v1 v2 v3 go path vg end edge either v4 v5 q reduce cost distance computation two pair object point several preprocess optimization technique used one method group point close together microcluster do first triangulating region r triangle grouping nearby point triangle microcluster used method similar birch dbscan show figure 1116 ( b ) process microcluster rather individual point overall computation reduce precomputation perform build two v4 v1 p v2 o1 v3 o2 vg q v5 vg ( ) ( b ) figure 1116 cluster obstacle object ( o1 o2 ) ( ) visibility graph ( b ) triangulation region microcluster source adapt tung hou han [ thh01 ] 
538 chapter 11 advanced cluster analysis kind join index base computation shortest path ( 1 ) vv index pair obstacle vertex ( 2 ) mv index pair microcluster obstacle vertex use index help optimize overall performance used precomputation optimization strategy distance two point ( granularity level microcluster ) compute efficiently thus cluster process perform manner similar typical efficient k-medoid algorithm claran achieve good cluster quality large datum set 115 summary conventional cluster analysis object assign one cluster exclusively however application need assign object one cluster fuzzy probabilistic way fuzzy cluster probabilistic model-based cluster allow object belong one cluster partition matrix record membership degree object belong cluster probabilistic model-based cluster assume cluster parameterized distribution used datum cluster observed sample estimate parameter cluster mixture model assume set observed object mixture instance multiple probabilistic cluster conceptually observed object generate independently first choose probabilistic cluster accord probability cluster choose sample accord probability density function choose cluster expectation-maximization algorithm framework approach maximum likelihood maximum posteriori estimate parameter statistical model expectation-maximization algorithms used compute fuzzy cluster probabilistic model-based cluster high-dimensional datum pose several challenge cluster analysis include model high-dimensional cluster search cluster two major category cluster method high-dimensional datum subspace cluster method dimensionality reduction method subspace cluster method search cluster subspace original space example include subspace search method correlation-based cluster method bicluster method dimensionality reduction method create new space lower dimensionality search cluster bicluster method cluster object attribute simultaneously type bicluster include bicluster constant value constant value column coherent value coherent evolution column two major type bicluster method optimization-based method enumeration method 
116 exercise 539 spectral cluster dimensionality reduction method general idea construct new dimension used affinity matrix cluster graph network datum many application social network analysis challenge include measure similarity object graph design cluster model method graph network datum geodesic distance number edge two vertex graph used measure similarity alternatively similarity graph social network measure used structural context random walk simrank similarity measure base structural context random walk graph cluster modeled compute graph cut sparsest cut may lead good cluster modularity used measure cluster quality scan graph cluster algorithm search graph identify well-connected component cluster constraint used express application-specific requirement background knowledge cluster analysis constraint cluster categorize constraint instance cluster similarity measurement constraint instance include must-link link constraint constraint hard soft hard constraint cluster enforce strictly respect constraint cluster assignment process cluster soft constraint consider optimization problem heuristic used speed constrain cluster 116 exercise 111 traditional cluster method rigid require object belong exclusively one cluster explain special case fuzzy cluster may use k-mean example 112 allelectronic carry 1000 product p1 p1000 consider customer ada bob cathy ada bob purchase three product common p1 p2 p3 997 product ada bob independently purchase seven randomly cathy purchase 10 product randomly select 1000 product euclidean distance probability dist ( ada bob ) > dist ( ada cathy ) jaccard similarity ( chapter 2 ) used learn example 113 show × j bicluster coherent value i1 i2 ∈ j1 j2 ∈ j ei1 j1 − ei2 j1 = ei1 j2 − ei2 j2 114 compare maple algorithm ( section 1123 ) frequent close itemset mining algorithm closet ( pei han mao [ phm00 ] ) major similarity difference 
540 chapter 11 advanced cluster analysis 115 simrank similarity measure cluster graph network datum ( ) prove lim si ( u v ) = ( u v ) simrank computation i→∞ ( b ) show ( u v ) = p ( u v ) simrank 116 large sparse graph average node low degree similarity matrix used simrank still sparse sense deliberate answer 117 compare scan algorithm ( section 1133 ) dbscan ( section 1041 ) similarity difference 118 consider partition cluster follow constraint cluster number object cluster must nk ( 1 − δ ) nk ( 1 + δ ) n total number object datum set k number cluster desire δ [ 0 1 ) parameter extend k-mean method handle constraint discuss situation constraint hard soft 117 bibliographic note höppner klawonn kruse runkler [ hkkr99 ] provide thorough discussion fuzzy cluster fuzzy c-mean algorithm ( example 117 base ) propose bezdek [ bez81 ] fraley raftery [ fr02 ] give comprehensive overview model-based cluster analysis probabilistic model mclachlan basford [ mb88 ] present systematic introduction mixture model application cluster analysis dempster laird rubin [ dlr77 ] recognize first introduce em algorithm give name however idea em algorithm “ propose many time special circumstance ” admit dempster laird rubin [ dlr77 ] wu [ wu83 ] give correct analysis em algorithm mixture model em algorithms used extensively many datum mining application introduction model-based cluster mixture model em algorithms find recent textbook machine learn statistical learning—for example bishop [ bis06 ] marsland [ mar09 ] alpaydin [ alp11 ] increase dimensionality severe effect distance function indicated beyer et al [ bgrs99 ] also dramatic impact various technique classification cluster semisupervised learn ( radovanović nanopoulos ivanović [ rni09 ] ) kriegel kröger zimek [ kkz09 ] present comprehensive survey method cluster high-dimensional datum clique algorithm develop agrawal gehrke gunopulos raghavan [ aggr98 ] proclus algorithm propose aggawal procopiuc wolf et al [ + 99 ] technique bicluster initially propose hartigan [ har72 ] term bicluster coin mirkin [ mir98 ] cheng church [ cc00 ] introduce 
117 bibliographic note 541 bicluster gene expression datum analysis many study bicluster model method notion δ-pcluster introduce wang wang yang yu [ wwyy02 ] informative survey see madeira oliveira [ mo04 ] tanay sharan shamir [ tss04 ] chapter introduce δ-cluster algorithm cheng church [ cc00 ] maple pei zhang cho et al [ + 03 ] example optimization-based method enumeration method bicluster respectively donath hoffman [ dh73 ] fiedler [ fie73 ] pioneer spectral cluster chapter use algorithm propose ng jordan weis [ njw01 ] example thorough tutorial spectral cluster see luxburg [ lux07 ] cluster graph network datum important fast-growing topic schaeffer [ sch07 ] provide survey simrank measure similarity develop jeh widom [ jw02a ] xu et al [ xyfs07 ] propose scan algorithm arora rao vazirani [ arv09 ] discuss sparsest cut approximation algorithms cluster constraint extensively study davidson wagstaff basu [ dwb06 ] propose measure informativeness coherence copk-mean algorithm give wagstaff et al [ wcrs01 ] cvqe algorithm propose davidson ravi [ dr05 ] tung han lakshmanan ng [ thln01 ] present framework constraint-based cluster base user-specified constraint efficient method constraint-based spatial cluster existence physical obstacle constraint propose tung hou han [ thh01 ] 
13 datum mining trend research frontier young research field datum mining make significant progress cover broad spectrum application since 1980s today datum mining used vast array area numerous commercial datum mining system service available many challenge however still remain final chapter introduce mining complex datum type prelude in-depth study reader may choose addition focus trend research frontier datum mining section 131 present overview methodology mining complex datum type extend concept task introduce book mining include mining time-series sequential pattern biological sequence graph network spatiotemporal datum include geospatial datum moving-object datum cyber-physical system datum multimedium datum text datum web datum datum stream section 132 briefly introduce approach datum mining include statistical method theoretical foundation visual audio datum mining section 133 learn datum mining application business science include financial retail telecommunication industry science engineering recommender system social impact datum mining discuss section 134 include ubiquitous invisible datum mining privacy-preserve datum mining finally section 135 speculate current expect datum mining trend arise response new challenge field 131 mining complex datum type section outline major development research effort mining complex datum type complex datum type summarize figure section 1311 cover mining sequence datum time-series symbolic sequence biological sequence section 1312 discuss mining graph social information network section 1313 address mining kind datum include spatial datum spatiotemporal datum moving-object datum cyber-physical system datum multimedium datum text datum datum mining concept technique doi b978-0-12-381479-100013-7 c 2012 elsevier right re-serve 585 
586 chapter 13 datum mining trend research frontier c p l e x p e f sequence datum graph network mining kind datum time-series datum ( eg stock market datum ) symbolic sequence ( eg customer shopping sequence web click stream ) biological sequence ( eg dna protein sequence ) homogeneous ( link type ) heterogeneous ( link different type ) example graph social information network etc spatial datum spatiotemporal datum cyber-physical system datum multimedium datum text datum web datum datum stream figure 131 complex datum type mining web datum datum stream due broad scope theme section present high-level overview topic discuss in-depth book 1311 mining sequence datum time-series symbolic sequence biological sequence sequence order list event sequence may categorize three group base characteristic event describe ( 1 ) time-series datum ( 2 ) symbolic sequence datum ( 3 ) biological sequence let ’ consider type time-series datum sequence datum consist long sequence numeric datum record equal time interval ( eg per minute per hour per day ) time-series datum generate many natural economic process stock market scientific medical natural observation symbolic sequence datum consist long sequence event nominal datum typically observed equal time interval many sequence gap ( ie lapse record event ) matter much example include customer shopping sequence web click stream well sequence event science engineering natural social development biological sequence include dna protein sequence sequence typically long carry important complicate hide semantic meaning gap usually important let ’ look datum mining sequence datum type 
131 mining complex datum type 587 similarity search time-series datum time-series datum set consist sequence numeric value obtain repeat measurement time value typically measure equal time interval ( eg every minute hour day ) time-series databasis popular many application stock market analysis economic sale forecasting budgetary analysis utility study inventory study yield projection workload projection process quality control also useful study natural phenomena ( eg atmosphere temperature wind earthquake ) scientific engineering experiment medical treatment unlike normal database query find datum match give query exactly similarity search find datum sequence differ slightly give query sequence many time-series similarity query require subsequence match find set sequence contain subsequence similar give query sequence similarity search often necessary first perform datum dimensionality reduction transformation time-series datum typical dimensionality reduction technique include ( 1 ) discrete fourier transform ( dft ) ( 2 ) discrete wavelet transform ( dwt ) ( 3 ) singular value decomposition ( svd ) base principle component analysis ( pca ) touch concept chapter 3 thorough explanation beyond scope book go great detail technique datum signal map signal transform space small subset “ strongest ” transform coefficient save feature feature form feature space projection transform space index construct original transform time-series datum speed search query-based similarity search technique include normalization transformation atomic match ( ie find pair gap-free window small length similar ) window stitching ( ie stitching similar window form pair large similar subsequence allow gap atomic match ) subsequence order ( ie linearly order subsequence match determine whether enough similar piece exist ) numerous software package exist similarity search time-series datum recently researcher propose transform time-series datum piecewise aggregate approximation datum view sequence symbolic representation problem similarity search transform one match subsequence symbolic sequence datum identify motif ( ie frequently occur sequential pattern ) build index hashing mechanism efficient search base motif experiment show approach fast simple comparable search quality dft dwt dimensionality reduction method regression trend analysis time-series datum regression analysis time-series datum study substantially field statistic signal analysis however one may often need go beyond pure regression 
chapter 13 datum mining trend research frontier price 588 allelectronic stock 10-day move average time figure 132 time-series datum stock price allelectronic time trend show dash curve calculate move average analysis perform trend analysis many practical application trend analysis build integrate model used follow four major component movement characterize time-series datum trend long-term movement indicate general direction time-series graph move time example used weight move average least square method find trend curf dash curve indicated figure 132 cyclic movement long-term oscillation trend line curve seasonal variation nearly identical pattern time series appear follow corresponding season successive year holiday shopping season effective trend analysis datum often need “ deseasonalize ” base seasonal index compute autocorrelation random movement characterize sporadic change due chance event labor dispute announce personnel change within company trend analysis also used time-series forecasting find mathematical function approximately generate historic pattern time series used make long-term short-term prediction future value arima ( auto-regressive integrate move average ) long-memory time-series modele autoregression popular method analysis sequential pattern mining symbolic sequence symbolic sequence consist order set element event record without concrete notion time many application involve datum 
131 mining complex datum type 589 symbolic sequence customer shopping sequence web click stream program execution sequence biological sequence sequence event science engineering natural social development biological sequence carry complicate semantic meaning pose many challenge research issue investigation conduct field bioinformatic sequential pattern mining focuse extensively mining symbolic sequence sequential pattern frequent subsequence exist single sequence set sequence sequence α = ha1 a2 · · · subsequence another sequence β = hb1 b2 · · · bm exist integer 1 ≤ j1 < j2 < · · · < jn ≤ a1 ⊆ bj1 a2 ⊆ bj2 ⊆ bjn example α = h { ab } di β = h { abc } { } { de } ai b c e item α subsequence mining sequential pattern consist mining set subsequence frequent one sequence set sequence many scalable algorithms develop result extensive study area alternatively mine set close sequential pattern sequential pattern close exist sequential pattern 0 proper subsequence 0 0 ( frequency ) support s similar frequent pattern mining counterpart also study efficient mining multidimensional multilevel sequential pattern constraint-based frequent pattern mining user-specified constraint used reduce search space sequential pattern mining derive pattern interest user refer constraint-based sequential pattern mining moreover may relax constraint enforce additional constraint problem sequential pattern mining derive different kind pattern sequence datum example enforce gap constraint pattern derive contain consecutive subsequence subsequence small gap alternatively may derive periodic sequential pattern fold event proper-size window find recur subsequence window another approach derive partial order pattern relax requirement strict sequential order mining subsequence pattern besides mining partial order pattern sequential pattern mining methodology also extend mining tree lattice episode order pattern sequence classification classification method perform model construction base feature vector however sequence explicit feature even sophisticated feature selection technique dimensionality potential feature still high sequential nature feature difficult capture make sequence classification challenge task sequence classification method organized three category ( 1 ) featurebased classification transform sequence feature vector apply conventional classification method ( 2 ) sequence distance–based classification distance function measure similarity sequence determine 
590 chapter 13 datum mining trend research frontier quality classification significantly ( 3 ) model-based classification used hide markov model ( hmm ) statistical model classify sequence time-series numeric-valu datum feature selection technique symbolic sequence easily apply time-series datum without discretization however discretization cause information loss recently propose time-series shapelet method used time-series subsequence maximally represent class feature achieve quality classification result alignment biological sequence biological sequence generally refer sequence nucleotide amino acid biological sequence analysis compare align index analyze biological sequence thus play crucial role bioinformatic modern biology sequence alignment base fact live organism related evolution imply nucleotide ( dna rna ) protein sequence species closer evolution exhibit similarity alignment process line sequence achieve maximal identity level also express degree similarity sequence two sequence homologous share common ancestor degree similarity obtain sequence alignment useful determine possibility homology two sequence alignment also help determine relative position multiple species evolution tree call phylogenetic tree problem alignment biological sequence describe follow give two input biological sequence identify similar sequence long conserve subsequence number sequence align exactly two problem know pairwise sequence alignment otherwise multiple sequence alignment sequence compare align either nucleotide ( rna ) amino acid ( protein ) nucleotide two symbol align identical however amino acid two symbol align identical one derive substitution likely occur nature two kind alignment local alignment global alignment former mean portion sequence align whereas latter require alignment entire length sequence either nucleotide amino acid insertion deletion substitution occur nature different probability substitution matrix used represent probability substitution nucleotide amino acid probability insertion deletion usually use gap character − indicate position preferable align two symbol evaluate quality alignment score mechanism typically defined usually count identical similar symbol positive score gap negative one algebraic sum score take alignment measure goal alignment achieve maximal score among possible alignment however expensive ( exactly np-hard problem ) find optimal alignment therefore various heuristic method develop find suboptimal alignment 
131 mining complex datum type 591 dynamic programming approach commonly used sequence alignment among many available analysis package blast ( basic local alignment search tool ) one popular tool biosequence analysis hide markov model biological sequence analysis give biological sequence biologist would like analyze sequence represent represent structure statistical regularity sequence class biologist construct various probabilistic model markov chain hide markov model model probability state depend previous state therefore particularly useful analysis biological sequence datum common method construct hide markov model forward algorithm viterbi algorithm baum-welch algorithm give sequence symbol x forward algorithm find probability obtain x model viterbi algorithm find probable path ( corresponding x ) model whereas baum-welch algorithm learn adjust model parameter best explain set training sequence 1312 mining graph network graph represent general class structure set sequence lattice tree broad range graph application web social network information network biological network bioinformatic chemical informatic computer vision multimedium text retrieval hence graph network mining become increasingly important heavily research overview follow major theme ( 1 ) graph pattern mining ( 2 ) statistical modele network ( 3 ) datum clean integration validation network analysis ( 4 ) cluster classification graph homogeneous network ( 5 ) cluster ranking classification heterogeneous network ( 6 ) role discovery link prediction information network ( 7 ) similarity search olap information network ( 8 ) evolution information network graph pattern mining graph pattern mining mining frequent subgraph ( also call ( sub ) graph pattern ) one set graph method mining graph pattern categorize apriori-based pattern growth–base approach alternatively mine set close graph graph g close exist proper supergraph g 0 carry support count g moreover many variant graph pattern include approximate frequent graph coherent graph dense graph user-specified constraint push deep graph pattern mining process improve mining efficiency graph pattern mining many interesting application example used generate compact effective graph index structure base concept 
592 chapter 13 datum mining trend research frontier frequent discriminative graph pattern approximate structure similarity search achieve explore graph index structure multiple graph feature moreover classification graph also perform effectively used frequent discriminative subgraph feature statistical modele network network consist set node corresponding object associate set property set edge ( link ) connect node represent relationship object network homogeneous node link type friend network coauthor network web page network network heterogeneous node link different type publication network ( link together author conference paper content ) health-care network ( link together doctor nurse patient disease treatment ) researcher propose multiple statistical model modele homogeneous network well-known generative model random graph model ( ie erdös-rényi model ) watts-strogatz model scale-free model scalefree model assume network follow power law distribution ( also know pareto distribution heavy-tailed distribution ) large-scale social network small-world phenomenon observed network characterize high degree local cluster small fraction node ( ie node interconnect one another ) degree separation remain node social network exhibit certain evolutionary characteristic tend follow densification power law state network become increasingly dense time shrink diameter another characteristic effective diameter often decrease network grow node out-degree in-degree typically follow heavytailed distribution datum clean integration validation information network analysis real-world datum often incomplete noisy uncertain unreliable information redundancy may exist among multiple piece datum interconnect large network information redundancy explore network perform quality datum clean datum integration information validation trustability analysis network analysis example distinguish author share name examine networked connection heterogeneous object coauthor publication venue term addition identify inaccurate author information present bookseller explore network build base author information provide multiple bookseller sophisticated information network analysis method develop direction many case portion datum serve “ training ” relatively clean reliable datum consensus datum multiple information 
131 mining complex datum type 593 provider used help consolidate remain unreliable portion datum reduce costly effort labele datum hand training massive dynamic real-world datum set cluster classification graph homogeneous network large graph network cohesive structure often hide among massive interconnect node link cluster analysis method develop large network uncover network structure discover hide community hub outlier base network topological structure associate property various kind network cluster method develop categorize either partition hierarchical density-based algorithms moreover give human-labele training datum discovery network structure guide human-specify heuristic constraint supervised classification semi-supervised classification network recent hot topic datum mining research community cluster ranking classification heterogeneous network heterogeneous network contain interconnect node link different type interconnect structure contain rich information used mutually enhance node link propagate knowledge one type another cluster ranking heterogeneous network perform hand-inhand context highly rank link cluster may contribute lower-rank counterpart evaluation cohesiveness cluster cluster may help consolidate high ranking link dedicate cluster mutual enhancement ranking cluster prompt development algorithm call rankclus moreover user may specify different ranking rule present labele link certain datum type knowledge one type propagate type propagation reach link type via heterogeneous-type connection algorithms develop supervised learn semi-supervised learn heterogeneous network role discovery link prediction information network exist many hide role relationship among different link heterogeneous network example include advisor–advisee leader–follower relationship research publication network discover hide role relationship expert specify constraint base background knowledge enforce constraint may help crosscheck validation large interconnect network information redundancy network often used help weed link follow constraint 
594 chapter 13 datum mining trend research frontier similarly link prediction perform base assessment ranking expect relationship among candidate link example may predict paper author may write read cite base author ’ recent publication history trend research similar topic study often require analyze proximity network link trend connection similar neighbor roughly speaking person refer link prediction link mining however link mining cover additional task include link-based object classification object type prediction link type prediction link existence prediction link cardinality estimation object reconciliation ( predict whether two object fact ) also include group detection ( cluster object ) well subgraph identification ( find characteristic subgraph within network ) metadata mining ( uncover schema-type information regard unstructured datum ) similarity search olap information network similarity search primitive operation database web search engine heterogeneous information network consist multityped interconnect object example include bibliographic network social medium network two object consider similar link similar way multityped object general object similarity within network determine base network structure object property similarity measure moreover network cluster hierarchical network structure help organize object network identify subcommunity well facilitate similarity search furthermore similarity defined differently per user consider different linkage path derive various similarity semantic network know path-based similarity organize network base notion similarity cluster generate multiple hierarchy within network online analytical process ( olap ) perform example drill dice information network base different level abstraction different angle view olap operation may generate multiple interrelate network relationship among network may disclose interesting hide semantic evolution social information network network dynamic constantly evolve detect evolve community evolve regularity anomaly homogeneous heterogeneous network help person better understand structural evolution network predict trend irregularity evolve network homogeneous network evolve community discover subnetwork consist object type set friend coauthor however heterogeneous network community discover subnetwork consist object different type connect set paper author venue term also derive set evolve object type like evolve author theme 
131 mining complex datum type 595 1313 mining kind datum addition sequence graph many kind semi-structure unstructured datum spatiotemporal multimedium hypertext datum interesting application datum carry various kind semantic either store dynamically stream system call specialize datum mining methodology thus mining multiple kind datum include spatial datum spatiotemporal datum cyber-physical system datum multimedium datum text datum web datum datum stream increasingly important task datum mining subsection overview methodology mining kind datum mining spatial datum spatial datum mining discover pattern knowledge spatial datum spatial datum many case refer geospace-related datum store geospatial datum repository datum “ vector ” “ raster ” format form imagery geo-reference multimedium recently large geographic datum warehouse construct integrate thematic geographically reference datum multiple source construct spatial datum cube contain spatial dimension measure support spatial olap multidimensional spatial datum analysis spatial datum mining perform spatial datum warehouse spatial databasis geospatial datum repository popular topic geographic knowledge discovery spatial datum mining include mining spatial association co-location pattern spatial cluster spatial classification spatial modele spatial trend outlier analysis mining spatiotemporal datum move object spatiotemporal datum datum relate space time spatiotemporal datum mining refer process discover pattern knowledge spatiotemporal datum typical example spatiotemporal datum mining include discover evolutionary history city land uncover weather pattern predict earthquake hurricane determine global warm trend spatiotemporal datum mining become increasingly important far-reaching implication give popularity mobile phone gps device internet-based map service weather service digital earth well satellite rfid sensor wireless video technology among many kind spatiotemporal datum moving-object datum ( ie datum move object ) especially important example animal scientist attach telemetry equipment wildlife analyze ecological behavior mobility manager emb gps car better monitor guide vehicle meteorologist use weather satellite radar observe hurricane massive-scale moving-object datum become rich complex ubiquitous example moving-object datum mining include mining movement pattern multiple move object ( ie discovery relationship among multiple move object move cluster leader follower merge convoy swarm pincer well collective movement pattern ) example 
596 chapter 13 datum mining trend research frontier moving-object datum mining include mining periodic pattern one set move object mining trajectory pattern cluster model outlier mining cyber-physical system datum cyber-physical system ( cp ) typically consist large number interact physical information component cp system may interconnect form large heterogeneous cyber-physical network example cyber-physical network include patient care system link patient monitoring system network medical information emergency handle system transportation system link transportation monitoring network consist many sensor video camera traffic information control system battlefield commander system link reconnaissance network battlefield information analysis system clearly cyber-physical system network ubiquitous form critical component modern information infrastructure datum generate cyber-physical system dynamic volatile noisy inconsistent interdependent contain rich spatiotemporal information critically important real-time decision make comparison typical spatiotemporal datum mining mining cyber-physical datum require link current situation large information base perform real-time calculation return prompt response research area include rare-event detection anomaly analysis cyber-physical datum stream reliability trustworthiness cyber-physical datum analysis effective spatiotemporal datum analysis cyber-physical network integration stream datum mining real-time automate control process mining multimedium datum multimedium datum mining discovery interesting pattern multimedium databasis store manage large collection multimedium object include image datum video datum audio datum well sequence datum hypertext datum contain text text markup linkage multimedium datum mining interdisciplinary field integrate image process understand computer vision datum mining pattern recognition issue multimedium datum mining include content-based retrieval similarity search generalization multidimensional analysis multimedium datum cube contain additional dimension measure multimedium information topic multimedium mining include classification prediction analysis mining association video audio datum mining ( section 1323 ) mining text datum text mining interdisciplinary field draw information retrieval datum mining machine learn statistic computational linguistic substantial portion information store text news article technical paper book digital library email message blog web page hence research text mining active important goal derive high-quality information text 
131 mining complex datum type 597 typically do discovery pattern trend mean statistical pattern learn topic modele statistical language modele text mining usually require structuring input text ( eg parse along addition derive linguistic feature removal other subsequent insertion database ) follow derive pattern within structure datum evaluation interpretation output “ high quality ” text mining usually refer combination relevance novelty interestingness typical text mining task include text categorization text cluster entity extraction production granular taxonomy sentiment analysis document summarization entity-relation modele ( ie learn relation name entity ) example include multilingual datum mining multidimensional text analysis contextual text mining trust evolution analysis text datum well text mining application security biomedical literature analysis online medium analysis analytical customer relationship management various kind text mining analysis software tool available academic institution open-source forum industry text mining often also used wordnet sematic web wikipedia information source enhance understand mining text datum mining web datum world wide web serve huge widely distribute global information center news advertisement consumer information financial management education government e-commerce contain rich dynamic collection information web page content hypertext structure multimedium hyperlink information access usage information provide fertile source datum mining web mining application datum mining technique discover pattern structure knowledge web accord analysis target web mining organized three main area web content mining web structure mining web usage mining web content mining analyze web content text multimedium datum structure datum ( within web page link across web page ) do understand content web page provide scalable informative keyword-based page indexing concept resolution web page relevance ranking web page content summary valuable information related web search analysis web page reside either surface web deep web surface web portion web index typical search engine deep web ( hide web ) refer web content part surface web content provide underlie database engine web content mining study extensively researcher search engine web service company web content mining build link across multiple web page individual therefore potential inappropriately disclose personal information study privacy-preserve datum mining address concern development technique protect personal privacy web web structure mining process used graph network mining theory method analyze node connection structure web extract pattern hyperlink hyperlink structural component connect 
598 chapter 13 datum mining trend research frontier web page another location also mine document structure within page ( eg analyze treelike structure page structure describe html xml tag usage ) kind web structure mining help us understand web content may also help transform web content relatively structure datum set web usage mining process extract useful information ( eg user click stream ) server log find pattern related general particular group user understand user ’ search pattern trend association predict user look internet help improve search efficiency effectiveness well promote product related information different group user right time web search company routinely conduct web usage mining improve quality service mining datum stream stream datum refer datum flow system vast volume change dynamically possibly infinite contain multidimensional feature datum store traditional database system moreover system may able read stream sequential order pose great challenge effective mining stream datum substantial research lead progress development efficient method mining datum stream area mining frequent sequential pattern multidimensional analysis ( eg construction stream cube ) classification cluster outlier analysis online detection rare event datum stream general philosophy develop single-scan a-few-scan algorithms used limit compute storage capability include collect information stream datum slide window tilt time window ( recent datum register finest granularity distant datum register coarser granularity ) explore technique like microcluster limit aggregation approximation many application stream datum mining explored—for example real-time detection anomaly computer network traffic botnet text stream video stream power-grid flow web search sensor network cyber-physical system 132 methodology datum mining due broad scope datum mining large variety datum mining methodology methodology datum mining thoroughly cover book section briefly discuss several interesting methodology fully address previous chapter methodology list figure 133 1321 statistical datum mining datum mining technique describe book primarily draw computer science discipline include datum mining machine learn datum warehousing algorithms design efficient handle huge amount datum 
132 methodology datum mining h e r n n g e h l g e statistical datum mining foundation datum mining visual audio datum mining 599 regression generalized linear model analysis variance mixed-effect model factor analysis discriminant analysis survival analysis datum reduction datum compression probability statistical theory microeconomic view pattern discovery inductive database datum visualization datum mining result visualization datum mining process visualization interactive visual datum mining audio datum mining figure 133 datum mining methodology typically multidimensional possibly various complex type however many well-established statistical technique datum analysis particularly numeric datum technique apply extensively scientific datum ( eg datum experiment physics engineering manufacturing psychology medicine ) well datum economic social science technique principal component analysis ( chapter 3 ) cluster ( chapter 10 11 ) already address book thorough discussion major statistical method datum analysis beyond scope book however several method mentioned sake completeness pointer technique provide bibliographic note ( section 138 ) regression general method used predict value response ( dependent ) variable one predictor ( independent ) variable variable numeric various form regression linear multiple weight polynomial nonparametric robust ( robust method useful error fail satisfy normalcy condition datum contain significant outlier ) generalized linear model model generalization ( generalized additive model ) allow categorical ( nominal ) response variable ( transformation 
600 chapter 13 datum mining trend research frontier ) related set predictor variable manner similar modele numeric response variable used linear regression generalized linear model include logistic regression poisson regression analysis variance technique analyze experimental datum two population describe numeric response variable one categorical variable ( factor ) general anova ( single-factor analysis variance ) problem involve comparison k population treatment mean determine least two mean different complex anova problem also exist mixed-effect model model analyze group data—data classify accord one grouping variable typically describe relationship response variable covariate datum group accord one factor common area application include multilevel datum repeat measure datum block design longitudinal datum factor analysis method used determine variable combine generate give factor example many psychiatric datum possible measure certain factor interest directly ( eg intelligence ) however often possible measure quantity ( eg student test score ) reflect factor interest none variable designate dependent discriminant analysis technique used predict categorical response variable unlike generalized linear model assume independent variable follow multivariate normal distribution procedure attempt determine several discriminant function ( linear combination independent variable ) discriminate among group defined response variable discriminant analysis commonly used social science survival analysis several well-established statistical technique exist survival analysis technique originally design predict probability patient undergo medical treatment would survive least time t method survival analysis however also commonly apply manufacturing setting estimate life span industrial equipment popular method include kaplanmeier estimate survival cox proportional hazard regression model extension quality control various statistic used prepare chart quality control shewhart chart cusum chart ( display group summary statistic ) statistic include mean standard deviation range count move average move standard deviation move range 1322 view datum mining foundation research theoretical foundation datum mining yet mature solid systematic theoretical foundation important help provide coherent 
132 methodology datum mining 601 framework development evaluation practice datum mining technology several theory basis datum mining include follow datum reduction theory basis datum mining reduce datum representation datum reduction trade accuracy speed response need obtain quick approximate answer query large databasis datum reduction technique include singular value decomposition ( drive element behind principal component analysis ) wavelet regression log-linear model histogram cluster sampling construction index tree datum compression accord theory basis datum mining compress give datum encode term bit association rule decision tree cluster encode base minimum description length principle state “ best ” theory infer datum set one minimize length theory datum encode used theory predictor datum encode typically bit probability statistical theory accord theory basis datum mining discover joint probability distribution random variable example bayesian belief network hierarchical bayesian model microeconomic view microeconomic view consider datum mining task find pattern interesting extent used decision-make process enterprise ( eg regard marketing strategy production plan ) view one utility pattern consider interesting act enterprise regard face optimization problem object maximize utility value decision theory datum mining become nonlinear optimization problem pattern discovery inductive databasis theory basis datum mining discover pattern occur datum association classification model sequential pattern area machine learn neural network association mining sequential pattern mining cluster several subfield contribute theory knowledge base view database consist datum pattern user interact system query datum theory ( ie pattern ) knowledge base knowledge base actually inductive database theory mutually exclusive example pattern discovery also see form datum reduction datum compression ideally theoretical framework able model typical datum mining task ( eg association classification cluster ) probabilistic nature able handle different form datum consider iterative interactive essence datum mining effort require establish well-defined framework datum mining satisfy requirement 
602 chapter 13 datum mining trend research frontier 1323 visual audio datum mining visual datum mining discover implicit useful knowledge large datum set used datum or knowledge visualization technique human visual system controlled eye brain latter thought powerful highly parallel process reasoning engine contain large knowledge base visual datum mining essentially combine power component make highly attractive effective tool comprehension datum distribution pattern cluster outlier datum visual datum mining view integration two discipline datum visualization datum mining also closely related computer graphic multimedium system human–computer interaction pattern recognition high-performance compute general datum visualization datum mining integrate follow way datum visualization datum database datum warehouse view different granularity abstraction level different combination attribute dimension datum present various visual form boxplot 3-d cube datum distribution chart curf surface link graph show datum visualization section chapter figure 134 135 statsoft show figure 134 boxplot show multiple variable combination statsoft source wwwstatsoftcom 
132 methodology datum mining 603 figure 135 multidimensional datum distribution analysis statsoft source wwwstatsoftcom datum distribution multidimensional space visual display help give user clear impression overview datum characteristic large datum set datum mining result visualization visualization datum mining result presentation result knowledge obtain datum mining visual form form may include scatter plot boxplot ( chapter 2 ) well decision tree association rule cluster outlier generalized rule example scatter plot show figure 136 sas enterprise miner figure 137 mineset used plane associate set pillar describe set association rule mine database figure 138 also mineset present decision tree figure 139 ibm intelligent miner present set cluster property associate datum mining process visualization type visualization present various process datum mining visual form user see datum extract database datum warehouse extract well select datum clean integrate preprocessed mine moreover may also show method select datum mining result store may view figure 1310 show visual presentation datum mining process clementine datum mining system 
604 chapter 13 datum mining trend research frontier figure 136 visualization datum mining result sas enterprise miner interactive visual datum mining ( interactive ) visual datum mining visualization tool used datum mining process help user make smart datum mining decision example datum distribution set attribute display used colored sector ( whole space represent circle ) display help user determine sector first select classification good split point sector may example show figure 1311 output perception-based classification ( pbc ) system develop university munich audio datum mining used audio signal indicate pattern datum feature datum mining result although visual datum mining may disclose interesting pattern used graphical display require user concentrate watch pattern identify interesting novel feature within sometimes quite tiresome pattern transform sound music instead watch picture listen pitch rhythm tune melody identify anything interesting unusual may relieve burden visual concentration 
132 methodology datum mining figure 137 visualization association rule mineset figure 138 visualization decision tree mineset 605 
606 chapter 13 datum mining trend research frontier figure 139 visualization cluster grouping ibm intelligent miner figure 1310 visualization datum mining process clementine 
133 datum mining application 607 figure 1311 perception-based classification interactive visual mining approach relax visual mining therefore audio datum mining interesting complement visual mining 133 datum mining application book study principle method mining relational datum datum warehouse complex datum type datum mining relatively young discipline wide diverse application still nontrivial gap general principle datum mining application-specific effective datum mining tool section examine several application domain list figure discuss customize datum mining method tool develop application 1331 datum mining financial datum analysis bank financial institution offer wide variety banking investment credit service ( latter include business mortgage automobile loan credit card ) also offer insurance stock investment service 
608 chapter 13 datum mining trend research frontier financial datum analysis retail telecommunication industry science engineering datum mining application intrusion detection prevention recommender system figure 1312 common datum mining application domain financial datum collect banking financial industry often relatively complete reliable high quality facilitate systematic datum analysis datum mining present typical case design construction datum warehouse multidimensional datum analysis datum mining like many application datum warehouse need construct banking financial datum multidimensional datum analysis method used analyze general property datum example company ’ financial officer may want view debt revenue change month region sector factor along maximum minimum total average trend deviation statistical information datum warehouse datum cube ( include advanced datum cube concept multifeature discovery-driven regression prediction datum cube ) characterization class comparison cluster outlier analysis play important role financial datum analysis mining loan payment prediction customer credit policy analysis loan payment prediction customer credit analysis critical business bank many factor strongly weakly influence loan payment performance customer credit rating datum mining method attribute selection attribute relevance ranking may help identify important factor eliminate irrelevant one example factor related risk loan payment include loan-to-value ratio term loan debt ratio ( total amount monthly debt versus total monthly income ) payment-to-income ratio customer income level education level residence region credit history analysis customer payment history may find say payment-to-income ratio dominant factor education level debt ratio bank may decide adjust loan-grant policy 
133 datum mining application 609 grant loan customer whose application previously deny whose profile show relatively low risk accord critical factor analysis classification cluster customer target marketing classification cluster method used customer group identification target marketing example use classification identify crucial factor may influence customer ’ decision regard banking customer similar behavior regard loan payment may identify multidimensional cluster technique help identify customer group associate new customer appropriate customer group facilitate target marketing detection money launder financial crime detect money launder financial crime important integrate information multiple heterogeneous databasis ( eg bank transaction databasis federal state crime history databasis ) long potentially related study multiple datum analysis tool used detect unusual pattern large amount cash flow certain period certain group customer useful tool include datum visualization tool ( display transaction activity used graph time group customer ) linkage information network analysis tool ( identify link among different customer activity ) classification tool ( filter unrelated attribute rank highly related one ) cluster tool ( group different case ) outlier analysis tool ( detect unusual amount fund transfer activity ) sequential pattern analysis tool ( characterize unusual access sequence ) tool may identify important relationship pattern activity help investigator focus suspicious case detailed examination 1332 datum mining retail telecommunication industry retail industry well-fit application area datum mining since collect huge amount datum sale customer shopping history good transportation consumption service quantity datum collect continue expand rapidly especially due increase availability ease popularity business conduct web e-commerce today major chain store also web site customer make purchase online business amazoncom ( wwwamazoncom ) exist solely online without brick-and-mortar ( ie physical ) store location retail datum provide rich source datum mining retail datum mining help identify customer buy behavior discover customer shopping pattern trend improve quality customer service achieve better customer retention satisfaction enhance good consumption ratio design effective good transportation distribution policy reduce cost business example datum mining retail industry outlined follow design construction datum warehouse retail datum cover wide spectrum ( include sale customer employee good transportation consumption 
610 chapter 13 datum mining trend research frontier service ) many way design datum warehouse industry level detail include vary substantially outcome preliminary datum mining exercise used help guide design development datum warehouse structure involve decide dimension level include preprocess perform facilitate effective datum mining multidimensional analysis sale customer product time region retail industry require timely information regard customer need product sale trend fashion well quality cost profit service commodity therefore important provide powerful multidimensional analysis visualization tool include construction sophisticated datum cube accord need datum analysis advanced datum cube structure introduce chapter 5 useful retail datum analysis facilitate analysis multidimensional aggregate complex condition analysis effectiveness sale campaign retail industry conduct sale campaign used advertisement coupon various kind discount bonuse promote product attract customer careful analysis effectiveness sale campaign help improve company profit multidimensional analysis used purpose compare amount sale number transaction contain sale item sale period versus contain item sale campaign moreover association analysis may disclose item likely purchase together item sale especially comparison sale campaign customer retention—analysis customer loyalty use customer loyalty card information register sequence purchase particular customer customer loyalty purchase trend analyze systematically good purchase different period customer group sequence sequential pattern mining used investigate change customer consumption loyalty suggest adjustment pricing variety good help retain customer attract new one product recommendation cross-referencing item mining association sale record may discover customer buy digital camera likely buy another set item information used form product recommendation collaborative recommender system ( section 1335 ) use datum mining technique make personalize product recommendation live customer transaction base opinion customer product recommendation also advertised sale receipt weekly flyer web help improve customer service aid customer select item increase sale similarly information “ hot item week ” attractive deal display together associative information promote sale fraudulent analysis identification unusual pattern fraudulent activity cost retail industry million dollar per year important ( 1 ) identify potentially fraudulent user atypical usage pattern ( 2 ) detect attempt gain fraudulent entry unauthorized access individual organizational 
133 datum mining application 611 account ( 3 ) discover unusual pattern may need special attention many pattern discover multidimensional analysis cluster analysis outlier analysis another industry handle huge amount datum telecommunication industry quickly evolved offer local long-distance telephone service provide many comprehensive communication service include cellular phone smart phone internet access email text message image computer web datum transmission datum traffic integration telecommunication computer network internet numerous mean communication compute way change face telecommunication compute create great demand datum mining help understand business dynamic identify telecommunication pattern catch fraudulent activity make better use resource improve service quality datum mining task telecommunication share many similarity retail industry common task include construct large-scale datum warehouse perform multidimensional visualization olap in-depth analysis trend customer pattern sequential pattern task contribute business improvement cost reduction customer retention fraud analysis sharpen edge competition many datum mining task customize datum mining tool telecommunication flourishing expect play increasingly important role business datum mining popularly used many industry insurance manufacturing health care well analysis governmental institutional administration datum although industry characteristic datum set application demand share many common principle methodology therefore effective mining one industry may gain experience methodology transfer industrial application 1333 datum mining science engineering past many scientific datum analysis task tend handle relatively small homogeneous datum set datum typically analyze used “ formulate hypothesis build model evaluate result ” paradigm case statistical technique typically employ analysis ( see section 1321 ) massive datum collection storage technology recently change landscape scientific datum analysis today scientific datum amassed much higher speed lower cost result accumulation huge volume high-dimensional datum stream datum heterogenous datum contain rich spatial temporal information consequently scientific application shift “ hypothesize-and-test ” paradigm toward “ collect store datum mine new hypothesis confirm datum experimentation ” process shift bring new challenge datum mining vast amount datum collect scientific domain ( include geoscience astronomy meteorology geology biological science ) used sophisticated 
612 chapter 13 datum mining trend research frontier telescope multispectral high-resolution remote satellite sensor global position system new generation biological datum collection analysis technology large datum set also generate due fast numeric simulation various field climate ecosystem modele chemical engineering fluid dynamic structural mechanic look challenge bring emerge scientific application datum mining datum warehouse datum preprocess datum preprocess datum warehouse critical information exchange datum mining create warehouse often require find mean resolve inconsistent incompatible datum collect multiple environment different time period require reconcile semantic reference system geometry measurement accuracy precision method need integrate datum heterogeneous source identify event instance consider climate ecosystem datum spatial temporal require cross-referencing geospatial datum major problem analyze datum many event spatial domain temporal domain example el nino event occur every four seven year previous datum might collect systematically today method also need efficient computation sophisticated spatial aggregate handle spatial-related datum stream mining complex datum type scientific datum set heterogeneous nature typically involve semi-structure unstructured datum multimedium datum georeference stream datum well datum sophisticated deeply hide semantic ( eg genomic proteomic datum ) robust dedicate analysis method need handle spatiotemporal datum biological datum related concept hierarchy complex semantic relationship example bioinformatic research problem identify regulatory influence gene gene regulation refer gene cell switch ( ) determine cell ’ function different biological process involve different set gene act together precisely regulate pattern thus understand biological process need identify participate gene regulator require development sophisticated datum mining method analyze large biological datum set clue regulatory influence specific gene find dna segment ( “ regulatory sequence ” ) mediate influence graph-based network-based mining often difficult impossible model several physical phenomena process due limitation exist modele approach alternatively labele graph network may used capture many spatial topological geometric biological relational characteristic present scientific datum set graph network modele object mine represent vertex graph edge vertex represent relationship object example graph used model chemical structure biological pathway datum generate numeric 
133 datum mining application 613 simulation fluid-flow simulation success graph network modele however depend improvement scalability efficiency many graph-based datum mining task classification frequent pattern mining cluster visualization tool domain-specific knowledge high-level graphical user interface visualization tool require scientific datum mining system integrate exist domain-specific datum information system guide researcher general user search pattern interpret visualize discover pattern used discover knowledge decision make datum mining engineering share many similarity datum mining science practice often collect massive amount datum require datum preprocess datum warehousing scalable mining complex type datum typically use visualization make good use graph network moreover many engineering process need real-time response mining datum stream real time often become critical component massive amount human communication datum pour daily life communication exist many form include news blog article web page online discussion product reviews twitter message advertisement communication web various kind social network hence datum mining social science social study become increasingly popular moreover user reader feedback regard product speech article analyze deduce general opinion sentiment view society analysis result used predict trend improve work help decision make computer science generate unique kind datum example computer program long execution often generate huge-size trace computer network complex structure network flow dynamic massive sensor network may generate large amount datum varied reliability computer system databasis suffer various kind attack data access may raise security privacy concern unique kind datum provide fertile land datum mining datum mining computer science used help monitor system status improve system performance isolate software bug detect software plagiarism analyze computer system fault uncover network intrusion recognize system malfunction datum mining software system engineering operate static dynamic ( ie stream-based ) datum depend whether system dump trace beforehand postanalysis must react real time handle online datum various method develop domain integrate extend method machine learn datum mining system engineering pattern recognition statistic datum mining computer science active rich domain datum miner unique challenge require development sophisticated scalable real-time datum mining system engineering method 
614 chapter 13 datum mining trend research frontier 1334 datum mining intrusion detection prevention security computer system datum continual risk extensive growth internet increase availability tool trick intrude attack network prompt intrusion detection prevention become critical component networked system intrusion defined set action threaten integrity confidentiality availability network resource ( eg user account file system system kernel ) intrusion detection system intrusion prevention system monitor network traffic or system execution malicious activity however former produce report whereas latter place in-line able actively block intrusion detected main function intrusion prevention system identify malicious activity log information say activity attempt stop activity report activity majority intrusion detection prevention system use either signaturebased detection anomaly-based detection signature-based detection method detection utilize signature attack pattern preconfigured predetermine domain expert signature-based intrusion prevention system monitor network traffic match signature match find intrusion detection system report anomaly intrusion prevention system take additional appropriate action note since system usually quite dynamic signature need update laboriously whenever new software version arrive change network configuration situation occur another drawback detection mechanism identify case match signature unable detect new previously unknown intrusion trick anomaly-based detection method build model normal network behavior ( call profile ) used detect new pattern significantly deviate profile deviation may represent actual intrusion simply new behavior need add profile main advantage anomaly detection may detect novel intrusion yet observed typically human analyst must sort deviation ascertain represent real intrusion limit factor anomaly detection high percentage false positive new pattern intrusion add set signature enhance signature-based detection datum mining method help intrusion detection prevention system enhance performance various way follow new datum mining algorithms intrusion detection datum mining algorithms used signature-based anomaly-based detection signature-based detection training datum labele either “ normal ” “ ” classifier derive detect know intrusion research area 
133 datum mining application 615 include application classification algorithms association rule mining cost-sensitive modele anomaly-based detection build model normal behavior automatically detect significant deviation method include application cluster outlier analysis classification algorithms statistical approach technique used must efficient scalable capable handle network datum high volume dimensionality heterogeneity association correlation discriminative pattern analysis help select build discriminative classifier association correlation discriminative pattern mining apply find relationship system attribute describe network datum information provide insight regard selection useful attribute intrusion detection new attribute derive aggregate datum may also helpful summary count traffic match particular pattern analysis stream datum due transient dynamic nature intrusion malicious attack crucial perform intrusion detection datum stream environment moreover event may normal consider malicious view part sequence event thus necessary study sequence event frequently encounter together find sequential pattern identify outlier datum mining method find evolve cluster build dynamic classification model datum stream also necessary real-time intrusion detection distribute datum mining intrusion launch several different location target many different destination distribute datum mining method may used analyze network datum several network location detect distribute attack visualization query tool visualization tool available view anomalous pattern detected tool may include feature view association discriminative pattern cluster outlier intrusion detection system also graphical user interface allow security analyst pose query regard network datum intrusion detection result summary computer system continual risk break security datum mining technology used develop strong intrusion detection prevention system may employ signature-based anomaly-based detection 1335 datum mining recommender system today ’ consumer face million good service shopping online recommender system help consumer make product recommendation likely interest user book cds movie restaurant online news article service recommender system may use either contentbased approach collaborative approach hybrid approach combine content-based collaborative method 
616 chapter 13 datum mining trend research frontier content-based approach recommend item similar item user prefer query past rely product feature textual item description collaborative approach ( collaborative filter approach ) may consider user ’ social environment recommend item base opinion customer similar taste preference user recommender system use broad range technique information retrieval statistic machine learn datum mining search similarity among item customer preference consider example 131 example 131 scenario used recommender system suppose visit web site online bookstore ( eg amazon ) intention purchasing book want read type name book first time visit web site browse even make purchase last christmas web store remember previous visit store click stream information information regard past purchase system display description price book specify compare interest customer similar interest recommend additional book title say “ customer buy book specify also buy title ” survey list see another title spark interest decide purchase one well suppose go another online store intention purchasing digital camera system suggest additional item consider base previously mine sequential pattern “ customer buy kind digital camera likely buy particular brand printer memory card photo editing software within three ” decide buy camera without additional item week later receive coupon store regard additional item advantage recommender system provide personalization customer e-commerce promote one-to-one marketing amazon pioneer use collaborative recommender system offer “ personalize store every customer ” part marketing strategy personalization benefit consumer company involved accurate model customer company gain better understand customer need serve need result greater success regard cross-selling related product upsel product affinity one-to-one promotion larger basket customer retention recommendation problem consider set c user set item let u utility function measure usefulness item user c utility commonly represent rating initially defined item previously rate user example join movie recommendation system user typically ask rate several movie space c × possible user item huge recommendation system able extrapolate know unknown rating predict item–user combination item highest predict utility user recommend user 
133 datum mining application 617 “ utility item estimate user ” content-based method estimate base utility assign user item similar many system focus recommend item contain textual information web site article news message look commonality among item movie may look similar genre director actor article may look similar term content-based method root information theory make use keyword ( describe item ) user profile contain information user ’ taste need profile may obtain explicitly ( eg questionnaire ) learn user ’ transactional behavior time collaborative recommender system try predict utility item user u base item previously rate user similar u example recommend book collaborative recommender system try find user history agree u ( eg tend buy similar book give similar rating book ) collaborative recommender system either memory ( heuristic ) base model base memory-based method essentially use heuristic make rating prediction base entire collection item previously rate user unknown rating item–user combination estimate aggregate rating similar user item typically k-nearest-neighbor approach used find k user ( neighbor ) similar target user u various approach used compute similarity user popular approach use either pearson ’ correlation coefficient ( section 332 ) cosine similarity ( section 247 ) weight aggregate used adjust fact different user may use rating scale differently model-based collaborative recommender system use collection rating learn model used make rating prediction example probabilistic model cluster ( find cluster like-minded customer ) bayesian network machine learn technique used recommender system face major challenge scalability ensure quality recommendation consumer example regard scalability collaborative recommender system must able search million potential neighbor real time site used browse pattern indication product preference may thousand datum point customer ensure quality recommendation essential gain consumer ’ trust consumer follow system recommendation end liking product less likely use recommender system classification system recommender system make two type error false negative false positive false negative product system fail recommend although consumer would like false positive product recommend consumer like false positive less desirable annoy anger consumer content-based recommender system limit feature used describe item recommend 
618 chapter 13 datum mining trend research frontier another challenge content-based collaborative recommender system deal new user buy history yet available hybrid approach integrate content-based collaborative method achieve improve recommendation netflix prize open competition hold online dvd-rental service payout $ 1000000 best recommender algorithm predict user rating film base previous rating competition study show predictive accuracy recommender system substantially improve blending multiple predictor especially used ensemble many substantially different method rather refine single technique collaborative recommender system form intelligent query answer consist analyze intent query provide generalized neighborhood associate information relevant query example rather simply return book description price response customer ’ query return additional information related query explicitly ask ( eg book evaluation comment recommendation book sale statistic ) provide intelligent answer query 134 datum mining society us datum mining part daily life although may often unaware presence section 1341 look several example “ ubiquitous invisible ” datum mining affect everyday thing product stock local supermarket ad see surfing internet crime prevention datum mining offer individual many benefit improve customer service satisfaction well lifestyle general however also serious implication regard one ’ right privacy datum security issue topic section 1342 1341 ubiquitous invisible datum mining datum mining present many aspect daily life whether realize affect shop work search information even influence leisure time health well-being section look example ubiquitous ( ever-present ) datum mining several example also represent invisible datum mining “ smart ” software search engine customer-adaptive web service ( eg used recommender algorithms ) “ intelligent ” database system email manager ticket master incorporate datum mining functional component often unbeknownst user grocery store print personalize coupon customer receipt online store recommend additional item base customer interest datum mining innovatively influenced buy way shop experience shopping one example wal-mart hundred million customer visit ten thousand store every week wal-mart allow supplier access datum 
134 datum mining society 619 product perform analysis used datum mining software allow supplier identify customer buy pattern different store control inventory product placement identify new merchandize opportunity affect item ( many ) end store ’ shelves—something think next time wander aisle wal-mart datum mining shape online shopping experience many shopper routinely turn online store purchase book music movie toy recommender system discuss section 1335 offer personalize product recommendation base opinion customer amazoncom forefront used personalize datum mining–based approach marketing strategy observed traditional brick-and-mortar store hardest part get customer store customer likely buy something since cost go another store high therefore marketing brick-and-mortar store tend emphasize draw customer rather actual in-store customer experience contrast online store customer “ walk ” enter another online store click mouse amazoncom capitalize difference offer “ personalize store every ” use several datum mining technique identify customer ’ like make reliable recommendation topic shopping suppose lot buy credit card nowadays unusual receive phone call one ’ credit card company regard suspicious unusual pattern spending credit card company use datum mining detect fraudulent usage save billion dollar year many company increasingly use datum mining customer relationship management ( crm ) help provide customize personal service address individual customer ’ need lieu mass marketing study browse purchasing pattern web store company tailor advertisement promotion customer profile customer less likely annoyed unwanted mass mailing junk mail action result substantial cost saving company customer benefit likely notified offer actually interest result less waste personal time greater satisfaction datum mining greatly influenced way person use computer search information work get internet example decide check email unbeknownst several annoying email already delete thank spam filter used classification algorithms recognize spam process email go google ( wwwgooglecom ) provide access information billion web page index server google one popular widely used internet search engine used google search information become way life many person google popular even become new verb english language meaning “ search ( something ) internet used google search engine extension comprehensive search ” 1 decide type keyword 1 http open-dictionarycom 
620 chapter 13 datum mining trend research frontier topic interest google return list web site topic mine index organized set datum mining algorithms include pagerank moreover type “ boston new york ” google show bus train schedule boston new york however minor change “ boston paris ” lead flight schedule boston paris smart offering information service likely base frequent pattern mine click stream many previous query view result google query various ad pop relate query google ’ strategy tailor advertising match user ’ interest one typical service explore every internet search provider also make happier less likely pester irrelevant ad datum mining omnipresent see daily-encounter example can go scenario many case datum mining invisible user may unaware examine result return datum mining click actually fed new datum datum mining function datum mining become improve accept technology continue research development need many area mentioned challenge throughout book include efficiency scalability increase user interaction incorporation background knowledge visualization technique effective method find interesting pattern improve handle complex datum type stream datum realtime datum mining web mining addition integration datum mining exist business scientific technology provide domain-specific datum mining tool contribute advancement technology success datum mining solution tailor e-commerce application opposed generic datum mining system example 1342 privacy security social impact datum mining information accessible electronic form available web increasingly powerful datum mining tool develop put use increase concern datum mining may pose threat privacy datum security however important note many datum mining application even touch personal datum prominent example include application involve natural resource prediction flood drought meteorology astronomy geography geology biology scientific engineering datum furthermore study datum mining research focus development scalable algorithms involve personal datum focus datum mining technology discovery general statistically significant pattern specific information regard individual sense believe real privacy concern unconstrained access individual record especially access privacy-sensitive information credit card transaction record health-care record personal financial record biological trait justice investigation ethnicity datum mining application involve personal datum many case simple method remove sensitive id datum may protect privacy individual nevertheless privacy concern exist wherever 
134 datum mining society 621 personally identifiable information collect store digital form datum mining program able access datum even datum preparation improper nonexistent disclosure control root cause privacy issue handle concern numerous datum security-enhancing technique develop addition great deal recent effort develop privacypreserve datum mining method section look advance protect privacy datum security datum mining “ secure privacy individual collect mining datum ” many datum security–enhancing technique develop help protect datum databasis employ multilevel security model classify restrict datum accord various security level user permit access authorize level show however user execute specific query authorize security level still infer sensitive information similar possibility occur datum mining encryption another technique individual datum item may encode may involve blind signature ( build public key encryption ) biometric encryption ( eg image person ’ iris fingerprint used encode personal information ) anonymous databasis ( permit consolidation various databasis limit access personal information need know personal information encrypt store different location ) intrusion detection another active area research help protect privacy personal datum privacy-preserve datum mining area datum mining research response privacy protection datum mining also know privacy-enhance privacysensitive datum mining deal obtain valid datum mining result without disclose underlie sensitive datum value privacy-preserve datum mining method use form transformation datum perform privacy preservation typically method reduce granularity representation preserve privacy example may generalize datum individual customer customer group reduction granularity cause loss information possibly usefulness datum mining result natural trade-off information loss privacy privacy-preserve datum mining method classify follow category randomization method method add noise datum mask attribute value record noise add sufficiently large individual record value especially sensitive one re-cover however add skillfully final result datum mining basically preserve technique design derive aggregate distribution perturbed datum subsequently datum mining technique develop work aggregate distribution k-anonymity l-diversity method method alter individual record uniquely identify k-anonymity method granularity datum representation reduce sufficiently give record map onto least k record datum used technique like generalization suppression k-anonymity method weak homogeneity 
622 chapter 13 datum mining trend research frontier sensitive value within group value may infer alter record l-diversity model design handle weakness enforce intragroup diversity sensitive value ensure anonymization goal make sufficiently difficult adversary use combination record attribute exactly identify individual record distribute privacy preservation large datum set can partition distribute either horizontally ( ie datum set partition different subset record distribute across multiple site ) vertically ( ie datum set partition distribute attribute ) even combination individual site may want share entire datum set may consent limit information sharing use variety protocol overall effect method maintain privacy individual object derive aggregate result datum downgrading effectiveness datum mining result many case even though datum may available output datum mining ( eg association rule classification model ) may result violation privacy solution can downgrade effectiveness datum mining either modify datum mining result hiding association rule slightly distort classification model recently researcher propose new idea privacy-preserve datum mining notion differential privacy general idea two datum set close one another ( ie differ tiny datum set single element ) give differentially private algorithm behave approximately datum set definition give strong guarantee presence absence tiny datum set ( eg represent individual ) affect final output query significantly base notion set differential privacy-preserve datum mining algorithms develop research direction ongoing expect powerful privacy-preserve datum publish datum mining algorithms near future like technology datum mining misuse however must lose sight benefit datum mining research bring range insight gain medical scientific application increase customer satisfaction help company better suit client ’ need expect computer scientist policy expert counterterrorism expert continue work social scientist lawyer company consumer take responsibility build solution ensure datum privacy protection security way may continue reap benefit datum mining term time money saving discovery new knowledge 135 datum mining trend diversity datum datum mining task datum mining approach pose many challenge research issue datum mining development efficient effective datum 
135 datum mining trend 623 mining method system service interactive integrate datum mining environment key area study use datum mining technique solve large sophisticated application problem important task datum mining researcher datum mining system application developer section describe trend datum mining reflect pursuit challenge application exploration early datum mining application put lot effort help business gain competitive edge exploration datum mining business continue expand e-commerce e-marketing become mainstream retail industry datum mining increasingly used exploration application area web text analysis financial analysis industry government biomedicine science emerge application area include datum mining counterterrorism mobile ( wireless ) datum mining generic datum mining system may limitation deal application-specific problem may see trend toward development application-specific datum mining system tool well invisible datum mining function embed various kind service scalable interactive datum mining method contrast traditional datum analysis method datum mining must able handle huge amount datum efficiently possible interactively amount datum collect continue increase rapidly scalable algorithms individual integrate datum mining function become essential one important direction toward improve overall efficiency mining process increase user interaction constraint-based mining provide user add control allow specification use constraint guide datum mining system search interesting pattern knowledge integration datum mining search engine database system datum warehouse system cloud compute system search engine database system datum warehouse system cloud compute system mainstream information process compute system important ensure datum mining serve essential datum analysis component smoothly integrate information process environment datum mining service tightly couple system seamless unify framework invisible function ensure datum availability datum mining portability scalability high performance integrate information process environment multidimensional datum analysis exploration mining social information network mining social information network link analysis critical task network ubiquitous complex development scalable effective knowledge discovery method application large number network datum essential outlined section 1312 mining spatiotemporal moving-object cyber-physical system cyberphysical system well spatiotemporal datum mount rapidly due 
624 chapter 13 datum mining trend research frontier popular use cellular phone gps sensor wireless equipment outlined section 1313 many challenge research issue realize real-time effective knowledge discovery datum mining multimedium text web datum outlined section 1313 mining kind datum recent focus datum mining research great progress make yet still many open issue solve mining biological biomedical datum unique combination complexity richness size importance biological biomedical datum warrant special attention datum mining mining dna protein sequence mining highdimensional microarray datum biological pathway network analysis topic field area biological datum mining research include mining biomedical literature link analysis across heterogeneous biological datum information integration biological datum datum mining datum mining software engineering system engineering software program large computer system become increasingly bulky size sophisticated complexity tend originate integration multiple component develop different implementation team trend make increasingly challenge task ensure software robustness reliability analysis execution buggy software program essentially datum mining process—trace datum generate program execution may disclose important pattern outlier can lead eventual automate discovery software bug expect development datum mining methodology system debug enhance software robustness bring new vigor system engineering visual audio datum mining visual audio datum mining effective way integrate human ’ visual audio system discover knowledge huge amount datum systematic development technique facilitate promotion human participation effective efficient datum analysis distribute datum mining real-time datum stream mining traditional datum mining method design work centralize location work well many distribute compute environment present today ( eg internet intranet local area network high-speed wireless network sensor network cloud compute ) advance distribute datum mining method expect moreover many application involve stream datum ( eg e-commerce web mining stock analysis intrusion detection mobile datum mining datum mining counterterrorism ) require dynamic datum mining model build real time additional research need direction privacy protection information security datum mining abundance personal confidential information available electronic form couple increasingly powerful datum mining tool pose threat datum privacy security grow interest datum mining counterterrorism also add concern 
136 summary 625 development privacy-preserve datum mining method foresee collaboration technologist social scientist law expert government company need produce rigorous privacy security protection mechanism datum publish datum mining confidence look forward next generation datum mining technology benefit bring 136 summary mining complex datum type pose challenge issue many dedicate line research development chapter present high-level overview mining complex datum type include mining sequence datum time series symbolic sequence biological sequence mining graph network mining kind datum include spatiotemporal cyber-physical system datum multimedium text web datum datum stream several well-established statistical method propose datum analysis regression generalized linear model analysis variance mixed-effect model factor analysis discriminant analysis survival analysis quality control full coverage statistical datum analysis method beyond scope book interested reader refer statistical literature cite bibliographic note ( section 138 ) researcher strive build theoretical foundation datum mining several interesting proposal appear base datum reduction datum compression probability statistic theory microeconomic theory pattern discovery–based inductive databasis visual datum mining integrate datum mining datum visualization discover implicit useful knowledge large datum set visual datum mining include datum visualization datum mining result visualization datum mining process visualization interactive visual datum mining audio datum mining used audio signal indicate datum pattern feature datum mining result many customize datum mining tool develop domain-specific application include finance retail telecommunication industry science engineering intrusion detection prevention recommender system application domain-based study integrate domain-specific knowledge datum analysis technique provide mission-specific datum mining solution ubiquitous datum mining constant presence datum mining many aspect daily life influence shop work search information use computer well leisure time health well-being invisible datum mining “ smart ” software search engine customer-adaptive web service 
626 chapter 13 datum mining trend research frontier ( eg used recommender algorithms ) email manager incorporate datum mining functional component often unbeknownst user major social concern datum mining issue privacy datum security privacy-preserve datum mining deal obtain valid datum mining result without disclose underlie sensitive value goal ensure privacy protection security preserve overall quality datum mining result datum mining trend include effort toward exploration new application area improve scalable interactive constraint-based mining method integration datum mining web service database warehousing cloud compute system mining social information network trend include mining spatiotemporal cyber-physical system datum biological datum system engineering datum multimedium text datum addition web mining distribute real-time datum stream mining visual audio mining privacy security datum mining 137 exercise 131 sequence datum ubiquitous diverse application chapter present general overview sequential pattern mining sequence classification sequence similarity search trend analysis biological sequence alignment modele however cover sequence cluster present overview method sequence cluster 132 chapter present overview sequence pattern mining graph pattern mining method mining tree pattern partial order pattern also study research summarize method mining structure pattern include sequence tree graph partial order relationship examine kind structural pattern mining cover research propose application create new mining problem 133 many study analyze homogeneous information network ( eg social network consist friend link friend ) however many application involve heterogeneous information network ( ie network link multiple type object research paper conference author topic ) major difference methodology mining heterogeneous information network method homogeneous counterpart 134 research describe datum mining application present chapter discuss different form datum mining used application 135 establishment theoretical foundation important datum mining name describe main theoretical foundation propose datum mining comment satisfy ( fail satisfy ) requirement ideal theoretical framework datum mining 
137 exercise 627 136 ( research project ) build theory datum mining require set theoretical framework major datum mining function explain framework take one theory example ( eg datum compression theory ) examine major datum mining function fit framework function fit well current theoretical framework propose way extend framework explain function 137 strong linkage statistical datum analysis datum mining person think datum mining automate scalable method statistical datum analysis agree disagree perception present one statistical analysis method automate or scale nicely integration current datum mining methodology 138 difference visual datum mining datum visualization datum visualization may suffer datum abundance problem example easy visually discover interesting property network connection social network huge complex dense connection propose visualization method may help person see network topology interesting feature social network 139 propose implementation method audio datum mining integrate audio visual datum mining bring fun power datum mining possible develop video datum mining method state scenario solution make integrate audiovisual mining effective 1310 general-purpose computer domain-independent relational database system become large market last several decade however many person feel generic datum mining system prevail datum mining market think datum mining focus effort develop domain-independent datum mining tool develop domain-specific datum mining solution present reasoning 1311 recommender system way differ customer productbased cluster system differ typical classification predictive modele system outline one method collaborative filter discuss work limitation practice 1312 suppose local bank datum mining system bank study debit card usage pattern notice make many transaction home renovation store bank decide contact offer information regard special loan home improvement ( ) discuss may conflict right privacy ( b ) describe another situation feel datum mining infringe privacy ( c ) describe privacy-preserve datum mining method may allow bank perform customer pattern analysis without infringe customer ’ right privacy ( ) example datum mining can used help society think way can used may detrimental society 
628 chapter 13 datum mining trend research frontier 1313 major challenge face bring datum mining research market illustrate one datum mining research issue view may strong impact market society discuss approach research issue 1314 base view challenge research problem datum mining give number year good number researcher implementor would plan make good progress toward effective solution problem 1315 base experience knowledge suggest new frontier datum mining mentioned chapter 138 bibliographic note mining complex datum type many research paper book cover various theme list recent book well-cite survey research article reference time-series analysis study statistic computer science community decade many textbook box jenkin reinsel [ bjr08 ] brockwell davis [ bd02 ] chatfield [ cha03b ] hamilton [ ham94 ] shumway stoffer [ ss05 ] fast subsequence match method time-series databasis present faloutsos ranganathan manolopoulos [ frm94 ] agrawal lin sawhney shim [ alss95 ] develop method fast similarity search presence noise scaling translation time-series databasis shasha zhu present overview method high-performance discovery time series [ sz04 ] sequential pattern mining method study many researcher include agrawal srikant [ as95 ] zaki [ zak01 ] pei han mortazavi-asl et al [ + 04 ] yan han afshar [ yha03 ] study sequence classification include ji bailey dong [ jbd05 ] ye keogh [ yk09 ] survey xing pei keogh [ xpk10 ] dong pei [ dp07 ] provide overview sequence datum mining method method analysis biological sequence include markov chain hide markov model introduce many book tutorial waterman [ wat95 ] setubal meidanis [ sm97 ] durbin eddy krogh mitchison [ dekm98 ] baldi brunak [ bb01 ] krane raymer [ kr03 ] rabiner [ rab89 ] jone pevzner [ jp04 ] baxevanis ouellette [ bo04 ] information blast ( see also korf yandell bedell [ kyb03 ] ) find ncbi web site graph pattern mining study extensively include holder cook djoko [ hcd94 ] inokuchi washio motoda [ iwm98 ] kuramochi karypis [ kk01 ] yan han [ yh02 yh03a ] borgelt berthold [ bb02 ] huan wang bandyopadhyay et al [ + 04 ] gaston tool nijssen kok [ nk04 ] 
138 bibliographic note 629 great deal research social information network analysis include newman [ new10 ] easley kleinberg [ ek10 ] yu han faloutsos [ yhf10 ] wasserman faust [ wf94 ] watt [ wat03 ] newman barabasi watt [ nbw06 ] statistical modele network study popularly albert barbasi [ ab99 ] watt [ wat03 ] faloutsos faloutsos faloutsos [ fff99 ] kumar raghavan rajagopalan et al [ + 00 ] leskovec kleinberg faloutsos [ lkf05 ] datum clean integration validation information network analysis study many include bhattacharya getoor [ bg04 ] yin han yu [ yhy07 yhy08 ] cluster ranking classification network study extensively include brin page [ bp98 ] chakrabarti dom indyk [ cdi98 ] kleinberg [ kle99 ] getoor friedman koller taskar [ gfkt01 ] newman m girvan [ ng04 ] yin han yang yu [ yhyy04 ] yin han yu [ yhy05 ] xu yuruk feng schweiger [ xyfs07 ] kuli basu dhillon mooney [ kbdm09 ] sun han zhao et al [ + 09 ] neville gallaher eliassi-rad [ nge-r09 ] ji sun danilevsky et al [ + 10 ] role discovery link prediction information network study extensively well krebs [ kre02 ] kubica moore schneider [ kms03 ] liben-nowell kleinberg [ l-nk03 ] wang han jia et al [ + 10 ] similarity search olap information network study many include tian hankin patel [ thp08 ] chen yan zhu et al [ + 08 ] evolution social information network study many researcher chakrabarti kumar tomkin [ ckt06 ] chi song zhou et al [ + 07 ] tang liu zhang nazeri [ tlzn08 ] xu zhang yu long [ xzyl08 ] kim han [ kh09 ] sun tang han [ + 10 ] spatial spatiotemporal datum mining study extensively collection paper miller han [ mh09 ] introduce textbook shekhar chawla [ sc03 ] hsu lee wang [ hlw07 ] spatial cluster algorithms study extensively chapter 10 11 book research conduct spatial warehouse olap stefanovic han koperski [ shk00 ] spatial spatiotemporal datum mining koperski han [ kh95 ] mamouli cao kollio hadjieleftheriou et al [ + 04 ] tsoukatos gunopulos [ tg01 ] hadjieleftheriou kollio gunopulos tsotra [ hkgt03 ] mining moving-object datum study many vlachos gunopulos kollio [ vgk02 ] tao faloutsos papadia liu [ tfpl04 ] li han kim gonzalez [ lhkg07 ] lee han whang [ lhw07 ] li ding han et al [ + 10 ] bibliography temporal spatial spatiotemporal datum mining research see collection roddick hornsby spiliopoulou [ rhs01 ] multimedium datum mining deep root image process pattern recognition study extensively many textbook include gonzalez wood [ gw07 ] russ [ rus06 ] duda hart stork [ dhs01 ] z zhang r zhang [ zz09 ] search mining multimedium datum study many ( see eg fayyad smyth [ fs93 ] faloutsos lin [ fl95 ] natsev rastogi 
630 chapter 13 datum mining trend research frontier shim [ nrs99 ] zaı̈ane han zhu [ zhz00 ] ) overview image mining method give hsu lee zhang [ hlz02 ] text datum analysis study extensively information retrieval many textbook survey article croft metzler strohman [ cms09 ] s buttcher c clarke g cormack [ bcc10 ] man raghavan schutze [ mrs08 ] grossman frieder [ gr04 ] baeza-yate riberio-neto [ byrn11 ] zhai [ zha08 ] feldman sanger [ fs06 ] berry [ ber03 ] weis indurkhya zhang damerau [ wizd04 ] text mining fast-developing field numerous paper publish recent year cover many topic topic model ( eg blei lafferty [ bl09 ] ) sentiment analysis ( eg pang lee [ pl07 ] ) contextual text mining ( eg mei zhai [ mz06 ] ) web mining another focuse theme book like chakrabarti [ cha03a ] liu [ liu06 ] berry [ ber03 ] web mining substantially improve search engine influential milestone work brin page [ bp98 ] kleinberg [ kle99 ] chakrabarti dom kumar et al [ + 99 ] kleinberg tomkin [ kt99 ] numerous result generate since search log mining ( eg silvestri [ sil10 ] ) blog mining ( eg mei liu su zhai [ mlsz06 ] ) mining online forum ( eg cong wang lin et al [ + 08 ] ) book survey stream datum system stream datum process include babu widom [ bw01 ] babcock babu datar et al [ + 02 ] muthukrishnan [ mut05 ] aggarwal [ agg06 ] stream datum mining research cover stream cube model ( eg chen dong han et al [ + 02 ] ) stream frequent pattern mining ( eg manku motwani [ mm02 ] karp papadimitriou shenker [ kps03 ] ) stream classification ( eg domingo hulten [ dh00 ] wang fan yu han [ wfyh03 ] aggarwal han wang yu [ ahwy04b ] ) stream cluster ( eg guha mishra motwani ’ callaghan [ gmmo00 ] aggarwal han wang yu [ ahwy03 ] ) many book discuss datum mining application financial datum analysis financial modele see example benninga [ ben08 ] higgin [ hig08 ] retail datum mining customer relationship management see example book berry linoff [ bl04 ] berson smith thearle [ bst99 ] telecommunication-related datum mining see example horak [ hor08 ] also book scientific datum analysis grossman kamath kegelmeyer et al [ + 01 ] kamath [ kam09 ] issue theoretical foundation datum mining address many researcher example mannila present summary study foundation datum mining [ man00 ] datum reduction view datum mining summarize new jersey datum reduction report barbará dumouchel faloutos et al [ + 97 ] datum compression view find study minimum description length principle grunwald rissanen [ gr07 ] pattern discovery point view datum mining address numerous machine learn datum mining study range association mining decision tree induction sequential pattern mining cluster probability theory point view popular statistic machine learn literature 
138 bibliographic note 631 bayesian network hierarchical bayesian model chapter 9 probabilistic graph model ( eg koller friedman [ kf09 ] ) kleinberg papadimitriou raghavan [ kpr98 ] present microeconomic view treat datum mining optimization problem study inductive database view include imielinski mannila [ im96 ] de raedt gun nijssen [ rgn10 ] statistical method datum analysis describe many book hastie tibshirani friedman [ htf09 ] freedman pisani purf [ fpp07 ] devore [ dev03 ] kutner nachtsheim neter li [ knnl04 ] dobson [ dob01 ] breiman friedman olshen stone [ bfos84 ] pinheiro bate [ pb00 ] johnson wichern [ jw02b ] huberty [ hub94 ] shumway stoffer [ ss05 ] miller [ mil98 ] visual datum mining popular book visual display datum information include tufte [ tuf90 tuf97 tuf01 ] summary technique visualize datum present cleveland [ cle93 ] dedicate visual datum mining book visual datum mining technique tool datum visualization mining soukup davidson [ sd02 ] book information visualization datum mining knowledge discovery edit fayyad grinstein wierse [ fgw01 ] contain collection article visual datum mining method ubiquitous invisible datum mining discuss many text include john [ joh99 ] article book edit kargupta joshi sivakumar yesha [ kjsy04 ] book business @ speed thought succeed digital economy gate [ gat00 ] discuss e-commerce customer relationship management provide interesting perspective datum mining future mena [ men03 ] informative book use datum mining detect prevent crime cover many form criminal activity range fraud detection money launder insurance crime identity crime intrusion detection datum mining issue regard privacy datum security address popularly literature book privacy security datum mining include thuraisingham [ thu04 ] aggarwal yu [ ay08 ] vaidya clifton zhu [ vcz10 ] fung wang fu yu [ fwfy10 ] research article include agrawal srikant [ as00 ] evfimievski srikant agrawal gehrke [ esag02 ] vaidya clifton [ vc03 ] differential privacy introduce dwork [ dwo06 ] study many hay rastogi miklau suciu [ hrms10 ] many discussion trend research direction datum mining various forum several book collection article issue kargupta han yu et al [ + 08 ] 

concept [ music ] first introduce basic concept cluster validation 
cluster validation assessment  major issue cluster validation assessment  cluster evaluation   cluster stability   understand sensitivity cluster result various algorithm parameter eg # cluster cluster tendency  2 evaluate goodness cluster assess suitability cluster ie whether datum inherent grouping structure know cluster unsupervised method sense expert judge whether cluster good cluster important evaluate cluster quality mean evaluate goodness cluster also want evaluate cluster stability mean understand sensitivity cluster result various algorithm parameter example number cluster mean number cluster really good base evaluation third one cluster tendency mean whether suitable cluster mean whether datum inherent grouping structure kind cluster discover [ music ] 

measure cluster quality 
measure cluster quality  cluster evaluation evaluate goodness cluster result   commonly recognize best suitable measure practice three categorization measure external internal relative  external supervised employ criterium inherent dataset   internal unsupervised criterium derive datum   2 compare cluster prior expert-specify knowledge ( ie ground truth ) used certain cluster quality measure evaluate goodness cluster consider well cluster separated compact cluster eg silhouette coefficient relative directly compare different clustering usually obtain via different parameter setting algorithm 

cluster validation [ sound ] session give general introduction external measure cluster validation 
measure cluster quality external method give ground truth q ( c ) quality measure cluster c  q ( c ) good satisfy follow four essential criterium  cluster homogeneity  purer better  cluster completeness  assign object belong category ground truth cluster  rag bag better alien  putt heterogeneous object pure cluster penalize putt rag bag ( ie “ miscellaneous ” “ ” category )  small cluster preservation  splitting small category piece harmful splitting large category piece  2 know cluster unsupervised sense really want use good judgement external method give expert knowledge prior truth extent call ground truth t give ground truth want measure quality cluster c use quality measure q ( c ) function q ( ct ) good satisfy follow four essential criterium first one call cluster homogeneity mean want cluster form pure mean be cluster purer better course second one call cluster completeness mean want assign object belong category ground truth cluster mean may get object category assign two three cluster s good want complete third rag bag better alien simply say may heterogeneous object kay put pure cluster mix category s good want penalize s better even put rag bag rag bag mean s miscellaneous category s better mix pure cluster fourth measure course small cluster preservation simply say get pretty small cluster already want split piece piece likely represent noise case better split larger category smaller piece reason larger category likely may belong different cluster different smaller category s small cluster preservation see go examine external measure 
commonly used external measure  matching-based measure   purity maximum match f-measure entropy-based measure  conditional entropy  normalize mutual information ( nmi )  variation information  pairwise measure ( cover ) ground truth partition t1 ( cover ) ( cover )  four possibility true positive ( tp ) fn fp tn  jaccard coefficient rand statistic fowlkes-mallow measure  correlation measure  3 ( cover ) discretize huber static normalize discretize huber static cluster c1 cluster c2 t2 often use graph color graph brown point dark brown point suppose ground truth partition t1 mean expert label point t1 okay ground truth ground truth may labele blue one mean t2 dark blue one actually another category kay mean ground truth expert know different color belong different truth kay however cluster algorithm may generate thing circle light brown one orange one red one mean ideal may good enough match expert judgement want measure whether cluster method good kay measure one call matching-based measure sev several measure like purity maximum match f-measure another kind measure call entropy-based measure conditional entropy normalize mutual information nmi variation information certain kind measure call pairwise measure example may four possibility true positive false negative false positive true negative okay actually jaccard coefficient rand statistic fowlkes-mallow measure fourth category call correlation measure essentially discretize huber static normalize discretize huber static marked cover discuss detail subsequent session [ music ] 

matching-based measure [ music ] hi session go discuss first group external measure call matching-based measure matching-based measure first introduce purity versus maximum match still use similar notation suppose ground truth get three category t1 actually marked brown point t2 marked green point t3 marked black point ground truth cluster finally get cluster c sub 1 actually s marked ellipse orange color okay c2 marked surround red color c3 surround black color purity actually quantify extent cluster c sub contain point one ground truth s partition suppose r cluster finer get purity sub actually first count many point cluster try see maximum number ground truth maximum number ground truth actually black point see max number one two three four five six seven get one seven total number nine get purity seven nine okay total purity whole cluster c essentially total purity one s purity suppose r cluster one s purity get corresponding proportion add together s total purity transform formula easily one okay course perfect cluster case purity one mean everything pure add together s one number cluster obtain number group ground truth 
matching-based measure ( ) purity vs maximum match purity quantify extent cluster ci contain point k 1 one ( ground truth ) partition purityi = max { nij } ni j 1  total purity cluster c r n 1 r k  = purityi max { nij } ∑ ∑ n n 1 j 1 i 1 = = purity    maximum match one cluster match one partition w ( ) = ∑ w ( e )  match pairwise match weight w ( eij ) = nij e∈m w ( ) }  maximum weight match match = arg max { n  ex2 ( green ) match = purity = 075 ( orange ) match = 065 > 06  2 perfect cluster purity = 1 r = k ( number cluster obtain ground truth ) ex 1 ( green orange ) purity1 = 50 purity2 = 25 purity3 = 25 purity = ( 30 + 20 + 25 ) 100 = 075 two cluster may share majority partition t2 ground truth t1 cluster c1 c2 c3 t3 sum c\t t1 t2 c1 0 20 30 50 c2 0 20 5 25 c3 25 0 0 25 mj 25 40 35 100 c\t t1 t2 sum c1 0 30 20 50 c2 0 20 5 25 c3 25 0 0 25 mj 25 50 25 100 t3 look two table let s first look green table green table purity mean first cluster get maximum purity 30 total number point s 50 get 30 purity 2 obviously get 20 purity 3 25 25 s really total purity whole cluster accord formula will get maximum ( 30 + 20 + 25 ) total number point get interestingly look orange table orange colored table get exact number thing 30 50 20 25 25 25 finally get purity exactly formula may find one undesirable thing orange colored table see ground truth t2 total get 50 actual partition two cluster c1 c2 mean c1 c2 would say belong ground truth t2 green table probably see s different c1 pair t3 c2 pair t2 motivate person propose maximum match mean one cluster match one partition rule find pairwise match mean look look weight allow element belong one cluster ij ij ij must exactly element belong belong one need look maximum weight match maximum weight match simply say look whole group look n point try find final match together maximize example look green one max match 30 part t3 t2 pairwise c2 t1 pairwise c3 okay s green s maximum match equal purity however orange color table probably see assign t2 c1 assign t2 c2 assign t3 c2 finally get match however assign t3 c1 t2 c2 t1 c3 get match 065 extent assignment better maximum match value bigger s reason maximum match restriction pairwise assignment 
matching-based measure ( ii ) f-measure precision fraction point ci majority partition j ( ie purity ) ji partition contain maximum # point ci niji 1 k = preci = max { nij } ground truth  ex green table = j 1 ni ni cluster c c c  prec1 = 50 prec2 = 25 prec3 = 25 c\t t1 t2 t3 sum  recall fraction point partition j share common c1 0 20 30 50 niji niji cluster ci j | j | c2 0 20 5 25 recall i =  ex green table c3 25 0 0 25 | ji | ji mj 25 40 35 100  recall1 = 35 recall2 = 40 recall3 = 25 2nij  f-measure ci harmonic mean preci recalli fi = ni + j r 1  f-measure cluster c average cluster f = ∑ fi r 1  ex green table  f1 = 85 f2 = 65 f3 = 1 f = 0774  2 1 1 3 2 3 another popular use matching-based measure call f-measure f-measure popularly used information retrieval f-measure essentially compute precision recall let s look precision defined okay precision define particular match want see precision sub want find s maximum match particular cluster divide number total number point cluster sense precision actually purity mean get fraction c sub majority partition c sub j j s partition contain max number point c sub get formula formula get precision purity actually one get 50 25 25 okay s precision recall recall actually fraction point partition share common cluster c sub i simply say assign t3 c1 want see have get 30 t3 actually get 35 grant capture mean be recall 30 s exactly probably see formula similarly second partition assign ground truth t2 c2 see get 40 recall 40 similarly third one assign ground truth t1 c3 get 25 mean fraction point partition share common cluster c sub get recall f measure f measure actually harmonic mean precision recall f measure cluster c sub harmonic mean precision sub recall sub i transform formula become like one f measure whole cluster need need average cluster f measure mean suppose get r cluster get f measure 1 r add together divide r okay green table see get f1 get 2 time 30 divide total two add together get f2 get 2 time 20 divide two add together okay f3 course 1 matter calculate average three get f-measure number [ music ] 

entropy-based measure [ sound ] session go introduce another group external measure call entropy-based measure know entropy useful information theory measure information theory also used datum mining mission learn quite lot 
entropy-based measure ( ) conditional entropy  entropy cluster c pci = ni ( ie probability cluster ci ) n entropy partition  entropy respect cluster ci  conditional entropy respect cluster c  cluster ’ member split different partition higher conditional entropy  perfect cluster conditional entropy value 0 worst possible conditional entropy value log k  2 t2 ground truth t1 cluster c1 c2 c3 entropy essentially represent amount otherness information partition still use graph figure represent conceptually ground truth represent different color point cluster represent different ellipse entropy cluster c suppose get r cluster s s essentially entropy every particular cluster add cluster together get entropy cluster c cluster actually entropy base probability c sub i probability cluster c sub base number point cluster divide total number point entropy partition essentially defined similar way suppose ground truth j 1 k k group okay ground truth pt sub actually get probability ground truth defined p sub sub okay add entropy together k ground truth get ground truth whole partition be interested entropy respect cluster c sub i mean want see ground truth distribute within cluster probably see j represent ground truth represent really cluster probably want see distribution work entropy respect cluster c sub i want get conditional entropy respect whole cluster c add together one cluster c sub total r cluster add proportionally get whole conditional entropy respect whole cluster c 
entropy-based measure ( ii ) normalize mutual information ( nmi ) mutual information r k pij  quantify amount share info ( c ) = ∑∑ pij log ( ) pci ⋅ pt j i 1 j 1 cluster c partition  measure dependency observed joint probability pij c expect joint probability pci ptj independence assumption  c independent pij = pci ptj ( c ) = however upper bound mutual information ground truth  normalize mutual information ( nmi ) cluster c  1  3 value range nmi [ 01 ] value close 1 indicate good cluster t2 1 c2 c3 conceptually see cluster s membership split different partition higher conditional entropy s less desirable probably see be partition wide spread different cluster good okay perfect cluster conditional entropy value worse conditional entropy value log k use transformation formula like transfer conditional entropy joint-entropy minus cluster s entropy probably see be conditional formula transformation be get detail check another useful measure used external measure normalize mutual information use similar figure sketch idea mutual information also defined information s theory introduce s also useful machine learn datum mining okay essentially mutual information quantify amount share information cluster c partition t probably see formula r cluster k ground truth mutual information essentially add together similar formula entropy s different probably see part really ij s probability divide cross ring property partition s property mean want measure dependency observed joint probability p sub ij c expect joint probability pc sub pt sub j independence assumption course c really independent mean really want equal distance probably see one actually take log become 0 okay course case good imply ground truth actually scatter r around different cluster however s upper bound mutual information less desirable s need introduce normalize mutual information mean want normalize range 0 highest wine one value close 1 actually indicate good cluster value close 0 mean almost accompany random independent assignment okay normalize mutual information essentially take mutual information divide entropy cluster divide entropy partition product together take square root transfer way quite useful know cluster base external measure cluster become perfect value close one [ music ] 

measure session be go introduce certain kind external measure call pairwise measure 
pairwise measure four possibility truth assignment four possibility base agreement cluster label partition label  tp true positive—two point xi xj belong partition also cluster c ˆ yˆ j } | tp | { ( = x j ) yi = = j yi yi true partition label yˆi cluster label point xi = fn | { ( = x j ) yi j yˆi ≠ yˆ j } | ground truth  fn false negative cluster c c ˆ ˆ fp | { ( ) } | = x x ≠ =  fp false positive j j j  1 1 2 tn true negative tn = | { ( xi x j ) yi ≠ j yˆi ≠ yˆ j } | n  calculate four measure n =   total # pair point k r k mj nij  2 1 r k 2 tp ∑∑ ( ) ( ( ∑∑ nij ) − = ) fn ∑ ( ) − tp = = 2 2 i 1 j 1 j 1 i 1 j 1 2 r r k ni 1 2 r 2 k = fp ∑ ( ) − tp tn n − ( + fn + fp ) = ( n − ∑ ni − ∑ j 2 + ∑∑ nij 2 ) 2 1 2 i 1 j 1 i 1 j 1  2 2 c3 
pairwise measure jaccard coefficient rand statistic jaccard coefficient fraction true positive point pair ignore true negative ( thus asymmetric )  jaccard = ( tp + fn + fp ) [ ie denominator ignore tn ]  perfect cluster jaccard = 1  rand statistic  rand = ( tp + tn ) n  symmetric perfect cluster rand = 1  fowlkes-mallow measure  geometric mean precision recall  fm =  3 prec × recall = tp ( tp + fn ) ( tp + fp ) used formula one calculate measure green table ( leave exercise ) t2 ground truth t1 cluster c1 c2 c3 t3 sum c\t t1 t2 c1 0 20 30 50 c2 0 20 5 25 c3 25 0 0 25 mj 25 40 35 100 pairwise measure mean take two point see whether agree cluster label partition label four possible case pair true positive false negative false positive true negative look case example get two point x sub x sub j belong partition also cluster c case true positive example look case definition true positive number case okay example two point x sub x sub j true partition label also cluster label mean belong cluster s case true positive example look case two blue point belong ground truth t2 also belong cluster c2 s true positive okay false negative false negative mean ground-truth partition label hand cluster example look two brown point ground-truth t1 belong different cluster s false negative case well false positive false positive mean actually different partition label cluster example look blue one brown one actually partition label cluster okay true negative true negative pair actually mean partition label also cluster example look black one blue one two point partition label also cluster s good case see calculate four measure first give n point datum set possible pair need examine actually n choose two s reason formula true positive case get partition cluster agree s case n j choose case choose two get many case false negative false negative mean get many partition case belong true positive okay false positive mean many cluster case belong true positive true negative case mean case belong one three case true negative okay computation simply use formula introduction true positive false negative four measure calculate measure like jaccard coefficient rand statistic still take figure illustrative example jaccard coefficient remember define jaccard coefficient one definition pairwise measure jaccard coefficient similar kind flavor probably see true positive divide case except ignore true negative case okay mean fraction true positive point ignore true negative case therefore computation positive negative different asymmetric measure however perfect cluster jaccard coefficient one know case actually cover nt false ca case rand statistic take true case true positive plus true negative divide possible pair okay mean nt cover anything like negative okay s s perfect cluster rand statistic symmetrical take positive negative case equally another interesting measure call fowlkes-mallow measure measure geometric mean precision recall remember study f measure harmonic mean precision recall geometric mean fowlkes-mallow measure essentially precision time recall get square root introduce measure want calculate example table like one green table use measure calculate number leave calculation exercise instead spending time lecture [ music ] 

cluster validation [ sound ] session introduce internal measure cluster validation remember discuss several kind external measure lucky thing external measure [ inaudible ] expert knowledge prior unfortunately many case [ inaudible ] s reason rely internal measure internal measure base concept cluster mean want point within cluster mean intra-cluster compact close want point different cluster mean inter-cluster distance want far apart possible mean want far separated s reason want maximize intra-cluster compactness also maximize inter-cluster separation 
internal measure ( ) betacv measure  trade-off maximize intra-cluster compactness inter-cluster separation  give cluster c = { c1 ck } k cluster cluster ci contain ni = | point  let w ( r ) sum weight edge one vertex r 1 k  sum intra-cluster weight cluster win = ∑ w ( ci ci ) 2 1 k k −1 1  sum inter-cluster weight = wout = w ( ci ci ) ∑∑ w ( ci c j ) ∑ 2=i 1 i 1 j >  number distinct intra-cluster edge n  n = ∑   1  2  k number distinct inter-cluster edge n = k −1 k n n ∑ ∑ j win n = 1 j = 1 beta-cv measure betacv =   2 wout n  ratio mean intra-cluster distance mean inter-cluster distance  smaller better cluster want tradeoff thing let s look define suppose give cluster c k cluster c sub 1to c sub k cluster c sub contain n sub number point introduce function w r sum weight edge one verdict r example will get two point say point cluster point cluster want look weight weight usually defined like euclidean distance whatever used cluster base say sum intra-cluster weight cluster defined way reason see c sub c sub i mean vertex cluster s s inter-cluster one point point b actually use calculate twice b b be actually calculate twice s divide two okay cluster sum together one k s cluster define sum inter cluster weight calculate way obvious one point one vertex cluster c sub one c sub i simply say cluster s s intro cluster since two point calculate twice s divide sum essentially s one k cluster get wout okay s inter-cluster weight sum together also see number distinct intra-cluster edge s cluster n sub number point intra-cluster link end point choose two s get formula get cluster one k sum s get n sub m inter-cluster n sub notated one point cluster point cluster j never cluster want calculate edge among different cluster s number distinct inter-cluster edge betacv measure defined wi divide n think mean intra-cluster distance get wout divide number think mean inter-cluster distance betacv essentially ratio mean inter-cluster distance versus mean intra-cluster distance course measure smaller better cluster smaller mean pretty small quite compact bigger mean s separated 
internal measure ( ii ) normalize cut modularity  = cut nc  k k w ( ci ci ) k w ( ci ci ) k w ( ci ci ) 1 = = = ∑ vol ( c ) ∑ ∑ ∑ w ( ci ci ) ( v ) w c + ( ) ( ) w c c w c c i 1 = = = 1 1 1 1 w ( ci ci ) vol ( ci ) = w ( ci v ) volume cluster ci higher normalize cut value better cluster  w ( c c )  w ( c v ) 2   modularity ( graph cluster ) q ∑ = −    ( ) ( ) w v v w v v 1      modularity q defined k k   3 k k 2 ( win + wout ) w ( v v ) = w ( ci v ) = w ( ci ci ) + ∑ w ( ci ci ) = ∑ ∑ i 1 i 1 i 1 modularity measure difference observed expect fraction weight edge within cluster smaller value better clustering—the intra-cluster distance lower expect s betacv measure defined look definition one popular use one call normalize cut normalize cut defined follow normalize cut ratio defined sum k cluster every cluster calculate way explicitly interpret way way inter-cluster weight sum intra inter-cluster weight ratio actually sum one k will get normalize cut ratio course base want inter-cluster far separated compare intro one s bigger normalize cut value better cluster s another definition call modularity especially popular graft cluster defined way proceed part normalize cut cut formula okay like part square okay simply say one observed fraction weight within cluster expect one modularity actually measure difference observed one expect one smaller value better cluster inter-cluster distance lower expect one modularity definition used graph cluster [ music ] 

relative measure 
relative measure  relative measure directly compare different clustering usually obtain via different parameter setting algorithm silhouette coefficient internal measure check cluster cohesion separation min µout ( xi ) − µin ( xi ) =  point xi silhouette coefficient si min max { µout ( xi ) µin ( xi ) } µin ( xi ) mean distance xi point cluster min ( xi ) mean distance xi point closest cluster µout 1 n  silhouette coefficient ( sc ) mean value si across point sc = ∑ si n 1  sc close 1 imply good cluster  point close cluster far cluster  silhouette coefficient relative measure estimate # cluster datum 1 pick k value yield best cluster ie yielding high sci = ∑ j value sc sci ( 1 ≤ ≤ k ) ni x ∈c  j 2 

cluster stability [ sound ] session go study another issue cluster assessment call cluster stability 
cluster stability clustering obtain several dataset sample underlie distribution similar “ stable ”  typical approach  find good parameter value give cluster algorithm  example find good value k correct number cluster  bootstrapping approach find best value k ( judge stability )  generate sample size n sampling replacement  sample di run cluster algorithm k value 2 kmax  compare distance pair clustering ck ( di ) ck ( dj ) via distance function  compute expect pairwise distance value k  value * exhibit least deviation clustering obtain resample dataset best choice k since exhibit stability  2 cluster stability basically say whether get right parameter get stable cluster result mean assume get datum set take sample get several dataset datum set cluster assume be cluster finally get stable cluster result s good cluster usually use find good parameter value give cluster algorithm example find good value k good number cluster find probably look graph probably see set k three even two probably find quite reasonable cluster set four five matter smart probably cluster find may quite stable extent may try test cluster s stability introduce one method call bootstrapping approach find best value k judge base stability bootstrapping basically say take tt sample size n sampling sampling replacement s every time take almost take time get sample sub 1 sub sub j sub t okay run cluster algorithm k value two maximum k like okay compare distance pair cluster example k may try see whether sub sub j two different sample dataset see whether finer get sample want compute expect pairwise distance value k mean suppose k get ten get least ten cluster want see pairwise distance okay see value k exhibit least deviation cluster mean cluster sample sub sample sub j least deviation okay k stable one thus may want k value s one application test cluster stability find best k actually many method find k appropriate number cluster one method call empirical method actually person give empirical formula example total datum set n point may take square root half n point okay example n 200 expect value need get number cluster course may work small number point get really big number point example get many point get value will get k solvent probably may want get solvent cluster even many point 
method find k number cluster empirical method  # cluster k ≈ n 2 dataset n point ( eg n = 200 k = 10 )  elbow method use turn point curve sum within cluster variance respect # cluster  cross validation method  divide give datum set part  use – 1 part obtain cluster model  use remain part test quality cluster  example point test set find closest centroid use sum square distance point test set closest centroid measure well model fit test set  k > 0 repeat time compare overall quality measure wrt different k ’ find # cluster fit datum best  3 another method person use call elbow method elbow method mean try base number cluster go one two three four get number cluster get sum within square variance mean look average within cluster variance try look see know best number k want see elbow point turn point s one method another method find best number k use cross validation mean divide give dataset part take one part test datum remain part cluster okay time check overall quality cluster test quality example okay suppose use minus 1 part cluster find k cluster point testing set try find s closest centroid base try find point dataset try find sum square distance mean try find square error sse introduce usually try see whether get best fit test dataset mean get smallest sum square distance since cross validation repeat time compare overall quality measure respect different k s will find best number fit datum best mean get overall quality measure get lowest sse particular k usually right number cluster [ music ] 

cluster tendency [ sound ] session be go study another important issue call cluster tendency cluster tendency 
cluster tendency whether datum contain inherent grouping structure assess suitability cluster  ( ie whether datum inherent grouping structure )  determine cluster tendency clusterability  hard task many different definition cluster  eg partition hierarchical density-based graph-based etc  even fix cluster type still hard define appropriate null model datum set  still clusterability assessment method  spatial histogram contrast histogram datum generate cover random sample  distance distribution compare pairwise point distance datum randomly generate sample  hopkin statistic sparse sampling test spatial randomness  2 mean want study whether datum set really contain cluster whether datum contain inherent grouping structure otherwise even use random generate datum may find certain number cluster however really make sense mean want assess whether datum suitable cluster however determine cluster tendency clusterability hard task many definition cluster example study partition method hierarchical method density-based method graph-based method different method may different definition cluster s study whether datum clusterable pretty hard however even fix cluster type example study partition method still hard define appropriate null model datum set however still study clusterability assessment example method like spatial histogram method distant distribution method hopkin statistic general philosophy try compare measure measure generate random sample see whether rather different example spatial histogram method try contrast histogram datum histogram generate random sample see whether rather different distance distribution try compare pairwise point distance datum pairwise distance randomly generate sample hopkin statistic use sparse sampling test spatial randomness since carry quite similar spirit introduce one spatial histogram method 
testing cluster tendency spatial histogram approach spatial histogram approach contrast d-dimensional histogram input dataset histogram generate random sample  dataset clusterable distribution two histogram rather different  method outline  divide dimension equi-width bin count many point lie cell obtain empirical joint probability mass function ( epmf )  randomly sample datum  compute much differ used kullback-leibler ( kl ) divergence value  3 spatial histogram approach general philosophy try construct d-dimensional histogram input example two may construct two dimensional histogram input dataset d histogram generate random sample mean want compare figure generate left generate right figure okay datum clusterable two histogram distribution rather different mean really generate different distribution likely dataset clusterable okay concrete method try divide dimension equal waste bin try see cell many point cell generate empirical joint probability mass function okay random generate datum set will will generate empirical joint probability mass function try compare whether two differ quite lot mean be go compare much differ typical method use kl divergence mean calculate kl divergence value kl divergence kullback-leibler divergence defined base formula be go introduce detail kl divergence want learn check textbook statistic mission learn datum mining general introduce measure will find different random sample say datum set clusterable 
recommend reading  m j zaki w meira datum mining analysis fundamental concept algorithms cambridge university press 2014  l hubert p arabie compare partition journal classification 2193–218 1985  a k jain r c dube algorithms cluster datum printice hall 1988  m halkidi y batistakis m vazirgiannis cluster validation technique journal intelligent info system 17 ( 2-3 ) 107–145 2001  j han m kamber j pei datum mining concept technique morgan kaufmann 3rd ed 2011  h xiong z li cluster validation measure ( chapter 23 ) c aggarwal c k reddy ( ed ) datum cluster algorithms application crc press 2014 4 finally be go introduce textbook chapter general paper discuss cluster validation measure especially show way summary many research paper zaki s book give lot detail measure cover lecture thank [ music ] 

1 
motivation harness big text datum • text datum ubiquitous grow rapidly – internet – blog – news – email – literature – twitter –… knowledge many application 2 
main technique harness big text datum text retrieval + text mining text retrieval big text datum big text datum text mining small relevant small datum relevant datum knowledge many application 3 
design cs410 overview online video + high engagement mooc 1 text retrieval big text datum big text datum mooc 2 hi text mining project & tech review small relevant small datum relevant datum knowledge many application 4 
design cs410 goal • emphasize theory practice – theory basic concept general principle applicable application è lecture + quiz + exam – practice specific practical skill immediately useful è programming assignment – integration theory practice è course project 5 
design cs410 goal • personalize learn è self pace + choice project & technology review • collaborative learn è forum-based interaction collaboration è group project group technology reviews 6 
design cs410 format & grade synchronous weekly office hour via video-teleconference asychronous question answer & discussion via forum 25 % 30 % 25 % mooc 1 text retrieval mooc 2 text mining lecture video quiz hi lecture video quiz exam programming exam programming 20 % course project extra credit + 5 % tech review topic selection 5 % proposal 5 % progress report 5 % software deposit 65 % presentation 20 % 7 
complete control grade • • • • • • + [ 95100 ] [ 9094 ] - [ 85 89 ] + [ 80 84 ] b [ 75 79 ] - [ 7074 ] • c [ 60 69 ] • [ 5559 ] • f < 55 5 % extra credit would help move grade one bracket 8 
work load aug first day instruction sept oct nov dec thanksgiving break last day instruction lecture video quiz proctore exam programming assignment last 2 week project technology review 9 
already take mooc ( ) • encourage finish quiz programming assignment much earlier • however take exam earlier programming task may require synchronization thus finished earlier schedule • enjoy time work course project ( want ) 10 
forum discussion • forum ( piazza ) primary way interaction engagement – asynchronous discussion enable participation everyone – enable faster question answer without wait office hour – facilitate identification difficult concept cover office hour 11 
protocol question answer • soon question issue discuss post immediately forum • question answer timely manner forum address adequately email question us ( ie instructor ta ) used subject line contain keyword “ cs410dso ” • ’ receive reply us email timely manner join office-hour ( ie video-conference ) 12 
format office hour • ta instructor hold weekly office hour publish time slot used video-conference ( zoom ) • student leave office hour need time • priority list descend order – high issue post forum unresolved even email communication instructor – medium unresolved issue forum – low question issue post forum bring student join office hour ( first come first serve ) 13 
get cs410 dso • plan ahead base schedule act early – allocate sufficient time preparation two proctore exam – try complete quiz programming assignment ahead time whenever possible • post question forum immediately whenever difficulty understand part course material • leverage collaborative learn – actively participate forum discussion ( ’ learn read post forum ) – earn 5 % extra credit make effort answer other ’ question forum ( effort logged forum ) 14 
information visit course website https welcome 15 

university illinois urbana-champaign 
table content general information cs @ illinois history cs graduate program admission requirement current illinois grad admission process change curriculum within cs financial aid open offer policy graduate assistantship assignment teach assistantship appointment cs research assistantship appointment cs plan document assistantship cs graduate assistant benefit overview speak english proficiency requirement limit status sure contact university & department directory information professional mcs program ms program program requirement ms thesis advisor program purpose estimate timeline first year milestone coursework find thesis advisor qualify exam purpose deadline qualify exam policy qualify exam statement conditional pass qualify exam fail qualify exam guidance prepare committee purpose timeline establish committee committee member advice form committee preliminary exam purpose exam policy guidance write proposal 1 1 1 2 3 3 4 5 6 6 7 7 7 8 10 10 10 11 12 13 14 14 14 15 15 16 18 18 18 18 18 18 19 19 19 20 20 20 20 21 21 21 21 22 final exam 23 purpose exam policy guidance prepare 23 23 24 graduate annual evaluation thesis deposit graduation process registration 24 24 25 25 full-time status late registration dropping course withdraw no-credit minimum grade requirement 26 26 26 27 27 27 re-entry cs graduate program graduate petition transfer coursework probation special grade code conduct policy student ethic academic integrity grievance process visit academic office spr internship cs grad curricular practical training ( cpt ) occupational practical training ( opt ) graduate study committee counsele service college engineering career service engineering technical service group cs & ece corporate connection program graduate office space mailbox communication skill general advice cs graduate student opportunity leader academic office graduate program staff computer science student organization important website advice fellow grad & faculty important 2015-2016 date & deadline new graduate student checklist 27 28 29 29 29 30 30 31 31 32 32 35 35 35 35 36 36 36 36 38 38 39 40 41 42 44 47 
department computer science wwwcsillinoisedu general information computer science public website wwwcsillinoisedu computer science graduate home page http graduate-student cs @ illinois history department computer science one longest establish computer science department world digital computer laboratory home design construction first two illiacs evolved department 1964 go create illiac iii iv computer home one nation ’ lead computer science program thomas m siebel center computer science serve live laboratory explore evaluate emerge compute environment center encompass 225000 square foot classroom laboratory office feature compute communication infrastructure integrate ubiquitous digital video capture facility high-resolution display device touch screen information panel highperformance wire wireless networking handheld cluster desktop computer system department use center intelligent information system make helpful tool everyday research task user control build ’ system customize work environment new application environment continuously develop student faculty staff deploy center short department provide member integrate state art education research facility environment allow researcher investigate apply technology related pervasive compute social compute multimedium infrastructure build intelligence security privacy build – environment attract faculty student promise shape future compute illinois world currently department 60 faculty member four graduate program enrolling 500 student undergraduate program enroll approximately 1550 student cs major six “ cs + x ” blended program college liberal art science grainger engineering library information center locate conveniently block away department center one national digital library initiative work several publisher transform material electronic form make accessible via network grainger library part university illinois library ( one largest country ) together library 28 academic institution state illinois accessible terminal via computerize catalog university illinois international reputation research institution research facility abound many department allow ample scope interdisciplinary study also cultural center offer many amenity large city krannert art museum krannert center perform art year-round program concert theatre dance give local visit artist information visit university ’ main website wwwillinoisedu cs graduate program department computer science offer graduate program lead degree ms professional mcs ( online campus ) ms bioinformatic ms require 28 credit hour coursework 4 credit hour thesis may count toward require 2015-2016 cs graduate student handbook http graduate-student 1 
department computer science wwwcsillinoisedu minimum 96 credit hours—minimum 48 credit hour coursework minimum 32 credit hour thesis professional mcs require 32 credit hour coursework thesis department computer science also offer professional mcs degree online ( i2cs ) deliver course content digital audio video technology i2cs program offer flexibility student allow participate program either full-time part-time non-degree student full-time student typically enroll 9 credit hour coursework per semester part-time student usually employ full-time typically enroll 4 credit hour coursework per semester student need admit university illinois graduate program register offcampus non-degree student non-degree student interested complete mcs degree strongly encourage complete admission requirement within first semester 12 credit hour non-degree course may transfer degree program current university illinois undergraduate student enrolled college engineering junior standing enroll bs-ms program bs-mcs program five-year program student enter bachelor ’ degree may earn ms way doctorate within first four semester student demonstrate core knowledge particular area computer science complete specify course qualify examination must attempt fourth semester focus student ’ individual research interest two examination complete student preliminary examination thesis proposal initial research result final examination defense thesis conclusion research student complete ms degree 5 semester complete degree ( count summer term ) student complete professional mcs degree campus 3 semester complete degree ( count summer term ) online student ( i2cs program ) 5 year complete program student complete degree 7 year complete program 6 year approve ms admission requirement prospective student & current student student apply ms program consider fall admission december 15th application deadline professional mcs program ms bioinformatic program fall spring admission application deadline october 15th spring term january 15th fall term application material include transcript reference letter test score must receive academic office deadline regardless financial aid consideration information application process go http graduatestudent 2015-2016 cs graduate student handbook http graduate-student 2 
department computer science wwwcsillinoisedu current illinois ( urbana-champaign ) grad admission process current university illinois graduate student department may apply cs graduate degree program submit graduate college curriculum change form http curriculum_transfer_form_offlinepdf form must include support signature student ’ current advisor home department departmental authorize signature home department along graduate college petition student must also submit follow item  detailed “ statement purpose ” include background want complete cs graduate degree  two letter recommendation require two faculty member illinois apply ms program one letter cs faculty member willing serve advisor apply program one letter cs faculty member willing serve advisor provide funding  name one two additional illinois faculty staff may provide recommendation need  current cv resume include publication  cs interest form locate https loginasp ms phd applicant student must submit application material october 15th consider spring term december 15th consider fall term entry please note ms cs program fall term entry change curriculum within computer science ms phd cs student ms program wish switch program must go application process outlined current student must complete graduate college curriculum change form http curriculum_transfer_form_offlinepdf place apply application form petition form student must indicate wish switch ms curriculum petition must support comment sign cs faculty member agree serve thesis advisor along graduate college petition student must submit follow item  detailed “ statement purpose ” focus student wish complete phd  two letter recommendation require one letter must cs faculty member agree supervise doctoral research letter must indicate research assistantship provide letter another illinois faculty member  name one two additional illinois faculty staff may provide recommendation need  current cv resume include publication  cs interest form locate https loginasp  support information might give faa ( admission ) committee full picture potential successfully pursue doctorate student must submit application material december 15th consider fall term curriculum change 2015-2016 cs graduate student handbook http graduate-student 3 
department computer science wwwcsillinoisedu ms professional master ’ ( mcs ) student must submit graduate college curriculum change form http curriculum_transfer_form_offlinepdf academic office request curriculum change later end term prior graduation ( eg december may graduation ) thesis advisor already secure advisor must endorse petition state student inform advisor longer wish complete thesis research would like complete professional mcs degree program leave university student accept cse option either ms advisor ’ written consent change professional mcs also require please note student switch ms professional mcs allow switch back ms program allow hold assistantship cs department financial aid ms cs graduate student fellowship research assistantship teach assistantship department offer assistantship fellowship professional mcs graduate student event professional mcs student secure tuition waiver-generating assistantship another campus unit department computer science seek tuition reimbursement appoint unit number financial aid offer depend department ’ available financial resource competitively make student place top ranking faa committee renewal assistantship fellowship base solely academic progress student work performance student obligation department determine original letter admission award funding base degree program student apply department approve change degree program time period award renewal or continuation funding may affected assistant work supervision individual faculty member assist research teach assistantship department computer science usually 50 % appointment 50 % appointment require student devote average 20 hour per week appointment limit amount assistantship support available two-month summer session student teach ( ta ) graduate ( ga ) assistantship belong bargaining unit graduate employee ’ organization ( geo ) organization recognize university exclusive representative wage hour term condition employment employee within bargaining unit information regard policy go indexhtm fellowship competitively award base performance condition stipulate fellowship fellowship provide complete support include tuition fee waiver one academic year fellowship may combine research teach assistantship thereby provide fellow larger overall stipend qualify fellowship well assistantship 25 % 67 % time include tuition service fee waiver student hold fellowship require register minimum 8 credit hour qualify tuition service fee waiver fall spring term minimum 4 credit hour summer term term external fellowship require enrollment greater number credit hour fellow external fellowship must meet term external fellowship 2015-2016 cs graduate student handbook http graduate-student 4 
department computer science wwwcsillinoisedu student hold award give spring semester may also register follow summer session without pay tuition service fee approximately $ 600 per semester miscellaneous fee cover tuition service fee waiver current start salary half-time assistant department computer science around $ 19200 nine-month academic year student whose native language english regardless us citizenship require state law submit pass score tse ( pass score 50 ) epi ( pass score 5 higher ) toefl ibt-speaking subsection ( pass score 24 ) ielts-speaking subsection ( pass score 8 ) eligible teach assistantship ( ta ) please aware pass score english exam automatic award ta student interested explore teach assistantship opportunity contact kara macgregor kmacgreg @ illinoisedu graduate student department assistantship also employment opportunity department administrative unit campus however department mechanism report inform student employment possibility learn opportunity visit graduate college assistantship clearinghouse web page indexcfm list hourly position assistantship available graduate student professional mcs student secure tuition waiver-generating assistantship another campus unit department computer science seek tuition reimbursement appoint unit open offer policy department computer science offer incoming graduate student “ open offer ” mean hold either teach research assistantship fellowship duration program ms student 5 semester student 5 year student receive “ open offer ” financial assistance require meet follow criterium order maintain “ open offer ” status student must            secure thesis advisor end first academic year complete require esl course end first academic year ( international student ) attempt qualify exam 4th semester pass qualify exam 5th semester ( student ) take leave absence program ’ f ’ course less 3 c ’ coursework probation status ( overall graduate gpa 30 ) submit publish least one paper preliminary exam meet advisor least month complete plan document ( funding request upcoming term ) deadline complete graduate student self-evaluation deadline satisfactory progress graduate student annual evaluation time addition student expect pass prelim exam 8th semester 2015-2016 cs graduate student handbook http graduate-student 5 
department computer science wwwcsillinoisedu graduate assistantship assignment graduate student award research teach assistantship must complete require paperwork semester finalize appointment prior appointment start date – august 16th fall term january 1st spring term complete appointment process contact computer science business office 2210 siebel center failure complete process specify date semester delay student ’ appointment start date well first paycheck acceptance appointment require student present available supervisor appointment period – august 16th december 31st fall term january 1st may 15th spring term student must away responsibility must receive prior approval supervisor academic office failure present may result termination nonreappointment assistantship vacation must schedule university close holiday outside appointment date information regard graduate college university guideline graduate assistantship visit indexhtm http assistantship international student social security number issue student “ employ ” campus would include student hold assistantship hourly appointment prior secure social security number ( ssn ) university issue temporary control number ( tcn ) need process assistantship graduate hourly appointment student fellowship need tcn number process paperwork tcn number available id production office illini union bookstore number issue receive icard please return office request teach assistantship appointment cs department commit maintain high level quality teach assistant ( ta ) appointment program student meet follow requirement order eligible ta appointment computer science  must good academic standing graduate program  must pass speak english proficiency score ( international student ) + toefl ibt + ielt + epi ( university ’ speak exam ) must complete cs assistantship plan document state deadline  new ta must complete center innovation teach & learn ( citl ) ta orientation week prior start class enroll cs 591ta seminar  return ta receive satisfactory evaluation faculty supervisor previous ta appointment allow one semester improve performance addition require complete retake cs 591ta seminar  ta switch ta ra appointment july 15st fall term december 15th spring term ta appointment oversight professor chandra chekuri ( chekuri @ illinoisedu ) primary contact person inquire ta appointment kara macgregor ( kmacgreg @ illinoisedu ) ta contract fall term generally post end april spring term end november 2015-2016 cs graduate student handbook http graduate-student 6 
department computer science wwwcsillinoisedu ta evaluation process semester ta require complete self-evaluation addition faculty supervisor complete ta evaluation form include section share feedback ta effort help learn grow ta experience ta evaluation process take place end october fall end march spring end july summer continuous ta training ta include student complete summer internship vacation fall spring term need back campus begin appointment period generally week prior start class semester start mandatory ta kick-off meeting take place first day class addition kick-off meeting student opportunity attend additional teach workshop center innovation teach & learn ( citl ) time workshop post http 836 citl also offer teach certificate program student interested teach complete research assistantship appointment cs research assistant ( ra ) appointment make directly faculty student interested secure ra appointment need consult faculty kim bogle ( kbogle @ illinoisedu ) business office 2210 siebel center issue ra contract faculty member confirm ra appointment research area faculty inform student ra appointment december 15th spring term may 15th fall term plan document assistantship cs semester fall spring ms student department require complete “ plan document ” form inform department thesis advisor type appointment request upcoming term plan document locate mycsillinoisedu “ cs tool ” menu graduate assistant benefit overview high level overview type benefit available graduate assistant detailed information visit indexhtm insurance health care student eligible participate university graduate student health insurance plan university provide service mckinley health center counsele center coverage may also purchase spouse dependent same-sex domestic partner review student insurance policy premium visit http call 333-0165 tuition fee waiver student hold assistantship appointment 25-67 % time least three-fourth term ( 91 day fall spring term 41 day summer ) eligible tuition fee waiver student must enrolled good academic standing appointment university holiday follow holiday observed university student require work day appointment 2015-2016 cs graduate student handbook http graduate-student 7 
department computer science wwwcsillinoisedu  christmas eve & day  memorial day  thanksgiving & day  new year s day  labor day  day determine president university  martin luther king s birthday  fourth july sick leave accrue depend appointment please visit “ summary benefit employment category chart ” locate indexhtm information bereavement leave eligible three day paid leave immediate family member same-sex domestic partner household member in-law grandchild or grandparent unpaid leave may grant appointment upon request sole discretion department university speak english proficiency requirement ( international student ) section student whose native language english even permanent resident unite state ( green card ) check exemption requirement base country citizenship visit exemptcountry department computer science require student admit program fall 2011 later whose native language english pass speak english proficiency score take qualify exam phd student admit prior fall 2011 whose native language english must pass speak english proficiency score prior attempt preliminary exam student must also pass score eligible hold ta position student pass score toelf ibt ( 24 ) ielt ( 8 ) must complete university illinois ’ english proficiency interview ( epi ) exam retake toefl ibt ielt speak exam illinois ’ epi exam student allow three chance pass ( 5 higher ) department ’ guideline policy epi exam outlined      student must register epi exam kathy runck ( krunck @ illinoisedu ) academic office student come late exam show exam cancel less 7 day prior exam might able get another epi appointment semester incoming student toefl ibt speaking section score 22 ielt speaking section score 6 must complete either 1 ) 10 hour approve tutor session 2 ) esl 504 506 510 take epi exam student fail first attempt epi exam must complete either 1 ) 10 hour approve tutor session 2 ) complete esl 504 506 510 eligible take epi second time student receive “ conditional pass ” epi exam must complete esl 508 receive pass grade “ ” course earn pass epi score cs graduate student conditional pass must first complete pass esl 508 award ta assistantship ( department ’ policy ) 2015-2016 cs graduate student handbook http graduate-student 8 
department computer science wwwcsillinoisedu epi score process epi assess test taker ’ speaking ability term five feature fluency linguistic accuracy discourse management question handle listen listener effort  fluency smoothness delivery amount hesitation re-start  linguistic accuracy include clear pronunciation grammar without noticeable error sophisticated vocabulary  discourse management ability develop idea rhetorical organization quantity disclosure  question handle listen ability give appropriate answer negotiation skill communication  listener effort ease difficulty understand test taker ’ speech description epi score level level 6 5 4cp 4 3 2 description communication always effective speaker sophisticated language skill appropriate teach context communication generally effective speaker satisfactory language skill range appropriate teach context communication generally effective however due isolate weakness communication occasionally difficult esl coursework require first semester teach order refine speaker ’ language skill teach context communication somewhat effective inconsistent performance indicate speaker ready classroom instructor communication marginally effective speaker limit language skill teach context communication generally effective speaker unsatisfactory language teach context result pass student permit ta restriction conditional pass student require successfully complete esl 508 hold ta appointment non-passing student permit ta must retake exam score 2 3 4 4cp ( conditional pass ) rater determine follow aspect ( ) candidate ’ oral english proficiency critical concern  fluency ( flow smoothness speech )  language form accuracy  pronunciation  grammar  vocabulary  idea development organization  question handle listen skill  factor salient decision information epi exam locate http mainhtml review english language proficiency policy ta visit http taengprofhtm 2015-2016 cs graduate student handbook http graduate-student 9 
department computer science wwwcsillinoisedu limit status student meet one graduate admission requirement may approve admission “ limit status ” common reason limit status  course deficiency determine department  low undergraduate gpa ( 30 )  comparable bachelor degree  lack demonstrated english language proficiency student admit limit status must address deficiency first semester program order continue english language proficiency ( international student ) international student meet require english language proficiency standard may place “ limit status ” graduate college time admission international student limit status require take esl placement test ( ept ) arrive campus department linguistic ( formerly deil ) administer test result exam determine whether student require enroll english second language course ( ) may reduce number academic course give term enrollment esl course take place result receive student must meet condition include pass esl coursework limit status within first year graduate study student require complete requirement order earn degree illinois urbana-champaign information ept test visit http test schedule visit http grreghtml information el course visit http sure contact sure right person contact correct process stop computer science academic office 1210 siebel center staff direct right person process addition comment concern idea non-academic related topic please send email academic @ csillinoisedu stop office provide staff feedback feedback used help enhance non-academic part cs graduate student life student university & department directory information graduate student may withhold home address phone number university staff directory suppress information learn university directory system visit http additional information contact cite helpdesk consult @ illinoisedu 333-7500 2015-2016 cs graduate student handbook http graduate-student 10 
department computer science wwwcsillinoisedu professional master ’ ( mcs ) program professional master computer science ( mcs ) program non-thesis non-research program terminal degree design allow student complete 32 credit hour program little one year within maximum 3 semester on-campus student 5 year online student oncampus student require register 12 credit hour per semester student fewer 12 credit hour remain final semester may request permission register 12 credit hour addition student program eligible switch ms program student enrolled program eligible graduate assistantship department computer science mcs student secure tuition waiver-generating graduate assistantship campus unit computer science seek tuition reimbursement hire unit viveka p kudaligama kudaliga @ illinoisedu serve academic advisor mcs student assist matter relate academic program include course selection degree progress student encourage meet viveka review academic progress program requirement professional master computer science ( mcs ) non-thesis degree require 32 hour graduate coursework 1 ) breadth requirement 12-16 credit hour student must complete four different course different area follow eight core area minimum grade -  architecture compiler parallel compute cs 426 431 433 435 462 483 526 533 536  artificial intelligence cs 440 443 446 543 546 548 549  database information system bioinformatic cs 410 411 412 466 511 512  programming language formal method software engineering cs 421 422 427 428 476 477 522 524 527 528 576  hci cs 417 418 419 465 467 519 565  system networking ( include real-time system security ) cs 414 423 424 425 438 439 461 463 523 525 538 541 545 563  scientific compute cs 450 457 482 554 555 556 558  theoretical computer science cs 475 571 573 574 579 583 2 ) student must complete 12 hour 500-level computer science coursework-cs 500 cs 590 cs cs 597 approve non-computer science 500-level course may satisfy four credit hour requirement 3 ) maximum 4 hour cs 591 or cs 491 may apply toward degree 4 ) minimum 24 hour mcs coursework must take computer science course offer university illinois urbana-champaign 5 ) 12 semester credit hour previous graduate course work approve department may transfer apply professional mcs degree requirement 2015-2016 cs graduate student handbook http graduate-student 11 
department computer science wwwcsillinoisedu 6 ) degree requirement must complete three semester ( summer count ) on-campus student 5 year online student student wish complete program one year two example complete requirement example one ( fall spring ) term course fall term cs 598 cs 411 cs 440 cs 425 total credit spring term cs 511 cs 543 cs 418 cs 597 cs 591 total credit credit hour 4 4 4 4 16 4 4 3 4 1 16 requirement 500-level requirement breadth requirement ( dais group ) breadth requirement ( ai group ) breadth requirement ( system & networking group ) 500-level requirement 500-level requirement breadth requirement ( hci group ) elective course elective course example two ( fall spring summer ) term course fall term cs 598 cs 411 cs 440 cs 591 cs 591 total credit spring term total credit summer term total credit cs 511 cs 543 cs 418 cs 426 cs 591 cs 597 credit hour 4 3 3 1 1 12 4 4 3 4 1 16 4 requirement 500-level requirement breadth requirement ( dais group ) breadth requirement ( ai group ) elective course elective course 500-level requirement 500-level requirement breadth requirement ( hci group ) breadth requirement ( architecture group ) elective course elective course 4 student plan complete degree requirement three semester ( count summer term ) would complete 12 credit hour first semester 12 credit hour second semester 8 credit hour last semester international student need complete “ coursework under-load ” form isss office since final semester program 12 credit hour ms ( thesis ) program ms program research-thesis base master ’ program allow student opportunity apply continue education program degree require 28 credit hour graduate coursework 4 credit hour thesis research total 32 credit hour complete within 2015-2016 cs graduate student handbook http graduate-student 12 
department computer science wwwcsillinoisedu maximum 5 semester include summer term student enrolled ms program may award assistantship academic office serve advisor ms student submit thesis advisor agreement thesis advisor secure she assume responsibility academic advise addition direct thesis research recommend student meet advisor regular basis ensure track academically coursework research program requirement master science ( ms ) research-orient degree require 28 hour graduate coursework 4 hour thesis research fulfill first stage ( 32 hour ) 96-hour computer science phd program 1 ) breadth requirement 9 – 12 credit hour student must complete three different course different area follow eight core area minimum grade -  architecture compiler parallel compute cs 426 431 433 435 462 483 526 533 536  artificial intelligence cs 440 443 446 543 546 548 549  database information system bioinformatic cs 410 411 412 466 511 512  programming language formal method software engineering cs 421 422 427 428 476 477 522 524 527 528 576  hci cs 417 418 419 465 467 519 565  system networking ( include real-time system security ) cs 414 423 424 425 438 439 461 463 523 525 538 541 545 563  scientific compute cs 450 457 482 554 555 556 558  theoretical computer science cs 475 571 573 574 579 583 2 ) advanced coursework 12 credit hour  course must complete breadth requirement  additional course 500-level must complete one three core area student choose mean least two course complete one core area  two additional 500-level course must complete may choose cs course numbered 500-590 598 ( note cs 597 591 count towards advanced coursework )  cs 599 ( thesis ) may satisfy four credit hour requirement count towards need two course one core area count towards total 28 hour coursework require degree example breadth advanced coursework requirement  breadth requirement cs 433 ( architecture ) cs 523 ( system & networking ) cs 450 ( scientific compute )  advanced coursework cs 538 ( 2nd 500-level course system networking ) cs 598 cs 599 2015-2016 cs graduate student handbook http graduate-student 13 
department computer science wwwcsillinoisedu 3 ) master ’ thesis registration four credit hour cs 599 associate thesis research require student ’ responsibility secure ms thesis advisor start work thesis research later begin third semester program 4 ) minimum 16 hour coursework hour must take computer science course university illinois urbana-champaign 5 ) maximum 4 hour cs 591 or cs 491 may apply toward either degree 6 ) 12 semester credit hour previous graduate course work approve department may transfer apply ms degree requirement 7 ) degree requirement must complete five semester ( summer count ) student obtain ms degree along way flexibility complete ms 8 ) student ms program offer admission program must complete ms degree prior start degree 9 ) international student f-1 student visa must register semester deposit ms thesis 1-20 still valid semester ms thesis advisor student require secure thesis advisor later end first academic year thesis advisor identify “ cs 599 thesis advisor agreement ” form must complete student complete online form locate mycsillinoisedu “ grad student toolbox ” paper form locate http official-form complete form submit student give appropriate crn ( course registration number ) register advisor ’ section cs 4 graduate hour cs 599 allow ms degree program program purpose program design guide student difficult process become independent researcher educator goal student 1 ) become scholar absorb large body research literature critically analyze stateof-the-art include shortcoming 2 ) become effective communicator learn express idea clearly writing individual meeting public seminar 3 ) become innovator create new theory technology paradigm advance state art end program student become expert research field colleague faculty successful student drive passion develop creative idea make impact intellectual contribution 2015-2016 cs graduate student handbook http graduate-student 14 
department computer science wwwcsillinoisedu estimate timeline student take anywhere 4 7 year complete program student enter program  approve external ms degree maximum 6 year complete program ( see section minimal requirement )  bachelor ’ degree earn ms illinois along way maximum 7 year complete program  ms degree university illinois urbana-champaign prior enter phd program maximum 7 year time start ms degree complete program ( 2 year ms degree 5 year degree ) average student enter program bs degree computer science take 5 year complete program variation due many factor prior experience career goal type research student interested faculty position may take 6 year substantial time require develop solid publication record become know other research community student together advisor determine appropriate pace complete program study form ( see ) typical schedule student enter bs computer science show student must attempt qualify exam 4th semester milestone guideline year 1 semester 1 2 2 3 4 5 6 7 8 9 10 3 4 5 milestone design submit program study select advisor complete esl requirement pass speaking exam ( international student ) take qualify exam preliminary exam ( thesis proposal ) final exam ( thesis defense ) first year milestone  student determine research interest find advisor  student plan coursework prepare qualify exam research  student must complete program study form december 15th fall term obtain approval three program study committee member process describe “ coursework ” section  student must find thesis advisor student must secure thesis advisor take qualify exam agreement make faculty member student “ cs 599 thesis advisor agreement ” form locate http - 2015-2016 cs graduate student handbook http graduate-student 15 
department computer science wwwcsillinoisedu official-form paper version mycsillinoisedu electronic version must complete submit academic office student give appropriate crn ( course registration number ) register advisor ’ section cs 599 usually occur either semester qualify exam take qualify exam pass  student must complete self-evaluation form annual process april 1st self-evaluation student must outline accomplish first year program – coursework progress secure thesis advisor identify possible research project etc coursework coursework requirement redesign allow student flexibility customize program study close interaction program study committee purpose curriculum  provide student knowledge understand perspective helpful toward research  develop context student faculty get know help aid process select area research advisor minimal requirement prior master ’ degree another university may “ approve ” satisfying “ stage ” ( first 32 hour ) academic office certify master ’ degree “ approve ” often student enter however transcript proof prior master ’ degree complete must submit department order academic office carry certification graduate college require 96 hour ( 64 approve ms ) – minimum 48 ( 16 approve ms ) credit hour coursework minimum 32 hour thesis hour ( cs 599 ) leave 16 credit hour either coursework thesis cs department additionally require minimum 48 coursework hour ( 16 approve ms ) least 24 hour ( 16 approve ms ) must 500-level least 20 hour ( 12 approve ms ) must cs course least 12 hour must 500-level cs course independent study ( cs 597 ) seminar course ( cs 591 cs 491 ) give special consideration course either computer science department may apply toward requirement 1 student complete 4 credit hour cs 597 give semester however hour independent study seminar course may used satisfy requirement 2 3 4 furthermore 4 hour seminar course may count towards total hour need graduation finally student must take cs 591phd ( orientation seminar ) first fall semester coursework committee program study within first month first semester student assign program study committee threecomputer science faculty one member base suggestion student two member ( faculty member student ’ area interest faculty outside area interest ) assign academic office 2015-2016 cs graduate student handbook http graduate-student 16 
department computer science wwwcsillinoisedu student must prepare program study form online http mycsillinoisedu ( click “ program study ” link ) consultation program study committee individual meeting group meeting form correspondence form three section 1 ) require coursework—represent coursework student must take 2 ) intend coursework—indicate course student plan take 3 ) strategy—brief explanation motivation choice coursework online form complete student must “ save ” “ print ” form committee member program study committee sign student must turn complete sign program study form academic office december 15th student may alter program study time revise program study form must complete approve program study committee “ require coursework ” portion change revise program study form signature must turn academic office request change program study committee may submit academic office consideration detailed information regard complete program study form view sample form visit http phd-program-study-process student ’ progress respect program study quality program study open evaluation annual area meeting student evaluate student make satisfactory progress term qualify exam preparation depth coursework breadth coursework receive warning indicate requirement complete earn ms degree along way student pass qualify exam consider routine earn ms degree deposit ms thesis provide appropriate graduate college requirement number credit hour satisfied student wish earn ms degree must stop academic office complete petition add ms program code ms degree confer student leave program without pass qualify exam may petition switch ms program assume complete expediently ms previously award elsewhere computer science graduate study committee consider petition case-bycase basis student may need take one additional course ms requirement satisfied advice three important source advice  faculty ( especially program study committee )  student ( especially pass qualify exam )  coursework recommendation locate http phd-program-study-process critical first semester student spend significant time talk program study committee member develop good program study 2015-2016 cs graduate student handbook http graduate-student 17 
department computer science wwwcsillinoisedu find thesis advisor choose thesis advisor one critical decision graduate program advisor-student relationship fundamental success student advisor thesis advisor great influence research direction also promote career student time student-thesis advisor relationship one last lifetime important find thesis advisor match research interest work style career goal even personality student may find thesis advisor within first week student may carefully explore evaluate many option first year scenario consider “ normal ” ms phd student must secure thesis advisor end first academic year complete “ cs 599 thesis agreement ” form student complete online form locate mycsillinoisedu “ grad student toolbox ” paper form locate http official-form qualify exam qualify exam purpose purpose qualify exam student convince faculty consider candidate faculty evaluate whether student knowledge experience perspective determination complete program addition faculty evaluate student ’ presentation communication skill ensure mastery english sufficient teach us institution achieve end program researcher various area may assess quality differently therefore format content exam vary significantly across research area qualify exam deadline student must attempt qualify exam later fourth semester student arrive ms degree may want take sooner base faculty recommendation qualify exam hold four-week period start monday closest third week fall spring semester contact mary beth kelley academic office question – mkelley @ illinoisedu 333-3527 qualify exam policy  student must advisor time qualify exam advisor agreement form must file academic office—the “ cs 599 thesis advisor agreement ” form  student admit program fall 2011 later whose native language english regardless us citizenship must pass tse ( pass score 50 ) epi ( pass score + ) toefl ibt-speaking subsection ( pass score 24 ) ielts-speaking subsection ( pass score 8 ) prior attempt qualify exam highly recommend student complete requirement within first year program avoid surprise time qualify exam student receive 4cp epi exam eligible complete qualify exam long register esl 508 qualify exam statement semester prior qualify exam student ask submit “ qual statement ” form locate http general-policies-processes-and2015-2016 cs graduate student handbook http graduate-student 18 
department computer science wwwcsillinoisedu official-form outline research interest forward appropriate research area committee area committee appoint three faculty member whose research match state interest examine committee however general committee include thesis advisor student notified date time location necessary material exam result qualify exam may pass fail conditional pass ( condition usually requirement take course two ) qual result report academic office area-by-area basis may take week student learn result conditional pass qualify exam student receive conditional pass qualify exam must meet condition condition within specify timeline condition completion additional coursework course ( ) must complete within one academic year non-coursework condition may require student meet condition ( ) less one year student ’ responsibility notify mary beth kelley academic office condition meet mary beth confirm condition meet conditional pass change pass fail qualify exam student fail qualify exam may discretion examine committee allow one attempt pass semester immediately follow first attempt qualify exam pass student may petition switch ms program assume complete expediently ms previously award elsewhere computer science graduate college confer duplicate ms degree graduate study committee consider petition case-by-case basis guidance prepare qualify exam format content qualify exam vary dramatically depend area area guideline appear http phd-qualifyingexam prepare qualify exam highly recommend student talk 1 ) advisor 2 ) faculty area 3 ) student take particular exam 4 ) study student take qualify exam 5 ) complete practice qual either senior level student faculty information particularly helpful student research interest span multiple area occasionally one exam might appropriate student consult her thesis advisor exam best plan research addition highly recommend student especially international student start focus presentation communication skill student want work towards mastery english lead excellence presentation skill effective communication play important role preliminary final exam well profession start master skill make plan towards improve preliminary exam 2015-2016 cs graduate student handbook http graduate-student 19 
department computer science wwwcsillinoisedu committee purpose role committee provide frequent feedback advice student committee share responsibility guide student ’ research successful completion student view committee member obstacle rather additional mentor possible promoter thesis research apply job committee member often first choice seek recommendation letter expect advisor work closely student determine appropriate committee member timeframe establish committee semester qualify exam pass student expect form committee committee member may easily add remove time qualify exam final exam ( thesis defense ) contact mary beth kelley academic office question – mkelley @ illinoisedu 333-3527 committee member initial committee minimum three member require two must belong faculty department computer science advisor include committee preliminary exam ( thesis proposal ) stage reach committee must change satisfy requirement impose graduate college department computer science 1 ) must least four voting member ( normally designate ) 2 ) least three less half voting member must member illinois ( urbanachampaign ) graduate faculty 3 ) least two voting member must tenured illinois urbana-champaign 4 ) least three member must member extend faculty department computer science ( extend faculty dcs include regular faculty well faculty non-visiting ( associate ) professor appointment dcs carry one follow modifiers—adjunct affiliate research emeritus ) two must full-time computer science department member ( non-affiliate ) university illinois urbana-champaign 5 ) least one member must outside university illinois ( outside member require student pass qualify exam spring 2006 later however highly recommend student ) outside member student thesis advisor within last five year must independent relevant publication occur earn degree note outside member must need university faculty member example member can belong industrial government research lab necessary teleconference technology may used preliminary exam approval outside member computer science department graduate college require cv ( resume ) brief statement choose outside member alum must away least five year substantial current publication time five requirement impose committee preliminary final exam ( although committee may different ) also student designate co-chair chair co2015-2016 cs graduate student handbook http graduate-student 20 
department computer science wwwcsillinoisedu chair must member university illinois urbana-champaign graduate faculty must pre-approve department graduate college initial plan stage chair co-chair must present exam addition requirement graduate college requirement must satisfied advice form committee  good student involve additional researcher effort early possible remember student lock particular committee choice preliminary exam sense committee formal mechanism stimulate interaction faculty student  generally recommend student least one committee member specialist general area research pursue thesis ability explain justify research outsider crucial success researcher  fairly common committee member minimum four help enhance quality visibility work furthermore may easier satisfy five requirement committee member however case voting purpose majority vote cs faculty illinois urbana-champaign preliminary exam ( thesis proposal ) prelim exam purpose writing good proposal important part successful researcher thesis proposal ( preliminary exam ) view important milestone help student develop skill phd student write proposal submit committee prior exam thesis proposal presentation give committee formal opportunity evaluate research progress goal student thus two main purpose preliminary exam develop proposal-writing skill obtain feedback research plan committee prelim exam policy  thesis proposal presentation ( preliminary exam ) must take within 5 semester pass qualify exam approval advisor must least 4 month preliminary final exam furthermore preliminary final exam may take within semester student must register term preliminary exam occur  student enter program prior fall 2011 whose native language english regardless us citizenship must meet speak english proficiency requirement prior attempt preliminary exam requirement meet pass one follow tse ( pass score 50 ) epi ( pass score + ) toefl ibt-speaking subsection ( pass score 24 ) ielts-speaking subsection ( pass score 8 ) highly recommend student complete requirement prior take qualify exam avoid surprise time preliminary exam ( student admit program fall 2011 later whose native language english regardless us citizenship require meet speak english proficiency requirement prior attempt qualify examination policy detailed qualify exam section ) 2015-2016 cs graduate student handbook http graduate-student 21 
department computer science wwwcsillinoisedu  thesis proposal must submit committee least three week prior exam give committee sufficient time carefully read proposal evaluate idea failure submit proposal time may result reschedule exam  committee exam must satisfy five criterium give committee section propose committee exam date must submit mary beth kelley academic office least three week prior exam submit prelim-final form http officialform graduate college require time approve committee officially appoint  student chair ( co-chair one designate ) committee least one voting member must physically present exam remain committee may participate exam via teleconference electronic communication medium  student require register term take prelim include summer term  contact mary beth kelley academic office question – mkelley @ illinoisedu 3333527 guidance write proposal  balance must strike satisfying severe space limitation provide critical detail proposal bound agreement student committee precise task must accomplish frequent interaction committee member student adapt specific thesis accomplishment necessary  thesis proposal 15 25 page ( single-column single-spaced format ) bibliographic reference include page count ( reference encourage ) explicit page limit format requirement proposal much shorter longer norm committee question reason proposal long committee may recommend reschedule exam proposal rewrite  three main criterium usually apply evaluate proposal first two similar national science foundation ’ guideline evaluate research proposal intellectual merit importance activity advance knowledge understand expect impact impact expect term particular research community society general feasibility likely state goal achieve candidate  base criterium thesis proposal contain overview state art help show candidate good grasp relevant research field brief summary research result obtain far candidate include cite prior publication current submission produce student clear description remain problem goal detail propose technical approach clear argument work relevant term intellectual merit expect impact explanation goal accomplish within expect amount time 2015-2016 cs graduate student handbook http graduate-student 22 
department computer science wwwcsillinoisedu  thesis proposal preliminary draft thesis particular chapter part thesis survey candidate ’ research field exist publication technical report final exam ( thesis defense ) final exam purpose final exam represent last significant opportunity committee ask question provide comment thesis work also serve disseminate work public ( include faculty student colleague friend family ) many way represent celebration completion work unlike preliminary exam close final exam open public announce along public seminar final exam policy  thesis defense must take least four month pass preliminary exam complete semester prelim  committee guideline “ phd committee ” section must follow committee final exam preliminary exam although often  student chair ( co-chair one designate ) committee least one voting member must physically present exam remain committee may participate exam via teleconference electronic communication medium  full thesis draft must submit committee least three week prior schedule defense furthermore request schedule defense include name committee member must submit mary beth kelley academic office later 30 day prior submit thesis draft committee complete prelim-final form http officialform  defense usually proceed follow 1 ) minute private discussion committee 2 ) public presentation present candidate typically last 45 minute 3 ) question committee front public 4 ) question public 5 ) question committee without public present 6 ) private discussion committee 7 ) outcome decide announce candidate  contact mary beth kelley academic office question – mkelley @ illinoisedu 3333527 2015-2016 cs graduate student handbook http graduate-student 23 
department computer science wwwcsillinoisedu guidance prepare final exam  rush schedule final exam work completely finished thesis completely written  common committee suggest minor improvement correction manuscript however usually case substantial new work expect risk committee request work prepared allot time necessary make recommend change enhancement thesis consult advisor point  important attend defense understand whole process learn valuable skill student ’ defense graduate annual evaluation ms graduate student go annual evaluation faculty member area mcs student go annual evaluation department graduate student must complete self-evaluation last part march email send student academic office inform complete self-evaluation student fail complete self-evaluation deadline receive automatic lack-of-progress warning information complete self-evaluation visit http graduate-student-self-evaluation self-evaluation complete faculty within different area meet review ms student ’ academic progress within program performance research teach assistant publication self-evaluation process occur april addition mcs student review april department ensure academic progress coursework make graduate student view feedback regard progress first week may visit http mycsillinoisedu one four outcome  sufficient progress  condition meet ( eg pass prelim end fall )  lack-of-progress warning ( student issue two warning row dismiss phd program )  unsatisfactory progress unsatisfactory progress may result removal program termination department ’ financial commitment continue support thesis deposit thesis deposit require ms student mary beth kelley academic office ( 1210 siebel center ) complete thesis format review ( student must submit thesis format review post departmental deadline ) student submit thesis electronically graduate college thesis office student ’ responsibility ensure thesis dissertation approval form ( tda ) complete sign prior thesis deposit deadline academic office responsible collect require signature form except department head ’ signature order complete electronic deposit require form submit thesis office post deadline thesis dissertation approval ( tda ) form must contain necessary 2015-2016 cs graduate student handbook http graduate-student 24 
department computer science wwwcsillinoisedu original signature entire committee “ wet ” signature “ department head ” ms thesis form must contain necessary original signature thesis advisor “ wet ” signature “ department head ” form take cs department thesis reviewer mary beth kelley obtain department head signature form sign thesis reviewer contact student pick form deliver thesis office deadline form require thesis office submit electronically highly recommend student begin electronic deposit thesis least three day prior thesis office deadline student wait last minute deposit thesis may make deadline thesis submit electronically place queue upload thesis office database processed order receive thesis still queue 445 pm deadline need additional change accept deposit student confer degree next conferral term information graduate college thesis office process deadline please visit student responsible understand deadline add name graduation list departmental format check thesis deposit graduate college departmental format check contact mary beth kelley academic office mkelley @ illinoisedu 333-3527 graduation process student ready degree confer must place name degree conferral list used ui integrate self-service alert academic office graduate college student plan graduate within semester student must add name degree conferral list deadline term student fail add name deadline wait next semester graduate addition student must also add name commencement list wish participate commencement ceremoney learn college engineering commencement date process visit https convocation+graduation learn campus wide may commencement process visit http reminder student must register term complete final exam ms international student f-1 visa must register term deposit valid i-20 registration registration spring term begin late october early april summer fall term office admission record ( oar ) provide website http online-registration student find earliest registration time graduate student begin register several student group ( eg graduate senior honor student band group first priority registration time ) banner application system ( also know “ ui integrate self-service ” ) used registration modification course schedule 10th day instruction student also print unofficial transcript add name appropriate graduation list system important notice cs graduate student must register fall spring term 10th day class failure may result late fee loss valid visa status international student loss 2015-2016 cs graduate student handbook http graduate-student 25 
department computer science wwwcsillinoisedu assistantship unapproved leave absence cs student except approve leave absence must register fall spring term graduate student approve internship must register 0 thesis hour full-time status  graduate student expect register full-time student fall spring semester ( 12 graduate hour ) student assistantship 25 % register minimum 8 hour consider full-time student outstanding student loan may also require full-time avoid loan call professional mcs campus student require register 12 credit hour  student award qualify fellowship or tuition fee waiver associate assistantship require register minimum 8 credit hour student receive external fellowship may need register 8 credit hour order meet term fellowship please contact graduate academic office clarification need  student require register summer term unless cpt fellowship summer registration fellow 4 graduate hour ( unless external fellowship require higher number credit hour ) student hold 25 % higher assistantship may register 4 credit hour consider full-time student consider drop summer registration withdraw summer registration contact academic office early possible information financial implication  student less 12 hour complete degree program need register number hour require graduate however student undergraduate loan deferment strongly encourage consult oar regard whether enrollment constitute fulltime status purpose keep loan go repayment  credit hour form traditional coursework independent study ( faculty member ) thesis research seminar  international student must register full-time student later 10 calendar day semester noon 11th day isss obligated law terminate f-1 j-1 immigration status student register student assistantship provide tuition waiver need register 10th day term may lose assistantship late registration late registration begin 1155 pm first day class student register time approximately 2 week complete registration late registration period student lose ability register must use late registration form http official-form student ’ advisor academic office must approve form prior final approval graduate college student also assessed $ 1500 ( subject change ) late registration penalty dropping course  semester drop course deadline register student deadline particularly drop cs course may different set graduate college student find early deadline check oar website http academic-calendar 2015-2016 cs graduate student handbook http graduate-student 26 
department computer science wwwcsillinoisedu  department post date “ academic deadline ” calendar http 2654  late program change form require add drop course past deadline find http official-form instructor advisor academic office must approve change prior submit form graduate college note student must maintain full-time status throughout semester  department support petition drop course final exam complete semester end student require complete course “ ” grade give accept grade receive please sure evaluate status course prior read day withdraw student make sure understand consequence withdraw university prior complete process academic office cooperation student ’ advisor must approve withdrawal university banner application system allow student drop course constitute withdrawal student must complete graduate petition indicate complete withdrawal http official-form international student must approval office international student scholar service ( isss ) withdraw no-credit elect no-credit registration allow student enroll course simple fail grade rather conventional letter grade - higher earn grade enter transcript “ ” ( satisfactory ) department allow no-credit option cs course course related computer science engineering course 500-level course used toward degree minimum grade requirement cs coursework cs graduate student require earn c higher cs graduate level coursework order count toward total hour coursework require graduate degree re-entry cs graduate program approve leave absence formal leave absence graduate student emergency however student may sometimes grant “ approve leave absence ” department student emergency arise cause interruption program need work closely thesis faculty advisor academic office complete necessary paperwork please aware timeframe must complete graduate degree still effect approve leave unapproved leave absence student take unapproved leave absence subsequently register without approve petition re-entry whether degree program remain different one unapproved leave absence occur “ approve leave absence ” grant student 2015-2016 cs graduate student handbook http graduate-student 27 
department computer science wwwcsillinoisedu  domestic student enrolled past three semester ( include summer term )  international student enrolled last semester ( include summer ) leave one term student educational loan consult financial aid office or lender terminate student status student take unapproved leave absence jeopardize chance complete graduate degree student would like re-enter program must complete graduate petition re-entry petition must include follow information  reason left department prior complete degree  justification re-admission anticipate completion date addition must submit follow document petition  letter faculty academic advisor thesis advisor indicate support estimate time frame completion  copy recent thesis proposal applicable fellowship assistantship admission ( faa ) committee review petition next enrollment period decide recommendation send graduate college final approval re-entry faa committee may establish new time limit student successfully complete requirement may include additional coursework addition guarantee funding effect prior leave program may may grant upon re-entry international student note visa status may change unapproved leave absence discuss office international student scholar service ( isss ) graduate petition online graduate college petition form access http graduate-studentrequest-form used request exception graduate college rule policy example     transfer credit time extension absentia registration curriculum change student complete petition submit electronically cs academic office review petition ms student academic office may seek faculty advisor ’ recommendation prior submit departmental approval approve electronic petition forward graduate college final review student department notified via email graduate college ’ decision graduate college petition take roughly 10-14 business day process receive graduate college student encourage contact cs academic office prior submit petition question 2015-2016 cs graduate student handbook http graduate-student 28 
department computer science wwwcsillinoisedu transfer coursework two type credit graduate student may wish transfer student may wish transfer credit one graduate degree another graduate degree within graduate college university illinois urbana-champaign alternatively graduate student may wish transfer graduate credit complete another accredit institution different rule apply type generally maximum twelve semester hour graduate coursework complete outside university illinois graduate college may count toward graduate degree work complete outside university illinois graduate college transfer include four type graduate level work take undergraduate university illinois champaign used toward degree graduate level work take guide individual study university illinois champaign graduate level work take another accredit institution used toward degree graduate level work do enrolled non-degree student university illinois champaign 12 hour graduate credit take enrolled non-degree student describe point 4 may petition apply toward student ’ graduate degree addition additional 12 hour credit take another institution describe point 3 transfer coursework must less 5 year old equivalent course offer illinois ’ department computer science receive grade b higher apply another degree student must complete petition ( see direction submit petition ) request course transfer along petition must also submit official transcript ( send department computer science ’ academic office ) well letter appropriate authority state credit hour used towards prior degree student request transfer course point 4 start program student request transfer course must complete 8 credit hour within program submit petition probation ( gpa ) minimum 30 grade point average ( gpa ) corresponding grade b must maintain stay graduate program graduate student whose gpa fall 30 receive warning letter state unless gpa raise 30 subsequent semester permit continue program course take university illinois graduate student affect graduate gpa therefore advisable take non-cs undergraduate-level course no credit option special grade ( dfr ab nr )  dfr ( defer ) grade issue end term cs dfr grade change “ ” grade thesis deposit – 4 hour cs 599 ms cs 599 credit hour academic office complete grade change cs 599 hour 2015-2016 cs graduate student handbook http graduate-student 29 
department computer science wwwcsillinoisedu  ( incomplete ) grade issue end term student complete require work course time limit student complete work follow  500 pm read day next semester student register next semester registration within year  register grade course within year one year deadline graduate college automatically change “ ” grade “ f rule ” fail grade reflect student ’ gpa instructor change  student fail appear course final exam instructor must issue ab ( absent ) grade matter well poorly student do course ab grade fail case approve instructor student may allow take special exam ab grade change  nr ( record ) automatically assign grade instructor enter grade grade submission deadline  grade c higher must earn cs graduate level coursework consider pass grade breadth requirement ms mcs program require minimum grade - code conduct policy department computer science require student act professional manner include written verbal communication faculty staff student outside vendor research partner harassment kind prohibit message derogatory inflammatory remark individual group ’ race religion national origin physical attribute sexual preference permit addition student hold accountable university illinois ’ code student conduct outlined http violation policy may result disciplinary action may include dismissal department computer science graduate program student charge violation 8 day appeal department computer science grievance committee failure appeal within time frame appeal deny charge stand university disciplinary action enforce student ethic academic integrity department computer science expect graduate student member university illinois community uphold high standard conduct ethic integrity student expect fully familiarize standard include policy practice related cheating plagiarism course work exam publication thesis department used section 1-402 ( http article1_part4_1-402html ) university illinois student code identify six type academic integrity infraction cheating plagiarism fabrication facilitate infraction academic integrity bribe favor threat academic interference student ’ responsibility carefully read part section 1-402 student also encourage review additional information academic integrity plagiarism available https honor+code http academicintegrityhtml 2015-2016 cs graduate student handbook http graduate-student 30 
department computer science wwwcsillinoisedu http plagiarismhtml http plagiarismcfm addition ms student require attend mandatory responsible conduct research ethic training session offer department fall term student inform via email time location session department recognize may ambiguous situation right course action easy infer state rule regulation therefore student strongly encourage doubt seek clarification appropriate source course instructor research supervisor thesis advisor fellow student academic staff graduate academic office open door policy address student question concern please make use resource ensure uphold departmental university standard conduct ethic integrity student charge violate student code may face consequence 1 ) receive grade zero assignment exam 2 ) receive fail grade course 3 ) dismissal program allege violation document within student ’ departmental file well document college engineering graduate college student allege violation 8 day respond professor writing ( usually via email ) meantime professor alert academic office allege violation violation still hold discuss professor student right appeal college engineering within 15 day notification learn grievance process or file appeal please contact professor lenny pitt pitt @ illinoisedu student appeal matter shall consider close one consequence apply student receive penalty 2 3 allow drop course student charge violation student code face additional university-level disciplinary action can dismiss program university grievance process faculty staff student within department computer science diverse group time time conflict problem arise conflict problem arise resolve informally two party however may time conflict resolve informally case student faculty staff either elect file formal grievance department contact professor pitt ( pitt @ illinoisedu ) file one directly graduate college graduate college process locate http gc_grievance visit academic office-1210 siebel center student see academic office academic matter include limit one list  academic progress  graduate petition  degree time extension  degree audit 2015-2016 cs graduate student handbook http graduate-student 31 
department computer science wwwcsillinoisedu     scheduling qualify prelim final exam thesis format check i-20 extension change curricular practical training ( cpt ) optional practical training ( opt ) ( international student )  submit form ( cs599 thesis advisor cs 597 independent study prelim-final etc ) useful publication regard academic matter graduate student graduate college handbook view print spr internship cs grad student wish hold internship department encourage summer term pending approval thesis advisor student pass qualify exam may approve internship fall spring term depend internship relate phd research student want hold fall spring internship must follow step provide academic office letter thesis advisor explain internship help progress phd research prepare prelim exam approve domestic student must register 0 thesis hour ( cs 599 ) international student must register eng 510 semester internship mean student responsible pay tuition cost semester student eligible hold ta ra appointment student hold fellowship may eligible internship depend regulation fellowship registration requirement student still require make progress research publication student still require complete cs plan document graduate self-evaluation form deadline research area still evaluate student s progress program determine satisfactory progress make academic year student require complete milestone 6-year timeline student ’ responsibility ensure coursework complete milestone meet international student responsible ensure compliance visa regulation related internship visit office international student scholar service ms mcs student eligible complete fall spring internship must register full-time student term note international student approve fall spring internship must consult academic office complete cpt paperwork require hold internship register eng cpt visa requirement impose restriction upon long international student may intern still qualify opt end degree curricular practical training ( cpt ) ( international student ) cpt graduate student 1 ) f-1visa 2 ) complete one year academic coursework 3 ) wish complete internship summer term cpt paperwork must approve sign cs grad academic office failure go office result 2015-2016 cs graduate student handbook http graduate-student 32 
department computer science wwwcsillinoisedu unapproved leave absence may result dismissal program primary contact assist student cpt kara macgregor ( kmacgreg @ illinoisedu ) kara available please contact either mary beth kelley ( mkelley @ illinoisedu ) viveka p kudaligama ( kudaliga @ illinoisedu ) cpt start end date summer cpt start end date may 16th august 15th     start date prior may 16th require department approval approve later april 15th end date august 15th require department approval approve later may 15th start date must last day spring term end date must first day fall term ta appointment start end date adjust student ta appointment must start cpt may 15th must end cpt august 15th department fund ta appointment contract date adjust vacation cpt student must plan summer vacation cpt complete vacation date run past august 16th please seek approval ra supervisor department approve start end date fall outside may 16th august 15th due vacation middle internship company require 12-week internship start monday end friday easily accomplish may 16th august 15th application process cpt order apply cpt international student must complete step complete isss cpt form  student submit electronic isss cpt form http f1cpthtml  copy internship offer letter must upload cpt form submit  please make sure indicate course name number credit hour summer cpt experience - cs 599 eng 510 ( see important note ) student thesis advisor pass qualify exam thesis title establish register 0 4 credit hour cs 599 student pass qualify exam register 0 credit hour eng 510 ms mcs student register 0 credit hour eng 510 isss cpt form electronically rout cs academic office cs academic office serve ` official advisor come apply cpt training verify require information prior approval ms phd student cpt appointment date start may 16th end august 15th assistantship download department computer science cpt approval form complete section one two form submit complete form cs academic office 2015-2016 cs graduate student handbook http graduate-student 33 
department computer science wwwcsillinoisedu information summer registration tuition & fee mckinley health center coverage registration student must register summer fall term prior leave training addition department require student full-time status ( see p 26 ) fall spring term follow cpt training ( unless graduate end fall term ) student register eng 510 registration eng 510 two-step process conjunction complete isss cpt approval process 1 ) international student must first request permission enroll eng 510 visit engineeringinternship request submit point prior add deadline relevant semester student expect step take 1-2 business day therefore early submission request encourage 2 ) request review approve student receive e-mail college engineering grant permission enroll eng student must enroll eng 510 via ui integrate self-service add deadline early completion step 1 allow sufficient time student register eng 510 add deadline summer tuition fee number credit hour student enroll either cs 599 eng 510 determine tuition fee rate assessed summer tuition fee schedule graduate student access http tuitionhtml health service fee ( mckinley ) health service fee trigger summer enrollment 0-2 hour student need access mckinley service summer consult mckinley health center business office eligibility purchase summer extension information available business_officehtm additional information ( please read carefully ) eligibility eligible cpt training student must complete one academic year study ( fall spring term ) length training cpt training may begin may 16th run august 15th summer student participate twelve month full-time curricular practical training lose eligibility apply twelve month optional practical training ( opt ) graduate study complete participation part-time curricular training program affect student ’ eligibility post-completion opt change address cpt approve student need update address ui integrate system match address cpt training period training student return campus must update address back address within urbana area 2015-2016 cs graduate student handbook http graduate-student 34 
department computer science wwwcsillinoisedu occupational practical training ( opt ) ( international student ) opt graduate student f-1 visa wish complete one year practical job training graduation order start opt training paperwork student must follow step primary contact assist student opt mary beth kelley ( mkelley @ illinoisedu ) mary beth available please contact either viveka p kudaligama ( kudaliga @ illinoisedu ) kara macgregor ( kmacgreg @ illinoisedu ) complete isss opt advisor form http f1opthtml form electronically rout cs academic office academic office serve ` official advisor come apply opt training academic office  verify student meet degree requirement  indicate student s anticipate graduation date  sign form recommend student plan least 90 day advance apply opt training information please visit http f1opthtml graduate study committee matter relate proper run graduate program scope department computer science graduate study committee concern may send committee chair please contact cs graduate academic office committee chair information counsele service student university illinois access counsele center assist gain balanced illinois experience service range various counsele service educational programming initiative training program outreach consultation activity self-help material staff member extensive training experience assist college student addition visit counselor confidential share cs academic office faculty advisor learn service visit http call ( 217 ) 333-3704 college engineering career service college engineering career service offer variety service help prepare graduate student job market offer assistance identify internship resume writing mock interview employment search much learn service available either visit website http visit office suite 3270 digital computer laboratory ( dcl ) addition learn university career event workshop visit http 2015-2016 cs graduate student handbook http graduate-student 35 
department computer science wwwcsillinoisedu engineering technical service group college engineering support department computer science basic advanced service necessary support lead edge educational research mission department please visit http information resource guide additional help find engineering helpdesk ( 2302 siebel center 333-7408 ) submit helpdesk ticket engrithelp @ illinoisedu submit helpdesk ticket important clearly outline problem include netid cs ece corporate connection program corporate connection comprehensive program create help industry connect faculty student forefront engineering corporate connection provide main point entry recruit cs ece student illinois well liaison collaboration faculty corporate connection membership fee used support program activity educational outreach activity student project student organization scholarship investment strengthen infrastructure cs ece department student profile database key component benefit corporate connection member create ability target specific student inclusion recruit event internship opportunity much student benefit invite event job posting base indication profile ensure best match student company learn corporate connection program complete student profile visit http indexhtml contact cynthia coleman ccoleman @ illinoisedu graduate office space mailbox new graduate student office reassignment current graduate student office complete august ms student ra appointment receive office space assign space accord research group student ta appointment share ta office space meet work need office assignment send student via email graduate student must swipe university icard office door lock access office space difficulty access office space direct engineering 2302 siebel center emailing engrithelp @ illinoisedu question concern regard graduate office space departmental policy office space direct associate head administration office 2232 siebel center addition office space department provide graduate student mailbox 1334 siebel center mailbox used internal campus communication purpose checked regular basis please note mailbox unsecured therefore outside mail ( letter package etc ) send mailbox student unable find mailbox contact academic office academic @ csillinoisedu communication skill develop good written oral communication skill essential success graduate study beyond skill necessary student successfully complete various stage program include qualify exam preliminary exam final exam student 2015-2016 cs graduate student handbook http graduate-student 36 
department computer science wwwcsillinoisedu numerous opportunity develop skill university illinois course research group meeting public presentation social setting ability researcher communicate well important develop research result develop good written oral skill graduate study allow student effectively engage researcher scholar keep attention make strong argument present difficult concept clearly written communication skill critical produce article publish conference proceedings journal student develop good technical writing technique work closely advisor faculty critically read well-written research article obtain feedback fellow student seek outside help internet book writing workshop university illinois writer ’ workshop center writing study http purdue university ’ online writing lab https two example online resource improve writing skill several opportunity exist student develop good oral communication skill student observe evaluate presentation style attend research seminar department around university student also opportunity give presentation seminar part research group meeting colloquia within department strongly encourage give presentation every opportunity least semester conference workshop research meeting excellent venue expand scholarly network learn meet person express idea experience researcher give impromptu explanation research time length ( 1 minute 5 minute 12 minute ) even tailor particular audience hire committee much enthusiastic candidate know least often take year build good social network student start early teach experience another way develop good communication skill teach valuable art serve graduate student well master take position academia government lab industry student seek career academia acquire good teach experience highly recommended—particularly responsibility entire course least development delivery substantial number lecture course non-native speaker english access university resource improve oral communication skill resource include english second language course international instructor ( http englishhtml ) intl connect ( http ) ihc international friend program available isss ( http ihc_intlfriendshtml ) student develop improve oral communication social skill may appear daunting seek advice feedback presentation actively interact peer faculty staff student able overcome challenge student stand gain socialize other different cultural background improve cross-cultural communication skill obtain broader global cultural perspective many brilliant student fail achieve career deserve limit communication skill please use many resource available department university improve written oral communication skill 2015-2016 cs graduate student handbook http graduate-student 37 
department computer science wwwcsillinoisedu general advice cs graduate student  undergraduate student life center course interaction faculty minimal graduate school interaction faculty central course secondary therefore important allow time first year spend interact faculty overload coursework  first year graduate school stressful student join department come environment top student take advantage talent around learn enhance skill build network last lifetime  important graduate student isolate get involved csgso student organization participate social activity within department graduate student remember student faculty staff offer support  student feel overwhelmed personal academic concern hesitate contact free counsele service university http information keep confidential student counselor  international student strongly encourage attend activity host international student scholar service ( isss ) help adjustment us culture learn different event visit http isssillinoisedu  end first year within graduate program common goal ms phd student begin research thesis advisor opportunity leader cs graduate student department computer science illinois many opportunity help lead shape future cs graduate program experience great resume builder well networking opportunity cs graduate ambassador communicate assist prospective student question graduate student life within department illinois ambassador play important active role admission recruitment process frequent email correspondence recruit assist activity schedule visit student interested ambassador contact viveka p kudaligama kudaliga @ illinoisedu october 15th cs graduate application reviewer assist faa ( fellowship assistantship admission ) committee review application ms program help determine applicant look strong consider admission student must pass qualify exam good academic standing participate student interested need contact research area chair ( viveka p kudaligama kudaliga @ illinoisedu ) october 30th student participate process review application december 20th january 20th do remotely winter break grad academic council ( gac ) assist department enhancement cs graduate program review comment new initiative enhance graduate education organize academic seminar enhance communication network among cs graduate student council make current graduate student research area show commitment leadership teach scholarship research member appoint research area chair serve two2015-2016 cs graduate student handbook http graduate-student 38 
department computer science wwwcsillinoisedu year term meeting hold month various member assist different activity throughout academic year student interested need contact research area chair current council member new appointment make month may illinois student senate ( iss ) work faculty undergraduate staff support university student committee help address issue important student include tuition career readiness prestige university recreational facility many other initiative learn become involved iss visit http student advise graduate education ( sage ) assist graduate college provost review provide feedback regard academic policy process graduate program illinois learn contact graduate college ( 217 ) review current membership visit http sage engineering graduate student advisory committee ( egsac ) advise college engineering topic important graduate education impact graduate student experience campus committee meet regular basis dean engineering share idea concern engineering graduate student learn become involved visit http leadership opportunity visit http student-organization click “ engineering student club ” academic office graduate program staff professor chandra chekuri director graduate admission & advancement chekuri @ illinoisedu ( 217 ) 265-0705 viveka p kudaligama coordinator graduate program & mcs advisor kudaliga @ illinoisedu ( 217 ) 300-2276 kara macgregor academic advisor kmacgreg @ illinoisedu ( 217 ) 333-9706 mary beth kelley academic advisor mkelley @ illinoisedu ( 217 ) 333-3527 kathy runck office manager krunck @ illinoisedu ( 217 ) 333-4197 2015-2016 cs graduate student handbook http graduate-student 39 
department computer science wwwcsillinoisedu computer science student organization department computer science university illinois number student organization graduate student join student organization within department  csgso https home contact person michael robson & pranjal vachaspatus csgso @ csuiucedu computer science graduate student organization group dedicate improve life graduate student computer science organization sponsor talk seminar organize informational social event serve liaison graduate student body department administration every friday csgso host popular friday extravaganza room 4401 weekly pizza party student relax catch long week research  woman computer science wwwillinoiswcsorg contact person namrata prabhu nprabhu2 @ illinoisedu woman computer science technical social professional student organization group foster interaction undergraduate graduate student alumni company recruiter computer science department student organization wc facilitate technical project organize tech talk professional workshop participate outreach activity group also offer mentoring program social event academic grace hopper scholarship opportunity membership much  acm http contact person sujay khandekar acm @ illinoisedu student chapter association compute machinery currently largest country one oldest ( 1965 ) acm illinois one 430 student chapter close 600 member special interest group ( sig ) focus fifteen different area computer graphic mobile compute artificial intelligence besides sig group hold many social event yearly midwest conference workshop place hang relax  siam http contact person amanda bienz bienz2 @ illinoisedu society industrial apply mathematics aim advance application mathematics computational science engineering industry science society locally chapter seek build community member interested intersection mathematics compute engineering science social professional event  latino-a computer science club https latino-a+computer+science+club contact person gaston leandro gerchko2 @ illinoisedu latino-a computer science club great place interact fun fellow student club develop main goal create special community within department one 2015-2016 cs graduate student handbook http graduate-student 40 
department computer science wwwcsillinoisedu latino student meet also help successful another main goal club involve create outreach program recruit latino student study computer science lcsc host array social event include xbox night set casino night siebel traditional mexican fiesta also host exclusive event company professional technical skill range tech talk private technical tip presentation club great opportunity cs student addition several different organization within university student view complete list opportunity visit student-group important website visit  cs graduate course list – http current-student  computational science engineering concentration –  department electrical computer engineering –  graduate college –  university illinois – wwwillinoisedu  housing information –  international student scholar service – wwwisssillinoisedu  office admission record – http  office minority student affair –  office student financial aid –  get started-grad college quick guide - http quick-guide  graduate college handbook -  thesis handbook - http thesis-dissertation  graduate student petition instruction - http petition  code policy regulation apply student -  graduate employee organization ( geo ) - indexhtm  campus polouse – wwwdpsillinoisedu  counsele center – http  dre ( disability resource educational service ) – wwwdisabilityillinoisedu  office student conflict resolution – wwwosjaillinoisedu  office vice chancellor research – wwwresearchillinoisedu  office equal opportunity access – wwweoaillinoisedu  grievance policy - http gc_grievance  campus life opportunity ( student organization ) - http student-group  krannert center perform art - 2015-2016 cs graduate student handbook http graduate-student 41 
department computer science wwwcsillinoisedu advice fellow grad faculty best advice survive grad school come fellow graduate student faculty ( one time grad student ) take time read explore website recommend fellow grad student advice  mean end achieve publication fellowship academic job offer etc make scholar  take good class take seriously often hear person say roughly ` class nt research important reflection academic culture reward certain accomplishment other simply recognize decide important one hand large publication record work ` hot field yield great perk easier access fellowship job example hand investing rigorous class fundamental topic ( topic nt related research ) taught great professor best way become sophisticated thinker therefore scholar  do…how go…how get make really enthusiastic really love whatever industry whatever person work make happier want make ` decision first worry corresponding ` later  victory come along guy fight last second whatever challenge confront fight last second - take qual find advisor work paper  keep note  research know do important even do wrong  day night 9 different pace can take get job do sometimes work day night whenever awake fast short short push something really need – ’ unavoidable indeed run time time time need proof mean time work 9 nt want die early  great survival guide graduate student read http hitch4html faculty advice  professor amir post philosophy research student http paper-pathhtml  professor zille advice grad section webpage http grad_advicehtml 2015-2016 cs graduate student handbook http graduate-student 42 
department computer science wwwcsillinoisedu  professor hockenmaier recommend follow website grad student gain advice succeed graduate school  http advicehtml  marie desjardin paper classic relevant http how2bhtml  http suprstarhtm  randy pausch s time management lecture http randypauschtimemanagement2007pdf  http badstudenthtml  http phdhtml  cra-w grad cohort program female grad student http resource http  walking home late night use safe-ride program  advice graduate student ta  ta evaluate student sure ask copy ice survey understand type question ask  ta high ice score nominated faculty may receive award public recognition department  teach certificate available graduate student  underperform ta may ask teach therefore may lose financial support  volunteer give lecture eg professor go conference  course one ta remember part team future employer find ability work team important  remember geo set rule allowable vacation time 2015-2016 cs graduate student handbook http graduate-student 43 
department computer science wwwcsillinoisedu important date 2015-2016 academic year important academic deadline set graduate college important event internal department computer science addition date please visit http graduate student seminar workshop date upcoming academic year view department ’ calendar visit http 2654 fall 2015 term august 1-23 2015 august 10-20 2015 august 11-20 2015 august 13-20 2015 august 16 2015 august 17-18 2015 august 19 2015 august 19-21 2015 august 21 2015 incoming cs graduate student arrive new ms student meet graduate academic office staff new mcs student meet coordinator graduate program new student meet professor chekuri fall ta ra appointment begin graduate academy college teach workshop popcorn available 1 3 pm outside 1210 sc 90 minute micro-teach session new mcs grad orientation 2 3 pm 1404 sc new ms-phd grad orientation 3 4 pm 1404 sc ice cream social new grad 4 430 pm first floor atrium sc last business day cancel fall registration 5 pm august 24 2015 first day fall term fall ta kick-off meeting 1000 am-noon 2405 sc deadline degree-seek student register fall class without late charge ( $ 15 ) 1155 pm cs departmental bbq 430 730 pm siebel center lawn graduate student must register full-time fall term last day add semester course web self-service campus close – labor day fall qualify exam begin spring 2015 admission application due professional mcs ms bio petition application current grad student apply ms mcs ms bio program fall qualify exam end last day drop semester course web self-service ms 5-year program informational session 4 pm 3404 sc fall qualify exam result due faculty ta self-evaluation spring plan document process start ta faculty student evaluation start fall qualify exam result send student august 27 2015 september 4 2015 september 7 2015 september 21 2015 october 15 2015 october 16 2015 october 21 2015 october 23 2015 october 26 2015 october 27 2015 october 30 2015 2015-2016 cs graduate student handbook http graduate-student 44 
department computer science wwwcsillinoisedu november 2 2015 november 13 2015 november 16 2015 november 20 2015 november 21-29 2015 november 27-28 2015 november 30 2015 december 4 2015 december 9 2015 december 10 2015 december 11 2015 december 12-18 2015 december 15 2015 december 18 2015 december 21 2015 registration spring term begin last day withdraw fall term without “ w ” grade last day drop semester course without “ w ” grade last day take final exam december doctoral degree last day add name december degree list web self-service spring qualify exam statement due academic office last day doctoral thesis format check academic office deadline graduate student service office receive final exam certificate result fall break campus close – thanksgiving break class resume spring ta contract send last day deposit december doctoral thesis last day master ’ thesis format check academic office last day change grade option semester course instruction end read day last day drop semester course instructor departmental approval ( “ w ” record ) last day change grade dfr previous term prevent change f rule ( apply cs 599 ) last day deposit december master ’ thesis final exam fall 2015 admission application due ( include petition application ) last date receipt petition graduate college graduate student december conferral december degree conferral ( commencement ) spring 2016 term january 15 2016 january 19 2016 february 1 2016 february 15 2016 february 28-march 1 2016 march 10 2016 march 11 2016 march 15 2016 fall 2016 application due professional mcs ms bio deadline cancel spring registration student need assistance first day spring term graduate student must register full-time spring term last day add semester course without permission spring qualify exam begin cs @ illinois grad student visit weekend ( invitation ) ta self-evaluation fall plan document process start last day drop semester course web self-service spring qualify exam end graduate student access graduate student self-evaluation application ms 5-year program application due spring qualify exam end 2015-2016 cs graduate student handbook http graduate-student 45 
department computer science wwwcsillinoisedu march 16 2016 march 19-27 2016 march 20 2016 march 28 2016 april 4 2016 april 6 2016 april 7-may 1 2016 april 8 2016 april 15 2016 april 22 2016 april 29 2016 may 4 2016 may 5 2016 may 6-13 2016 may 9 2016 may 15 2016 may 20 2016 ta faculty student evaluation start spring break spring qualify exam result due faculty class resume spring qualify exam result send student registration summer fall term begin graduate student ’ self-evaluation due graduate student evaluation complete thesis advisor area committee last day take final exam may doctoral degree last day withdraw spring term without “ w ” grade last day drop semester course without “ w ” grade last day doctoral thesis format check academic office last day add name may degree list must use web self-service deadline graduate student service office receive final exam certificate result last day deposit may doctoral thesis last day master ’ thesis format check academic office fall ta contact send last day deposit may master ’ thesis last day instruction graduate student evaluation feedback send student read day last day drop semester course instructor departmental approval ( “ w ” record ) last day change grade dfr previous term prevent change f rule ( apply cs 599 ) final exam fall qualify exam statement due academic office may degree conferral ( commencement ) last day graduate petition related may degree conferral summer 2016 term may 18 2016 may 30 2016 july 1 2016 july 4 2016 july 8 2016 july 15 2016 july 22 2016 first day summer 10 week term campus close – memorial day last day take final exam august doctoral degree campus close – fourth july last day doctoral thesis format check academic office last day add name august degree list must use web self-service deadline graduate student service office receive final exam certificate result last day deposit august doctoral thesis last day master ’ thesis format check academic office last day deposit august master ’ thesis last day withdraw summer term without “ w ” grade 2015-2016 cs graduate student handbook http graduate-student 46 
department computer science wwwcsillinoisedu august 4 2016 august 5-6 2016 august 8 2016 instruction end noon read day begin 1 pm final exam august degree conferral ( commencement ) last day graduate petition related august degree conferral new graduate student checklist welcome department computer science arrived safely campus item complete prior start term please read carefully complete task august 24 question please contact kara macgregor kmacgreg @ illinoisedu academic office 1210 siebel center academic @ illinoisedu ( 217 ) 333-4428 international student *   office international student scholar service - international student upon arrival check office international student scholar service ( isss ) office check begin august 3 august 12 hold 610 e john street room 400 student service build isss close check-in august 13th isss reopen check-in august 14 – august 21 illini union include weekend check-in sunday august 16 additional information check-in hour orientation activity find http one first stop upon arrival campus check-in isss requirement incoming international student social security number – student hold ra ta must apply social security number quickly give instruction check isss apply new student departmental financial aid offer all graduate *  academic office – new student upon arrival check kathy runck room 1210 siebel center 201 n goodwin avenue urbana ( cs academic office ) packet material instruction set appointment meet staff advisor student specific question sign meeting graduate director professor chandra chekuri front desk cs academic office ms student daily walk-in appointment available august 10-19 100-400 pm ms student also sign-up specific time ( room 1210 siebel center ) e-mail one academic advisor schedule meeting mcs student daily walk-in appointment cs graduate program coordinator viveka kudaligama available august 11-19 100-400 pm student need schedule specific time e-mail viveka kudaliga @ illlinoisedu  payroll – award assistantship please see kim bogle business office room 2222 siebel center fill i-9 employment eligibility form federal law may perform duty associate assistantship appointment paid university form complete failure complete step timely manner may 2015-2016 cs graduate student handbook http graduate-student 47 
department computer science wwwcsillinoisedu result reduction salary can possibly affect tuition waiver benefit might receive appointment require form complete on-line program call nessie cs business office enter name payroll system receive email instruction fill form make sure hit “ submit ” button  mckinley health center – submit health form ( mail person upon arrival ) 1109 lincoln avenue urbana il 61801  i-card – student issue permanent photo identification card must retain student register university get id go first floor illini union bookstore 809 s wright street ( corner wright john street ) information find http  parking – need register car bicycle applicable semester fee include bus pass suggest use parking meter cost anywhere 75 cent one dollar per hour difficult find certain time parking office locate 1201 w university avenue urbana information available http  ui-integrate – registration class send email graduate admission office let know admission finalize log read letter apply ( application system ) – towards bottom invite review quick guide graduate life guide give instruction set netid enterprise id need registration make sure register university deadline monday august 24th ( 1155 pm - first day class ) date charge late fee register  timetable – view upcoming semester course offering course catalog description follow link https schedule  housing – check champaign news-gazette on-line http classified interested on-campus housing please visit university ’ housing website http wwwhousingillinoisedu  fall teach assistant orientation – assign teach assistantship ( ta ) require attend graduate academy college teach present center teach excellence schedule august 17th 18th ( monday tuesday ) receive detail assignment notification letter attendance orientation mandatory anyone hold teach assistantship campus also need attend fall cs ta kick-off meeting august 24 2015 10 am-noon 2405 siebel center  mailbox – mailbox set room 1334 siebel center ( right inside east entry door ) arrive campus please sure check mailbox important document check regularly mailbox arrange alphabetical order ( last name ) allow personal mail deliver box  payday – payday 16th month 16th fall saturday sunday receive check pay stub friday receive assistantship qualify fellowship tuition waiver include also waive service fee health service fee afmfa fee library technology fee also provide vision insurance dental insurance partial payment health insurance fee 2015-2016 cs graduate student handbook http graduate-student 48 
department computer science wwwcsillinoisedu    final credential – make sure either mail hand-carry final seal credential graduate admission office upon arrival ( outlined official admission letter ) locate room 301 coble hall 801 s wright champaign il 61820 new grad welcome - attend cs new grad orientation session august 21st professional master ’ ( mcs ) new grad orientation 2 3 pm 3 4 pm session hold 1404 siebel center 4 500 pm new grad invite attend “ ice cream social ” first floor atrium area siebel center meet greet bbq - attend cs “ meet greet ” bbq august 27th 430 730 pm siebel center courtyard please rsvp august 23rd http bbq 2015-2016 cs graduate student handbook http graduate-student 49 
contact cs academic office quick reference guide process contact person email address phone number graduate petition viveka kudaligama kudaliga @ illinoisedu 300-2276 general advise question viveka kudaligama kudaliga @ illinoisedu 300-2276 cs 597 agreement form kathy runck krunck @ illinoisedu 333-4197 thesis advisor agreement form kathy runck krunck @ illinoisedu 333-4197 final exam mary beth kelley mkelley @ illinoisedu 333-3527 thesis format check mary beth kelley mkelley @ illinoisedu 333-3527 academic degree audit viveka kudaligama kudaliga @ illinoisedu 300-2276 cpt opt kara macgregor mary beth kelley viveka kudaligama kmacgreg @ illinoisedu mkelley @ illinoisedu kudaliga @ illinoisedu 333-9706 333-3527 300-2276 i-20 extension viveka kudaligama kudaliga @ illinoisedu 300-2276 registration override holly bagwell hbagwell @ illinoisedu 333-4763 grad student evaluation process viveka kudaligama kudaliga @ illinoisedu 300-2276 ta assignment kara macgregor kmacgreg @ illinoisedu 333-9706 ra ta contract kim bogle kbogle @ illinoisedu 333-4230 frequently contact department quick reference guide department graduate college email address web site grad @ illinoisedu phone number 333-0035 international student & scholar service ( isss ) isss @ illinoisedu 333-1303 student health insurance wwwsiillinoisedu 333-0165 financial services-cashier office usfscohelp @ uillinoisedu 333-2180 university policy department dpscomment @ illinoisedu counsele center wwwcounselingcenterillinoisedu 333-3704 office admission & record wwwregistrarillinoisedu 333-0210 333-1216 ( 911 emergency ) 

1 [ sound ] > > lecture natural language 
course schedule small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 content analysis see picture really first step process text datum text datum natural language computer understand natural language extent order make use datum s topic lecture be go cover three thing first natural language process main technique process natural language obtain understand 
overview • natural language process ( nlp ) • state art nlp • nlp text retrieval 3 second state art nlp stand natural language process finally be go cover relation natural language process text retrieval first nlp well best way explain think see text foreign language understand order understand text basically computer face 
example nlp dog chasing boy playground det noun aux noun phrase complex verb semantic analysis dog ( d1 ) boy ( b1 ) playground ( p1 ) chasing ( d1 b1 p1 ) + scared ( x ) chasing ( _ x _ ) scared ( b1 ) inference verb det noun prep noun phrase det noun noun phrase lexical analysis ( part-of-speech tag ) prep phrase verb phrase syntactic analysis ( parse ) verb phrase sentence person say may remind another person get dog back… pragmatic analysis ( speech act ) 4 look simple sentence like dog chasing boy playground nt problem understand sentence imagine computer would order understand well general would follow first would know dog noun chasing s verb etc call lexical analysis part-of-speech tag need figure syntactic category word s first step be go figure structure sentence example show dog would go together form noun phrase wo nt dog go first structure right structure show might get look sentence try interpret sentence word would go together first go together word show noun phrase intermediate component verbal phrase finally sentence get structure need something call semantic analysis parse may parser accompany program would automatically create structure point would know structure sentence still nt know meaning sentence go semantic analysis mind usually map sentence already know knowledge base example might imagine dog look like s boy s activity computer would use symbol denote will use symbol ( d1 ) denote dog ( b ) 1 denote boy ( p ) 1 denote playground also chasing activity s happen relationship chasing connect symbol computer would obtain understand sentence representation can also infer thing might indeed naturally think something else read text call inference example believe someone s chase person might scared rule see computer can also infer boy maybe scared extra knowledge will infer base understand text even go understand person say sentence use language call pragmatic analysis order understand speak actor sentence right say something basically achieve goal s purpose use language case person say sentence might remind another person bring back dog can one possible intent reach level understand would require step computer would go step order completely understand sentence yet human trouble understand instantly would get everything reason s large knowledge base brain use common sense knowledge help interpret sentence computer unfortunately hard obtain understand nt knowledge base still incapable reasoning uncertainty 
nlp difficult • natural language design make human communication efficient result – omit lot “ common sense ” knowledge assume reader possess – keep lot ambiguity assume reader know resolve • make every step nlp hard – ambiguity “ killer ” – common sense reasoning pre-requir 5 make natural language process difficult computer fundamental reason natural language process difficult computer simply natural language design computer natural language design us communicate language design computer example programming language harder us right natural language design make communication efficient result omit lot common sense knowledge assume everyone know also keep lot ambiguity assume receiver hearer can know decipher ambiguous word base knowledge context s need demand different word different meaning can overload word different meaning without problem reason make every step natural language process difficult computer ambiguity main difficulty common sense reasoning often require s also hard 
example challenge • word-level ambiguity eg – “ design ” noun verb ( ambiguous pos ) – “ root ” multiple meaning ( ambiguous sense ) • syntactic ambiguity eg – “ natural language process ” ( modification ) – “ man see boy ” ( pp attachment ) • anaphora resolution “ john persuade bill buy tv • ” ( = john bill ) presupposition “ quit ” imply smoke 6 let give example challenge consider word level ambiguity word different syntactic category example design noun verb word root may multiple meaning square root math sense root plant might able think s meaning also syntactical ambiguity example main topic lecture natural language process actually interpreted two way term structure think moment see figure usually think process natural language can also think say language process natural example synaptic ambiguity different structure apply sequence word another common example ambiguous sentence follow man see boy telescope case question telescope call prepositional phrase attachment ambiguity pp attachment ambiguity generally nt problem ambiguity lot background knowledge help us disambiguate ambiguity another example difficulty anaphora resolution think sentence john persuade bill buy tv question refer john bill something use background context figure finally presupposition another problem consider sentence quit smoking obviously imply smoke imagine computer want understand subtle difference meaning would use lot knowledge figure also would maintain large knowledge base meaning word connect common sense knowledge world s difficult result steep perfect fact far perfect understand natural language used computer 
state art dog chasing boy playground det noun aux noun phrase verb complex verb det noun prep noun phrase det noun pos tag 97 % noun phrase prep phrase verb phrase parse partial > 90 % ( ) semantic aspect verb phrase - relation extraction - word sense disambiguation - sentiment analysis sentence speech act analysis inference 7 slide sort gain simplified view state art technology part speech tag pretty well show 97 % accuracy number obviously base certain dataset nt take literally show pretty well s still perfect term parse partial parse pretty well mean get noun phrase structure verb phrase structure segment sentence dude correct term structure evaluation result see 90 % accuracy term partial parse sentence say number relative dataset dataset number might lower exist work evaluate used news dataset lot number less bias toward news datum think social medium datum accuracy likely lower term semantical analysis far able complete understand sentence technique would allow us partial understand sentence can mention example technique allow us extract entity relation mentioned text article example recognize dimension person location organization etc text call entity extraction may able recognize relation example person visit place person meet person company acquire another company relation extract used computer current natural language process technique be perfect well entity entity harder other also word sense disintegration extend figure whether word sentence would certain meaning another context computer can figure different meaning s perfect something direction also sentiment analysis meaning figure whether sentence positive negative especially useful review analysis example example semantic analysis help us obtain partial understand sentence s give us complete understand show sentence would still help us gain understand content useful term inference yet probably general difficulty inference uncertainty general challenge artificial intelligence s probably also nt complete semantical representation natural [ inaudible ] text hard yet domain perhaps limit domain lot restriction word used may able perform inference extent general really reliably speech act analysis also far do analysis special case roughly give idea state art also talk little bit ca nt ca nt even 100 % part speech tag look like simple task think example two used may different syntactic category try make fine grained distinction s easy figure difference s also hard general complete parse sentence see example ambiguity hard disambiguate imagine example use lot knowledge context sentence background order figure actually telescope although sentence look simple actually pretty hard case sentence long imagine four five prepositional phrase even possibility figure 
’ • 100 % pos tag – “ turn ” vs “ turn fan ” • general complete parse – “ man see boy telescope ” • precise deep semantic analysis – ever able precisely define meaning “ ” “ john own ” robust & general nlp tend “ shallow ” “ deep ” understand ’ scale 8 s also harder precise deep semantic analysis s example sentence ` john own restaurant define own exactly word something understand s hard precisely describe meaning computer result robust general natural language process technique process lot text datum shallow way meaning superficial analysis example part speech tag partial parse recognize sentiment deep understand be really understand exact meaning sentence hand deep understand technique tend scale well meaning would fill restrict text nt restrict text domain use word technique tend work well may work well base machine learn technique datum similar training datum program trained generally would nt work well datum different training datum pretty much summarize state art natural language process course within short amount time ca nt really give complete view nlp big field will expect see multiple course natural language process topic relevance topic talk s useful know background case happen expose 
nlp text retrieval • must general robust & efficient  shallow nlp • “ bag word ” representation tend sufficient search task ( ) • text retrieval technique naturally address nlp problem • however deeper nlp need complex search task 9 mean text retrieval well text retrieval deal kind text s hard restrict text certain domain also often deal lot text datum mean nlp technique must general robust efficient imply today use fairly shallow nlp technique text retrieval fact search engine today use something call bag word representation probably simplest representation possibly think turn text datum simply bag word meaning will keep individual word will ignore order word will keep duplicate occurrence word call bag word representation represent text way ignore lot valid information make harder understand exact meaning sentence have lose order yet representation tend actually work pretty well search task partly search task difficult see match query word text document chance document topic although exception comparison task example machine translation would require understand language accurately otherwise translation would wrong comparison task relatively easy representation often sufficient s also representation major search engine today like google bing used course put parenthesis course many query answer well current search engine require replantation would go beyond bag word replantation would require natural language process do another reason used sophisticated nlp technique modern search engine s retrieval technique actually naturally solve problem nlp one example word sense disintegration think word like java can mean coffee can mean program language look word anome would ambiguous user used word query usually word example be look usage java applet applet imply java mean program language contest help us naturally prefer document java refer program language document would probably match applet well java occur document mean coffee would never match applet small probability case retrieval technique naturally achieve goal word another example technique call feedback talk later lecture technique would allow us add additional word query additional word can related query word word help match document original query word occur achieve extent semantic match term technique also help us bypass difficulty natural language process however long run still need deeper natural language process technique order improve accuracy current search engine s particularly need complex search task question answer google recently launch knowledge graph one step toward goal knowledge graph would contain entity relation go beyond simple bag word replantation technique help us improve search engine utility significantly although open topic research exploration 
summary • natural language process ( nlp ) • state art nlp • nlp text retrieval 10 sum lecture talk nlp have talk state technique finally also explain bag word replantation remain dominant replantation used modern search engine even though deeper nlp would need future search engine 
additional read chris man hinrich schütze foundation statistical natural language process mit press cambridge may 1999 11 want know take look additional reading cite one s good start point thank [ music ] 

1 [ sound ] lecture be go talk text access 
course schedule small relevant datum user text retrieval problem text retrieval method recommendation vector space model 2 access text text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 previous lecture talk natural language content analysis explain state natural language process technique still good enough process lot unrestricted text datum robust manner result bag word remain popular application like search engine lecture be go talk high-level strategy help user get access text datum also important step convert raw big text datum small random datum actually need specific application 
access relevant text datum text information system help user get access relevant text datum • push vs pull • query vs browse 3 main question will address text information system help user get access relevant text datum be go cover two complimentary strategy push versus pull be go talk two way implement pull mode 
two mode text access pull vs push • pull mode ( search engine ) – user take initiative – ad hoc information need • push mode ( recommender system ) – system take initiative – stable information need system good knowledge user ’ need 4 query versus browse first push versus pull two different way connect user right information right time difference take initiative party take initiative pull mode user take initiative start information access process case user typically would use search engine fulfill goal example user may type query browse result find relevant information usually appropriate satisfying user s ad hoc information need ad hoc information need temporary information need example want buy product suddenly need read reviews related product crack information purchase product generally longer need information s temporary information need case s hard system predict need s proper user take initiative s search engine useful today many person many information need time be speaking google probably process many query mostly adequate information need pull mode contrast push mode system would take initiative push information user recommend information user case usually support recommender system would appropriate user stable information example may research interest topic interest tend stay s rather stable hobby another example stable information need case system interact learn interest monitor information stream system nt see relevant item interest system can take initiative recommend information example news filter news recommend system can monitor news stream identify interesting news simply push news article mode information access may also property system good knowledge user need happen search context example search information web search engine might infer might also interested something related formation would recommend information remind example advertisement place search page two high level strategy two mode text access 
pull mode query vs browse • query – user enter ( keyword ) query – system return relevant document – work well user know keyword use • browse – user navigate relevant information follow path enabled structure document – work well user want explore information ’ know keyword use ’ conveniently enter query 5 let s look pull mode detail pull mode distinguish two way help user query versus browse query user would enter query typical keyword query search engine system would return relevant document use work well user know exactly keyword used know exactly look tend know right keyword query work well time also know sometimes nt work well nt know right keyword use query want browse information topic area use browse would useful case case browse user would simply navigate relevant information follow path support structure document system would maintain kind structure user can follow structure navigate really work well user want explore information space user nt know keyword used query simply user find inconvenient type query even user know query type user used cellphone search information s still harder enter query case browse tend convenient 
information seek sightseeing • sightseeing know address attraction – yes take taxi go directly site – walk around take taxi nearby place walk • information seek know exactly want find – yes use right keyword query find information directly – browse information space start rough query browse 6 relationship browse query best understood make imagine be site see imagine be tour city know exact address attraction take taxi perhaps fastest way go directly site nt know exact address may need walk around take taxi nearby place walk around turn exactly information study know exactly look use right keyword query find information be s usually fastest way find information nt know exact keyword use well clearly probably wo nt well related page need also walk around information space meaning follow link browse finally get relevant page want learn likely lot browse like look around area want see interesting attraction related [ inaudible ] analogy also tell us today good support query nt really good support browse order browse effectively need map guide us like need map chicago city chicago need topical map tour information space construct topical map fact interesting research question might bring us interesting browse experience web application 
summary small relevant datum user push text access recommender system + pull search engine query + browse natural language content analysis big text datum 7 summarize lecture have talk two high level strategy text access push pull push tend support recommender system pull tend support search engine course sophisticated [ inaudible ] information system combine two pull mode [ inaudible ] query browse generally want combine two way help assist support query nad browse 
additional read n j belkin w b croft information filter information retrieval two side coin commun acm 35 12 ( 1992 ) 2938 8 want know relationship pull push read article give excellent discussion relationship machine filter information retrieval informational filter similar information recommendation push mode information access [ music ] 

1 [ music ] lecture text retrieval problem 
course schedule small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 picture show overall plan lecture last lecture talk high level strategy text access talk push versus pull engine main tool support pull mode start lecture be go talk search engine work detail first s text retrieval problem be go talk three thing lecture 
overview • text retrieval • text retrieval vs database retrieval • document selection vs document ranking 3 first define text retrieval second be go make comparison text retrieval related task database retrieval finally be go talk document selection versus document ranking two strategy respond user s query 
text retrieval ( tr ) • collection text document exist • user give query express information need • search engine system return relevant document user • often call “ information retrieval ” ( ir ) ir actually much broader • know “ search technology ” industry 4 text retrieval task s familiar us be used web search engine time text retrieval basically task system would respond user s query relevant document basically s support query one way implement poll mode information access situation follow collection text retrieval document document can webpage web literature article digital library maybe text file computer user typically give query system express information need system would return relevant document user relevant document refer document useful user type query task phone call information retrieval literally information retrieval would broadly include retrieval non-textual information well example audio video etc s worth note text retrieval core information retrieval sense medias video retrieve exploit companion text datum example current image search engine actually match user s query companion text datum image problem also call search problem technology often call search technology industry 
tr vs database retrieval • information – free text vs structure datum – ambiguous vs well-defined semantic • query – ambiguous vs well-defined semantic – incomplete vs complete specification • answer – relevant document vs match record • tr empirically defined problem – ’ mathematically prove one method better another – must rely empirical evaluation involve user 5 ever take course databasis useful pause lecture point think difference text retrieval database retrieval two task similar many way important difference spend moment think difference two think datum information manage search engine versus manage database system think different query typically specify database system versus query type user search engine finally think answer s difference two okay think information datum manage two system see text retrieval datum unstructured s free text databasis structure datum clear defined schema tell column name person column age etc unstructured text obvious name person mentioned text difference also see text information tend ambiguous talk process chapter whereas databasis nt tend find semantic result important difference query partly due difference information datum test query tend ambiguous whereas research query typically well-defined think sql query would clearly specify record return well-defined semantic keyword query electronic query tend incomplete also nt really specify document retrieve whereas complete specification return difference answer would also different case text retrieval be look rather document database search retrieve record match record sequel query precisely case text retrieval right answer query well specify discuss s unclear right answer query important consequence textual retrieval empirically defined problem problem s empirically defined mathematically prove one method better another method also mean must rely empirical evaluation involve user know method work better s need one lecture cover issue evaluation important topic sir jenning without know evaluate heroism properly s way tell whether get better whether one system better another 
formal formulation tr • • • • • vocabulary = { w1 w2 … wn } language query q = q1 … qm qi  v document di = di1 … dimi dij  v collection = { d1 … dm } set relevant document r ( q )  c – generally unknown user-dependent – query “ hint ” doc r ( q ) • task = compute r ’ ( q ) approximation r ( q ) 6 let s look problem formal way slide show formal formulation text retrieval problem first vocabulary set set word language consider one language reality web might multiple natural language text kind language simplicity assume one kind language technique used retrieve datum multiple language less similar technique used retrieve document one end although important difference principle method similar next query sequence word see query defined sequence word q sub word vocabulary document defined way s also sequence word sub ij also word vocabulary typically document much longer query also case document may short think might example case hope think twitter search tweet short general document longer query collection document collection large think web can large goal text retrieval will find set relevant document denote r ( q ) depend query general subset document collection unfortunately set relevant document generally unknown user-dependent sense query type different user expect relevant document may different query give us user hint document set indeed user generally unable specify exactly set especially case web search connection s large user nt complete knowledge whole production best search system compute approximation relevant document set denote r ( q ) formerly see task compute r ( q ) approximation relevant document imagine ask write program would think moment right input query document compute answer query set document would useful user would solve problem 
compute r ’ ( q ) • strategy 1 document selection – r ’ ( q ) = { dc|f ( q ) 1 } f ( q )  { 01 } indicator function binary classifier – system must decide doc relevant ( absolute relevance ) • strategy 2 document ranking – r ’ ( q ) = { dc|f ( q ) >  } f ( q )  relevance measure function  cutoff determine user – system need decide one doc likely relevant another ( relative relevance ) 7 general two strategy use first strategy document selection be go binary classification function binary classifier s function would take document query input give zero one output indicate whether document relevant query case see document relevant document set defined follow basically document value 1 function case see system must decide document relevant basically say whether s one zero call absolute relevance basically need know exactly whether s go useful user alternatively s another strategy call document ranking case system go make call whether document random rather system go use real value function f would simply give us value would indicate document likely relevant s go make call whether document relevant rather would say document likely relevant function used random document be go let user decide stop user look document threshold theta determine document approximation set be go assume document rank threshold set effect document deliver user theta cutoff determine user have get collaboration user sense nt really make cutoff user kind help system make cutoff case system need decide one document likely relevant another need determine relative relevance opposed absolute relevance probably already sense relative relevance would easier determine absolute relevance first case say exactly whether document relevant turn ranking indeed generally prefer document selection let s look two strategy detail 
document selection vs ranking true r ( q ) + - - + - + + - - - - 1 doc selection f ( q ) = doc ranking f ( q ) = 0 + - + + r ’ ( q ) - - - - + - 098 d1 + 095 d2 + 083 d3 080 d4 + 076 d5 056 d6 034 d7 021 d8 + r ’ ( q ) 8 picture show work left side see document use pluse indicate relevant document see true relevant document consist set true relevant document consist process document document selection function be go basically classify two group relevant document non-relevant one course classify perfect make mistake see approximation relevant document get number document similarly relevant document be misclassify non-relevant case document ranking see system seem like simply rank document descend order score be go let user stop wherever user want stop user want examine document user scroll stop [ inaudible ] user want read random document user might stop top position case user stop d4 fact deliver four document user 
problem document selection • classifier unlikely accurate – “ over-constrained ” query  relevant document return – “ under-constrained ” query  delivery – hard find right position two extreme • even accurate relevant document equally relevant ( relevance matter degree ) – prioritization need • thus ranking generally prefer 9 say ranking generally prefer one reason 
theoretical justification ranking • probability ranking principle [ robertson 77 ] return rank list document descend order probability document relevant query optimal strategy follow two assumption – utility document ( user ) independent utility document – user would browse result sequentially • two assumption hold 10 classifier case document selection unlikely accurate clue usually query query may accurate sense can overly constrain example might expect relevant document talk topic used specific vocabulary result might match relevant document collection other discuss topic used vocabulary right case will see problem relevant document return case over-constrained query hand query under-constrained example query sufficient descriptive word find random document may actually end delivery thought word sufficient help find right document turn sufficient many distraction document used similar word case delivery unfortunately s hard find right position two extreme whether user look information general user good knowledge information find case user good knowledge vocabulary used relevent document s hard user pre-specify right level constraint even classifier accurate also still want rend relevant document generally equally relevant relevance often matter degree must prioritize document user examine note prioritization important user digest content user generally would look document sequentially therefore would make sense user relevant document s ranking reason ranking generally prefer preference also theoretical justification give probability ranking principle end lecture reference principle say return rank list document descend order probability document relevant query optimal strategy follow two assumption first utility document ( user ) independent utility document second user would assume browse result sequentially s easy understand assumption need order justify site ranking strategy document independent evaluate utility document s separate would allow computer score document independently go rank document base scroll second assumption say user would indeed follow rank list user go follow rank list go examine document sequentially obviously order would optimal two assumption theoretically justify ranking strategy fact best can have put one question two assumption hold suggest pause lecture moment think think example would suggest assumption nt necessarily true think moment may realize none assumption actually true example case independence assumption might document similar exactly content look alone relevant user already see one assume s generally useful user see another similar duplicate one clearly utility document dependent document user see case might see scenario one document may useful user three particular document put together provide answer user s question collective relevance also suggest value document might depend document sequential browse generally would make sense rank list even rank list evidence show user nt always go strictly sequentially entire list sometimes look bottom example skip think complicate interface can possibly use like two dimensional phase put additional information screen sequential browse restrict assumption point none assumption really true less probability ranking principle establish solid foundation ranking primary pattern search engine actually basis lot research work information retrieval many hour design base assumption despite assumption nt necessarily true address problem post process rank list example remove redundancy summarize lecture main point take away follow first text retrieval empirically defined problem mean algorithm better must judge user second document ranking generally prefer help user prioritize examination search result also bypass difficulty determine absolute relevance get help user determine make cut s flexible suggest main technical challenge design search engine design effective ranking function word need define value function f query document pair design function main topic follow lecture 
summary • text retrieval empirically defined problem – algorithm better must judge user • document ranking generally prefer – help user prioritize examination search result – bypass difficulty determine absolute relevance ( user help decide cutoff rank list ) • main challenge design effective ranking function f ( q ) = 11 
additional reading • se robertson probability ranking principle ir journal documentation 33 294-304 1977 • c j van rijsbergen information retrieval 2nd edition butterworth-heinemann newton usa 1979 – must-read anyone research information retrieval chapter 6 in-depth discussion prp 12 two suggest additional reading first classical paper probability ranking principle second one must-read anyone research information retrieval s classic ir book excellent coverage main research result early day time book written chapter six book in-depth discussion probability ranking principle probably retrieval model general [ music ] 

1 [ sound ] lecture overview text retrieval method 
course overview text schedule retrieval method small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 21 previous lecture introduce problem text retrieval explain main problem design ranking function rank document query lecture give overview different way design ranking function 
design ranking function query q = q1 … qm qi  v document = d1 … dn di  v ranking function f ( q )  good ranking function rank relevant document top non-relevant one • key challenge measure likelihood document relevant query q • retrieval model = formalization relevance ( give computational definition relevance ) • • • • 3 problem follow query sequence word document s also sequence word hope define function f compute score base query document main challenge hear design good ranking function rank relevant document top non-relevant one clearly mean function must able measure likelihood document relevant query q also mean way define relevance particular order implement program computational definition relevance achieve goal design retrieval model give us formalization relevance 
many different retrieval model • similarity-based model f ( q ) = similarity ( q ) – vector space model • probabilistic model f ( q ) = p ( r=1|d q ) r  { 01 } – classic probabilistic model – language model – divergence-from-randomness model • probabilistic inference model f ( q ) = p ( dq ) • axiomatic model f ( q ) must satisfy set constraint • different model tend result similar ranking function involve similar variable 4 many decade researcher design many different kind retrieval model fall different category first one family model base similarity idea basically assume document similar query another document say first document relevant second one case ranking function defined similarity query document one well know example case vector space model cover detail later lecture second kind model call probabilistic model family model follow different strategy assume query document observation random variable assume binary random variable call r indicate whether document relevant query define score document respect query probability random variable r equal 1 give particular document query different case general idea one classic probabilistic model another language model yet another divergence randomness model later lecture talk one case language model third kind model base probabilistic inference idea associate uncertainty inference rule quantify probability show query follow document finally also family model used axiomatic think idea define set constraint hope good retrieval function satisfy case problem seek good ranking function satisfy desire constraint interestingly although different model base different think end retrieval function tend similar function tend also involve similar variable let s take look common form state art retrieval model examine common idea used model first model base assumption used bag word represent text explain natural language process lecture bag word representation remain main representation used search engine assumption score query like presidential campaign news respect document would base score compute base individual word 
common idea state art retrieval model f ( = “ presidential campaign news ” ) g ( “ presidential ” ) g ( “ campaign ” ) “ bag word ” g ( “ news ” ) many time “ presidential ” occur term frequency ( tf ) c ( “ presidential ” ) long document length | often see “ presidential ” entire collection document frequency df ( “ presidential ” ) p ( “ presidential ” collection ) 5 mean score would depend score word presidential campaign news see three different component corresponding well document match query word inside function see number heuristic used example one factor affect function many time word presidential occur document call term frequency tf might also denote c presidential d general word occur frequently document value function would larger another factor long document use document length score general term occur long document many time s significant occur number time short document long document term expect occur frequently finally factor call document frequency also want look often presidential occur entire collection call document frequency df presidential model might also use probability characterize information show probability presidential collection try characterize popularity term collection general match rare term collection contribute overall score match common term capture main idea used pretty much older state art original model 
model work best • optimized follow model tend perform equally well [ fang et al 11 ] – pivot length normalization – bm25 – query likelihood – pl2 • bm25 popular 6 natural question model work best turn many model work equally well list four major model generally regard state art original model pivot length normalization bm25 query likelihood pl2 optimized model tend perform similarly discuss detail reference end lecture among bm25 probably popular s likely used virtually search engine also often see method discuss research paper will talk method later lecture summarize main point make lecture first design good ranking function pre-require computational definition relevance achieve goal design appropriate retrieval model second many model equally effective nt single winner yet researcher still active work problem try find truly optimal retrieval model 
summary • design ranking function f ( q ) pre-require computational definition relevance ( retrieval model ) • many model equally effective single winner • state art ranking function tend rely – bag word representation – term frequency ( tf ) document frequency ( df ) word – document length 7 finally state art ranking function tend rely follow idea first bag word representation second tf document frequency word information used weighting function determine overall contribution match word document length often combine interesting way will discuss exactly combine rank document lecture later 
additional reading • detailed discussion comparison state art model – hui fang tao tao chengxiang zhai diagnostic evaluation information retrieval model acm tran inf syst 29 2 article 7 ( april 2011 ) • broad review different retrieval model – chengxiang zhai statistical language model information retrieval morgan & claypool publisher 2008 ( chapter 2 ) 8 two suggest additional reading time first paper find detailed discussion comparison multiple state art model second book chapter give broad review different retrieval model [ music ] 

1 
course schedule small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 
many different retrieval model • similarity-based model f ( q ) = similarity ( q ) – vector space model 3 
vector space model ( vsm ) illustration programming query q d2 dm d3 d5 library presidential d4 d1 4 
vsm framework • represent query term vector – term basic concept eg word phrase – term define one dimension – n term define n-dimensional space – query vector = ( x1 …xn ) xi query term weight – doc vector = ( y1 …yn ) yj doc term weight • relevance ( q )  similarity ( q ) f ( q ) 5 
vsm ’ say • select “ basic concept ” – concept assume orthogonal • place docs query space ( = assign term weight ) – term weight query indicate importance term – term weight doc indicate well term characterize doc • define similarity measure 6 

1 lecture be go talk instantiate vector space model get specific ranking function 
course schedule small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 continue discussion vector space model one particular approach design ranking function be go talk use general framework vector space model guidance instantiate framework derive specific ranking function be go cover symbolist instantiation framework 
vsm ’ say = ( x1 …xn ) = = ( y1 …yn ) = sim ( q ) = 3 discuss previous lecture vector space model really framework nt say discuss previous lecture vector space model really framework say many thing example show say define dimension also say place document vector space say place query vector vector space finally say measure similarity query vector document vector imagine order implement model say specifically compute vector exactly xi exactly yi determine place document vector place query vector course also need say exactly similarity function provide definition concept would define dimension xi s yi s namely weight term query document able place document vector query vector well defined space also specify similarity function will well defined ranking function let s see think instantiation actually would suggest pause lecture point spend couple minute think suppose ask implement idea come idea vector space model still nt figured compute vector exactly define similarity function would think couple minute proceed 
dimension instantiation bag word ( bow ) w1 w3 w2 vocabulary = ( w1 …wn ) 4 let s think simplest way instantiate vector space model first define dimension well obvious choice use word vocabulary define dimension show n word vocabulary therefore n dimension word define one dimension basically bag word 
vector placement bit vector w1 xi yi  { 01 } = ( x1 …xn ) 1 word wi present 0 word wi absent = ( y1 …yn ) w3 w2 5 let s look place vector space simplest strategy use bit vector represent query document mean element xi yi take value either zero s 1 mean corresponding word present document query s 0 s go mean s absent imagine user type word query query vector 1 s many many zero document vector generally 1 s course also many zero since vocabulary generally large many word nt really occur document many word occasionally occur document lot word absent particular document place document query vector space 
similarity instantiation dot product sim ( q ) = 𝒙𝟏 𝒚𝟏 + … + 𝒙𝑵 𝒚𝑵 = w1 𝑵 𝒊=𝟏 𝒙𝒊 𝒚𝒊 = ( x1 …xn ) = ( y1 …yn ) w3 w2 6 let s look measure similarity commonly used similarity measure dot product dot product two vector simply defined sum product corresponding element two vector see product x1 y1 x2 multiply y2 finally xn multiply yn take sum s dot product represent general way used sum one many different way measure similarity 
simplest = bit-vector + dot-product + bow = ( x1 …xn ) = ( y1 …yn ) xi yi  { 01 } 1 word wi present 0 word wi absent sim ( q ) = 𝒙𝟏 𝒚𝟏 + … + 𝒙𝑵 𝒚𝑵 = 𝑵 𝒊=𝟏 𝒙𝒊 𝒚𝒊 ranking function intuitively capture good ranking function 7 see defined dimension defined vector also defined similarity function finally simplest vector space model base bit vector [ inaudible ] dot product similarity bag word [ inaudible ] formula look like formula s actually particular retrieval function ranking function right finally implement function used program language rank document query point pause lecture think interpreted score go process modele retrieval problem used vector space model make assumption place vector vector space define similarity end have get specific retrieval function show next step think whether retrieval function actually make sense right expect function actually perform well used rank document user s query s worth think value calculate end will get number number mean meaningful spend couple minute sort think course general question believe good ranking function would actually work well think interpret value actually meaningful mean something related well document match query 
example would rank document query = “ news presidential campaign ” d1 … news … d2 … news organic food campaign… d3 … news presidential campaign … d4 … news presidential campaign … … presidential candidate … d5 … news organic food campaign… campaign…campaign…campaign… ideal ranking d4 + d3 + d1 d2 d5 8 order assess whether simplest vector space model actually work well let s look example show sample document sample query query news presidential campaign five document cover different term query look document moment may realize document probably relevant other probably relevant ask rank document would rank basically ideal ranking human examine document try rank think moment take look slide perhaps pause lecture think would agree d4 d3 probably better other really cover query well match news presidential campaign look like document probably better other rank top three d2 d1 d5 really relevant also say d4 d3 relevant document d1 d2 d5 non-relevant let s see simplest vector space model can can something closer let s first think actually use model score document right show two document d1 d3 query also vector space model course want first compute vector document query show vocabulary well end dimension will think think vector query note be assume use zero 1 indicate whether term absent present query document zero1 bit vector think query vector well query four word 
ranking used simplest vsm query = “ news presidential campaign ” d1 d3 … news … … news presidential campaign … = { news presidential campaign food … } = ( 1 1 1 1 0 … ) = ( 1 1 0 0 0 … ) f ( q d1 ) = 1*1+1*1 * 0+1*0+0*0 + = 2 = ( 1 0 1 1 0 … ) f ( q d3 ) = 1*1+1*0 1*1 + … = 3 four word rest zero document s d1 two row news two 1 s rest zero similarly two vector let s compute similarity be go use product see use dot product multiply corresponding element right two formal product two generate another product two generate yet another product forth easily see actually nt care zero whenever zero product zero take sum pair zero entry go long one zero product would zero fact be count many pair 1 case see two result mean well mean number value score function simply count many unique query term match document term match document two one s s zero document side similarly document term term query zero query vector nt count result score function basically measure many unique query term match document interpret score also take look d3 case see result 3 d3 match three distinctive query word news presidential campaign whereas d1 match two case seem reasonable rank d3 top d1 simplest vector space model indeed 
simplest vsm effective query = “ news presidential campaign ” d1 … news … d2 … news organic food campaign… d3 … news presidential campaign … f ( q d1 ) 2 f ( q d2 ) 3 f ( q d3 ) 3 d4 … news presidential campaign … … presidential candidate … f ( q d4 ) 3 d5 … news organic food campaign… campaign…campaign…campaign… f ( q d5 ) 2 10 look pretty good however examine model detail likely find problem be go show score five document easily verify be correct be basically count number unique query term match document note measure actually make sense right basically mean document match unique query term document assume relevant seem make sense problem note three document d2 d3 d4 tie 3 score s problem look carefully seem d4 rank d3 d3 mention presidential d4 mentioned multiple time case d3 presidential can dimension d4 clearly presidential campaign another problem d2 d3 also score look three word match case d2 match news campaign case d3 match news presidential campaign intuitively read better match presidential important match even though presidential query intuitively would like d3 rank d2 model nt mean model still good enough 
summary • vsm instantiation dimension vector placement similarity • simplest vsm – dimension = word – vector = 0-1 bit vector ( word absence ) – similarity = dot product – f ( q ) = number distinct query word match 11 solve problem summarize lecture talk instantiate vector space model mainly need three thing one define dimension second decide place document vector vector space also place query vector space vector third define similarity two vector particularly query vector document vector also talk various simple way instantiate vector space model indeed s probably simplest vector space model derive case use word define dimension use zero 1 bit vector represent document query case basically care word presence absence ignore frequency use dot product similarity function instantiation show score function basically score document base number distinct query word match document also show simple vector space model still nt work well need improve topic be go cover next lecture [ music ] 

1 [ sound ] lecture go talk improve instantiation vector space model 
course schedule small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 continue discussion vector space model be go focus improve instantiation model 
two problem simplest vsm query = “ news presidential campaign ” d2 … news organic food campaign… f ( q d2 ) 3 d3 … news presidential campaign … f ( q d3 ) 3 d4 … news presidential campaign … … presidential candidate … f ( q d4 ) 3 match “ presidential ” time deserve credit match “ presidential ” important match “ ” 3 previous lecture see simple instantiation vector space model come simple score function would give us basically account many unique query term match document also see function problem show slide particular look three document get score match three unique query word intuitively would like d4 rank d3 d2 really relevant problem function can nt capture follow heuristic first would like give credit d4 match presidential time d3 second intuitively match presidential important match common word occur everywhere nt really carry much content lecture let s see improve model solve two problem s worth think point problem look back assumption make instantiate vector space model will realize problem really come assumption particular place vector vector space naturally order fix problem revisit assumption perhaps use different way instantiate vector space model particular place vector different way 
improve vector placement term frequency vector w1 = ( x1 …xn ) xi = count word wi query yi = count word wi doc = ( y1 …yn ) w3 w2 4 let s see improve one natural thought order consider multiple time term document consider term frequency instead absence presence order consider difference document query term occur multiple time one query term occur consider term frequency count term document simplest model modeled presence absence term ignore actual number time term occur document let s add back be go represent document vector term frequency element say element query vector document vector 0 1s instead count word query document would bring additional information document see accurate representation document 
improve vsm term frequency weighting = ( x1 …xn ) = ( y1 …yn ) xi = count word wi query yi = count word wi doc sim ( q ) = 𝒙𝟏 𝒚𝟏 + … + 𝒙𝑵 𝒚𝑵 = 𝑵 𝒊=𝟏 𝒙𝒊 𝒚𝒊 ranking function intuitively capture fix problem simplest vsm 5 let s see formula would look like change representation will see slide still use dot product formula look similar form fact look identical inside sum course x different count word query document point also suggest pause lecture moment think interpret score new function s something similar simplest vsm change vector new score different interpretation see difference consideration multiple occurrence term document importantly would like know whether would fix problem simplest vector space model 
ranking used term frequency ( tf ) weighting d2 … news organic food campaign… = ( 1 = ( 1 d3 1 0 1 1 … news presidential campaign … = ( 1 = ( 1 d4 1 1 1 0 1 1 1 1 … news presidential campaign … … presidential candidate … = ( 1 = ( 1 1 0 1 2 1 1 f ( q d2 ) 3 0 … ) 1 … ) f ( q d3 ) 3 0 … ) 0 … ) f ( q d4 ) 4 0 … ) 0 … ) 6 let s look example suppose change vector representation term frequency vector let s look three document query vector word occur exactly query vector still 01 vector fact d2 also essentially represent way none word repeat many time result score also still true d3 still d4 would different presidential occur twice end presidential document vector would 2 instead result score d4 higher s 4 mean used term frequency rank d4 d2 d3 hope solve problem d4 also see d2 d3 still filter way still identical score fix problem 
fix problem 2 ( “ presidential ” vs “ ” ) d2 d3 … news organic food campaign… … news presidential campaign … = { news presidential campaign food … } = ( 1 = ( 1 1 1 1 0 1 1 0 … ) 1 … ) = ( 1 = ( 1 1 0 1 1 1 1 0 … ) 0 … ) f ( q d2 ) < 3 f ( q d3 ) > 3 7 fix problem intuitively would like give credit match presidential match solve problem general way way determine word treat importantly word basically ignore word really carry much content essentially ignore sometimes call word stock word generally frequent occur everywhere match nt really mean anything computationally capture encourage think little bit come statistical approach somehow distinguish presidential think moment will realize one difference word like occur everywhere count occurrence word whole collection see much higher frequency presidential tend occur document idea suggest can somehow use global statistic term information try down-weight element vector representation d2 time hope somehow increase weight presidential vector d3 expect d2 get overall score less 3 d3 get score would able rank d3 top d2 systematically rely statistical count 
improvement vector placement add inverse document frequency ( idf ) w1 = ( x1 …xn ) xi = count word wi query yi = c ( wi ) idf ( wi ) = ( y1 …yn ) w3 w2 8 case particular idea call inverse document frequency see document frequency one signal used modern retrieval function discuss previous lecture specific way used document frequency count document contain particular term say inverse document frequency actually want reward word nt occur many document way incorporate vector representation modify frequency count multiply idf corresponding word show penalize common word generally lower idf reward rare word higher idf 
idf weighting penalize popular term idf ( w ) log ( m+1 ) total number docs collection idf ( w ) = log [ ( m+1 ) k ] total number docs contain w ( doc frequency ) 1 k ( doc freq ) 9 specifically idf defined logarithm m+1 divide k total number document collection k df document frequency total number document contain word w plot function vary k would see curve would look like general see would give higher value low df word rare word also see maximum value function log m+1 would interesting think s minimum value function can interesting exercise specific function may important heuristic simply penalize popular term turn particular function form also work well whether s better form function open research question s also clear use linear penalization like s show line may reasonable standard idf particular see difference standard idf somehow turn point point be go say term essentially useful essentially ignore make sense term occur frequently let s say term occur 50 % document term unlikely important s basically common term s important match word standard idf see s basically assume low weight s difference look linear penalization point still difference intuitively will want focus discrimination low df word rather common word well course one work better still validate used empirically correlated dataset use user judge result better 
solve problem 2 ( “ presidential ” vs “ ” ) d2 d3 … news organic food campaign… … news presidential campaign … = { news idf ( w ) = 15 = ( 1 presidential campaign food … } 10 25 31 18 1 = ( 1*15 = ( 1 = ( 1*15 1*10 1 0 … ) 1 1 0 0 1*31 0 … ) 1 1*25 1 1*31 0 … ) 0 … ) f ( q d2 ) = 56 < f ( q d3 ) 71 10 let s see solve problem let s look two document without idf weighting term frequency vector idf weighting adjust tf weight multiply idf value example see adjustment particular s adjustment used idf value smaller idf value presidential look idf distinguish two word result adjustment would larger would make weight larger score new vector would happen course share weight news campaign match discriminate result idf weighting d3 rank d2 match rare word whereas d2 match common word show idf weighting solve problem 2 
effective vsm tf-idf weighting query = “ news presidential campaign ” d1 … news … f ( q d1 ) 25 d2 … news organic food campaign… f ( q d2 ) 56 d3 … news presidential campaign … f ( q d3 ) 71 d4 … news presidential campaign … … presidential candidate … f ( q d4 ) 96 d5 … news organic food campaign… campaign…campaign…campaign… f ( q d5 ) 139 11 effective model general used tf-idf weighting well let s look document see new score new document effective new weighting method new score function point let s see overall effective new ranking function tf-idf weighting show five document see score see score first four document seem quite reasonable expect however also see new problem d5 high score simplest vector space model actually high score fact highest score create new problem actually common phenomenon design retrieval function basically try fix one problem tend introduce problem s s tricky design effective ranking function s best ranking function open research question researcher still work next lecture be go also talk additional idea improve model try fix problem 
summary • improve vsm – dimension = word – vector = tf-idf weight vector – similarity = dot product – work better simplest vsm – still problem 12 summarize lecture have talk improve vector space model have get improve instantiation vector space model base td-idf weighting improvement mostly placement vector give high weight term occur many time document infrequently whole collection see improve model indeed look better simplest vector space model also still problem next lecture be go look address additional problem [ music ] 

1 [ music ] 
course schedule small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 lecture continue discussion vector space model particular be go talk tf transformation 
vsm tf-idf weighting still problem query = “ news presidential campaign ” d1 … news … f ( q d1 ) 25 d2 d3 d4 … news organic food campaign… f ( q d2 ) 56 … news presidential campaign … f ( q d3 ) 71 … news presidential campaign … … presidential candidate … f ( q d4 ) 96 d5 … news organic food campaign… campaign…campaign…campaign… f ( q d5 ) 139 3 previous lecture derive tf idea weighting formula used vector space model assume model actually work pretty well example show slide except d5 receive high score indeed receive highest score among document document intuitive non-relevant desirable lecture be go talk be go use tf transformation solve problem discuss detail let s take look formula simple tf-idf weighting ranking function see document receive high score 
ranking function tf-idf weighting total # docs collection 1 f ( q )   xi yi   c ( w q ) c ( w ) log df ( w ) 1 wq d n match query word d5 … news organic food campaign… campaign…campaign…campaign… doc frequency c ( “ campaign ” d5 ) 4 f ( q d5 ) 139 4 formula look formula carefully see involve sum match query term inside sum match query term particular weight weight tf-idf weighting idea component see two variable one total number document collection m document frequency number document contain word w variable involved formula include count query term w query count word document look document s hard realize reason receive high score high count campaign count campaign document 4 much higher document contribute high score document treat amount lower score document need somehow restrict contribution match term document think match term document carefully actually would realize probably nt reward multiple occurrence generously mean first occurrence term say lot match term go zero count count one increase mean lot see word document s likely document talk word see extra occurrence top first occurrence go one two also say well second occurrence kind confirm accidental manage word sure document talk word imagine see let s say 50 time word document add one extra occurrence go test evidence be already sure document word be think way seem restrict contribution 
tf transformation c ( w ) tf ( w ) term frequency weight = x y=tf ( w ) = log ( 1+x ) = log ( 1+log ( 1+x ) ) 2 1 bit vector 1 0 1 2 3 … x=c ( w ) 5 high count term idea tf transformation transformation function go turn real count word term frequency weight word document show x axis will count axis show term frequency weight previous breaking function actually imprison rate use kind transformation example 1 bit vector recantation actually use transformation function show basically count 0 0 weight otherwise would weight s flat used term count tf weight well s linear function exactly weight count see desirable want something like example algorithm function ca nt sublinear transformation look like control influence really high weight s go lower inference yet retain inference small count might want even bend curve apply logarithm twice person try method indeed work better linear form transformation 
tf transformation bm25 transformation term frequency weight large k y=tf ( w ) k+1 = 𝑘+1 𝑥 𝑥+𝑘 2 k=0 1 0 1 2 3 … x=c ( w ) 6 far work best seem special transformation call bm25 transformation bm stand best match transformation see s parameter k k control upper bound function s easy see function upper bound look x divide x + k k non-active number numerator never able exceed denominator right s upper bound k+1 also difference transformation function logarithm transformation nt upper bound furthermore one interesting property function vary k actually simulate different transformation function include two extreme show 1 bit transformation linear transformation example set k 0 see function value precisely recover 1 bit transformation set k large number hand s go look like linear transformation function sense transformation flexible allow us control shape transformation also nice property upper bound upper bound useful control inference particular term prevent spammer increase count one term spam query might match term word upper bound might also ensure term would count aggregate weight compute score say transformation function work well far 
summary • sublinear tf transformation need – capture intuition “ diminish return ” higher tf – avoid dominance one single term other • bm25 transformation – upper bound – robust effective • ranking function bm25 tf ( k > 0 ) n f ( q ) = å xi yi = i=1 å wîqçd c ( w q ) ( k 1 ) c ( w ) 1 log c ( w ) + k df ( w ) 7 summarize lecture main point need sublinear tf transformation need capture intuition diminish return higher term count s also avoid dominance one single term other bm25 transformation talk interesting s far one best-perform tf transformation formula upper bound s also robust effective be plug function tf-idf weighting vector space model will end follow ranking function bm25 tf component already close state odd ranking function call bm25 will discuss improve formula next lecture [ music ] 

1 
course schedule small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 
document length query = “ news presidential campaign ” d4 d6 … news presidential campaign … … presidential candidate … 100 word d6 > d4 … campaign… campaign 5000 word ………………………………………………………………… news …………………………………………………………………… …………………………………………………… …………………………………………………………………… …………………………………… ………………………… …………………… presidential …… presidential…… 
document length normalization • penalize long doc doc length normalizer – long doc better chance match query – need avoid over-penalization • document long – used word  penalization – content  less penalization • pivot length normalizer average doc length “ pivot ” – normalizer = 1 | average doc length ( avdl ) 4 
pivot length normalization b  [ 01 ] d | normalizer  1  b  b avdl b > > 0 b > 0 reward 10 penalization 0 1 2 … shorter avdl avdl … longer avdl b=0 | 5 
state art vsm ranking function • pivot length normalization vsm [ singhal et al 96 ] f ( q )   c ( w q ) wq d ln [ 1  ln [ 1  c ( w ) ] ] 1 log d | df ( w ) 1 b  b avdl • okapi [ robertson & walker 94 ] f ( q )   wq d c ( w q ) b  [ 01 ] k1 k3  [ 0  ) ( k  1 ) c ( w ) c ( w )  k ( 1  b  b d | ) avdl log 1 df ( w ) 6 
improvement vsm • improve instantiation dimension – stem word stop word removal phrase latent semantic indexing ( word cluster ) character n-grams … – bag-of-words phrase often sufficient practice – language-specific domain-specific tokenization important ensure “ normalization term ” • improve instantiation similarity function – cosine angle two vector – euclidean – dot product seem still best ( sufficiently general especially appropriate term weighting ) 7 
improvement bm25 • bm25f [ robertson & zaragoza 09 ] – use bm25 document structure ( “ f ” field ) – key idea combine frequency count term field apply bm25 ( instead way ) • + [ lv & zhai 11 ] – address problem penalization long document bm25 add small constant tf – empirically analytically show better bm25 8 
summary vector space model • • • • relevance ( q ) = similarity ( q ) query document represent vector heuristic design ranking function major term weighting heuristic – tf weighting transformation – idf weighting – document length normalization • bm25 pivot normalization seem effective 9 
additional reading • a singhal c buckley m mitra pivot document length normalization proceedings acm sigir 1996 • s e robertson s walker simple effective approximation 2-poisson model probabilistic weight retrieval proceedings acm sigir 1994 • s robertson h zaragoza probabilistic relevance framework bm25 beyond find trend inf retr 3 4 ( april 2009 ) • y lv c zhai lower-bound term frequency normalization proceedings acm cikm 2011 10 

1 [ music ] lecture implementation text retrieval system 
implementation text retrieval system text retrieval problem small relevant datum user text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 lecture discuss implement text retrieval method build search engine main challenge manage lot text datum enable query answer quickly respond many query 
typical tr system architecture docs query offline & online feedback tokenizer doc rep ( index ) indexer offline query rep index judgment user scorer result online 3 typical text retrieval system architecture see document first processed tokenizer get tokenize unit example word word token processed indexer create index datum structure search engine use quickly answer query query would go similar process step tokenizer would apprise query well text processed way unit would match query s representation would give scorer would use index quickly answer user s query score document ranking result give user user look result provide us feedback explicit judgement document good document bad implicit feedback user nt anything extra end user look result skip click result view interact signal used system improve ranking accuracy assume view document better skip one search engine system divide three part first part indexer second part scorer respond user query third part feedback mechanism typically indexer do offline manner pre-process correct datum build inventory index introduce moment datum structure used online module scorer process user s query dynamically quickly generate search result feedback mechanism do online offline depend method implementation indexer scorer standard main topic lecture next lecture feedback mechanism hand variation depend method used usually do algorithms specific way let s first talk tokenizer 
tokenization • normalize lexical unit word similar meaning map indexing term • stem mapping inflectional form word root form eg – computer - > compute – computation - > compute – compute - > compute • language ( eg chinese ) pose challenge word segmentation 4 tokernization normalize lexical unit form semantically similar word match language like english stem often used map inflectional form word root form example computer computation compute match root form compute way different form compute match normally good idea increase coverage document match query s also always beneficial sometimes subtlest difference computer computation might still suggest difference coverage content case stem seem beneficial tokenize text language example chinese might face special challenge segment text find word boundary s obvious boundary s space separate course use language specific process technique tokenization would index text document will convert document datum structure enable faster search basic idea precompute much basically 
indexing • indexing = convert document datum structure enable fast search ( precompute much ) • inverted index dominate indexing method support basic search algorithms • index ( eg document index ) may need feedback 5 commonly used index call inverted index used many search engine support basic search algorithms sometimes index example document index might need order support feedback like say kind technique really standard vary lot accord feedback method understand want use inverted index useful think would respond single term query quickly want use time think pause video think pre process text datum quickly respond query one word 
inverted index example dictionary ( lexicon ) doc 1 … news doc id freq 1 1 # docs total freq news 3 3 2 1 campaign 2 2 3 1 presidential 1 2 2 1 food 1 1 3 1 … … … 3 2 2 1 doc 3 … … … news presidential campaign … … presidential candidate … … … doc 2 … news organic food campaign… term posting position p1 p2 p3 p4 p5 p6 p7 p8 6 thought question might realize best simply create list document match every term vocabulary way basically pre-construct answer see term simply fetch random list document term return list user s fastest way respond single term idea invert index actually basically like be go pre-construct search index allow us quickly find document match particular term let s take look example three document document see previous lecture suppose want create inverted index document want maintain dictionary dictionary one entry term be go store basic statistic term example number document match term total number code frequency term mean would kind duplicate occurrence term example news term occur three document count document three might also realize need count document document frequency compute statistic used vector space model think weighting heuristic would need count well s idea right inverse document frequency idf property term compute right document count s easy compute idea either time old index random time see query addition basic statistic will also store document match news entry store file call posting case match three document store information three document document id document 1 frequency tf one news second document s also 1 et cetera list get document match term news also know frequency news document query one word news easily look table find entry go quicker posting fetch document match let s take look another term time let s take look word presidential would occur one document document document frequency 1 occur twice document frequency count two frequency count used reachable method might use frequency assess popularity term collection similarly will pointer posting case one entry term occur one document s document id 3 occur twice basic idea inverted index s actually pretty simple right structure easily fetch document match term basis score document query sometimes also want store position term many case term occur document s one position example case case term occur twice s two position position information useful check whether match query term actually within small window let s say five word ten word whether match two query term fact phrase two word checked quickly used position 
inverted index fast search • single-term query • multi-term boolean query – must match term “ ” term “ b ” – must match term “ ” term “ b ” • multi-term keyword query – similar disjunctive boolean query ( “ ” “ b ” ) – aggregate term weight • efficient sequentially scanning docs ( ) 7 inverted index good fast search well talk possibility used two answer single-term query s easy multiple term query well let s first look special case boolean query boolean query basically boolean expression like want value document match term term b s one conjunctive query want web document match term term b s disjunctive query answer query used inverted index well think bit would obvious simply fetch document match term also fetch document match term b take intersection answer query like b take union answer query b easy answer s go quick multi-term keyword query talk vector space model example match query document generate score score base aggregate term weight case s boolean query score actually do similar way basically s similar disjunctive boolean query basically s like b take union document match least one query term would aggregate term weight basic idea used inverted index score document general be go talk detail later let s look question index good idea basically efficient sequentially scanning document obvious approach compute score document sort straightforward method go slow imagine wealth s lot document take long time answer query question would invert index much faster well word distribution text 
empirical distribution word • stable language-independent pattern person use natural language • word occur frequently occur rarely eg news article – top 4 word 10~15 % word occurrence – top 50 word 35~40 % word occurrence • frequent word one corpus may rare another 8 s common phenomena word distribution text language independent pattern seem stable pattern basically characterize follow pattern word like common word like occur frequently text account large percent occurrence word word would occur rarely many word occur let s say document collection many s also true frequent word one corpus rare another mean although general phenomenon applicable observed many case exact word common may vary context context 
zipf ’ law • rank * frequency  constant word freq highest frequency biggest datum structure ( stop word ) f ( w )  c r ( w )    1 c  01 intermediate frequency word many rare word word rank ( freq ) 9 phenomena characterize s call zipf s law law say rank word multiply frequency word roughly constant formally use f ( w ) denote frequency r ( w ) denote rank word formula basically say thing mathematical term c basically constant also parameter alpha might adjust better fit empirical observation plot word frequency sort order see easily x axis basically word rank r ( w ) axis word frequency f ( w ) curve show product two roughly constant look word see separated three group middle s intermediary frequency word word tend occur quite document like frequent word also rare tend often used query also tend high tf-idf weight intermediate frequency word look left part curve highest frequency word cover frequently usually word like etc word frequent fact two frequent discriminate generally useful retrieval often remove call stop word removal use pretty much kind word collection kind infer word might stop word basically highest frequency word also occupy lot space inverted index imagine post entry word would long therefore remove word save lot space inverted index also show tail part lot rare word word nt occur frequently many word word actually useful search also user happen interested topic be rare s often true user nt necessarily interested word retain would allow us match document accurately generally high idf 
datum structure inverted index • dictionary modest size – need fast random access – prefer memory – hash table b-tree trie … • posting huge – sequential access expect – stay disk – may contain docid term term pos etc – compression desirable 10 kind datum structure use store inverted index well two part right recall dictionary also posting dictionary modest size although web s still go large compare posting s distinct also need fast random access entry be go look query term quickly therefore will prefer keep dictionary memory s possible collection large feasible collection large s general possible vocabulary size large obviously ca nt general s go datum structure often use store dictionary would direct access structure like hash table b-tree ca nt store everything memory use disk try build structure would allow quickly look entry posting huge general nt direct access specific entry generally would look sequence document id frequency document match query term would read entry sequentially therefore s large generally store posting disc stay disc would contain information document id term frequency term position etcetera large compression often desirable save disc space course one benefit compression go occupy much space s also help improve speed see well know input output would cost lot time comparison time take cpu cpu much faster io take time compress inverter index oppose file become smaller entry reading memory process query term would smaller reduce amount tracking io save lot time course process datum uncompress datum memory say cpu fast still save time compression save disc space speed load index [ music ] 

1 [ sound ] lecture inverted index construction 
system implementation inverted index construction text retrieval problem small relevant datum user text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 lecture continue discussion system implementation particular be go discuss construct inverted index 
construct inverted index • main difficulty build huge index limit memory • memory-based method usable large collection • sort-based method – step 1 collect local ( termid docid freq ) tuple – step 2 sort local tuple ( make “ run ” ) – step 3 pair-wise merge run – step 4 output inverted file 3 construction inverted index actually easy dataset small s easy construct dictionary store posting file problem datum able fit memory use special method deal unfortunately retrieval application dataset large generally load memory many approach solve problem sorting-based method quite common work four step show first collect local termid documentid frequency tuple basically locate term small set document collect account sort count base term able local partial inverted index call round write temporary file disk merge step pairwise merge run eventually merge run generate single inverted index 
sort-based inversion sort doc-id doc1 doc2 doc300 < 113 > < 212 > < 311 > < 122 > < 323 > < 422 > … sort term-id < 113 > < 122 > < 212 > < 243 > < 153 > < 162 > … info term 1 < 113 > < 122 > < 152 > < 163 > < 13003 > < 212 > … < 13003 > < 33001 > < 12993 > < 13001 > < 50002991 > < 50003001 > parse & count “ local ” sort merge sort term lexicon 1 campaign 2 news 3 a4 docid lexicon doc1 1 doc2 2 doc3 3 4 illustration method left see document right term lexicon document id lexicon lexicon map string-based representation document id term integer representation map back integer stream representation reason want interest used integer present id integer often easier handle example integer used index array also easy compress one reason tend map string integer nt carry string around approach work well s simple be go scan document sequentially parse document count frequency term stage generally sort frequency document id process document sequentially will first encounter term first document therefore document id one case follow document id two natural result process datum sequential order point run memory would write disc be go sort use whatever memory sort time be go sort base term id note be used term id key sort entry share term would group together case see id document match term 1 would group together be go write temporary file would allow use memory process make batch document be go document be go write lot temporary file disc next stage merge sort basically be go merge sort eventually get single inverted index entry sort base term id top be go see older entry document match term id basically construction inverted index even though datum load manner 
inverted index compression • general leverage skewer distribution value use variable-length encode • tf compression – small number tend occur far frequently large number ( ) – fewer bit small ( high frequency ) integer cost bit large integer • doc id compression – “ d-gap ” ( store difference ) d1 d2-d1 d3-d2 … – feasible due sequential access • method binary code unary code -code -code … 5 mention earlier hosting large s desirable compress let s take little bit compress inverted index well idea compression general leverage skewer distribution value generally use variable-length encode instead fixed-length encode use default program manager like + leverage skewer distribution value compress value well general use bit encode frequent word cost used longer bit string code rare value case let s think compress tf tone frequency picture inverted index look like see post thing lot tone frequency frequency term document think kind value frequent probably able guess small number tend occur far frequently large number well think distribution word sip slope many word occur rarely see lot small number therefore use fewer bit small highly frequent integer s cost used bit larger integer trade course value distribute uniform wo nt save us space tend see many small value frequent save average even though sometimes see large number use lot bit document id also see posting well distribute skewer way deal well turn use trick call d-gap store difference term id imagine term match many document longest document id take gap take difference adjacent document id gap small see lot small number whereas term occur document gap would large large number would frequent create skewer distribution would allow us compress value also possible order uncover uncompress document id sequentially process datum store difference order recover exact document id first recover previous document id add difference previous document id restore current document id possible need sequential access document id look term look document id match term sequentially process s natural s trick actually work many different method encode binary code commonly used code program language use basically fix glance coding unary code gamma code delta code possibility many possibility 
integer compression method • binary equal-length coding • unary x1 code x-1 one bit follow 0 eg = > 110 = > 11110 • -code = > unary code 1+log x follow uniform code x-2 log x log x bit eg = > 101 = > 11001 • -code -code replace unary prefix -code eg = > 1001 = > 10101 6 let s look detail binary coding really equal length coding s property randomly distribute value unary coding variable length coding method case integer 1 encode x 1 1 bit follow example 3 encode 2 1s follow 0 whereas 5 encode 4 1s follow 0 etc imagine many bit use large number like 100 many bit use exactly number like 100 well exactly use 100 bit s number bit value number inefficient likely see large number imagine occasionally see number like 1000 use 1000 bit work well absolutely sure large number mostly often see small number decode code since variable length encode method ca nt count many bit stop ca nt say 8-bit 32-bit start another code variable length rely mechanism case unary see s easy see boundary easily see 0 would signal end encode count many 1s see end hit finished one number start another number see unary coding aggressive reward small number occasionally see big number would disaster less aggressive method well gamma coding s one method use unary coding transform form s 1 plus floor log x magnitude value much lower original x s afford used unary code first unary code coding log x would follow uniform code binary code basically uniform code binary code be go use coder code remain part value x basically precisely x-1 floor log x unary code basically call flow log x well add one remain part will used uniform code actually code difference x 2 log x s easy show difference need use many bit floor log x bit easy understand difference large would higher floor log x example example 3 encode first two digit unary code nt value 2 10 encode 2 unary coding mean floor log x 1 wo nt actually use unary code code 1 plus flow log x since two know flow log x actually 3 still larger 2 difference 1 1 encode end s 101 similarly 5 encode 110 follow case unary code code unary code 110 flow log x mean be go compute difference 5 2 2 s 1 end time be go use 2 bit level flow log x can number 5 6 7 would share prefix order differentiate use 2 bit end differentiate imagine 6 would 10 end instead 01 s also true form gamma code always first odd number bit center s end unary code left side 0 1s right side 0 s binary coding uniform coding decode code well first unary coding hit 0 get unary code also tell many bit read decode uniform code decode gamma code also delta code be basically gamma code except replace unary prefix gamma code s even less conservative gamma code term wording small integer mean s okay occasionally see large number s okay delta code s also fine gamma code s really big loss unary code operate course different degree favore short favore small integer also mean would appropriate sort distribution none perfect distribution method work best would depend actual distribution dataset inverted index compression person find gamma coding seem work well 
uncompress inverted index • decode encode integer – unary decode count 1 ’ see zero – -decode • first decode unary part let value k+1 • read k bit decode binary code let value r • value encode number 2k+r • decode doc id encode used d-gap – let encode id list x1 x2 x3 … – decode x1 obtain doc id1 decode x2 add re-cover value doc id1 obtain – repeatedly decode x3 x4 re-cover value previous doc id 7 uncompress inverted index talk firstly decode encode integer think discuss decode unary coding gamma coding document id might compress used d-gap well be go sequential decode suppose encode list x1 x2 x3 etc first decode x1 obtain first document id id1 decode x2 actually difference second id first one add decoder value x2 id1 recover value id secondary position see advantage convert document id integer allow us kind compression repeat decode document every time use document id previous position help recover document id next position [ music ] 

1 
system implementation fast search text retrieval problem small relevant datum user text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 
score document quickly general form score function final score adjustment f q = fa ( 𝒉 𝒈 𝒕𝟏 𝒅 𝒒 … 𝒈 𝒕𝒌 𝒅 𝒒 weight aggregation 𝑓𝑑 𝑑 𝑓𝑞 𝑞 ) weight match query term 3 
general algorithm ranking document f q = fa ( 𝒉 𝒈 𝒕𝟏 𝒅 𝒒 … 𝒈 𝒕𝒌 𝒅 𝒒 𝑓𝑑 𝑑 𝑓𝑞 𝑞 ) • 𝑓𝑑 𝑑 𝑓𝑞 𝑞 pre-compute • maintain score accumulator compute h • query term ti – fetch inverted list { ( d1 f1 ) … ( dn fn ) } – entry ( dj fj ) compute g ( ti dj q ) update score accumulator doc di incrementally compute h • adjust score compute fa sort 4 
example ranking base tf sum f ( q ) g ( t1 q ) + g ( tk q ) g ( ti q ) = c ( ti ) query = “ info security ” info ( d1 3 ) ( d2 4 ) ( d3 1 ) ( d4 5 ) security ( d2 3 ) ( d41 ) ( d5 3 ) accumulator d1 0 ( d13 ) = > 3 ( d24 ) = > 3 info ( d31 ) = > 3 ( d45 ) = > 3 ( d23 ) = > 3 ( d41 ) = > 3 security ( d53 ) = > 3 d2 0 0 4 4 4 7 7 7 d3 0 0 0 1 1 1 1 1 d4 0 0 0 0 5 5 6 6 d5 0 0 0 0 0 0 0 3 5 
improve efficiency • cach ( eg query result list inverted index ) • keep promising accumulator • scaling web-scale ( need parallel process ) 6 
text retrieval toolkit • • • • • lucene http indri http terrier http meta http find http resource 7 
summary system implementation • inverted index construction – preprocess datum much – compression appropriate • fast search used inverted index – exploit inverted index accumulate score document match query term – exploit zipf ’ law avoid touch many document match query term – support wide range ranking algorithms • great potential scaling used distribute file system parallel process cach 8 
additional reading • ian h witten alistair moffat timothy c bell manage gigabyte compress indexing document image second edition morgan kaufmann 1999 • stefan büttcher charle l a clarke gordon v cormack information retrieval - implement evaluate search engine mit press 2010 9 

1 [ music ] 
evaluation textschedule retrieval system course small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 lecture evaluation text retrieval system previous lecture talk number text retrieval method different kind ranking function know one work best order answer question compare mean evaluate retrieval method main topic lecture 
evaluation • reason 1 assess actual utility tr system – measure reflect utility user real application – usually do user study ( interactive ir evaluation ) • reason 2 compare different system method – measure need correlated utility actual user thus ’ accurately reflect exact utility user – usually do test collection ( test set ir evaluation ) 3 first let think evaluation already give one reason use evaluation figure retrieval method work better important advance knowledge otherwise would nt know whether new idea work better old idea begin course talk problem text retrieval compare datum base retrieval mentioned text retrieval empirically defined problem evaluation must rely user system work better would judge user become challenge problem get user involved evaluation fair comparison different method go back reason evaluation list two reason second reason basically say also another reason assess actual utility text regional system imagine be build annual application would interesting know well search engine work user case match must reflect utility actual user real occasion typically do used user starter used real search engine second case second reason measure actually need collate utility actually use thus nt accurately reflect exact utility user measure need good enough tell method work better usually do test collection main idea will talk course important compare different algorithms improve search engine system general 
measure • accuracy accurate search result – measure system ’ ability ranking relevant docucment top non-relevant one • efficiency quickly user get result much compute resource need answer query – measure space time overhead • usability useful system real user task – user study 4 let s talk measure many aspect search measure evaluate list three major aspect one effectiveness accuracy accurate search result case be measure s capability ranking relevant document top non relevant one second efficiency quickly get result much compute resource need answer query case need measure space time overhead system third aspect usability basically question useful system new user task obviously interface many thing also important would typically user study course be go talk mostly effectiveness accuracy measure efficiency usability dimension really unique search engine need without software system also good coverage cause evaluate search engine s quality accuracy something unique text retrieval be go talk lot 
cranfield evaluation methodology • methodology laboratory testing system component develop 1960s • idea build reusable test collection & define measure – sample collection document ( simulate real document collection ) – sample set topic ( simulate user query ) – relevance judgment ( ideally make user formulate query )  ideal rank list – measure quantify well system ’ result match ideal rank list • test collection reus many time compare different system 5 main idea person propose used test set evaluate text retrieval algorithm call cranfield evaluation methodology one actually develop long time ago develop 1960s s methodology laboratory test system component sampling methodology useful search engine evaluation also evaluate virtually kind empirical task example natural language process field problem empirical find typically would need use methodology today big datum challenge use machine learn everywhere methodology popular first develop search engine application 1960s basic idea approach build reusable test collection define measure test collection build used test different algorithms be go define measure allow quantify performance system algorithm exactly work well sample collection document adjust simulate real document collection search application be go also sample set query topic little simulator used query will relevance judgment judgment document return query ideally make user formulate query person know exactly document would used finally match quantify well system s result match ideal rank list would construct base user s relevance judgement methodology useful start retrieval algorithms test reus many time also provide fair comparison method criterium dataset used compare different algorithms allow us compare new algorithm old algorithm divide many year ago used standard 
test collection evaluation query ra q1 q2 q3 … q50 system … d2 + d1 + d4 - rb d2 d1 relevance judgment = q1 system b d3 d48 document collection d1 + d4 d3 d5 + d2 + better ra rb quantify q1 d1 + q1 d2 + q1 d3 – q1 d4 – q1 d5 + … q2 d1 – q2 d2 + q2 d3 + q2 d4 – … q50 d1 – q50 d2 – q50 d3 + 6 illustration work say need query show q1 q2 etc also need document s call document cach right side see need relevance judgment basically binary judgment document respect query example d1 judge relevant q1 d2 judge relevant well d3 judge relevant q1 etc create user basically test collection two system want compare run system query document system would return result let s say query q1 would result show r sub result system a remember talk task compute approximation relevant document set r sub system s approximation r sub b system s approximation relevant document let s take look result better imagine user one would like let s take look result difference document return system look result feel maybe better sense many number element document among three document return two relevant s good s precise hand one council say maybe b better have get document have get three instead two one better quantify well obviously question highly depend user s task depend user well might even imagine user may system better user interested get random document right case user read million user see relevant document hand one also imagine user might need many random document possible example be literature survey might sigma category might find system b better case also define measure quantify might need define multiple measure user different perspective look result [ music ] 

1 [ sound ] lecture basic measure evaluation text retrieval system 
evaluation tr system basic measure small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 lecture be go discuss design basic measure quantitatively compare two retrieval system 
test collection evaluation query ra q1 q2 q3 … q50 system … d2 + d1 + d4 - rb d2 d1 relevance judgment = q1 system b d3 d48 document collection d1 + d4 d3 d5 + d2 + better ra rb quantify q1 d1 + q1 d2 + q1 d3 – q1 d4 – q1 d5 + … q2 d1 – q2 d2 + q2 d3 + q2 d4 – … q50 d1 – q50 d2 – q50 d3 + slide see earlier lecture talk granville evaluation methodology test faction consist query document [ inaudible ] run two system datum set contradict evaluator performance raise question set result better system better system b better let s talk accurately quantify performance suppose total 10 relevant document collection query relevant judgment show right [ inaudible ] obviously see 3 [ inaudible ] [ inaudible ] document imagine random document judge query intuitively thought system better much noise particular see among three result two relevant system b five result three relevant intuitively look like system accurate infusion capture match holder position simply compute extent retrieval result relevant 100 % position would mean retrieval document relevant case system position two three system b sweet hold 5 show system better frequency also talk system b might prefer unit would like retrieve many random document possible case will compare number relevant document 
test collection evaluation query ra q1 q2 q3 … q50 system … d2 + d1 + d4 - 3 10 rb d2 d1 relevance judgment = q1 total # rel docs = 10 system b d3 d48 document collection d1 + d4 d3 d5 + d2 + 5 10 q1 d1 + q1 d2 + q1 d3 – q1 d4 – q1 d5 + … q2 d1 – q2 d2 + q2 d3 + q2 d4 – … q50 d1 – q50 d2 – q50 d3 + retrieve s another method call recall method used completeness coverage random document retrieval result assume ten relevant document collection have get two system a recall 2 whereas system b call 3 s 3 see recall system b better two measure turn basic measure evaluate search engine important also widely used many test evaluation problem example look application machine learn tend see precision recall number report kind task 
evaluate set retrieve docs precision recall action doc relevant relevant precision  recall  ac ab retrieve relevant retrieve irrelevant retrieve c retrieve relevant reject b irrelevant reject ideal result precision=recall=10 reality high recall tend associate low precision set defined cutoff ( eg precision @ 10 docs ) 5 okay let s define two measure precisely measure evaluate set retrieve document mean consider approximation set relevant document distinguish 4 case depend situation document document retrieve retrieve right talk set result document also relevant relevant depend whether user think useful document count document four category represent number document retrieve relevant b document retrieve rather etc table define precision ratio relevant retrieve document total relevant retrieve document divide sum c sum column singularly recall defined divide sum b s divide sum row instead column right see precision recall focuse look s number retrieve relevant document be go use different denominator okay would ideal result well easily see ideal case would precision recall oil mean get 1 % relevant document result result return relevant least s single relevant document return reality however high recall tend associate low precision imagine s case go try get many random document possible tend encounter lot document precision go note set also defined cut rest s although two measure defined retrieve document actually useful evaluate rank list fundamental measure task retrieval many task often interested precision ten document web search mean look many document among top ten result actually relevant meaningful measure tell us many relevant document user expect see first page typically show ten result precision recall basic match need use evaluate search engine build block 
combine precision recall f-measure f  1 1  r 2 pr f1  pr 2  2 1 1  2 1 05*p+05*r (  2  1 ) p * r  1  2p  r p p precision r recall  parameter ( often set 1 ) 6 say tend trailoff precision recall naturally would interesting combine s one method s often used call f-measure s [ inaudible ] mean precision recall defined slide see first compute inverse r p would interpret 2 used coefficient depend parameter beta transformation easily see would form case become agent precision recall beta parameter s often set control emphasis precision recall always set beta 1 end special case f-measure often call f1 popular measure s often used combine precision recall formula look simple s s easy see larger precision larger recall f measure would high s interesting trade precision recall capture interesting way f1 order understand first look natural combine used symbol arithmetically efficient would likely natural way combine think want think pause video good f1 s problem think arithmetic mean see sum multiple term case s sum precision recall case sum total value tend dominate large value mean high p high r really nt care whether value low whole sum would high desirable one easily perfect recall perfect recall easily imagine s probably easy imagine simply retrieve document collection perfect recall give us 05 average result clearly useful user even though average used formula would relevantly high contrast see f 1 would reward case precision recall roughly seminar would case extremely high value one mean f one encode different trade example show actually important methodology try solve problem might naturally think one solution let s say error mechanism s important settle source s important think whether way combine think multiple variance s important analyze difference think one make sense case think carefully think f1 probably make sense simple although case may different result case seem reasonable nt pay attention subtle difference might take easy way combine go ahead later find measure nt seem work well right methodology actually important general solve problem try think best solution try understand problem well know need measure need combine precision recall use guide find good way solve problem 
summary • • • • precision retrieve result relevant recall relevant document retrieve f measure combine precision recall tradeoff precision recall depend user ’ search task 7 summarize talk precision address question retrievable result relevant also talk recall address question relevant document retrieve two two basic match text retrieval used many task well talk f measure way combine precision precision recall also talk tradeoff precision recall turn depend user s search task will discuss point later lecture [ music ] 

1 [ music ] lecture evaluate rank list 
evaluation tr system evaluate rank list small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum lecture continue discussion evaluation particular go look evaluate rank list result previous lecture talk precision-recall two basic measure quantitatively measure performance search result talk ranking frame text retrieval problem ranking problem also need evaluate quality rank list use precision-recall evaluate rank list well naturally look precision-recall different cut-off end approximation relevant document set give rank list determine user stop browse right assume user securely browse list result user would stop point point would determine set s important cut-off consider compute precision-recall without know exactly user would stop consider position user can stop let s look position look slide let s look user stop first document s precision-recall point think well s easy see document precision one one get one document s relevent recall well note be assume ten relevant document query collection s one ten user stop second position top two well precision 100 % two two record two ten user stop third position well interesting case get additional relevant document record change precision lower have get number [ inaudible ] s exactly precision well s two three right recall two ten would see another point recall would different look list well wo nt happen see another relevant document case d5 point recall increase three ten precision three five see keep also get d8 precision four eight eight document four relevant recall four ten get recall five ten well list nt go list nt know convenience often assume precision zero othe precision zero level recall beyond search result course pessimistic assumption actual position would higher make make assumption order easy way compute another measure call average precision discuss later also say see make assumption clearly accurate okay purpose compare text method relative comparison s okay actual measure actual actual number deviate little bit true number long deviation bias toward particular retrieval method okay still accurately tell method work better important point keep mind compare different algorithms key s avoid bias toward method long avoid s okay transformation measure anyway preserve order okay will talk get lot precision-recall number different position 
evaluate ranking precision-recall ( pr ) curve total number relevant document collection 10 precision d1 + d2 + d3 – d4 – d5 + d6 – d7 – d8 + d9 – d10 – recall 1 2 3 10 10 10 5 10 8 10 10 10 06 01 02 03 … 10 assume precision=0 3 imagine plot curve show x-axis show recall y-axis show precision precision line marked 1 2 3 right different level recall y-axis also different amount s precision plot precision-recall number get point picture link point form curve will see assume precision high-level recall zero s zero actual curve probably something like discuss nt matter much compare two method would underestimated method okay precision-recall curve 
compare pr curf precision ideal system precision b x b x x x better x x x recall x recall 4 compare rank back list right mean compare two pr curf show two case system show red system b show blue s cross right one better hope see system clearly better level recall see level recall see precision point system better system b s question imagine code look like ideal search system well perfect precision recall point line would ideal system general higher curve better right problem might see case like actually happen often like two curf cross case one better think real problem actually might face suppose build search engine old algorithm s show blue system b come new idea test result show red curve a question new method better old method practically replace algorithm be already used search engine another new algorithm use system method replace method b go real decision make make replacement search engine would behave like system whereas nt like system b want spend time think pause video s actually useful think say s real decision make build search engine be work company care search thought moment might realize well case s hard say user might like system user might like like system b s difference well difference know low level recall region system b better s higher precision high recall region system better also mean depend whether user care high recall low recall high precision imagine someone go check s happen today want find something relevant news well one better think case clearly system b better user unlikely examine lot result user nt care high recall hand think case user start problem want find whether idea ha start case emphasize high recall want see many relevant document possible therefore might favor system a mean one better actually depend user precisely user task mean may necessarily able come one number would accurately depict performance look overall picture yet say practical decision make whether replace another may actually come single number quantify method compare many different method research ideally one number compare easily make lot comparison reason desirable one single number match 
summarize ranking total number relevant document collection 10 precision d1 + d2 + d3 – d4 – d5 + d6 – d7 – d8 + d9 – d10 – recall 1 2 3 10 10 10 5 10 8 10 0 1 1 10  2 2  3 5  4 8 000000 10 average precision 06 01 02 03 … 10 10 5 need number summarize range precision-recall curve right one way summarize whole rank list whole curve look area underneath curve right one way measure way measure turn particular way match popular used since long time ago text basically way s call average precision basically be go take look every different recall point look precision know know one precision another different recall nt count one recall level be go look number s precision different recall level et cetera know add precision different point corresponding retrieve first relevant document second third follow et cetera miss many relevant document case assume zero precision finally take average divide ten total number relevant document collection note be divide sum four number retrieve relevant document imagine divide four would happen think moment s common mistake person sometimes overlook right divide four s actually good fact favore system would retrieve random document case denominator would small would good match note denomina denominator ten total number relevant document basically compute area need occur standard method used evaluate rank list note actually combine recall precision first know precision number secondly also consider recall miss many would many zero right combine precision recall furthermore see measure sensitive small change position relevant document let s say move relevant document little bit would increase mean average precision whereas move relevant document let s say move relevant document would decrease uh average precision good s sensitive ranking every relevant document tell small difference two rank list want sometimes one algorithm work slightly better another want see difference contrast look precision ten document look whole set well s precision think well s easy see s four ten right precision meaningful tell us user would see s pretty useful right s meaningful measure user perspective use measure compare system would nt good would nt sensitive four relevant document rank move around precision ten still right good measure compare different algorithms contrast average precision much better measure tell difference different difference rank list subtle way [ music ] 

1 [ sound ] 
evaluation tr system evaluate rank list small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 
mean average precision ( map ) • average precision – average precision every cutoff new relevant document retrieve – normalizer = total # relevant docs collection – sensitive rank relevant document • mean average precision ( map ) – map = arithmetic mean average precision set query – gmap = geometric mean average precision set query – better map gmap 6 average precision computer one one query generally experiment many different query avoid variance across query depend query use might make different conclusion right s better used query use query also take average average precision query well naturally think arithmetic mean always tend think way would give us s call ` mean average position map case take arithmetic mean average precision several query topic mentioned another lecture good call talk different way combine precision recall conclude arithmetic mean good map measure s also think alternative way aggregate number nt automatically assume though let s also take arithmetic mean average position query let s think be best way aggregate think different way naturally probably able think another way geometric mean call kind average gmap another way think two different way thing natural question ask one better use map gmap s important question imagine testing new algorithm compare way old algorithms make search engine test multiple topic have get average precision topic think look overall performance take average strategy would use first also think question well make difference think scenario used one would make difference would give different ranking method also mean depend way average detect average average position get different conclusion make question become even important right one would use well look difference different way aggregate average position will realize arithmetic mean sum dominate large value large value mean mean query relatively easy high pre average position whereas gmap tend affected low value query nt good performance average precision low think improve search engine difficult query gmap would prefer right hand want improve lot kind query particular popular query might easy want make perfect maybe map would prefer answer depend user user task pref preference point think multiple way solve problem compare think carefully difference one make sense often one might make sense one situation another might make sense different situation s important pick situation one prefer 
special case mean reciprocal rank • ’ one relevant document collection ( eg know item search ) – average precision = reciprocal rank = r r rank position single relevant doc – mean average precision  mean reciprocal rank – simply use r 7 special case mean average position also think case precisely one rank document happen often example s call know item search know target page s say find amazon homepage one relevant document hope find s call ` know item search case s precisely one relevant document another application like question answer maybe s one answer rank answer goal rank one particular answer top right case easily verify average position basically boil reciprocal rank 1 r r rank position single relevant document document rank top 1 s 1 reciprocal rank s rank second s 1 et cetera also take average average precision reciprocal rank set topic would give us something call mean reciprocal rank s popular measure item search know problem one relevant item see r actually meaningful r basically indicate much effort user would make order find relevant document s rank top s low effort make little effort s rank 100 actually read presumably 100 document order find sense r also meaningful measure reciprocal rank take reciprocal r instead used r directly natural question simply used r imagine design ratio measure performance random system one relevant item might thought used r directly measure measure user s effort right think take average large number topic would make difference right one single topic used r used 1 r make difference s larger r correspond small 1 r right difference would show show many topic think average mean reciprocal rank versus average r s difference see difference would would difference change oath system conclusion turn actually big difference think want think pause video basically difference take directory dominate large value r value basically large value indicate lower rank result mean relevant item rank low list sum s also average would dominate relevant document rank lower portion rank user perspective care highly rank document take transformation used reciprocal rank emphasize difference top know think difference 1 2 would make big difference 1 r think 100 1 wo nt make much difference use use big difference 100 let s say 1000 right desirable hand 1 2 wo nt make much difference yet another case may multiple choice thing need figure one make sense 
summary • precision-recall curve characterize overall accuracy rank list • actual utility rank list depend many top-ranked result user would examine • average precision standard measure compare two ranking method – combine precision recall – sensitive rank every relevant document multiple level relevance judgment 8 summarize show precision-recall curve characterize overall accuracy rank list emphasize actual utility rank list depend many top rank result user would actually examine user examine other average person used standard measure compare two ranking method combine precision recall s sensitive rank every random document [ music ] 

university illinois urbana-champaign [ music ] lecture evaluate text retrieval system multiple level judgement 
evaluation tr system multi-level judgment small relevant datum text retrieval problem user text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 lecture continue discussion evaluation be go look evaluate text retrieval system multiple level judgement 
multi-level relevance judgment relevance level r=1 ( non-relevant ) 2 ( marginally relevant ) 3 ( relevant ) gain cumulative gain 3 3+2 3+2+1 3+2+1+1 discount cumulative gain 3 d1 3 normalize = log 2 d2 2 log log 3 d3 1 d4 1 d5 3 d6 1 d7 1 dcg @ 10 = log log 3 + log 10 d8 2 idealdcg @ 10 = log log 3 + log + log 10 d9 1 d10 1 assume 9 document rate “ 3 ” total collection … … 3 far talk binary judgement mean document judge relevant relevant earlier also talk relevance medal degree often distinguish high relevant document useful document moderately relevant document okay useful perhaps be add document useful imagine rating page would multiple level rating example show example three level 3 relevant sorry 3 relevant 2 marginally relevant 1 non-relevant evaluate search engine system used judgement obvious map nt work average precision nt work precision recall nt work rely binary judgement let s look top rank result used judgement imagine user would mostly care top ten result marked rating level relevance level document show 3 2 1 1 3 etcetera call gain reason call gain measure infuse call ndcg normalize accumulate gain gain basically measure much gain random information user obtain look document right look first document user gain 3 point look non-relevant document user would gain 1 point look moderator marginally relevant document user would get 2 point etcetera gain measure utility document user s perspective course assume user stop 10 document be look cutoff 10 look total gain user s well s simply sum call cumulative gain user stop position 1 s user look another document s 3+2 user look document cumulative gain course cost spending time examine list cumulative gain give us idea much total gain user would user examine document ndcg also another letter discount cumulative gain want discounting well look cumulative gain one deficiency consider rank position document example look sum know 1 highly relevant document 1 marginally relevant document 2 non-relevant document nt really care rank ideally want two rank top case capture intuition well say well 3 good 3 top mean contribution gain different position weight position idea discounting basically be go say well first one need discount user assume always see document second one one discount little bit s small possibility user would nt notice divide gain weight base position log 2 2 rank position document go third position discount even normalizer log 3 forth take sum lower rank document would contribute much highly rank document mean example switch position let s say position one would get discount put example relevant document opposed imagine put 3 would discount s good would put 3 idea discounting okay point get discount cumulative gain measure utility rank list multiple level judgement happy well use rank system still need little bit order make measure comparable across different topic last step way show dcg 10 total sum dcg 10 document last step call n normalization will get normalize dcg well idea be go normalize dcg ideal dcg cutoff ideal dcg well dcg ideal ranking imagine 9 document whole collection rate 3 mean total 9 document rate ideal rank lister would put 9 document top would 3 would follow 2 s best can run position would right would ideal rank list compute dcg ideal rank list would give formula see ideal dcg would used normalizer dcg idea dcg would used normalizer imagine normalization essentially compare actual dcg best dcg possibly get topic want well will map dcg value range 0 best value highest value every query would s rank list fact ideal list otherwise general lower one nt well see transformation normalization nt really affect relative comparison system one topic ideal dcg system ranking system base dcg would exactly rank base normalize dcg difference however multiple topic nt normalization different topic different scale dcg topic like one 9 highly relevant document dcg get really high imagine another case two relevant document total whole collection highest dcg system can achieve topic would high face problem different scale dcg value take average nt want average dominate high value easy query normalization avoid problem make query contribute equal average idea ndcg s used measure rank list base multiple level relevance judgement 
normalize discount cumulative gain ( ndcg ) • applicable multi-level judgment scale [ 1 r ] r > 2 • main idea ndcg @ k document – measure total utility top k document user – utility lowly rank document discount – normalize ensure comparability across query 4 general way basically measure apply rank task multiple level judgement scale judgement multiple binary binary much multiple level like 1 0 5 even depend application main idea measure summarize measure total utility top k document always choose cutoff measure total utility would discount contribution lowly rank document finally would normalization ensure comparability across query [ music ] 

1 [ sound ] lecture practical issue would address evaluation text retrieval system 
2 lecture continue discussion evaluation will cover practical issue solve actual evaluation text retrieval system 
challenge create test collection relevance judgment query representative & many q1 q2 q3 … q50 existence relevant docs d2 d1 … d1 + judgment q1 q1 d2 + completenessq1vsd3 – q1 d4 – minimum human work q1 d5 + measure capture perceive utility user d3 d48 docs representative & many … q2 d1 – q2 d2 + q2 d3 + q2 d4 – … q50 d1 – q50 d2 – q50 d3 + 3 order create test collection create set query set document set relevance judgment turn actually challenge create first document query must representative must represent real query real document user handle also use many query many document order avoid bias conclusion match relevant document query also need ensure exist lot relevant document query query one be relevant option actually s informative compare different method used query s much room us see difference ideally relevant document clatch yet query also represent real query care term relevance judgment challenge ensure complete judgment document query yet minimize human fault use human labor label document s labor intensive result s impossible actually label document query especially consider giant datum set like web actually major challenge s difficult challenge measure s also challenge want measure would accurately reflect perceive utility user consider carefully user care design measure measure measure measure right thing conclusion would misled 
statistical significance test • sure observed difference ’ simply result particular query choose experiment 1 query system system b experiment 2 query system system b 1 2 3 4 5 6 7 020 021 022 019 017 020 021 040 041 042 039 037 040 041 1 2 3 4 5 6 7 002 039 016 058 004 009 012 076 007 037 021 002 091 046 average 020 040 average 020 040 slide doug oard & philip resnik 4 s important be go talk couple issue one statistical significance test also reason use lot query question sure observe difference nt simply result particular query choose sample result average position system system b different experiment see bottom mean average position mean look mean average position mean average position exactly experiment right see 020 040 system b s also 020 040 identical yet look exact average position different query look number detail would realize one case would feel trust conclusion give average another case case feel well be sure nt take look number moment pause medium look average mean average position easily say well system b better right s 040 twice much 020 s better performance look two experiment look detailed result see have confident say case one experiment one case number seem consistently better system b whereas experiment 2 be sure look result like system better another case system better yet look average system b better think reliable conclusion look average case intuitively feel experiment 1 reliable quantitate answer question need statistical significance test 
statistical significance testing query system system b sign test wilcoxon 074 - 032 021 - 037 - 002 082 034 1 2 3 4 5 6 7 002 039 016 058 004 009 012 076 007 037 021 002 091 046 + + + + average 020 040 p=10 p=09375 95 % outcome 0 slide doug oard & philip resnik try http statisticshtml 5 idea statistical significance test basically assess variant across different query big variance mean result can fluctuate lot accord different query believe unless used lot query result might change use another set query right c high variance s reliable let s look result second case show two different way compare one sign test look sign system b better system plus sign system better minus sign etc used case see well seven case actually four case system b better three case system better intuitively almost like random result right take random sample flip seven coin use plus denote head minus denote tail can easily result randomly flip seven coin fact average larger nt tell us anything ca nt reliably conclude quantitatively measure p value basically mean probability result fact random fluctuation case probability mean surely random fluctuation willcoxan test s non-parametric test would look sign will also look magnitude difference draw similar conclusion say s likely random illustrate let s think distribution call distribution assume mean zero let say start assumption s difference two system assume random fluctuation depend query might observe difference actual difference might left side right side right curve kind show probability actually observe value deviate zero look picture see difference observed chance high fact random observation right define region likely observation random fluctuation 95 % outcome observed may still random fluctuation observe value region difference side difference unlikely random fluctuation right s small probability observe difference random fluctuation case conclude difference must real system b indeed better idea statical significance test takeaway message use many query avoid jump conclusion case say system b better many different way statistical significance test 
pool avoid judge document • ’ afford judge document collection subset judge • pool strategy – choose diverse set ranking method ( tr system ) – return top-k document – combine top-k set form pool human assessor judge – ( unjudged ) document usually assume non-relevant ( though ’ ) – okay compare system contribute pool problematic evaluate new system 6 let s talk problem make judgment say earlier s hard judge document completely unless small datum set question afford judge document collection subset judge solution pool strategy used many case solve problem idea pool follow would first choose diverse set ranking method text retrieval system hope method help us nominate like relevant document goal pick relevant document want make judgement relevant document useful document user perspective be go return top-k document k vary system point ask suggest likely relevant document simply combine top-k set form pool document human assessor judge imagine many system ten k document take top-k document form union course many document duplicate many system might retrieve random document duplicate document also unique document return one system idea diverse set ranking method ensure pool broad include many possible relevant document possible user would human assessor would make complete judgment datum set pool unjudged document usually assume non relevant pool large enough assumption okay pool large actually reconsider might use strategy deal indeed method handle case strategy generally okay compare system contribute pool mean participate contribute pool s unlikely would penalize system problematic document judge however problematic evaluate new system may contribute pool case new system might penalize might nominated read document judge document might assume non relevant 
summary tr evaluation • extremely important – tr empirically defined problem – inappropriate experiment design misguide research application – make sure get right research application • cranfield evaluation methodology main paradigm – map ndcg appropriate compare ranking algorithms – precision @ 10docs easier interpret user ’ perspective • cover – a-b test [ sanderson 10 ] – user study [ kelly 09 ] 7 s unfair summarize whole part textual evaluation s extremely important problem empirically defined problem nt rely user s way tell whether one method work better property experiment design might misguide research application might draw wrong conclusion see discussion make sure get right research application main methodology cranfield evaluation methodology main paradigm used kind empirical evaluation task search engine variation map ndcg two main measure definitely know appropriate compare ranking algorithms see often research paper precision 10 document easier interpret user s perspective s also often useful s cover evaluation strategy like a-b test system would mix two result two method randomly would show mixed result user course user nt see result method user would judge result click document search engine application case search engine check click document see one method contribute click document user tend click one result one method suggest message may better leverage real user search engine evaluation s call a-b test s strategy often used modern search engine commercial search engine another way evaluate ir textual retrieval user study nt cover have put reference look want know 
additional reading • donna harman information retrieval evaluation synthesis lecture information concept retrieval service morgan & claypool publisher 2011 • mark sanderson test collection base evaluation information retrieval system foundation trend information retrieval 4 ( 4 ) 247-375 ( 2010 ) • diane kelly method evaluate interactive information retrieval system user foundation trend information retrieval 3 ( 1-2 ) 1-224 ( 2009 ) 8 three additional reading three mini book evaluation excellent cover broad review information retrieval evaluation cover thing discuss also lot other offer [ music ] 

1 [ sound ] lecture probabilistic retrieval model 
probabilistic retrieval model basic idea lecture be go continue discussion text retrieval method be go look another kind different way design ranking function vector space model discuss 
many different retrieval model • probabilistic model f ( q ) = p ( r=1|d q ) r  { 01 } – classic probabilistic model  bm25 – language model query likelihood – divergence-from-randomness model pl2 p ( r=1|d q ) p ( q|d r=1 ) user like document likely would user enter query q ( order retrieve ) 3 probabilistic model define ranking function base probability document relevant query word introduce binary random variable variable r also assume query document observation random variable note vector-based model assume vector assume datum observed random variable problem retrieval become estimate probability relevance category model different variant classic probabilistic model lead bm25 retrieval function discuss vectors-based model form actually similar backward space model lecture discuss another sub class p class call language modele approach retrieval particular be go discuss query likelihood retrieval model one effective model probabilistic model also another line call divergence randomness model lead pl2 function s also one effective state art retrieval function query likelihood assumption probability relevance approximate probability query give document relevance intuitively probability capture follow probability user like document likely would user enter query q order retrieve document assume user like relevance value ask question likely will see particular query user basic idea understand idea let s take look general idea basic idea probabilistic retrieval model list imagined relevance status value relevance judgment query document example line show q1 query user type d1 document user see 1 mean user think d1 relevant q1 r also approximate click-through datum search engine collect watch interact search result case let s say user click document s 1 similarly user click d2 also 1 word d2 assume relevant q1 hand d3 non-relevant s 0 d4 non-relevant d5 relevant forth part maybe datum collect different user user type q1 find d1 actually useful d1 actually non-relevant contrast see s relevant can query type user different time d2 also relevant etc see datum query imagine lot datum ask question estimate probability relevance compute probability relevance well intuitively mean look entry see particular particular q likely will see one column basically mean collect count first count many time see q pair table count many time actually also see 1 third column compute ratio let s take look specific example suppose try compute probability d1 d2 d3 q1 estimate probability think pause video need try take look table try give estimate probability see interested q1 d1 will look two pair case well actually one case user say 1 relevant r = 1 one two case case s s one two d1 d2 well d1 d2 d1 d2 case case r = s two two forth see approach actually score document query right score d1 d2 d3 query simply rank base probability s basic idea probabilistic retrieval model see make lot sense case s go rank d2 document case c q1 d2 r = user click document also show lot click-through datum search engine learn lot datum improve search engine simple example show even small amount entry already estimate probability probability would give us sense document might relevant useful user typing query 
probabilistic retrieval model basic idea query doc rel q r 𝑐𝑜𝑢𝑛𝑡 ( 𝑞 𝑑 𝑅 = 1 ) q1 q1 q1 q1 q1 … q1 q1 q1 q2 q3 q4 d1 d2 d3 d4 d5 1 1 0 0 1 d1 d2 d3 d3 d1 d2 0 1 0 1 1 1 f ( q ) p ( r=1|d q ) = 𝑐𝑜𝑢𝑛𝑡 ( 𝑞 𝑑 ) p ( r=1|q1 d1 ) = 2 p ( r=1|q1 d2 ) = 2 p ( r=1|q1 d3 ) = 2 unseen document unseen query 4 course problem nt observe query document relevance value right would lot unseen document general collect datum document show user even unseen query predict query type user obviously approach wo nt work apply unseen query unseen document nevertheless show basic idea probabilistic retrieval model make sense intuitively case lot unseen document unseen query well solution approximate way 
query likelihood retrieval model query doc rel user like q r q1 d1 1 f ( q ) p ( r=1|d q )  p ( q|d r=1 ) q1 q1 q1 q1 … q1 q1 q1 q2 q3 q4 d2 d3 d4 d5 1 0 0 1 d1 d2 d3 d3 d1 d2 0 1 0 1 1 1 likely user enter q assumption user formulate query base “ imaginary relevant document ” 5 particular case call query likelihood retrieval model approximate another conditional probability p ( q give r=1 ) condition part assume user like document see user click document part show be interested likely user would actually enter query likely see query row note make interesting assumption basically be go assume whether user type query something whether user like document word actually make follow assumption user formulate query base imaginary relevant document look conditional probability s obvious make assumption really meant use new conditional probability help us score new conditional probability somehow able estimate conditional probability without rely big table otherwise would similar problem make assumption way bypass big table try model user formulate query okay simplify general model derive specific relevant function later 
doc likely “ imaginary relevant doc ” q = “ news presidential campaign ” d1 d2 d3 … news … p ( q|d1 ) p ( q|d2 ) … news organic food campaign… … news presidential campaign … d4 … news presidential campaign … … presidential candidate … d5 … news organic food campaign… campaign…campaign…campaign… p ( q|d4 ) p ( q|d5 ) 6 let s look model work example basically go case ask follow question document likely imaginary relevant document user s mind user formulate query ask question quantify probability probability conditional probability observe query particular document fact imaginary relevant document user s mind see have compute query likelihood probability likelihood query give document value rank document base value 
summary • relevance ( q ) = p ( r=1|q ) p ( q|d r=1 ) • query likelihood ranking function f ( q ) p ( q|d ) – probability user like would pose query q • compute p ( q|d ) compute probability text general  language model p ( = “ presidential campaign ” = … news presidential campaign … presidential candidate … ) 7 summarize general idea modern relevance proper risk model assume introduce binary random variable r let score function defined base conditional probability also talk approximate used query likelihood case ranking function s basically base probability query give document probability interpreted probability user like document would pose query q question course compute conditional probability general compute probability text q text model call language model kind model propose model text specifically interested follow conditional probability show user like document likely user would pose query next lecture be go give introduction language model see model text probable risk model general [ music ] 

1 [ sound ] lecture statistical language model 
probabilistic retrieval model statistical language model lecture be go give introduction statistical language model model text datum probabilistic model s related model query base document 
overview • language model • unigram language model • used language model 3 be go talk language model be go talk simplest language model call unigram language model also happen useful model text retrieval finally class use language model 
statistical language model ( lm ) • probability distribution word sequence – p ( “ today wednesday ” )  0001 – p ( “ today wednesday ” )  00000000000001 – p ( “ eigenvalue positive ” )  000001 • context-dependent • also regard probabilistic mechanism “ generate ” text thus also call “ generative ” model today wednesday … eigenvalue positive today wednesday 4 language model well s probability distribution word sequence will show one model give sequence today wednesday probability give today wednesday small probability s non-grammatical see probability give sentence sequence word vary lot depend model therefore s clearly context dependent ordinary conversation probably today wednesday popular among sentence imagine context discuss apply math maybe eigenvalue positive would higher probability mean used represent topic text model also regard probabilistic mechanism generate text s also often call generate model mean imagine mechanism s visualised stochastic system generate sequence word ask sequence s send sequence device want might generate example today wednesday can generate sequence example many possibility right sense view datum basically sample observed generate model model useful well s mainly quantify uncertainty natural language uncertainty come well one source simply ambiguity natural language discuss earlier lecture another source complete understand lack knowledge understand language case uncertainty well let show example question answer language model would interesting application different way give see john feel likely see happy opposed habit next word sequence word obviously would useful speech recognition happy habit would similar acoustic sound acoustic signal look language model know john feel happy would far likely john feel habit another example give observe baseball three time game news article likely sport obviously related text categorization information retrieval 
lm useful • quantify uncertainty natural language • allow us answer question like – give see “ john ” “ feel ” likely see “ happy ” opposed “ habit ” next word ( speech recognition ) – give observe “ baseball ” three time “ game ” news article likely “ sport ” ( text categorization information retrieval ) – give user interested sport news likely would user use “ baseball ” query ( information retrieval ) 5 also give user interested sport news likely would user use baseball query clearly related query likelihood discuss previous lecture let s look simplest language model call unigram language model case assume generate text generate word independently mean probability sequence word would product probability word normally be independent right single word like language would make far likely observe model nt see language assumption necessarily true make assumption simplify model model precisely n parameter n vocabulary size one probability word probability must sum strictly speaking actually n-1 parameter say text assume assemble draw word distribution example ask device model stochastically generate word us instead sequence instead give whole sequence like today wednesday give us one word get kind word assemble word sequence 
simplest language model unigram lm • • • • generate text generate word independently thus p ( w1 w2 wn ) p ( w1 ) p ( w2 ) …p ( wn ) parameter { p ( wi ) } p ( w1 ) …+p ( wn ) 1 ( n voc size ) text = sample draw accord word distribution p ( “ today wed ” ) today = p ( “ today ” ) p ( “ ” ) p ( “ wed ” ) … eigenvalue = 00002  0001  0000015 wednesday 6 still allow compute probability today wednesday product three probability see even though ask model generate sequence actually allow us compute probability sequence model need n parameter characterize mean specify probability word s behavior completely specify whereas nt make assumption would specify probability kind combination word sequence make assumption make much easier estimate parameter 
text generation unigram lm unigram lm p ( w| ) … text 02 mining 01 001 topic 1 association cluster 002 text mining … food 000001 … topic 2 health … food 025 nutrition 01 healthy 005 diet 002 … sampling document = text mining paper food nutrition paper 7 let s see specific example show two unigram language model probability high probability word show top first one clearly suggest topic text mining high probability related topic second one related health ask question likely observe particular text two model suppose sample word form document let s say take first distribution would like sample word word think would generate make text maybe mining maybe another word even food small probability might still able show general high probability word likely show often imagine general text look like text mining fact small probability might able actually generate actual text mining paper actually meaningful although probability small extreme case might imagine might able generate text mining paper would accept major conference case probability would even smaller s non-zero probability assume none word non-zero probability similarly second topic imagine generate food nutrition paper nt mean generate paper text mining distribution probability would small maybe smaller even generate paper accept major conference text mining point keep distribution talk probability observe certain kind text text higher probability other 
estimation unigram lm unigram lm p ( w| ) = estimation … 100 text 100 mining 100 association 100 database 100 … query … maximum likelihood ( ml ) estimator p ( w |  )  p ( w | )  c ( w ) d | text mining paper total # words=100 text 10 mining 5 association 3 database 3 algorithm 2 … query 1 efficient 1 best estimate 8 let s look problem different way suppose available particular document case many abstract text mining table see word count total number word question ask estimation question ask question model one distribution used generate text assume text generate assemble word distribution would guess decide probability text mining etc would suppose view second try think best guess be like lot person would guess well best guess text probability 10 100 have see text 10 time total 100 word simply normalize count s fact word justified intuition consistent mathematical derivation call maximum likelihood estimator estimator assume parameter setting would give observe datum maximum probability mean change probability probability observe particular text datum would somewhat smaller see simple formula basically need look count word document divide total number word document document lens normalize frequency consequence course be go assign zero probability unseen word observed word incentive assign non-zero probability used approach would take away probability mass observed word obviously would nt maximize probability particular observed text datum one still question whether best estimate well answer depend kind model want find right estimator give best model base particular datum interested model explain content full paper abstract might second thought right thing word body article zero probability even though be observed abstract be go cover little bit later class query likelihood model let s take look possible used language model one use simply use represent topic show general english background text use text estimate language model model might look like right top common word etc will see common word like rare word bottom background language model represent frequency word english general background model 
lm topic representation b general background english text 003 002 0015 001 food 0003 computer 000001 text 0000006 … computer science c paper 0032 0019 0014 0011 computer 0004 software 00001 text 000006 … text mining paper 0031 … text 004 mining 0035 association 003 cluster 0005 computer 00009 … food 0000001 collection lm p ( w|c ) document lm p ( | background lm p ( w|b ) 9 let s look another text maybe time will look computer science research paper collection computer science research paper mentioned use maximum likelihood estimator simply normalize frequency case will get distribution look like top look similar word occur everywhere common go will see word related computer science computer software text etc although might also see word example computer imagine probability much smaller probability see many word would common general english see distribution characterize topic corresponding text look even smaller text case let s look text mining paper another distribution expect occur top sooner see text mining association cluster word relatively high probability contrast distribution text relatively small probability mean base different text datum different model model capture topic call document language model call collection language model later see be used retrieval function let s look another use model statistically find word semantically related computer find word well first thought let s take look text match computer take look document contain word computer let s build language model see word see well surprisingly see common word top always case language model give us conditional probability see word context computer common word naturally high probability also see computer software relatively high probability use model say word semantically related computer ultimately will like get rid common word turn s possible use language model suggest think know word common want kind get rid model tell us well maybe think background language model precisely tell us information tell us common general use background model would know word common word general s surprising observe context computer whereas computer small probability general s surprising see computer probability true software use two model somehow figure word related computer 
lm association analysis word semantically related “ computer ” topic lm p ( | “ computer ” ) normalize topic lm 0032 p ( | “ computer ” ) p ( w|b ) 0019 document 0014 computer 400 contain word 0008 software 150 “ computer ” computer 0004 program 104 software 00001 … background lm p ( w|b ) text 30 … 003 11 002 099 0015 general background 09 001 english text 08 10 computer 000001 b example simply take ratio group probability normalize topic language model probability word background language model take ratio will see top computer rank follow software program word related computer occur frequently context computer frequently whole collection whereas common word high probability fact ratio 1 really related computer take sample text contain computer nt really see occurrence general show even simple language model limit analysis semantic 
summary • language model = probability distribution text • unigram language model = word distribution • used language model – represent topic – discover word association 11 lecture talk language model basically probability distribution text talk simplest language model call unigram language model also word distribution talk two used language model one represent topic document collection general discover word association next lecture be go talk language model used design retrieval function 
additional reading • chris man hinrich schütze foundation statistical natural language process mit press cambridge may 1999 • rosenfeld r ` two decade statistical language modele go proceedings ieee vol88 no8 pp12701278 2000 12 two additional reading first textbook statistical natural language process second article survey statistical language model lot pointer research work [ music ] 

1 
probabilistic retrieval model query likelihood small relevant datum user text retrieval problem text retrieval method recommendation 43 probabilistic model text access recommender system search engine evaluation system implementation web search natural language content analysis big text datum 2 
query generation sampling word doc p ( = “ presidential campaign ” = … news presidential campaign … presidential candidate … ) “ presidential ” campaign user think doc likely would pose query 3 
unigram query likelihood p ( = “ presidential campaign ” = … news presidential campaign … presidential candidate … ) = p ( “ presidential ” d ) p ( “ campaign ” d ) c ( ` presidential ) c ( ` campaign )  * d | d | assumption query word generate independently 4 
query likelihood make sense p ( q  presidential campaign | )  p ( = c ( ` presidential ) c ( ` campaign ) * d | d | … news presidential campaign … presidential candidate … ) p ( = … news presidential campaign … p ( = … news organic food campaign… ) 𝟐 = | ) 𝟎 = | 𝟏 = | ∗ ∗ ∗ 𝟏 | 𝟏 | 𝟏 0 | d4 > d3 > d2 expect 5 
try different query q = “ presidential campaign update ” p ( = … news presidential campaign … presidential candidate … ) p ( = … news presidential campaign … p ( = … news organic food campaign… ) 𝟐 = | 𝟏 | ) = 𝟎 = | ∗ 𝟏 | ∗ 𝟏 𝒅𝟒 ∗ 𝟎 𝒅𝟒 0 ∗ 𝟏 | ∗ 𝟎 𝒅𝟑 0 𝟎 𝒅𝟐 ∗ 0 assumption cause problem fix 6 
improve model sampling word doc model likely would observe query doc model p ( = “ presidential campaign ” = “ presidential ” campaign update … news presidential campaign … presidential candidate … ) … presidential 02 campaign 01 news 001 candidate 002 … update 000001 … 7 
computation query likelihood document d1 text mining paper d2 food nutrition paper document lm p ( w|d1 ) … text 02 mining 01 association 001 cluster 002 … food 000001 … p ( w|d2 ) … food 025 nutrition 01 healthy 005 diet 002 … query q = “ datum mining algorithms ” p ( “ datum mining alg ” d1 ) = p ( “ datum ” d1 )  p ( “ mining ” d1 )  p ( “ alg ” d1 ) p ( “ datum mining alg ” d2 ) = p ( “ datum ” d2 )  p ( “ mining ” d2 )  p ( “ alg ” d2 ) 8 
summary ranking base query likelihood q  w1w2 wn f ( q )  log p ( q | )  p ( q | )  p ( w1 | )   p ( wn | ) n  log p ( w 1 d )   c ( w q ) log p ( w | ) wv document language model retrieval problem  estimation p ( wi|d ) different estimation method  different ranking function 9 

1 
probabilistic retrieval model smooth small relevant datum user text retrieval problem text retrieval method recommendation 43 probabilistic model text access recommender system search engine evaluation system implementation web search natural language content analysis big text datum 2 
ranking function base query likelihood q w1w2 wn f ( q ) log p ( q | ) p ( q | ) n 1 p ( w1 | ) log p ( wi | ) p ( wn | ) c ( w q ) log p ( w | ) w v estimate p ( w|d ) 3 
estimate p ( w|d ) p ( w|d ) max likelihood estimate c ( w ) d | pml ( w | ) smooth lm p ( w | ) 0 even c ( w ) 0 w word 4 
smooth lm • key question probability assign unseen word • let probability unseen word proportional probability give reference lm • one possibility reference lm = collection lm discount ml estimate p ( w | ) pseen ( w | ) p ( w | c ) w see otherwise collection language model 5 
rewrite ranking function smooth log p ( q | ) c ( w q ) log p ( w | ) w v c ( w q ) log pseen ( w | ) w v c ( w ) 0 c ( w q ) log p ( w | c ) w v c ( w ) 0 query word match c ( w q ) log query word match p ( w | c ) w v c ( w q ) log c ( w q ) log p ( w | c ) w v c ( w ) 0 query word match query word w v c ( w ) 0 pseen ( w | ) p ( w | c ) | q | log c ( w q ) log p ( w | c ) w v 6 
benefit rewrite • better understand ranking function – smooth p ( w|c ) tf-idf weighting + length norm tf weighting log p ( q | ) pseen ( w | ) [ log ] p ( w | c ) doc length normalization n n log wi wi q match query term idf weighting log p ( w | c ) 1 ignore ranking • enable efficient computation 7 

1 [ sound ] 
probabilistic retrieval model smooth small relevant datum user text retrieval problem text retrieval method recommendation 43 probabilistic model text access recommender system search engine evaluation system implementation web search natural language content analysis big text datum 2 
benefit rewrite • better understand ranking function – smooth p ( w|c ) tf-idf weighting + length norm tf weighting log p ( q | ) pseen ( w | ) [ log ] p ( w | c ) doc length normalization n n log wi wi q match query term idf weighting log p ( w | c ) 1 ignore ranking • enable efficient computation 7 show rewrite query like holder function form look like formula slide make assumption smooth language model base collection language model look rewrite actually give us two benefit first benefit help us better understand ranking function particular be go show formula see smooth collection language model would give us something like tf-idf weighting length normalization second benefit also allow us compute query like holder efficiently particular see main part formula sum match query term much better take sum word smooth document damage model essentially non zero problem word new form formula much easier score compute s also interesting note last term actually independent document since goal rank document query ignore term ranking s go document ignore would nt affect order document inside sum also see match query term would contribute weight weight actually interesting look like tf-idf weighting first already see frequency word query like vector space model take thought product see word frequency query show sum naturally part would correspond vector element document vector indeed see actually encode weight similar factor tf-idf weight will let examine see see part capture tf part capture idf weighting want pause video think noticed p sub see related term frequency sense word occur frequently document make probability tend larger mean term really something like tf weight also noticed term denominator actually achieve factor idf popularity term collection s denominator probability collection larger weight actually smaller mean popular term actually smaller weight precisely idf weighting different form tf idf remember idf logarithm document frequency something different intuitively achieve similar effect interestingly also something related length libation see factor related document length formula say term related idf weighting collection probability turn term actually related document length normalization particular f sub might related document length encode much probability mass want give unseen world much smooth want intuitively document long need less smooth assume datum large enough probably observed word author can written document short r sub can expect large need smooth s likey word written yet author term appear paralyze non document sub would tend longer larger long document note alpha sub also occur may actually necessary paralyze long document effect clear yet see later consider specific smooth method turn paralyze long document like tf-idf weighting document length normalization formula vector space model s interesting observation mean nt even think specific way smooth need assume smooth collection memory model would formula look like tf-idf weighting document length violation s also interesting fix form ranking function see heuristically put logarithm fact think would logarithm look assumption make would clear s used logarithm query like score turn product sum logarithm probability s logarithm note want heuristically implement tf weighting idf weighting nt necessary logarithm imagine drop logarithm would still tf idf weighting s nice problem risk modele automatically give logarithm function s basically fix form formula really heuristically design case try drop logarithm model probably work well keep logarithm nice property problem risk modele follow assumption probability rule will get formula automatically formula would particular form like case heuristically design formula may necessarily end specific formula 
summary • smooth p ( w|d ) necessary query likelihood • general idea smooth p ( w|c ) – probability unseen word assume proportional p ( w|c ) – lead general ranking formula query likelihood tfidf weighting document length normalization – score primarily base sum weight match query term • however exactly smooth 8 summarize talk need smooth document imaging model otherwise would give zero probability unseen word document s good store query unseen word s also necessary general improve accuracy estimate model represent topic document general idea smooth retrieval use connect memory model give us clue unseen word higher probability probability unseen word assume proportional probability collection assumption have show derive general ranking formula query likelihood effect tf-idf weighting document length normalization also see rewrite score ranking function primarily base sum weight match query term like vector space model actual ranking function give us automatically probability rule assumption make like vector space model heuristically think form function however still need address question exactly smooth document model exactly use reference model base connection adjust probability maximum micro make topic next batch [ music ] 

1 [ sound ] lecture specific smooth method language model used probabilistic retrieval model 
probabilistic retrieval model smooth method small relevant datum user text retrieval problem text retrieval method recommendation 43 probabilistic model text access recommender system search engine evaluation system implementation web search natural language content analysis big text datum 2 lecture continue discussion language model information retrieval particularly query likelihood retrieval method be go talk specifically smooth method used retrieval function 
query likelihood + smooth p ( w|c ) p ( wi | ) c ( w q ) [ log see ] p ( w | c ) log p ( q | ) n n log log p ( w | c ) wi wi q pseen ( w | ) c ( w q ) [ log ] p ( w | c ) f ( q ) 1 n log wi wi q pseen ( w | ) smooth p ( w|d ) 3 slide previous lecture show query likelihood ranking smooth collection language model add retrieval function look like follow retrieval function base assumption discuss see s sum match query term inside sum count term query weight term document f weight another constant n clearly want implement function used programming language still need figure variable particular be go need know estimate probability word exactly set alpha order answer question think specific smooth method main topic lecture 
linear interpolation ( jelinek-mercer ) smooth unigram lm p ( | ) = 100 100 100 100 100 100 p ( w | ) ( 1 p ( ` text | ) … text mining association database … query network c ( w ) ) ( 1 d | ) 10 100 document total # words=100 text 10 mining 5 association 3 database 3 algorithm 2 … query 1 efficient 1 p ( w | c ) 𝜆 ∈ [ 01 ] * 0001 p ( ` network | ) collection lm p ( w|c ) 01 008 computer 002 database 001 …… text 0001 network 0001 mining 00009 … * 0001 4 be go talk two smooth method first simple linear interpolation fix coefficient also call jelinek-mercer smooth idea actually simple picture show estimate document language model used maximum likelihood estimate give us word count normalize total number word text idea used method maximize probability observed text result word like network observed text s go get 0 probability show idea smooth rely collection language model word go zero probability help us decide nonzero probability assign word note network nonzero probability approach linear interpolation maximum likelihood placement collection language model compute smooth parameter lambda 0 smooth parameter larger lambda smooth mix together achieve goal assign nonzero probability word like network let s see work word example compute smooth probability text maximum likelihood estimate give us 10 100 s go collection probability will combine together simple formula also see word network used zero probability get non-zero probability value s count go zero network part nonzero s basically method work think easily see alpha sub smooth method basically lambda s remember coefficient front probability word give collection language model okay first smooth method second one similar tie-in coefficient linear interpolation s often call dirichlet prior bayesian smooth face problem zero probability unseen word like network use collection language model case be go combine somewhat different way formula first see interpolation maximum likelihood estimate collection language model j-m smooth method coefficient lambda fix number dynamic coefficient form mu parameter s non-negative value see set mu constant effect long document would actually get smaller coefficient long document longer length therefore coefficient actually smaller long document would less smooth would expect seem make sense fix coefficient smooth course part would form two coefficient would sum one way understand smooth basically mean s dynamic coefficient interpolation another way understand formula even easier remember s side s easier see rewrite smooth method form form easily see change make maximum likelihood estimate would part normalize count document length form see add count every word mean well basically something related probability word collection multiply parameter mu combine count essentially add pseudocount observed text pretend every word get many pseudocount total count would sum pseudocount actual count word document result total would add many pseudocount take somewhat one word will see probability word would sum 1 give us mu total number pseudocount add probability would still sum case easily see method essentially add pseudocount datum pretend actually augment datum include pseudo datum defined collection language model result count total count word would like result even word zero count let s say zero count would still nonzero count part method work let s also take look specific example 
dirichlet prior ( bayesian ) smooth document total # words=100 unigram lm p ( | ) = 100 100 100 100 100 100 p ( w | ) … text mining association database … query network c ( w ) p ( w | c ) d | | p ( ` text | ) d | 10 * 0001 100 text 10 mining 5 association 3 database 3 algorithm 2 … query 1 efficient 1 c ( w ) d | d | p ( w | c ) p ( ` network | ) collection lm p ( w|c ) 01 008 computer 002 database 001 …… text 0001 network 0001 mining 00009 … ∈ [ 0 + ) 100 * 0001 5 text 10 original count actually observe also add pseudocount probability text would form naturally probability network would part also see s alpha sub see want think pause video will notice part basically alpha sub d see case alpha sub depend document length depend document whereas linear interpolation j-m smooth method constant [ music ] 

1 
probabilistic retrieval model smooth method small relevant datum user text retrieval problem text retrieval method recommendation 43 probabilistic model text access recommender system search engine evaluation system implementation web search natural language content analysis big text datum 2 
ranking function jm smooth pseen ( w | ) c ( w q ) [ log ] p ( w | c ) f ( q ) n log wi wi q p ( w | ) pseen ( w | ) p ( w | c ) ( 1 ( 1 f jm ( q ) c ( w ) ) d | ) p ml ( w | ) p ( w | c ) p ( w | c ) c ( w q ) log [ 1 w w q 𝜆 ∈ [ 01 ] p ( w | c ) 1 1 1 c ( w ) | | p ( w | c ) c ( w ) ] | | p ( w | c ) 6 
ranking function dirichlet prior smooth pseen ( w | ) c ( w q ) [ log ] p ( w | c ) f ( q ) n log wi wi q p ( w | ) c ( w ) p ( w | c ) | pseen ( w | ) p ( w | c ) f dir ( q ) | | c ( w ) d | d | c ( w ) p ( w | c ) d | p ( w | c ) d | [ c ( w q ) log [ 1 w w q d | 1 p ( w | c ) ∈ [ 0 + ) c ( w ) p ( w | c ) c ( w ) ] ] p ( w | c ) n log d | d | 7 
summary • two smooth method – jelinek-mercer fix coefficient linear interpolation – dirichlet prior add pseudo count adaptive interpolation • lead state art retrieval function assumption clearly articulate ( less heuristic ) – also implement tf-idf weighting doc length normalization – precisely one ( smooth ) parameter 8 
summary query likelihood probabilistic model • effective ranking function obtain used pure probabilistic modele – assumption 1 relevance ( q ) = p ( r=1|q ) p ( q|d r=1 ) p ( q|d ) – assumption 2 query word generate independently – assumption 3 smooth p ( w|c ) – assumption 4 jm dirichlet prior smooth • less heuristic compare vsm • many extension make [ zhai 08 ] 9 
additional reading • chengxiang zhai statistical language model information retrieval ( synthesis lecture series human language technology ) morgan & claypool publisher 2008 http s00158 ed1v01y200811hlt001 10 

1 
text retrieval method feedback tr small relevant datum user text retrieval problem text retrieval method recommendation 43 feedback tr text access recommender system search engine evaluation system implementation web search natural language content analysis big text datum 2 
relevance feedback user make explicit relevance judgment initial result ( judgment reliable user ’ want make extra effort ) retrieval engine query update query document collection feedback result d1 35 d2 24 … dk 05 judgment d1 + d2 d3 + … dk user 3 
automatic feedback top-k initial result simply assume relevant ( judgment ’ reliable user activity require ) retrieval engine query update query document collection feedback result d1 35 d2 24 … dk 05 judgment d1 + d2 + d3 + … dk top 10 assume relevant 4 
implicit feedback user-clicked docs assume relevant skip one non-relevant ( judgment ’ completely reliable extra effort user ) retrieval engine query update query document collection feedback result d1 35 d2 24 … dk 05 clickthrough d1 + d2 d3 + … dk user 5 

1 [ sound ] lecture feedback vector space model 
feedback text retrieval feedback vsm 2 lecture continue talk feedback text retrieval particularly be go talk feedback vector space model 
feedback vector space model • tr system learn example improve retrieval accuracy – positive example docs know relevant – negative example docs know non-relevant • general method query modification – add new ( weight ) term ( query expansion ) – adjust weight old term 3 discuss case feedback task text retrieval system remove example improve retrieval accuracy positive example document assume would relevant charge relevant document view user also negative example document know non-relevant also document skip user general method vector space model feedback modify query vector want place query vector better position make accurate mean exactly well think query vector would mean would something vector element general would mean might add new term might weight old term assign weight new term result general query term often call query expansion 
rocchio feedback illustration centroid relevant document centroid non-relevant document - - + + + + - - - + + q - - + + + + + + + + + + - - - 4 effective method vector space model feedback call rocchio feedback actually propose several decade ago idea quite simple illustrate idea used two dimensional display document collection also query vector see query vector center document use query back use narrative function find similar document basically circle document would basically top-ranked document process relevant document relevant document example s relevant etc minuse negative document like goal try move query back position improve retrieval accuracy look diagram think move query vector improve retrieval accuracy intuitively want move query vector want think pause video think picture realize order work well case want query vector close positive vector possible mean ideally want place query vector somewhere want move query vector closer point exactly point well want relevant document rank top want center relevant document right draw circle around one will get relevant document mean move query vector towards centroid relevant document vector basically idea rocchio course consider centroid negative document want move away negative document match be talk move vector closer vec 
rocchio feedback formula new query origial query parameter rel docs non-rel docs 5 away vector mean formula see original query vector average basically centroid vector relevant document take average vector compute centroid vector similarly average non-relevant document like s essentially non-relevant document three parameter alpha beta gamma control amount movement add two vector together be move query vector closer centroid add together subtract part kind move query vector away centroid main idea rocchio feedback do get new query vector used score document new query vector reflect move original query vector toward relevant centroid vector away non-relevant value 
example rocchio feedback = { news presidential camp food … } query = “ news presidential campaign ” = ( 1 1 1 1 0 0 … ) new *1-*0067 *1+*35 *1+*20-*26 *13 0 0 … ) d1query q ’ … ( *1+*15-*15 news … d2 d3 d4 - = ( 15 01 0 0 0 0 … ) … news organic food campaign… - = ( 15 01 0 20 20 0 … ) … news presidential campaign … + = ( 15 0 30 20 0 0 … ) … = presidential campaign … + centroid ( ( 15+15 ) 2 0 ( 30+40 ) 2 ( 20+20 ) 2 0 0 … ) … presidential candidate … = ( 15 0 35 20 0 0 … ) + = ( 15 0 40 20 0 0 … ) … news organic food campaign… campaign…campaign…campaign… d5 centroid = ( ( 15+15+15 ) 3 ( 01+01+0 ) 3 0 ( 0+20+60 ) 3 ( 0+20+20 ) 3 0 … ) = ( 15 0067 0 26 13 0 … ) - = ( 15 0 0 60 20 0 … ) 6 okay let s take look example example have see earlier deem display actual document show vector representation document five document read document right be display red term vector assume weight lot term zero weight course negative argument two another one rocchio method first compute centroid category let s see look centroid vector positive document simply s easy see add one corresponding element s take average be go add corresponding element take average end one average vector two s centroid two let s also look centroid negative document basically be go take average three element corresponding element three vector forth end one rocchio feedback method be go combine original query vector let s see combine together well s basically parameter alpha control original query time weight s one beta control inference positive centroid weight s come right go also negative weight gamma way come course negative centroid exactly term one term new vector be go use new query vector one rank document imagine would happen right movement one would match red document much better moved vector closer s go penalize black document non relevent document precisely want feedback course apply method practice see one potential problem original query four term zero query explain merge will many time would non zero weight calculation involve term practice often truncate matter retain term highest weight 
rocchio practice • negative ( non-relevant ) example important ( ) • often truncate vector ( ie consider small number word highest weight centroid vector ) ( efficiency concern ) • avoid “ over-fitting ” ( keep relatively high weight original query weight ) ( ) • used relevance feedback pseudo feedback (  set larger value relevance feedback pseudo feedback ) • usually robust effective 7 let s talk use method practice mentioned be often truncate vector consider small number word highest weight centroid vector efficiency concern also say negative example non-relevant example tend useful especially compare positive example think one reason negative document tend distract query direction take average nt really tell exactly move whereas positive document tend cluster together point consistent direction also mean sometimes use negative example note case difficult query result negative negative feedback useful another thing avoid over-fitting mean keep relatively high weight original query term sample see feedback relatively small sample nt want overly trust small sample original query term still important term heighten user user decide term important order prevent us over-fitting drift prevent topic drift due bias toward feed back symbol generally would keep pretty high weight original term safe especially true pseudo relevance feedback method used relevance feedback pseudo-relevance feedback case pseudo-feedback prime beta set smaller value relevant example assume relevant be reliable relevance feedback case relevance feedback obviously can use larger value parameter set empirically rocchio method usually robust effective s still popular method feedback [ music ] 

1 [ sound ] lecture feedback language modele approach 
feedback text retrieval feedback lm small relevant datum user text retrieval problem text retrieval method recommendation 43 feedback tr text access recommender system search engine evaluation system implementation web search natural language content analysis big text datum 2 lecture continue discussion feedback text retrieval particular be go talk feedback language modele approach 
feedback language model • query likelihood method ’ naturally support relevance feedback • solution – kullback-leibler ( kl ) divergence retrieval model generalization query likelihood – feedback achieve query model update 3 derive query likelihood ranking function make various assumption basic retrieval function formula work well think feedback information s little bit awkward use query likelihood perform feedback lot time feedback information additional information query assume query generate assemble word language model query likelihood method s kind unnatural sample word form feedback document result researcher propose way generalize query likelihood function s call kullback-leibler divergence retrieval model model actually go make query likelihood retrieval function much closer vector space model yet form language model regard generalization query likelihood sense cover query likelihood special case case feedback achieve simply query model estimation update similar rocchio update query vector 
kullback-leibler ( kl ) divergence retrieval model query likelihood f ( q )  kl-divergence ( cross entropy ) f ( q )   wi  w q query lm c ( w q ) [ log pseen ( w | ) ]  n log d d p ( w | c ) ˆ ) log pseen ( w | ) ]  log  [ p ( w |   q  p ( w | c ) wd p ( w|q ) 0 c ( w q ) ˆ p ( w |  q )  | 4 let s see kl-divergence retrieval model top see query likelihood retrieval function one kl-divergence also call cross entropy retrieval model basically generalize frequency part language model basically s difference give probabilistic model characterize user look versus count query word difference allow us plug various different way estimate estimate many different way include used feedback information call kl-divergence interpreted match kl-divergence two distribution one query model denote distribution one document language model smooth collection language model course go talk detail will find reference s also call cross entropy fact ignore term kl-divergence function end actually cross entropy term information theory anyway purpose see two formula look almost identical except probability word give query language model sum word document also nonzero probability query model s kind generalization sum match query word also easily see recover query likelihood retrieval function simply set query model relative frequency word query easy see plug eliminate query length constant get exactly like see equivalence s also kl-divergence model regard generalization query likelihood cover query likelihood special case would also allow us much 
feedback model interpolation d document (  q |  ) result q query q  q  ( 1   )  q    f =0 =1 q  q q  f feedback full feedback f feedback docs = { d1 d2 … dn } generative model 5 use kl-divergence model feedback picture show first estimate document language model estimate query language model compute kl-divergence often denote basically mean exactly like vector space model compute vector document compute another vector query compute distance vector special form probability distribution get result find feedback document let s assume mostly positive document although can also consider kind document can like rocchio be go compute another language model call feedback language model go another vector like compute centroid vector rocchio model combine original query model used linear interpolation would give us update model like rocchio see parameter alpha control amount feedback s set zero essentially feedback s set one get full feedback ignore original query generally desirable right unless absolutely sure see lot relevant document query term important course main question compute theta f big question rest easy talk one approach many approach course approach base generative model 
generative mixture model background word  p ( | c ) w = { d1 … dn } p ( source ) topic word 1- p ( |  ) w log p ( f |  )    c ( w ) log [ ( 1   ) p ( w |  )   p ( w | c ) ] w maximum likelihood  f  arg ax log p ( f |  )   = noise feedback document 6 be go show work use generative mixture model picture show model feedback model want estimate basis feedback document let s say observe positive document click document user random document judge user simply top rank document assume relevant imagine compute centroid document used language model one approach simply assume document generate language model can normalize word frequency get word distribution question whether distribution good feedback well imagine top rank word would think well word would common word always see language model top rank word actually common word like etc s good feedback would add lot word query interpolate original query model good need something particular try get rid common word see actually one way used background language model case learn association word word related word computer can would another way go talk another approach principled approach case be go say well say common word document belong topic model right assume well word generate background language model generate word like example use maximum likelihood estimate note word must generate model model force assign high probability word like occur frequently note order reduce probability model another model one help explain word case s appropriate use background language model achieve goal model would assign high probability common word approach assume machine generate word would work follow source control imagine flip coin decide distribution use probability lambda coin show head be go use background language model be go sample word model probability 1 minus lambda be go decide use know topic model would like estimate be go generate word make assumption whole thing one model call mixture model two distribution mixed together actually nt know distribution used think whole thing one model still ask word still give us word random manner course word show depend distribution distribution addition would also depend lambda say lambda high s go always use background distribution get different word say well lambda small be go use parameter model be think way basically exactly be go use maximum likelihood estimator adjust model estimate parameter basically be go adjust parameter best explain datum difference ask model know explain rather go ask whole model mixture model explain datum get help background model nt assign high probability word like result assign higher probability word common high probability would common be common would high probability accord maximum likelihood estimate method rare nt get much help background model result topic model must assign high probability high probability word accord topic model would common rare background basically little bit like idea weighting would allow us achieve effect remove topic word meaningless feedback mathematically compute likelihood local likelihood feedback document note also another parameter lambda assume lambda denote noise feedback document go let s say set parameter let s say 50 % word noise 90 % noise assume fix assume fix probability parameter like simple unigram language model n parameter n number word likelihood function would look like s similar global likelihood function see except inside logarithm s sum sum consider two distribution one used would depend lambda s form mathematically function theta unknown variable function value know except guy choose probability distribution maximize log likelihood idea maximum likelihood estimate mathematical problem solve optimization problem essentially would try theta value find one give whole thing maximum probability s well-defined math problem do obtain theta f interpolate original query model feedback 
example pseudo-feedback query model query “ airport security ” =09 mixture model w security airport beverage alcohol bomb terrorist author license bond counter-terror terror newsnet attack operation headline p ( w| f ) 00558 00546 00488 00474 00236 00217 00206 00188 00186 00173 00142 00129 00124 00121 00121 approach web database top 10 docs =07 w security airport beverage alcohol author bomb terrorist license state p ( w| f ) 00405 00377 00342 00305 00304 00268 00241 00214 00156 00150 00137 00135 00127 00127 00125 7 example feedback model learn web document collection pseudo-feedback use top ten document use mixture model query airport security first retrieve ten document web database course pseudo-feedback be go feed mixture model ten document set word learn used approach probability word give feedback model case case see highest probability word include relevant word query airport security example query word still show high probability case naturally occur frequently top rank document also see beverage alcohol bomb terrorist etc relevant topic combine original query help us much accurately document also help us bring document mention word maybe example airport bomb example pseudo-feedback work show model really work pick related word query s also interesting look two table compare will see case lambda set small value will see common word mean well nt use background model often remember lambda confuse probability used background model generate text nt rely much background model still use topic model account common word whereas set lambda high value use background model often explain word s burden expand common word feedback document topic model result topic model discriminative contain relevant word without common word add original query achieve feedback 
summary feedback text retrieval • feedback = learn example • three major feedback scenario – relevance pseudo implicit feedback • rocchio vsm • query model estimation lm – mixture model – many method ( eg relevance model ) propose [ zhai 08 ] 8 summarize lecture talk feedback language model approach general feedback learn example example assume example pseudo-example like assume top ten document assume relevant can base user interaction like feedback base clickthrough implicit feedback talk three major feedback scenario relevance feedback pseudo feedback implicit feedback talk use rocchio feedback vector space model use query model estimation feedback language model briefly talk mixture model basic idea many method example relevance model effective model estimate query model read method reference list end lecture 
additional reading • chengxiang zhai statistical language model information retrieval ( synthesis lecture series human language technology ) morgan & claypool publisher 2008 http s00158ed1v 01y200811hlt001 • victor lavrenko w bruce croft relevance base language model proceedings acm sigir 2011 9 two additional reading first one book systematic review discussion language model information retrieval second one important research paper s relevance base language model s effective way compute query model [ music ] 

1 lecture web search 
web search small relevant datum user text retrieval problem text retrieval method recommendation text access recommender system search engine evaluation system implementation web search natural language content analysis big text datum 2 lecture be go talk one important application text retrieval web search engine 
web search challenge & opportunity • challenge – scalability  parallel indexing & search ( mapreduce ) • handle size web ensure completeness coverage • serve many user query quickly spam detection – low quality information spam & robust ranking – dynamic web • new page constantly create page may update quickly • opportunity – many additional heuristic ( eg link ) leverage improve search accuracy link analysis & multi-feature ranking 3 let s first look general challenge opportunity web search many informational retrieval algorithms develop web born web born create best opportunity apply algorithms major application problem everyone would care naturally extension classical search algorithms address new challenge encounter web search general challenge first scalability challenge handle size web ensure completeness coverage information serve many user quickly answer query s one major challenge web born scale search relatively small second problem s quality information often spam third challenge dynamic web new page constantly create page may update quickly make harder keep index fresh challenge solve order deal high quality web search hand also interesting opportunity leverage include search result many additional heuristic example used link leverage improve score everything talk vector space model general algorithms apply search application s advantage hand also nt take advantage special characteristic page document specific application web search web page link obviously link something also leverage challenge opportunity new technique develop web search due need web search one parallel indexing search address issue scalability particular google s imaging map reduce influential helpful aspect second technique develop address problem spam spam detection will prevent spam page rank high also technique achieve robust ranking be go use lot signal rank page s easy spam search engine particular trick third line technique link analysis technique allow us improve result leverage extra information general web search be go use multiple feature ranking link analysis also explore kind crawl like layout anchor text describe link another page s picture show basic search engine technology basically web left user right side be go help user get access web information first component crawler would crawl page second 
basic search engine technology user web … browser query host info result retriever crawler cach page indexer - - - … - - - - … - - - … - - - - ( inverted ) index 4 component indexer would take page create inverted index third component retriever would use inverted index answer user s query talk user s browser search result give user browser would show result allow user interact web be go talk component 
component robot • build “ toy crawler ” easy – – – – start set “ seed page ” priority queue fetch page web parse fetch page hyperlink add queue follow hyperlink queue • real crawler much complicated… – – – – – – robustness ( server failure trap etc ) crawl courtesy ( server load balance robot exclusion etc ) handle file type ( image pdf file etc ) url extension ( cgi script internal reference etc ) recognize redundant page ( identical duplicate ) discover “ hide ” url ( eg truncating long url ) 5 first be go talk crawler also call spider software robot would something like crawl page web build toy crawler relatively easy need start set seed page fetch page web parse page figure new link add priority que explore additional link able real crawler actually tricky complicate issue deal example robustness server nt respond s trap generate dynamically generate webpage might attract crawler keep crawl side fetch dynamic generate page result issue crawl courtesy nt want overload one particular server many crawl request respect robot exclusion protocol also need handle different type file image pdf file kind format web also consider url extension sometimes cgi script internal reference etc sometimes javascript page also create challenge ideally also recognize redundant page nt duplicate page finally may interested discover hide url url may link page truncate url shorter path might able get additional page 
major crawl strategy • breadth-first common ( balance server load ) • parallel crawl natural • variation focuse crawl – target subset page ( eg page “ automobile ” ) – typically give query • find new page ( may link old page ) • repeat crawl – need minimize resource overhead – learn past experience ( update daily vs monthly ) – target 1 ) frequently update page 2 ) frequently access page 6 major crawl strategy general breadth-first common naturally balance sever load would keep probe particular server many request also parallel crawl natural task easy parallelize variation crawl task one interesting variation call focuse crawl case be go crawl page particular topic example page automobile right typically go start query use query get result major search engine start result gradually crawl one channel crawl find new channel person create person probably create new page time challenge new page actually link old page probably find re-crawle old page also interesting challenge solve finally might face scenario incremental crawl repeat crawl right let s say want build web search engine first crawl lot datum web crack datum future need crawl update page general nt re-crawl everything right s necessary case goal minimize resource overhead used minimum resource update page actually interesting research question open research question nt many standard algorithms establish yet task general imagine learn past experience two major factor consider first page update frequently quote page page static page nt change month probably nt re-crawl everyday s unlikely change frequently hand s sport score page get update frequently may need re-crawl maybe even multiple time day factor consider page frequently access user mean high utility page thus s important ensure page refresh compare another page never fetch user year even though page change lot s probably necessary crawl page least s urgent maintain freshness frequently access page user 
summary • web search one important application text retrieval – new challenge scalability efficiency quality information – new opportunity rich link information layout etc • crawler essential component web search application – initial crawl complete vs focuse – incremental crawl resource optimization 7 summarize web search one important application text retrieval new challenge particularly scalability efficiency quality information also new opportunity particularly rich link information layout etc crawler essential component web search application general find two scenario one initial crawl want complete crawl web general search engine focuse crawl want target certain type page another scenario be incremental update crawl datum incremental crawl case need optimize resource try use minimum resource get [ inaudible ] [ music ] 

1 [ sound ] lecture web indexing 
web search web indexing small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 lecture continue talk web search be go talk create web scale index 
basic search engine technology web crawler cach page indexer - - - … - - - - - - - … - - - ( inverted ) index … 3 crawl web have get lot web page next step use indexer create inverted index 
overview web indexing • standard ir technique basis insufficient – scalability – efficiency • google ’ contribution – google file system ( gfs ) distribute file system – mapreduce software framework parallel computation – hadoop open source implementation mapreduce 4 general use information retrieval technique create index talk previous lecture new challenge solve web scale indexing two main challenge scalability efficiency index would large actually fit single machine single disk store datum virtual machine also datum large s beneficial process datum parallel produce index quickly address challenge google make number innovation one google file system be general file system help programmer manage file store cluster machine second mapreduce general software framework support parallel computation hadoop well know open source implementation mapreduce used many application 
gfs architecture simple centralize management fix chunk size ( 64 mb ) chunk replicate ensure reliability datum transfer directly application chunk server ghemawat s gobioff h leung google file system sosp ’ 03 proceedings nineteenth acm symposium operate system principle ( new york ny usa 2003 ) acm pp 29–43 http gfs-sosp2003pdf architecture google file system used simple centralize management mechanism manage specific location file maintain file namespace look table know exactly file store application client talk gfs master obtain specific location file want process gfs file kind obtain specific location file application client talk specific server whether datum actually sit directly avoid involve node network file system store file machine system also great fix size chunk datum file separated many chunk chunk 64 mb s pretty big s appropriate large datum process chunk replicate ensure reliability something programmer nt worry s take care file system application perspective programmer would see s normal file programmer nt know exactly store invoke high level operator process file another feature datum transfer directly application chunk server s efficient sense 
mapreduce framework parallel programming • minimize effort programmer simple parallel • process task feature • hide many low-level detail ( network storage ) • built-in fault tolerance • automatic load balancing 6 top google file system google also propose mapreduce general framework parallel programming useful support task like build inverted index framework hiding lot low-level feature program result programmer make minimum effort create application run large cluster parallel low level detail hide framework include specific network communication load balancing task execute detail hide programmer also nice feature build fault tolerance one server break server task may finished mapreduce mapper know task do 
mapreduce computation pipeline input  key value key value key value map ( k v ) map ( k v ) map ( k v ) key value key value key value key value key value key value … … … mapreduce internal sort reduce ( k v [ ] ) key value key value key value output slide adapt alexander behm & ajey shah ’ presentation ( http behm-shah-pagerank ) 7 automatically dispatch task server job therefore program nt worry s mapreduce work input datum would separated number key value pair exactly value would depend datum s actually fairly general framework allow partition datum different part part processed parallel key value pair would send map function program right map function course map function process key value pair generate number key value pair course new key usually different old key s give map input key value pair output map function output map function would collect sort base key result value associate key group together have get pair key separate value attach key would send reduce function course reduce function handle different key send output value multiple reduce function handle unique key reduce function would process input key set value produce another set key value output output value would correct together form final output general framework mapreduce programmer need write map function reduce function everything else actually take care mapreduce framework see program really need minimum work framework input datum partition multiple part process parallel first map process reach reduce stage much reduce be [ inaudible ] also process different key associate value parallel achieve achieve purpose parallel process large datum set 
word count input text datum hello world bye world hello hadoop bye hadoop bye hadoop hello hadoop … … output count word bye 3 hadoop 4 hello 3 world 2 … within mapreduce framework slide adapt alexander behm & ajey shah ’ presentation ( http behm-shah-pagerank ) 8 let s take look simple example s word count input contain word output want generate number occurrence word s word count know kind count would useful example assess popularity word large collection useful achieve factor idf wading search solve problem well one natural thought well task do parallel simply count different part file parallel end combine count s precisely idea mapreduce 
word count map function input 1 “ hello world bye world ” 2 “ hello hadoop bye hadoop ” … output map ( k v ) map ( k v ) < hello1 > < world1 > < bye1 > < world1 > < hello1 > < hadoop1 > < bye1 > < hadoop1 > map ( k v ) { word w v collect ( w 1 ) } slide adapt alexander behm & ajey shah ’ presentation ( http behm-shah-pagerank ) 9 parallelize line input file specifically assume input map function key value pair represent line number string line first line example key one another word word four word line key value pair would send map function map function would count word line case course four word world get count one output see slide map function map function really simple look pseudocode look like right side see simply need iterate word line collect function mean would send word count collector collector would try sort key value pair different map function right function simple programmer specify function way process part datum course second line handled different map function produce single output 
word count reduce function map output < hello1 > < world1 > < bye1 > < world1 > < hello1 > < hadoop1 > < bye1 > < hadoop1 > … internal grouping output < bye  1 1 1 > reduce ( k v [ ] ) < bye 3 > < hadoop  1 1 1 1 > reduce ( k v [ ] ) < hadoop 4 > < hello  1 1 1 > reduce ( k v [ ] ) < hello 3 > reduce ( k v [ ] ) { int count = 0 v v count = v collect ( k count ) } slide adapt alexander behm & ajey shah ’ presentation ( http behm-shah-pagerank ) 10 okay output map function send collector collector would internal grouping sort stage see collect match pair pair word count line see pair sort base key word collect count word like bye together similarly word like hadoop hello etc word attach number value number count count represent occurrence solve word different light get new pair key set value pair fed reduce function reduce function would finish job count total occurrence word ready get puzzle account need simply add reduce function simple well counter iterate word will see array accumulate account right finally output p proto account s precisely want output whole program see ready similar build invert index think output index already get dictionary basically get count s miss document specific frequency count word document modify slightly actually able index parallel 
inverted indexing mapreduce d1 java resource java class map key java resource class value ( d1 2 ) ( d1 1 ) ( d11 ) d2 java travel resource key java travel resource d3 … value ( d2 1 ) ( d21 ) ( d21 ) built-in shuffle sort aggregate value key reduce key value java { ( d12 ) ( d2 1 ) } resource { ( d1 1 ) ( d21 ) } class { ( d11 ) } travel { ( d21 ) } … slide adapt jimmy lin ’ presentation 11 s one way case assume input map function pair key denote document id value denote screen document s word document map function would something similar see word campaign example simply group count word document together would generate set key value pair key word value count word document plus document id easily see need add document id later inverted index would like keep formation map function keep track send reduce function later similarly another document d2 processed way end sort mechanism would group together key like java associate document match key document java occur count count java document collect together fed reduce function see reduce function already get input look like inverted index entry s word document contain word frequency word document need simply concatenate continuous chunk datum do written file system basically reduce function go minimal work 
inverted indexing pseudo-code slide adapt jimmy lin ’ presentation 12 pseudo-code [ inaudible ] s construction see two function procedure map procedure reduce programmer would specify two function program top map reduce see basically describe case map s go count occurrence word used associativearray would output count together document id reduce function hand simply concatenate input give put together one single entry key simple mapreduce function yet would allow us construct inverted index large scale datum processed different machine program nt take care detail parallel index construction web search 
summary • web scale indexing require – store index multiple machine ( gfs ) – create index parallel ( mapreduce ) • gfs mapreduce general infrastructure 13 summarize web scale indexing require new technique go beyond standard traditional indexing technique mainly store index multiple machine usually do used filing system like google file system file system secondly require create index parallel s large take long time create index document parallel much faster do used mapreduce framework note gfs mapreduce framework general also support many application [ music ] 

1 
web search link analysis small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 
ranking algorithms web search • standard ir model apply ’ sufficient – different information need – document additional information – information quality vary lot • major extension – exploit link improve score – exploit clickthrough massive implicit feedback – general rely machine learn combine kind feature 3 
exploit inter-document link “ extra text ” summary doc description ( “ anchor text ” ) link indicate utility doc hub link tell us authority 4 
pagerank capture page “ popularity ” • intuition – link like citation literature – page cite often expect useful general • pagerank essentially “ citation count ” improve simple count – consider “ indirect citation ” ( cite highly cite paper count lot… ) – smooth citation ( every page assume nonzero pseudo citation count ) • pagerank also interpreted random surfing ( thus capture popularity ) 5 
pagerank algorithm random surfing model page prob  randomly jump another page prob ( 1- ) randomly pick link follow p ( di ) pagerank score di = average probability visit page di d1 d3 transition matrix 0 1  0  1 2 d2 d4 0 0 1 2 2 0 0 0 1 2 0   0   0  n m j 1 ij 1 probability page di time probability visit page dj time t+1 “ equilibrium equation ” mij = probability go di dj n n 1 1 pt 1 ( j )  ( 1   )  ij pt ( )    n1 pt ( ) = # page reach dj via follow link drop ntime index p ( j )   [ n1   ( 1   ) ij ] p ( ) reach dj via random jump   p  ( i  ( 1   ) ) p 1 solve equation iterative algorithm iij = n 
pagerank example d1 d3 n p ( j )   [ n1   ( 1   ) ij ] p ( ) d2 1   p  ( i  ( 1   ) ) p d4 0 1  ( 1  02 )  02  08   0  1 2  p n 1 ( d1 )   p n ( d1 )  005  n 1   n  p ( 2 )  005  p ( 2 )     p n 1 ( )    p n ( )   045 3 3      n 1 n     045 p ( ) p ( ) 4  4    0 0 2 0 1 0 2 0 1 2 1 4  1 4 0   02   1 4 0    0  1 4 085 005 005 085 005 005 005 005 4 4 4 4 4 4 4 4 n 045   p ( d1 )   n  p ( 2 )  045   005  p n ( 3 )     005  p n ( )  4   p n 1 ( d1 )  005 * p n ( d1 )  085 * p n ( 2 )  005 * p n ( 3 )  045 * p n ( 4 ) initial value p ( ) n iterate converge see score propagate graph 1 4 1 4  1 4  1 4 
pagerank practice • computation quite efficient since usually sparse • normalization ’ affect ranking lead variant formula • zero-outlink problem p ( di ) ’ ’ sum 1 – one possible solution = page-specific damp factor ( =10 page outlink ) • many extension ( eg topic-specific pagerank ) • many application ( eg social network analysis ) 8 
hit capture authority & hub • intuition – page widely cite good authority – page cite many page good hub • key idea hit ( hypertext-induced topic search ) – good authority cite good hub – good hub point good authority – iterative reinforcement… • many application network analysis 9 
hit algorithm d1 d3 d2 d4 0 0 1 0 a 0 1  1 1 h ( )  1 1 0 0  0 0  0 0  ( j ) j out ( ) ( di )   j in ( ) h  aa h ( j )  h h  aat h  aa “ adjacency matrix ” initial value ( di ) h ( di ) 1 iterate normalize  ( ) 2   h ( )  1 2 10 
summary • link information useful – anchor text – pagerank – hit • pagerank hit many application analyze graph network 11 

1 
web search learn rank small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 
combine many feature ( learn rank ) • general idea – give query-doc pair ( q ) define various kind feature xi ( q ) – example feature number overlapping term bm25 score q p ( q|d ) pagerank p ( q|di ) di may anchor text big font text “ url contain ‘ ~ ’ ” … – hypothesize p ( r=1|q ) s ( x1 ( q ) … xn ( q )  )  set parameter – learn  fitting function training datum ie 3-tuple like ( q 1 ) ( relevant q ) ( q0 ) ( non-relevant q ) 3 
regression-based approach logistic regression xi ( q ) feature  ’ parameter log n p ( r  1 | q )  0   i x 1  p ( r  1 | q ) 1 p ( r  1 | q )  1 n 1  exp (   0    x ) 1 p ( { ( q d1 1 ) ( q d2 0 ) } )   estimate  ’ maximize likelihood training datum x1 ( q ) x2 ( q ) x3 ( q ) bm25 pagerank bm25anchor d1 ( r=1 ) 07 011 065 d2 ( r=0 ) 03 005 04 1 1 * ( 1  ) 1  exp (   0  07 1  011 2  065 3 ) 1  exp (   0  031  005 2  04  3 )  *  arg max  p ( { ( q1 d11 r11 ) ( q1 d12 r12 ) ( qn dm1 rm1 ) } )  ’ know take xi ( q ) compute base new query new document generate score wrt q 4 
advanced learn algorithms • attempt directly optimize retrieval measure ( eg map ndcg ) – difficult optimization problem – many solution propose [ liu 09 ] • apply many ranking problem beyond search – recommender system – computational advertising – summarization –… 5 
summary • machine learn apply text retrieval since many decade ago ( eg rocchio feedback ) • recent use machine learn drive – large-scale training datum available – need combine many feature – need robust ranking ( spam ) • modern web search engine use kind ml technique combine many feature optimize ranking • learn rank still active research topic 6 
additional reading • tie-yan liu learn rank information retrieval foundation trend information retrieval 3 3 ( 2009 ) 225-331 • hang li short introduction learn rank ieice tran inf & syst e94-d 10 ( 2011 ) np 7 

1 [ sound ] lecture future web search 
web search future web search small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 lecture be go talk possible future trend web search intelligent information retrieval system general 
next generation search engine • customize ( vertical search engine ) – special group user ( community engine eg citeseer ) – personalize ( better understand user ) – special domain ( better understand document ) • learn time ( evolve ) • integration search navigation filter ( full-fledged information management ) • beyond search support task ( eg shopping ) • many opportunity innovation 3 order improve accuracy search engine s important consider special case information need one particular trend can specialize customize search engine call vertical search engine vertical search engine expect effective current general search engine can assume user special group user might common information need search engine customize ser user customization s also possible personalization search personalize better understand user restriction domain also advantage handle document better understand document example particular word may ambiguous domain bypass problem ambiguity another trend expect see search engine able learn time s like lifetime learn lifelong learn course attractive mean search engine self-improve person used search engine become better better already happen search engine learn [ inaudible ] feedback user use quality search engine allow popular query type many user allow become better sort another feature see third trend might integration bottle information access search navigation recommendation filter might combine form full-fledged information management system begin course talk push versus pull different mode information access mode combine similarly pull mode query browse can also combine fact be basically today [ inaudible ] search ending query sometimes browse click link sometimes have get information recommend although case information recommend advertising future imagine seamlessly integrate system multi-mode information access would convenient person another trend might see system try go beyond search support user task reason person want search solve problem make decision perform task example consumer might search opinion product order purchase product choose good product case would beneficial support whole workflow purchasing product choose product era common search engine already provide good support example sometimes look reviews want buy click button go shopping site directly get do provide good task support many task example researcher might want find realm literature site literature s much support finishing task writing paper general think many opportunity wait follow slide will talk little bit specific idea thought hopefully help imagine new application possibility might already relevant currently work 
data-user-service ( dus ) triangle lawyer scientist uiuc employee online shopper … user search browse mining task support … datum web page news article blog article literature email … service 4 general think intelligent system especially intelligent information system specify three node connect three triangle will able specify information system call data-user-service triangle basically three question ask would serve kind datum manage kind service provide right would help us basically specify system many different way connect depend connect different kind system let give example top see different kind user left side see different type datum information bottom see different service function imagine connect different way example connect everyone web page support search browse get well s web search right 
million way connect dus triangle customer uiuc everyone scientist online … service employee shopper web page literature web search organization docs enterprise search literature assistant opinion advisor blog article product reviews … person customer rel man customer email search browse alert mining decision support … 5 connect uiuc employee organization document enterprise document support search browse s enterprise search connect scientist literature information provide kind service include search browse alert new random document mining analyze research trend provide task support decision support example might might able provide support automatically generate related work section research paper would closer task support right imagine would literature assistant connect online shopper blog article product reviews help person improve shopping experience provide example datum mining capability analyze reviews compare product compare sentiment product provide task support decision support choose product buy connect customer service person email customer imagine system provide analysis email find major complaint customer imagine system can provide task support automatically generate response customer email maybe intelligently attach also promotion message appropriate detect be positive message complaint might take opportunity attach promotion information whereas s complaint might able automatically generate generic response first tell customer expect detailed response later etc try help person improve productivity show opportunity really lot s restrict imagination picture show trend technology also characterize intelligent information system three angle see center s triangle connect keyword query search bag word representation 
future intelligent information system task support intelligentmining & interactive task support access search current search engine keyword query search history personalization complete model ( useruser modele ) bag word entities-relation large-scale semantic analysis knowledge representation ( vertical search engine ) 6 mean current search engine basically provide search support user mostly model user base keyword query see datum bag word representation s simple approximation actual information document s current system connect three node simple way provide basic search function nt really understand user nt really understand much information document show trend push node toward advanced function think user node right go beyond keyword query look user search history model user completely understand user s task environment task need context information okay push personalization complete user model major direction research order build intelligent information system document side also see go beyond bag word implementation entity relation representation mean will recognize person s name relation location etc already feasible today s natural process technique google reason initiative knowledge graph nt hear good step toward direction get level without initiate robust manner larger scale enable search engine provide much better service future would like knowledge representation add perhaps inference rule search engine would become intelligent call large-scale semantic analysis perhaps feasible vertical search engine s easier make progress particular domain service side see need go beyond search support information access general search one way get access information well recommender system push pull different way get access random information go beyond access also need help person digest information information find step analysis information datum mining find pattern convert text information real knowledge used application actionable knowledge used decision make furthermore knowledge used help user improve productivity finishing task example decision-make task right trend basically dimension anticipate future intelligent information system provide intelligent interactive task support also emphasize interactive s important optimize combine intelligence user system get help user natural way nt assume system everything human user machine collaborate intelligent way efficient way combine intelligence high general minimize user s overall effort solve problem big picture future intelligent information system hopefully provide us insight make innovation top handled today [ music ] 

1 
recommender system text retrieval problem small relevant datum user text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 
two mode text access pull vs push • pull mode ( search engine ) – user take initiative – ad hoc information need • push mode ( recommender system ) – system take initiative – stable information need system good knowledge user ’ need 3 
recommender  filter system • stable & long term interest dynamic info source • system must make delivery decision immediately document “ arrive ” interest … filter system 4 
basic filter question user u like item x • two different way answer – look item u like check x similar item similarity = > content-based filter – look like x check u similar user similarity = > collaborative filter • combine 5 
typical content-based filter system initialization doc source accumulate docs binary classifier user profile text accept docs user user interest profile utility func learn feedback linear utility = * # good - 2 * # bad # good ( # bad ) number good ( bad ) document deliver user coefficient ( 3 2 ) reasonable ( 10 1 ) ( 1 10 ) 6 
three basic problem content-based filter • make filter decision ( binary classifier ) – doc text profile text  no • initialization – initialize filter base profile text example • learn – limit relevance judgment ( “ yes ” docs ) – accumulate document • try maximize utility 7 
extend retrieval system information filter • “ reuse ” retrieval technique score document • use score threshold filter decision • learn improve score traditional feedback • new approach threshold set learn 8 
general vector-space approach doc vector score utility evaluation threshold yes profile vector vector learn threshold threshold learn feedback information 9 
difficulty threshold learn 365 334 321 299 273 … rel nonrel rel • =300 • • censor datum ( judgment available deliver document ) none labele datum exploration vs exploitation judgment available document 10 
empirical utility optimization • basic idea – compute utility training datum candidate score threshold – choose threshold give maximum utility training datum set • difficulty bias training sample – get upper bound true optimal threshold – can discard item possibly interesting user • solution – heuristic adjustment ( lower ) threshold 11 
beta-gamma threshold learn utility θo p tima l encourage exploration zero  θ  α * θzero  ( 1 - α * θo p tima l θzero  0123… k   n α  β  ( 1 - β  * e  n γ n  # training example cutoff position ( descend order doc score )    [ 01 ] example less exploration ( closer optimal ) 12 
beta-gamma threshold learn ( cont ) • pro – explicitly address exploration-exploitation tradeoff ( “ safe ” exploration ) – arbitrary utility ( appropriate lower bound ) – empirically effective • con – purely heuristic – zero utility lower bound often conservative 13 
summary • two strategy filter – content-based ( item similarity ) – collaborative filter ( user similarity ) • content-based recommender system build base search engine system – add threshold mechanism – add adaptive learn algorithms 14 

1 
recommender system collaborative filter small relevant datum user text retrieval problem text retrieval method recommendation vector space model text access recommender system search engine natural language content analysis system implementation evaluation probabilistic model feedback web search big text datum 2 
basic filter question user u like item x • two different way answer – look item u like check x similar item similarity = > content-based filter – look like x check u similar user similarity = > collaborative filter • combine 
collaborative filter ( cf ) • make filter decision individual user base judgment user • infer individual ’ preference similar user • general idea – give user u find similar user { u1 … um } – predict u ’ preference base preference u1 … um – user similarity judge base similarity preference common set item 4 
cf assumption • user interest similar preference • user similar preference probably share interest • example – “ interest information retrieval ” = > “ favor sigir paper ” – “ favor sigir paper ” = > “ interest information retrieval ” • sufficiently large number user preference available ( “ cold start ” problem ) 5 
collaboration filter problem rating object o1 o2 u1 u2 3 15 … … … 2 user u ui 1 … oj … 2 task um xij=f ( ui oj ) = 3 unknown function f u x o r • • • assume know f value ( u ) ’ predict f value ( u ) ’ essentially function approximation like learn problem 6 
memory-based approach • general idea – xij rating object oj user ui – ni average rating object user ui – normalize rating vij = xij – ni – prediction rating object oj user ua vˆaj  k  w ( ) vij 1 xˆaj  vˆaj  na k   w ( ) 1 • specific approach differ w ( ) - similarity user ua ui 7 
user similarity measure • pearson correlation coefficient ( sum commonly rate item )  ( x  n ) ( x  n ) aj w p ( )  ij j 2 2 ( x  n ) ( x  n )  aj  ij j j • cosine measure n w c ( )  • many possibility x j 1 n  x aj j 1 aj 2 x ij n  x ij 2 j 1 8 
improve user similarity measure • deal miss value set default rating ( eg average rating ) • inverse user frequency ( iuf ) similar idf 9 
summary recommender system • recommendation “ easy ” – user ’ expectation low – recommendation better none • filter “ hard ” – must make binary decision though ranking also possible – datum sparseness ( limit feedback information ) – “ cold start ” ( little information user begin ) • content-based vs collaborative filter vs hybrid • recommendation combine search  push + pull • many advanced algorithms propose use context information advanced machine learn 10 
additional reading • francesco ricci lior rokach bracha shapira paul b kantor recommender system handbook springer 2011 http recommender_syste ms_handbookpdf 11 

1 [ noise ] lecture summary course 
course summary major topic cover small relevant datum user text retrieval problem text retrieval method recommendation text access recommender system search engine natural language content analysis vector space model system implementation evaluation probabilistic model feedback big text datum web search 2 map show major topic cover course key high-level take-away message first talk natural language content analysis main take-away message natural language process foundation text retrieval currently nlp nt robust enough battle war generally main method used modern search engine s often sufficient search task obviously complex search task need deeper natural language process technique talk high level strategy text access talk push versus pull pull talk query versus browse general future search engine integrate technique provide math involved information access will talk number issue related search engine talk search problem frame ranking problem talk number retrieval method start overview vector space model probabilistic model talk vector space model depth also later talk language modele approach s probabilistic model many take-away message modele retrieval function tend look similar generally use various heuristic important one tf-idf weighting document length normalization tf often transform sub medium transformation function talk implement retrieval system main technique talk construct inverted index prepare system answer query quickly talk faster search used inverted index talk evaluate text retrieval system mainly introduce cranfield evaluation methodology important evaluation methodology apply many task talk major evaluation measure important measure search engine map mean average precision ndcg summarize discount accumulative gain also precision recall two basic measure talk feedback technique talk rocchio vector space model mixture model language modele approach feedback important technique especially consider opportunity learn lot pixel web 
key high-level take-away message 33tr  retrieval ranking problem text problem small relevant datum recommendation recommendation content-based + collaborative filter user 44many retrieval method text retrieval method vsm lm 5 vector space model tf-idf length norm text access vs browse push vs pull query recommender system search engine nlp foundation tr language current nlp ’ robust natural content analysis enough bow sufficient search task big text datum inverted index + system fast search implementation cranfield eval method evaluation map ndcg prec recall probabilistic model feedback rocchio mixture model feedback mapreduce parallel indexing web search pagerank hit learn rank future web search 3 talk web search talk use parallel scene solve scalability issue scene be go use net reduce talk use link permission model app improve search talk page rank hit major hour analyze link web talk learn rank use machine learn combine multiple feature improvement score effectiveness improve used approach also improve robustness ranking function easy expand search engine feature promote page finally talk future web search major reaction might see future improve count regeneration engine finally talk recommend system system increment push mode will talk two approach one content-based one collaborative filter combine together obvious miss piece picture user user interface also important component search engine even though current search interface relatively simple actually do lot study user interface visualization example 
search user interface marti hearst cambridge university press 2009 http 4 topic learn read book s excellent book kind study search used face 
additional reading • synthesis digital library many excellent short long tutorial relevant topic http forthcomingsynthesislecture – information concept retrieval service http 1 – human language technology http 1 – artificial intelligence & machine learn http 1 • journal acm tois irj ipm … • conference sigir cikm ecir wsdm www kdd acl … info check http resource 5 want know topic talk also read additional reading list short course manage cover basic topic text retrieval search engine resource provide additional information advanced topic give thorough treatment topic talk main source synthesis digital library see lot short textbook textbook long tutorial tend provide lot information explain topic lot series related cause one information concept retrieval service one human langauge technology yet another artificial intelligence machine learn also major journal conference list tend lot research paper need topic course finally information resource include reading tool kit etc check url 
main technique harness big text datum text retrieval + text mining course text mining & analytic course text retrieval text mining big text big text datum datum small relevant datum small relevant datum knowledge many application take text mining course datum mining specialization series naturally next step take course picture show mine big text datum generally need two kind technique one text retrieval cover course technique help us convert raw big text datum small relevant text datum actually need specific application human play important role mining text datum text datum written human consume involve human process datum mining important course cover various strategy help user get access relevant datum technique always essential text mining system help provide prominence help user interpret inner pattern user define text datum mining general user would go back original datum better understand pattern text mining cause rather text mining analytic course deal user follow information second step picture would convert text datum actionable knowledge help user digest find information find pattern reveal knowledge text knowledge used application system help decision make help user finish task take course natural step natural next step would take course thank take course hope fun find course useful look forward interact future opportunity [ music ] 

1 
text mining analytic • text mining  text analytic • turn text datum high-quality information actionable knowledge – minimize human effort ( consume text datum ) – supply knowledge optimal decision make • related text retrieval essential component text mining system – text retrieval preprocessor text mining – text retrieval need knowledge provenance 2 
text vs non-text datum human subjective “ sensor ” real world sense weather report sensor thermometer 3c 15f … geo sensor location 41°n 120°w … network sensor network perceive datum 01000100011100 express “ human sensor ” 3 
general problem datum mining actionable knowledge sensor 1 real world sensor 2 … sensor k … non-text datum numerical categorical relational video datum mining software general datum mining … video mining text datum text mining 4 
problem text mining actionable knowledge non-text datum text datum real world … joint mining text + non-text text mining 5 
landscape text mining analytic infer real-world variable ( predictive analytic ) + non-text datum mining content text datum observed world real world text datum + context perceive express ( perspective ) ( english ) mining knowledge observer mining knowledge language 6 
topic cover course text-based prediction topic mining & analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining & sentiment analysis natural language process & text representation word association mining & analysis 7 

1 [ sound ] 
text mining analytic • text mining  text analytic • turn text datum high-quality information actionable knowledge – minimize human effort ( consume text datum ) – supply knowledge optimal decision make • related text retrieval essential component text mining system – text retrieval preprocessor text mining – text retrieval need knowledge provenance 2 look text mining problem closely see problem similar general datum mining except will focuse text datum 
text vs non-text datum human subjective “ sensor ” real world sense weather report sensor thermometer 3c 15f … geo sensor location 41°n 120°w … network sensor network perceive datum 01000100011100 express “ human sensor ” 3 be go text mining algorithms help us turn text datum actionable knowledge use real world especially decision make complete whatever task require text datum support general many real world problem datum mining also tend kind datum non-textual 
general problem datum mining actionable knowledge sensor 1 real world sensor 2 … sensor k … non-text datum numerical categorical relational video datum mining software general datum mining … video mining text datum text mining 4 general picture would include non-text datum well reason might concern joint mining text non-text datum course be go focus text mining be also go also touch joint analysis text datum non-text datum problem definition look landscape topic text mining analytic slide show process generate text datum detail specifically human sensor human observer would look word perspective different person would look world different angle will pay attention different thing person different time might also pay attention different aspect observed world human able perceive world perspective human sensor would form view world call observed world course would different real world perspective person take often bias also observed world represent example entity-relation graph general way used knowledge representation language general basically person mind world nt really know exactly look like course human would express person observed used natural language english result text datum course person can used different language express observed case might text datum mixed language different language main goal text mining actually revert process generate text datum hope able uncover aspect process specifically think mining example knowledge language mean look text datum english may able discover something english usage english pattern english one type mining problem result knowledge language may useful various way look picture 
problem text mining actionable knowledge non-text datum text datum real world … joint mining text + non-text text mining 5 also mine knowledge observed world much mining content text datum be go look text datum try get essence extract high quality information particular aspect world be interested example everything say particular person particular entity regard mining content describe observed world user s mind person s mind look also imagine 
landscape text mining analytic infer real-world variable ( predictive analytic ) + non-text datum mining content text datum observed world real world text datum + context perceive express ( perspective ) ( english ) mining knowledge observer mining knowledge language 6 mine knowledge observer also used text datum infer property person property can include mood person sentiment person note distinguish observed word person text datum ca nt describe person observed objective way description also subject sentiment general imagine text datum would contain factual description world plus subjective comment s s also possible text mining mine knowledge observer finally look picture left side picture see certainly also say something real world right indeed text mining infer real world variable often call predictive analytic want predict value certain interesting variable picture basically cover multiple type knowledge mine text general infer real world variable can also use result mining text datum intermediate result help prediction example mine content text datum might generate summary content summary can used help us predict variable real world course still generate original text datum want emphasize often process text datum generate feature help prediction important s show result mining task include mining content text datum mining knowledge observer helpful prediction fact non-text datum can also use non-text datum help prediction course depend problem general non-text datum important prediction task example want predict stock price change stock price base discussion news article social medium example used text datum predict real world variable case obviously historical stock price datum would important prediction s example non-text datum would useful prediction be go combine kind datum make prediction non-text datum also used analyze text supply context look text datum alone will mostly look content or opinion expressed text text datum generally also context associate example time location associate text datum useful context information context provide interesting angle analyze text datum example might partition text datum different time period availability time analyze text datum time period make comparison similarly partition text datum base location meta datum s associate form interesting comparison area sense non-text datum actually provide interesting angle perspective text datum analysis help us make context-sensitive analysis content language usage opinion observer author text datum can analyze sentiment different context fairly general landscape topic text mining analytic course be go selectively cover topic actually hope cover general topic 
topic cover course text-based prediction topic mining & analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining & sentiment analysis natural language process & text representation word association mining & analysis 7 first be go cover natural language process briefly understand text datum determine represent text datum text mining second be go talk mine word association text datum word association form use lexical knowledge language third be go talk topic mining analysis one way analyze content text s useful way analyze content s also one useful technique text mining be go talk opinion mining sentiment analysis regard one example mining knowledge observer finally be go cover text-based prediction problem try predict real world variable base text datum slide also serve road map course be go use outline topic will cover rest course [ music ] 

1 
natural language content analysis text-based prediction topic mining & analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining & sentiment analysis natural language process & text representation word association mining & analysis 2 
basic concept nlp lexical analysis ( part-of-speech tag ) dog chasing boy playground det semantic analysis noun aux det noun prep det noun phrase complex verb dog ( d1 ) boy ( b1 ) playground ( p1 ) chasing ( d1 b1 p1 ) + scared ( x ) chasing ( _ x _ ) scared ( b1 ) inference verb noun phrase noun noun phrase prep phrase verb phrase verb phrase sentence syntactic analysis ( parse ) person say may remind another person get dog back pragmatic analysis ( speech act ) 3 
nlp difficult • natural language design make human communication efficient result – omit lot common sense knowledge assume reader possess – keep lot ambiguity assume reader know resolve • make every step nlp hard – ambiguity killer – common sense reasoning pre-requir 4 
example challenge • word-level ambiguity – “ design ” noun verb ( ambiguous pos ) – “ root ” multiple meaning ( ambiguous sense ) • syntactic ambiguity – “ natural language process ” ( modification ) – “ man see boy ” ( pp attachment ) • anaphora resolution “ john persuade bill buy tv ” ( = john bill ) • presupposition “ quit smoking ” imply smoke 5 
state art dog chasing boy playground det noun aux noun phrase verb complex verb det noun prep noun phrase det noun pos tag 97 % noun phrase prep phrase verb phrase parse partial > 90 % ( ) semantic aspect verb phrase - relation extraction - word sense disambiguation - sentiment analysis sentence speech act analysis inference 6 
’ • 100 % pos tag – “ turn ” vs “ turn fan ” • general complete parse – “ man see boy telescope ” • precise deep semantic analysis – ever able precisely define meaning “ ” “ john own restaurant ” robust general nlp tend shallow deep understand ’ scale 7 
summary • nlp foundation text mining • computer far able understand natural language – deep nlp require common sense knowledge inference thus work limit domain – shallow nlp base statistical method do large scale thus broadly applicable • practice statistical nlp basis human provide help need 8 
additional read man chris hinrich schütze foundation statistical natural language process cambridge mit press 1999 9 

1 
natural language content analysis text-based prediction topic mining & analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining & sentiment analysis natural language process & text representation word association mining & analysis 2 
basic concept nlp lexical analysis ( part-of-speech tag ) dog chasing boy playground det semantic analysis noun aux det noun prep det noun phrase complex verb dog ( d1 ) boy ( b1 ) playground ( p1 ) chasing ( d1 b1 p1 ) + scared ( x ) chasing ( _ x _ ) scared ( b1 ) inference verb noun phrase noun noun phrase prep phrase verb phrase verb phrase sentence syntactic analysis ( parse ) person say may remind another person get dog back pragmatic analysis ( speech act ) 3 
nlp difficult • natural language design make human communication efficient result – omit lot common sense knowledge assume reader possess – keep lot ambiguity assume reader know resolve • make every step nlp hard – ambiguity killer – common sense reasoning pre-requir 4 
example challenge • word-level ambiguity – “ design ” noun verb ( ambiguous pos ) – “ root ” multiple meaning ( ambiguous sense ) • syntactic ambiguity – “ natural language process ” ( modification ) – “ man see boy ” ( pp attachment ) • anaphora resolution “ john persuade bill buy tv ” ( = john bill ) • presupposition “ quit smoking ” imply smoke 5 
state art dog chasing boy playground det noun aux noun phrase verb complex verb det noun prep noun phrase det noun pos tag 97 % noun phrase prep phrase verb phrase parse partial > 90 % ( ) semantic aspect verb phrase - relation extraction - word sense disambiguation - sentiment analysis sentence speech act analysis inference 6 
’ • 100 % pos tag – “ turn ” vs “ turn fan ” • general complete parse – “ man see boy telescope ” • precise deep semantic analysis – ever able precisely define meaning “ ” “ john own restaurant ” robust general nlp tend shallow deep understand ’ scale 7 
summary • nlp foundation text mining • computer far able understand natural language – deep nlp require common sense knowledge inference thus work limit domain – shallow nlp base statistical method do large scale thus broadly applicable • practice statistical nlp basis human provide help need 8 
additional read man chris hinrich schütze foundation statistical natural language process cambridge mit press 1999 9 

1 
text representation text-based prediction topic mining & analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining & sentiment analysis natural language process & text representation word association mining & analysis 2 
dog chasing boy playground dog chasing boy playground det noun aux verb det noun prep noun phrase complex verb det + syntactic structure prep phrase verb phrase sequence word + pos tag noun noun phrase noun phrase string character verb phrase sentence dog animal chase boy person playground location + logic predicate dog ( d1 ) boy ( b1 ) playground ( p1 ) chasing ( d1 b1 p1 ) speech act = request + entity relation + speech act deeper nlp require human effort less accurate closer knowledge representation 3 
text representation enabled analysis course text rep generality enabled analysis example application string word string process word relation analysis topic analysis sentiment analysis compression thesaurus discovery topic opinion related application + syntactic structure + entity & relation syntactic graph analysis stylistic analysis structurebased feature extraction discovery knowledge opinion specific entity + logic predicate integrative analysis scatter knowledge assistant knowledge logic inference biologist knowledge graph analysis information network analysis 4 
summary • text representation determine kind mining algorithms apply • multiple way represent text possible – string word syntactic structure entity-relation graph predicates… – should combine real application • course focus word-base representation – general robust applicable natural language – little manual effort – “ surprisingly ” powerful many application ( ) – combine sophisticated representation 5 

1 
text representation text-based prediction topic mining & analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining & sentiment analysis natural language process & text representation word association mining & analysis 2 
dog chasing boy playground dog chasing boy playground det noun aux verb det noun prep noun phrase complex verb det + syntactic structure prep phrase verb phrase sequence word + pos tag noun noun phrase noun phrase string character verb phrase sentence dog animal chase boy person playground location + logic predicate dog ( d1 ) boy ( b1 ) playground ( p1 ) chasing ( d1 b1 p1 ) speech act = request + entity relation + speech act deeper nlp require human effort less accurate closer knowledge representation 3 
text representation enabled analysis course text rep generality enabled analysis example application string word string process word relation analysis topic analysis sentiment analysis compression thesaurus discovery topic opinion related application + syntactic structure + entity & relation syntactic graph analysis stylistic analysis structurebased feature extraction discovery knowledge opinion specific entity + logic predicate integrative analysis scatter knowledge assistant knowledge logic inference biologist knowledge graph analysis information network analysis 4 
summary • text representation determine kind mining algorithms apply • multiple way represent text possible – string word syntactic structure entity-relation graph predicates… – should combine real application • course focus word-base representation – general robust applicable natural language – little manual effort – “ surprisingly ” powerful many application ( ) – combine sophisticated representation 5 

1 
word association mining & analysis text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
outline • word association • mine word association • mine word association 3 
basic word relation paradigmatic vs syntagmatic • paradigmatic & b paradigmatic relation substitute ( ie & b class ) – eg “ cat ” “ dog ” “ monday ” “ tuesday ” • syntagmatic & b syntagmatic relation combine ( ie & b related semantically ) – eg “ cat ” “ sit ” “ car ” “ drive ” • two basic complementary relation generalized describe relation item language 4 
mine word association • useful improve accuracy many nlp task – pos tag parse entity recognition acronym expansion – grammar learn • directly useful many application text retrieval mining – text retrieval ( eg use word association suggest variation query ) – automatic construction topic map browse word node association edge – compare summarize opinion ( eg word strongly associate “ battery ” positive negative reviews iphone 6 respectively ) 5 
mining word association intuition paradigmatic similar context cat eat fish saturday cat eat turkey tuesday dog eat meat sunday dog eat turkey tuesday … cat _ eat fish saturday _ eat turkey tuesday … dog _ eat meat sunday _ eat turkey tuesday … similar left context similar right context similar general context similar context ( “ cat ” ) context ( “ dog ” ) similar context ( “ cat ” ) context ( “ computer ” ) 6 
mining word association intuition syntagmatic correlated occurrence cat eat fish saturday cat eat turkey tuesday dog eat meat sunday dog eat turkey tuesday … _ _ _ _ … eat _ eat _ eat _ eat _ word tend occur left “ eat ” saturday tuesday sunday tuesday word right whenever “ eat ” occur word also tend occur helpful occurrence “ eat ” predict occurrence “ meat ” helpful occurrence “ eat ” predict occurrence “ text ” 7 
mining word association general idea • paradigmatic – represent word context – compute context similarity – word high context similarity likely paradigmatic relation • syntagmatic – count many time two word occur together context ( eg sentence paragraph ) – compare co-occurrence individual occurrence – word high co-occurrence relatively low individual occurrence likely syntagmatic relation • paradigmatically related word tend syntagmatic relation word  joint discovery two relation • idea implement many different way 8 

1 
paradigmatic relation discovery text-based prediction topic mining & analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
word context “ pseudo document ” left1 ( “ cat ” ) = { “ ” “ ” “ big ” “ ” “ ” … } cat _ eat fish saturday _ eat turkey tuesday … window8 ( “ cat ” ) = { “ ” “ ” “ big ” “ eat ” “ fish ” … } right1 ( “ cat ” ) = { “ eat ” “ eat ” “ ” “ ” … } context = pseudo document = “ bag word ” context may contain adjacent non-adjacent word 3 
measure context similarity sim ( “ cat ” “ dog ” ) = sim ( left1 ( “ cat ” ) left1 ( “ dog ” ) ) + sim ( right1 ( “ cat ” ) right1 ( “ dog ” ) ) + … + sim ( window8 ( “ cat ” ) window8 ( “ dog ” ) ) = high sim ( word1 word2 )  word1 word2 paradigmatically related 4 
bag word  vector space model ( vsm ) word1 pseudo doc ( “ cat ” ) = ( x1 …xn ) n vocabulary size = ( y1 …yn ) pseudo doc ( “ dog ” ) wordn word2 term vector “ eat ” “ eat ” ( 5 3 “ ” “ ” … 10 3 … ) 5 
vsm paradigmatic relation mining word1 compute vector = ( x1 …xn ) xi = = ( y1 …yn ) sim ( d1 d2 ) = word2 yj = wordn many approach possible ( develop originally text retrieval ) 6 
expect overlap word context ( eowc ) probability randomly picked word d1 wi = ( x1 …xn ) = ( y1 …yn ) count word wi d1 xi c ( wi d1 ) | yi = c ( wi d2 ) | total count word d1 sim ( d1 d2 ) = 𝒙𝟏 𝒚𝟏 + … + 𝒙𝑵 𝒚𝑵 = 𝑵 𝒊=𝟏 𝒙𝒊 𝒚𝒊 probability two randomly picked word d1 d2 respectively identical 7 
would eowc work well • intuitively make sense overlap two context document higher similarity would • however – favor match one frequent term well match distinct term – treat every word equally ( overlap “ ” ’ meaningful overlap “ eat ” ) 8 
expect overlap word context ( eowc ) probability randomly picked word d1 wi = ( x1 …xn ) = ( y1 …yn ) count word wi d1 xi c ( wi d1 ) | yi = c ( wi d2 ) | total count word d1 sim ( d1 d2 ) = 𝒙𝟏 𝒚𝟏 + … + 𝒙𝑵 𝒚𝑵 = 𝑵 𝒊=𝟏 𝒙𝒊 𝒚𝒊 probability two randomly picked word d1 d2 respectively identical 2 
improve eowc retrieval heuristic • favor match one frequent term well match distinct term  sublinear transformation term frequency ( tf ) • treat every word equally ( overlap “ ” ’ meaningful overlap “ eat ” )  reward match rare word idf term weighting 3 
tf transformation c ( w ) tf ( w ) term frequency weight = x y=tf ( w ) = log ( 1+x ) 2 1 bit vector ( ignore count ) 1 0 1 2 3 … x=c ( w ) 4 
tf transformation bm25 transformation term frequency weight large k y=tf ( w ) k+1 = 𝑘+1 𝑥 𝑥+𝑘 2 k=0 1 0 1 2 3 … x=c ( w ) 5 
idf weighting penalize popular term idf ( w ) log ( m+1 ) total number docs collection idf ( w ) = log [ ( m+1 ) k ] total number docs contain w ( doc frequency ) 1 k ( doc freq ) 6 
adapt bm25 retrieval model paradigmatic relation mining = ( x1 …xn ) bm 25 ( w d1 )  xi  = ( y1 …yn ) ( k  1 ) c ( w d1 ) c ( w d1 )  k ( 1  b  * | d1 | avdl ) bm 25 ( w d1 ) n  bm 25 ( w d1 ) j1 j b  [ 01 ] k  [ 0  ) yi defined similarly sim ( d1 d2 ) = 𝑵 𝒊=𝟏 𝑰𝑫𝑭 ( 𝒘𝒊 ) 𝒙𝒊 𝒚𝒊 7 
bm25 also discover syntagmatic relation = ( x1 …xn ) bm 25 ( w d1 )  xi  ( k  1 ) c ( w d1 ) c ( w d1 )  k ( 1  b  * | d1 | avdl ) bm 25 ( w d1 ) n  bm 25 ( w d1 ) j1 j b  [ 01 ] k  [ 0  ) idf-weighted = ( x1*idf ( w1 ) … xn*idf ( wn ) ) highly weight term context vector word w likely syntagmatically related w 8 
summary • main idea discover paradigmatic relation – collect context candidate word form pseudo document ( bag word ) – compute similarity corresponding context document two candidate word – highly similar word pair assume paradigmatic relation • many different way implement general idea • text retrieval model easily adapt compute similarity two context document – bm25 + idf weighting represent state art – syntagmatic relation also discover “ product ” 9 

1 
paradigmatic relation discovery text-based prediction topic mining & analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
word context “ pseudo document ” left1 ( “ cat ” ) = { “ ” “ ” “ big ” “ ” “ ” … } cat _ eat fish saturday _ eat turkey tuesday … window8 ( “ cat ” ) = { “ ” “ ” “ big ” “ eat ” “ fish ” … } right1 ( “ cat ” ) = { “ eat ” “ eat ” “ ” “ ” … } context = pseudo document = “ bag word ” context may contain adjacent non-adjacent word 3 
measure context similarity sim ( “ cat ” “ dog ” ) = sim ( left1 ( “ cat ” ) left1 ( “ dog ” ) ) + sim ( right1 ( “ cat ” ) right1 ( “ dog ” ) ) + … + sim ( window8 ( “ cat ” ) window8 ( “ dog ” ) ) = high sim ( word1 word2 )  word1 word2 paradigmatically related 4 
bag word  vector space model ( vsm ) word1 pseudo doc ( “ cat ” ) = ( x1 …xn ) n vocabulary size = ( y1 …yn ) pseudo doc ( “ dog ” ) wordn word2 term vector “ eat ” “ eat ” ( 5 3 “ ” “ ” … 10 3 … ) 5 
vsm paradigmatic relation mining word1 compute vector = ( x1 …xn ) xi = = ( y1 …yn ) sim ( d1 d2 ) = word2 yj = wordn many approach possible ( develop originally text retrieval ) 6 
expect overlap word context ( eowc ) probability randomly picked word d1 wi = ( x1 …xn ) = ( y1 …yn ) count word wi d1 xi c ( wi d1 ) | yi = c ( wi d2 ) | total count word d1 sim ( d1 d2 ) = 𝒙𝟏 𝒚𝟏 + … + 𝒙𝑵 𝒚𝑵 = 𝑵 𝒊=𝟏 𝒙𝒊 𝒚𝒊 probability two randomly picked word d1 d2 respectively identical 7 
would eowc work well • intuitively make sense overlap two context document higher similarity would • however – favor match one frequent term well match distinct term – treat every word equally ( overlap “ ” ’ meaningful overlap “ eat ” ) 8 
expect overlap word context ( eowc ) probability randomly picked word d1 wi = ( x1 …xn ) = ( y1 …yn ) count word wi d1 xi c ( wi d1 ) | yi = c ( wi d2 ) | total count word d1 sim ( d1 d2 ) = 𝒙𝟏 𝒚𝟏 + … + 𝒙𝑵 𝒚𝑵 = 𝑵 𝒊=𝟏 𝒙𝒊 𝒚𝒊 probability two randomly picked word d1 d2 respectively identical 2 
improve eowc retrieval heuristic • favor match one frequent term well match distinct term  sublinear transformation term frequency ( tf ) • treat every word equally ( overlap “ ” ’ meaningful overlap “ eat ” )  reward match rare word idf term weighting 3 
tf transformation c ( w ) tf ( w ) term frequency weight = x y=tf ( w ) = log ( 1+x ) 2 1 bit vector ( ignore count ) 1 0 1 2 3 … x=c ( w ) 4 
tf transformation bm25 transformation term frequency weight large k y=tf ( w ) k+1 = 𝑘+1 𝑥 𝑥+𝑘 2 k=0 1 0 1 2 3 … x=c ( w ) 5 
idf weighting penalize popular term idf ( w ) log ( m+1 ) total number docs collection idf ( w ) = log [ ( m+1 ) k ] total number docs contain w ( doc frequency ) 1 k ( doc freq ) 6 
adapt bm25 retrieval model paradigmatic relation mining = ( x1 …xn ) bm 25 ( w d1 )  xi  = ( y1 …yn ) ( k  1 ) c ( w d1 ) c ( w d1 )  k ( 1  b  * | d1 | avdl ) bm 25 ( w d1 ) n  bm 25 ( w d1 ) j1 j b  [ 01 ] k  [ 0  ) yi defined similarly sim ( d1 d2 ) = 𝑵 𝒊=𝟏 𝑰𝑫𝑭 ( 𝒘𝒊 ) 𝒙𝒊 𝒚𝒊 7 
bm25 also discover syntagmatic relation = ( x1 …xn ) bm 25 ( w d1 )  xi  ( k  1 ) c ( w d1 ) c ( w d1 )  k ( 1  b  * | d1 | avdl ) bm 25 ( w d1 ) n  bm 25 ( w d1 ) j1 j b  [ 01 ] k  [ 0  ) idf-weighted = ( x1*idf ( w1 ) … xn*idf ( wn ) ) highly weight term context vector word w likely syntagmatically related w 8 
summary • main idea discover paradigmatic relation – collect context candidate word form pseudo document ( bag word ) – compute similarity corresponding context document two candidate word – highly similar word pair assume paradigmatic relation • many different way implement general idea • text retrieval model easily adapt compute similarity two context document – bm25 + idf weighting represent state art – syntagmatic relation also discover “ product ” 9 

1 
syntagmatic relation discovery entropy text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
syntagmatic relation = correlated occurrence whenever “ eat ” occur word also tend occur cat eat fish saturday cat eat turkey tuesday dog eat meat sunday dog eat turkey tuesday … _ _ _ _ … eat _ eat _ eat _ eat _ word tend occur left “ eat ” saturday tuesday sunday tuesday word right 3 
word prediction intuition prediction question word w present ( absent ) segment text segment ( unit eg sentence paragraph document ) … … word easier predict other 1 ) w = “ meat ” 2 ) = “ ” 3 ) = “ unicorn ” 4 
word prediction formal definition binary random variable xw  { 0 1 } 1 xw   0 w present w absent p ( x w  1 )  p ( x w  0 )  1 random xw difficult prediction would one quantitatively measure “ randomness ” random variable like xw 5 
entropy h ( x ) measure randomness x h ( x w )    p ( x w v ) log2 p ( x w  v ) v { 0 1 } 1 xw   0 w present w absent   p ( x w  0 ) log2 p ( x w  0 )  p ( x w  1 ) log2 p ( x w  1 ) define 0 log2 0  0 h ( xw ) xw h ( xw ) reach minimum eg p ( xw=1 ) 1 p ( xw=1 ) 05 10 05 10 p ( xw=1 ) equivalently p ( xw=0 ) ( ) 6 
entropy h ( x ) coin toss h ( x coin )   p ( x coin  0 ) log2 p ( x coin  0 )  p ( x coin  1 ) log2 p ( x coin  1 ) xcoin toss coin 1 head x coin   0 tail h ( x ) fair coin p ( x=1 ) p ( x=0 ) 2 1 1 1 1 h ( x )   log2  log2  1 2 2 2 2 completely bias p ( x=1 ) 1 10 05 10 p ( x=1 ) h ( x )  0 * log2 0  1 * log2 1  0 7 
entropy word prediction word w present ( absent ) segment … 1 ) w = “ meat ” 2 ) w = “ ” … 3 ) w = “ unicorn ” low h ( xmeat ) h ( xthe ) h ( xunicorn ) h ( xthe ) 0  uncertainty since p ( xthe=1 )  1 high entropy word harder predict 8 

1 
syntagmatic relation discovery conditional entropy text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
know text segment prediction question “ meat ” present ( absent ) segment … eat … presence “ eat ” help predict presence “ meat ” reduce uncertainty “ meat ” ie h ( xmeat ) know absence “ eat ” also help 3 
conditional entropy know nothing segment know “ eat ” present ( xeat 1 ) p ( x meat  1 ) p ( x meat  1 | x eat  1 ) p ( x meat  0 ) p ( x meat  0 | x eat  1 ) h ( x meat )   p ( x meat  0 ) log2 p ( x meat  0 )  p ( x meat  1 ) log2 p ( x meat  1 ) h ( x meat | x eat  1 )   p ( x meat  0 | x eat  1 ) log2 p ( x meat  0 | x eat  1 ) - p ( x meat  1 | x eat  1 ) log2 p ( x meat  1 | x eat  1 ) h ( x meat | x eat  0 ) defined similarly 4 
conditional entropy complete definition h ( x meat | x eat )    [ p ( x u { 0 1 } eat  [ p ( x u { 0 1 }  u ) eat  u ) h ( x meat | x eat  u ) ]  [  p ( x v { 0 1 } meat  v | x eat  u ) log2 p ( x meat  v | x eat  u ) ] ] general discrete random variable x h ( x )  h ( x|y ) ’ minimum possible value h ( x|y ) 5 
conditional entropy capture syntagmatic relation h ( x meat | x eat )   [ p ( x u { 0 1 } eat  u ) h ( x meat | x eat  u ) ] h ( x meat | x meat )  smaller h ( xmeat|xthe ) h ( xmeat|xeat ) word w h ( xmeat|xw ) reach minimum ( ie 0 ) word w h ( xmeat|xw ) reach maximum h ( xmeat ) 6 
conditional entropy mining syntagmatic relation • word w1 – every word w2 compute conditional entropy h ( xw1|xw2 ) – sort candidate word ascend order h ( xw1|xw2 ) – take top-ranked candidate word word potential syntagmatic relation w1 – need use threshold w1 • however h ( xw1|xw2 ) h ( xw1|xw3 ) comparable h ( xw1|xw2 ) h ( xw3|xw2 ) ’ mine strongest k syntagmatic relation collection 7 

1 
syntagmatic relation discovery mutual information text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
mutual information ( x ) measure entropy reduction much reduction entropy x obtain know mutual information ( x ) = h ( x ) – h ( x|y ) = h ( ) h ( y|x ) property - non-negative ( x ) 0 - symmetric ( x ) i ( x ) - ( x ) 0 iff x & independent fix x rank different y ( x ) h ( x|y ) give order ( x ) allow us compare different ( x ) pair 3 
mutual information ( x ) syntagmatic relation mining mutual information ( x ) = h ( x ) – h ( x|y ) = h ( ) h ( y|x ) whenever “ eat ” occur word also tend occur word high mutual information “ eat ” ( xeat xmeat ) = ( xmeat xeat ) > ( xeat xthe ) = ( xthe xeat ) ( xeat xeat ) h ( xeat )  ( xeat xw ) 4 
rewrite mutual information ( mi ) used kl-divergence observed joint distribution xw1 xw2 ( x w1 x w 2 )    p ( x w1  u x w 2  v ) log2 u { 0 1 } v { 0 1 } p ( x w1  u x w 2  v ) p ( x w1  u ) p ( x w 2  v ) expect joint distribution xw1 xw2 xw1 xw2 independent mi measure divergence actual joint distribution expect distribution independence assumption larger divergence higher mi would 5 
probability involved mutual information ( x w1 x w 2 )    p ( x w1  u x w 2  v ) log2 u { 0 1 } v { 0 1 } p ( x w1  u x w 2  v ) p ( x w1  u ) p ( x w 2  v ) presence & absence w1 p ( xw1=1 ) + p ( xw1=0 ) = 1 presence & absence w2 p ( xw2=1 ) + p ( xw2=0 ) = 1 co-occurrence w1 w2 p ( xw1=1 xw2=1 ) + p ( xw1=1 xw2=0 ) p ( xw1=0 xw2=1 ) + p ( xw1=0 xw2=0 ) = 1 w1 & w2 occur w1 occur w2 occur none occur 6 
relation different probability presence & absence w1 p ( xw1=1 ) + p ( xw1=0 ) = 1 presence & absence w2 p ( xw2=1 ) + p ( xw2=0 ) = 1 co-occurrence w1 w2 p ( xw1=1 xw2=1 ) + p ( xw1=1 xw2=0 ) p ( xw1=0 xw2=1 ) + p ( xw1=0 xw2=0 ) = 1 constraint p ( xw1=1 xw2=1 ) + p ( xw1=1 xw2=0 ) = p ( xw1=1 ) p ( xw1=0 xw2=1 ) + p ( xw1=0 xw2=0 ) = p ( xw1=0 ) p ( xw1=1 xw2=1 ) + p ( xw1=0 xw2=1 ) = p ( xw2=1 ) p ( xw1=1 xw2=0 ) + p ( xw1=0 xw2=0 ) = p ( xw2=0 ) 7 
computation mutual information presence & absence w1 p ( xw1=1 ) + p ( xw1=0 ) = 1 p ( xw2=1 ) + p ( xw2=0 ) = 1 presence & absence w2 co-occurrence w1 w2 p ( xw1=1 xw2=1 ) + p ( xw1=1 xw2=0 ) p ( xw1=0 xw2=1 ) + p ( xw1=0 xw2=0 ) = 1 p ( xw1=1 xw2=1 ) + p ( xw1=1 xw2=0 ) = p ( xw1=1 ) p ( xw1=0 xw2=1 ) + p ( xw1=0 xw2=0 ) = p ( xw1=0 ) p ( xw1=1 xw2=1 ) + p ( xw1=0 xw2=1 ) = p ( xw2=1 ) p ( xw1=1 xw2=0 ) + p ( xw1=0 xw2=0 ) = p ( xw2=0 ) need know p ( xw1=1 ) p ( xw2=1 ) p ( xw1=1 xw2=1 ) 8 
estimation probability ( depend datum ) w1 count ( w1 ) p ( x w1  1 )  n count ( w 2 ) p ( x w 2  1 )  n count ( w1 w 2 ) p ( x w1  1 x w 2  1 )  n w2 segment_1 segment_2 segment_3 1 1 1 0 w1 occur 1 occur 1 occur segment_4 0 0 neither occur … segment_n 0 1 w2 occur count ( w1 ) = total number segment contain w1 count ( w2 ) = total number segment contain w2 count ( w1 w2 ) = total number segment contain w1 w2 9 
smooth accommodate zero count count ( w1 )  05 n 1 count ( w 2 )  05 p ( x w 2  1 )  n 1 count ( w1 w 2 )  025 p ( x w1  1 x w 2  1 )  n 1 p ( x w1  1 )  smooth add pseudo datum event zero count ( pretend observed extra datum ) w1 w2 ¼ pseudoseg_1 ¼ pseudoseg_2 ¼ pseudoseg_3 0 1 0 0 0 1 ¼ pseudoseg_4 1 1 1 0 0 1 segment_1 … segment_n actually observed datum 10 
summary syntagmatic relation discovery • syntagmatic relation discover measure correlation occurrence two word • three concept information theory – entropy h ( x ) measure uncertainty random variable x – conditional entropy h ( x|y ) entropy x give know – mutual information ( x ) entropy reduction x ( ) due know ( x ) • mutual information provide principled way discover syntagmatic relation 11 
summary word association mining • two basic association paradigmatic syntagmatic – generally applicable item language ( eg phrase entity unit ) • pure statistical approach available discover ( combine perform joint analysis ) – generally applicable text human effort – different way define “ context ” “ segment ” lead interesting variation application • discover association support many application 12 
additional read • chris man hinrich schütze foundation statistical natural language process mit press cambridge may 1999 ( chapter 5 collocation ) • chengxiang zhai exploit context identify lexical atom statistical view linguistic context proceedings international interdisciplinary conference model used context ( context-97 ) rio de janeiro brzil 4-6 pp 119-129 • jiang chengxiang zhai random walk adjacency graph mining lexical relation big text datum proceedings ieee bigdata conference 2014 pp 549-554 13 

1 
syntagmatic relation discovery mutual information text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
mutual information ( x ) measure entropy reduction much reduction entropy x obtain know mutual information ( x ) = h ( x ) – h ( x|y ) = h ( ) h ( y|x ) property - non-negative ( x ) 0 - symmetric ( x ) i ( x ) - ( x ) 0 iff x & independent fix x rank different y ( x ) h ( x|y ) give order ( x ) allow us compare different ( x ) pair 3 
mutual information ( x ) syntagmatic relation mining mutual information ( x ) = h ( x ) – h ( x|y ) = h ( ) h ( y|x ) whenever “ eat ” occur word also tend occur word high mutual information “ eat ” ( xeat xmeat ) = ( xmeat xeat ) > ( xeat xthe ) = ( xthe xeat ) ( xeat xeat ) h ( xeat )  ( xeat xw ) 4 
rewrite mutual information ( mi ) used kl-divergence observed joint distribution xw1 xw2 ( x w1 x w 2 )    p ( x w1  u x w 2  v ) log2 u { 0 1 } v { 0 1 } p ( x w1  u x w 2  v ) p ( x w1  u ) p ( x w 2  v ) expect joint distribution xw1 xw2 xw1 xw2 independent mi measure divergence actual joint distribution expect distribution independence assumption larger divergence higher mi would 5 
probability involved mutual information ( x w1 x w 2 )    p ( x w1  u x w 2  v ) log2 u { 0 1 } v { 0 1 } p ( x w1  u x w 2  v ) p ( x w1  u ) p ( x w 2  v ) presence & absence w1 p ( xw1=1 ) + p ( xw1=0 ) = 1 presence & absence w2 p ( xw2=1 ) + p ( xw2=0 ) = 1 co-occurrence w1 w2 p ( xw1=1 xw2=1 ) + p ( xw1=1 xw2=0 ) p ( xw1=0 xw2=1 ) + p ( xw1=0 xw2=0 ) = 1 w1 & w2 occur w1 occur w2 occur none occur 6 
relation different probability presence & absence w1 p ( xw1=1 ) + p ( xw1=0 ) = 1 presence & absence w2 p ( xw2=1 ) + p ( xw2=0 ) = 1 co-occurrence w1 w2 p ( xw1=1 xw2=1 ) + p ( xw1=1 xw2=0 ) p ( xw1=0 xw2=1 ) + p ( xw1=0 xw2=0 ) = 1 constraint p ( xw1=1 xw2=1 ) + p ( xw1=1 xw2=0 ) = p ( xw1=1 ) p ( xw1=0 xw2=1 ) + p ( xw1=0 xw2=0 ) = p ( xw1=0 ) p ( xw1=1 xw2=1 ) + p ( xw1=0 xw2=1 ) = p ( xw2=1 ) p ( xw1=1 xw2=0 ) + p ( xw1=0 xw2=0 ) = p ( xw2=0 ) 7 
computation mutual information presence & absence w1 p ( xw1=1 ) + p ( xw1=0 ) = 1 p ( xw2=1 ) + p ( xw2=0 ) = 1 presence & absence w2 co-occurrence w1 w2 p ( xw1=1 xw2=1 ) + p ( xw1=1 xw2=0 ) p ( xw1=0 xw2=1 ) + p ( xw1=0 xw2=0 ) = 1 p ( xw1=1 xw2=1 ) + p ( xw1=1 xw2=0 ) = p ( xw1=1 ) p ( xw1=0 xw2=1 ) + p ( xw1=0 xw2=0 ) = p ( xw1=0 ) p ( xw1=1 xw2=1 ) + p ( xw1=0 xw2=1 ) = p ( xw2=1 ) p ( xw1=1 xw2=0 ) + p ( xw1=0 xw2=0 ) = p ( xw2=0 ) need know p ( xw1=1 ) p ( xw2=1 ) p ( xw1=1 xw2=1 ) 8 
estimation probability ( depend datum ) w1 count ( w1 ) p ( x w1  1 )  n count ( w 2 ) p ( x w 2  1 )  n count ( w1 w 2 ) p ( x w1  1 x w 2  1 )  n w2 segment_1 segment_2 segment_3 1 1 1 0 w1 occur 1 occur 1 occur segment_4 0 0 neither occur … segment_n 0 1 w2 occur count ( w1 ) = total number segment contain w1 count ( w2 ) = total number segment contain w2 count ( w1 w2 ) = total number segment contain w1 w2 9 
smooth accommodate zero count count ( w1 )  05 n 1 count ( w 2 )  05 p ( x w 2  1 )  n 1 count ( w1 w 2 )  025 p ( x w1  1 x w 2  1 )  n 1 p ( x w1  1 )  smooth add pseudo datum event zero count ( pretend observed extra datum ) w1 w2 ¼ pseudoseg_1 ¼ pseudoseg_2 ¼ pseudoseg_3 0 1 0 0 0 1 ¼ pseudoseg_4 1 1 1 0 0 1 segment_1 … segment_n actually observed datum 10 
summary syntagmatic relation discovery • syntagmatic relation discover measure correlation occurrence two word • three concept information theory – entropy h ( x ) measure uncertainty random variable x – conditional entropy h ( x|y ) entropy x give know – mutual information ( x ) entropy reduction x ( ) due know ( x ) • mutual information provide principled way discover syntagmatic relation 11 
summary word association mining • two basic association paradigmatic syntagmatic – generally applicable item language ( eg phrase entity unit ) • pure statistical approach available discover ( combine perform joint analysis ) – generally applicable text human effort – different way define “ context ” “ segment ” lead interesting variation application • discover association support many application 12 
additional read • chris man hinrich schütze foundation statistical natural language process mit press cambridge may 1999 ( chapter 5 collocation ) • chengxiang zhai exploit context identify lexical atom statistical view linguistic context proceedings international interdisciplinary conference model used context ( context-97 ) rio de janeiro brzil 4-6 pp 119-129 • jiang chengxiang zhai random walk adjacency graph mining lexical relation big text datum proceedings ieee bigdata conference 2014 pp 549-554 13 

1 [ sound ] > > lecture topic mining analysis be go talk motivation task definition 
topic mining analysis motivation task definition text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 lecture be go talk different kind mining task see road map cover mining knowledge language namely discovery word association paradigmatic relation syntagmatic relation start lecture be go talk mining another kind knowledge content mining try discover knowledge main topic text call topic mining analysis lecture be go talk motivation task definition 
topic mining analysis motivation • topic  main idea discuss text datum – subject discussion conversation – different granularity ( eg topic sentence article etc ) • many application require discovery topic text – twitter user talk today – current research topic datum mining different 5 year ago – person like iphone 6 dislike – major topic debate 2012 presidential election 3 first let s look concept topic topic something understand think s actually easy formally define roughly speaking topic main idea discuss text datum think theme subject discussion conversation also different granularity example talk topic sentence topic article aa topic paragraph topic research article research library right different grand narrative topic obviously different application indeed many application require discovery topic text be analyze example example might interested know twitter user talk today talk nba sport talk international event etc interested know research topic example one might interested know current research topic datum mining different five year ago involve discovery topic datum mining literature also want discover topic today s literature past make comparison might also also interested know person like product like iphone 6 dislike involve discover topic positive opinion iphone 6 also negative reviews perhaps be interested know major topic debate 2012 presidential election discover topic text analyze be go talk lot technique 
topic knowledge world knowledge world text datum real world … non-text datum + context time location … topic 1 topic 2 … topic k 4 general view topic knowledge world text datum expect discover number topic topic generally provide description world tell us something world product person etc non-text datum context analyze topic example might know time associate text datum location text datum produce author text source text etc meta datum context variable associate topic discover use context variable help us analyze pattern topic example look topic time would able discover whether s trend topic topic might fading away soon look topic different location might know insight person s opinion different location s mining topic important let s look task topic mining analysis general would involve first discover lot topic case k topic also would like know topic cover document extent 
task topic mining analysis task 2 figure document cover topic text datum doc 1 doc 2 … doc n topic 1 topic 2 … topic k task 1 discover k topic 5 example document one might see topic 1 cover lot topic 2 topic k cover small portion topic perhaps cover document two hand cover topic 2 well cover topic 1 also cover topic k extent etc right see generally two different task sub-task first discover k topic collection text layer k topic okay major topic text second task figure document cover topic extent 
formal definition topic mining analysis • input – collection n text document = { d1 … dn } – number topic k • output – k topic { 1 … k } – coverage topic di { i1 … ik } – ij = prob di cover topic j define i k  j1 ij 1 6 formally define problem follow first input collection n text document denote text collection c denote text article i generally also need input number topic k may technique automatically suggest number topic technique discuss also useful technique often need specify number topic output would k topic would like discover order theta sub one theta sub k also want generate coverage topic document sub denote pi sub j pi sub ij probability document sub cover topic theta sub j obviously document set value indicate extent document cover topic assume probability sum one document wo nt able cover topic outside topic discuss discover question define theta sub define topic problem completely defined define exactly theta next lecture be go talk different way define theta [ music ] 

1 
formal definition topic mining analysis • input – collection n text document = { d1 … dn } – number topic k • output – k topic { 1 … k } – coverage topic di { i1 … ik } – ij=prob di cover topic j define i k  j1 ij 1 2 
initial idea topic = term text datum 1 “ sport ” 2 “ travel ” … k “ science ” doc 1 doc 2 30 % 11 21=0 … doc n n1=0 12 12 % 22 n2 1k 2k nk 8 % 3 
mining k topical term collection c • parse text c obtain candidate term ( eg term = word ) • design score function measure good term topic – – – – favor representative term ( high frequency favore ) avoid word frequent ( eg “ ” “ ” ) tf-idf weighting retrieval useful domain-specific heuristic possible ( eg favor title word hashtag tweet ) • pick k term highest score try minimize redundancy – multiple term similar closely related pick one ignore other 4 
compute topic coverage ij doc di 1 “ sport ” 2 “ travel ” … k “ science ” i1 i2 count ( “ sport ” di ) 4 count ( “ travel ” di ) 2 ij  count (  j ) k  count (  l 1 l di ) ik count ( “ science ” di ) 1 5 
well approach work doc di cavalier vs golden state warrior nba playoff final … basketball game … travel cleveland … star … 1 “ sport ” i1  c ( ` sport )  0 2 “ travel ” i 2  c ( ` travel )  1  0 … k “ science ” need count related word also 2 “ star ” ambiguous ( eg star sky ) ik  c ( ` science )  0 mine complicate topic 6 
problem “ term topic ” • lack expressive power – represent general topic – ’ represent complicate topic • incompleteness vocabulary coverage – ’ capture variation vocabulary ( eg related word ) • word sense ambiguity – topical term related term ambiguous ( eg basketball star vs star sky ) 7 

1 
topic mining analysis probabilistic topic model text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
problem “ term topic ” • lack expressive power  topic = { multiple word } – represent general topic – ’ represent complicate topic • incompleteness vocabulary coverage + weight word – ’ capture variation vocabulary ( eg related word ) • word sense ambiguity  split ambiguous word – topical term related term ambiguous ( eg basketball star vs star sky ) probabilistic topic model 3 
improve idea topic = word distribution 1 “ sport ” p ( w|1 ) sport 002 game 001 basketball 0005 football 0004 play 0003 star 0003 … nba 0001 … travel 00005 …  p ( w |  )  1 wv 2 “ travel ” … p ( w|2 ) travel 005 attraction 003 trip 001 flight 0004 hotel 0003 island 0003 … culture 0001 … play 00002 … k “ science ” p ( w|k ) science 004 scientist 003 spaceship 0006 telescope 0004 genomic 0004 star 0002 … genetic 0001 … travel 000001 … vocabulary set = { w1 w2 … } 4 
probabilistic topic mining analysis • input – collection n text document = { d1 … dn } – vocabulary set = { w1 … wm } – number topic k • output – k topic word distribution { 1 … k }  p ( w | i )  1 wv – coverage topic di { i1 … ik } k – ij=prob di cover topic j  ij  1 j1 5 
computation task input c k v output { 1 … k } { i1 … ik } doc 1 text datum 1 2 … sport 002 game 001 basketball 0005 football 0004 … travel 005 attraction 003 trip 001 … science 004 003 k scientist spaceship 0006 … 30 % 11 doc 2 21=0 % … doc n n1=0 % 12 % 12 22 n2 8 % 1k 2k nk 6 
generative model text mining modele datum generation p ( datum model  ) = ( { 1 … k } { 11 … 1k } … { n1 … nk } ) many parameter total parameter inference * = argmax  p ( | model  ) p ( datum model  ) *  7 
summary • topic represent word distribution – multiple word allow describe complicate topic – weight word model subtle semantic variation topic • task topic mining analysis – input collection c number topic k vocabulary set v – output set topic word distribution coverage topic document = ( { 1 … k } { 11 … 1k } … { n1 … nk } ) j  [ 1 k ]  p ( w |  j )  1 wv k i  [ 1 n ]  ij  1 j1 8 
summary ( cont ) • generative model text mining – model datum generation prob model p ( datum model  ) – infer likely parameter value * give particular datum set * = argmax  p ( | model  ) – take * “ knowledge ” mine text mining problem – adjust design model discover different knowledge 9 

university illinois urbana-champaign 
probabilistic topic model overview statistical language model text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
statistical language model ( lm ) • probability distribution word sequence – p ( “ today wednesday ” )  0001 – p ( “ today wednesday ” )  00000000000001 – p ( “ eigenvalue positive ” )  000001 • context-dependent • also regard probabilistic mechanism “ generate ” text – thus also call “ generative ” model today wednesday … today wednesday eigenvalue positive 3 
simplest language model unigram lm • • • • generate text generate word independently thus p ( w1 w2 wn ) p ( w1 ) p ( w2 ) …p ( wn ) parameter { p ( wi ) } p ( w1 ) …+p ( wn ) 1 ( n voc size ) text = sample draw accord word distribution wednesday today … eigenvalue p ( “ today wed ” ) = p ( “ today ” ) p ( “ ” ) p ( “ wed ” ) = 00002  0001  0000015 4 
text generation unigram lm unigram lm p ( w| ) … text 02 mining 01 association 001 topic 1 cluster 002 text mining … food 000001 … topic 2 health … food 025 nutrition 01 healthy 005 diet 002 … sampling document p ( |  ) = text mining paper food nutrition paper 5 
estimation unigram lm unigram lm p ( w| ) = … 100 100 100 100 100 estimation text mining paper total # words=100 text mining association database … query … maximum likelihood estimate best estimate define “ best ” text 10 mining 5 association 3 database 3 algorithm 2 … query 1 efficient 1 6 
maximum likelihood vs bayesian • maximum likelihood estimation – “ best ” mean “ datum likelihood reach maximum ” ˆ  arg max p ( x |  )  – problem small sample • bayesian estimation  baye rule p ( x | )  p ( | x ) p ( x ) p ( ) – “ best ” mean consistent “ prior ” knowledge explain datum well ˆ  arg max p (  | x )  arg max p ( x |  ) p (  )   – problem define prior  maximum posteriori ( map ) estimate 7 
illustration bayesian estimation bayesian inference f (  ) = f̂ (  )   f (  ) p (  | x ) posterior p ( |x )  p ( x| ) p (  )  posterior mean ˆ    * p (  | x ) likelihood p ( x| ) = ( x1 … xn )  prior p (  ) 0 prior mode  1 posterior mode ml ml estimate 8 
summary • language model = probability distribution text = generative model text datum • unigram language model = word distribution • likelihood function p ( x| ) – give   x higher likelihood – give x   maximize p ( |  ) [ ml estimate ] • bayesian inference – must define prior p (  ) – posterior distribution p ( |x )  p ( x| ) p (  )  allow infer “ derive value ”  9 

university illinois urbana-champaign 
probabilistic topic model overview statistical language model text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
statistical language model ( lm ) • probability distribution word sequence – p ( “ today wednesday ” )  0001 – p ( “ today wednesday ” )  00000000000001 – p ( “ eigenvalue positive ” )  000001 • context-dependent • also regard probabilistic mechanism “ generate ” text – thus also call “ generative ” model today wednesday … today wednesday eigenvalue positive 3 
simplest language model unigram lm • • • • generate text generate word independently thus p ( w1 w2 wn ) p ( w1 ) p ( w2 ) …p ( wn ) parameter { p ( wi ) } p ( w1 ) …+p ( wn ) 1 ( n voc size ) text = sample draw accord word distribution wednesday today … eigenvalue p ( “ today wed ” ) = p ( “ today ” ) p ( “ ” ) p ( “ wed ” ) = 00002  0001  0000015 4 
text generation unigram lm unigram lm p ( w| ) … text 02 mining 01 association 001 topic 1 cluster 002 text mining … food 000001 … topic 2 health … food 025 nutrition 01 healthy 005 diet 002 … sampling document p ( |  ) = text mining paper food nutrition paper 5 
estimation unigram lm unigram lm p ( w| ) = … 100 100 100 100 100 estimation text mining paper total # words=100 text mining association database … query … maximum likelihood estimate best estimate define “ best ” text 10 mining 5 association 3 database 3 algorithm 2 … query 1 efficient 1 6 
maximum likelihood vs bayesian • maximum likelihood estimation – “ best ” mean “ datum likelihood reach maximum ” ˆ  arg max p ( x |  )  – problem small sample • bayesian estimation  baye rule p ( x | )  p ( | x ) p ( x ) p ( ) – “ best ” mean consistent “ prior ” knowledge explain datum well ˆ  arg max p (  | x )  arg max p ( x |  ) p (  )   – problem define prior  maximum posteriori ( map ) estimate 7 
illustration bayesian estimation bayesian inference f (  ) = f̂ (  )   f (  ) p (  | x ) posterior p ( |x )  p ( x| ) p (  )  posterior mean ˆ    * p (  | x ) likelihood p ( x| ) = ( x1 … xn )  prior p (  ) 0 prior mode  1 posterior mode ml ml estimate 8 
summary • language model = probability distribution text = generative model text datum • unigram language model = word distribution • likelihood function p ( x| ) – give   x higher likelihood – give x   maximize p ( |  ) [ ml estimate ] • bayesian inference – must define prior p (  ) – posterior distribution p ( |x )  p ( x| ) p (  )  allow infer “ derive value ”  9 

1 [ sound ] lecture continue discussion probabilistic topic model 
probabilistic topic model mining one topic text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 lecture be go continue discuss probabilistic model be go talk simple case interested mining one topic one document 
simplest case topic model mining one topic output {  } input = { } v p ( w| ) text datum  text mining association database … query … doc 100 % 3 simple setup interested analyze one document try discover one topic simplest case topic model input longer k number topic know one topic collection one document also output also longer coverage assume document cover topic 100 % main goal discover world probability single topic show 
language model setup • datum document = x1 x2 … | xi = { w1 … wm } word • model unigram lm  ( topic ) { i=p ( wi  ) } i=1 … 1+…+m=1 • likelihood function p ( |  )  p ( |  )   p ( x |  )  p ( w1 |  ) c ( w1 )   p ( w |  ) c ( w )   p ( w |  ) 1 • ml estimate c ( w )   i c ( w ) 1 ( ˆ 1 ˆ )  arg max 1 m p ( |  )  arg max 1 m  ci ( wi ) 1 4 always think used generate model solve problem start think kind datum go model perspective be go model datum datum representation be go design specific model generate datum perspective perspective mean want take particular angle look datum model right parameter discover knowledge want will think microfunction write microfunction capture formally likely datum point obtain model likelihood function parameter function argue interest estimate parameter example maximize likelihood lead maximum likelihood estimate estimator parameter become output mining hour mean will take estimate parameter knowledge discover text let s look step simple case later will look procedure complicate case datum case document sequence word word denote x sub i model unigram language model word distribution hope denote topic s goal many parameter many word vocabulary case m convenience be go use theta sub denote probability word w sub i obviously theta sub s sum likelihood function look like well probability generate whole document give model assume independence generate word probability document product probability word since word might repeat occurrence also rewrite product different form line rewrite formula product unique word vocabulary w sub 1 w sub m different previous line well product different position word document transformation would need introduce counter function denote count word one document similarly count word n document word might repeat occurrence also see word occur document zero count therefore corresponding term disappear useful form writing likelihood function often use later want pay attention get familiar notation s change product different word vocabulary end course will use theta sub express likelihood function would look like next be go find theta value probability word would maximize likelihood function let take look maximum likelihood estimate problem closely line copy previous slide s likelihood function goal maximize likelihood function find often easy maximize local likelihood instead original likelihood purely mathematical convenience logarithm transformation function become sum instead product also constraint probability sum make easier take derivative often need find optimal solution function please take look sum form function often see later also general topic model s sum word vocabulary inside sum count word document macroed logarithm probability let s see solve problem point problem purely mathematical problem go find optimal solution constrain maximization problem objective function likelihood function constraint probability must sum one 
computation maximum likelihood estimate maximize p ( d| ) ( ˆ 1 ˆ )  arg max 1 m p ( |  )  arg max 1 m  ci ( wi ) 1 max log-likelihood ( ˆ 1 ˆ )  arg max 1 m log [ p ( |  ) ]  arg max 1 m  c ( w ) log i 1 subject constraint  1 1 use lagrange multiplier approach i=1 i=1 lagrange function f ( q | ) = å c ( wi ) logq + l ( åq 1 ) ¶f ( q | ) c ( wi ) = l 0 ¶q qi åi=1 c ( wi ) l ® q = n c ( wi ) l = 1 ® l = å c ( wi ) ® qˆ = p ( wi | qˆ ) = i=1 normalize count c ( wi ) å c ( w ) i=1 = c ( wi ) d | one way solve problem use lagrange multiplier approace command beyond scope course since lagrange multiplier useful approach also would like give brief introduction interested approach construct lagrange function function combine objective function another term encode constraint introduce lagrange multiplier lambda s additional parameter idea approach turn constraint optimization sense unconstrained optimize problem interested optimize lagrange function may recall calculus optimal point would achieve derivative set zero necessary condition s sufficient though see partial derivative respect theta equal part come derivative logarithm function lambda simply take set zero easily see theta sub related lambda way since know theta s must sum one plug constraint allow us solve lambda net sum count allow us solve optimization problem eventually find optimal set theta sub i look formula turn s actually intuitive normalize count word document ns also sum count word document mess obtain something s intuitive intuition want maximize datum assign much probability mass possible observed word might also notice general result maximum likelihood raise estimator general estimator would normalize count s sometimes count do particular way also see later basically analytical solution optimization problem general though likelihood function complicate be go able solve optimization problem close form formula instead use numerical algorithms 
topic look like p ( |  ) text mining paper 0031 0018 … text 004 mining 0035 association 003 cluster 0005 computer 00009 … food 0000001 … get rid common word 6 be go see case later also imagine would get use maximum likelihood estimator estimate one topic single document let s imagine document text mining paper might see something look like top see high probability word tend common word often functional word english follow content word really characterize topic well like text mining etc end also see probability word really related topic might extraneously mentioned document topic representation see ideal right high probability word functional word really characterize topic question get rid common word topic next module be go talk use probabilistic model somehow get rid common word [ music ] 

1 
probabilistic topic model mixture unigram lm text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
factor background word p ( |  ) text mining paper 0031 0018 … text 004 mining 0035 association 003 cluster 0005 computer 00009 … food 0000001 … get rid common word 3 
generate used two word distribution topic d p ( | d ) text mining paper p ( | b ) background ( topic ) b text 004 mining 0035 association 003 cluster 0005 … 0000001 003 002 0015 001 food 0003 … text 0000006 … p ( d ) + ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 4 
’ probability observe word w topic d “ ” “ text ” text 004 mining 0035 p ( d ) + ( b ) 1 003 p ( “ ” ) p ( d ) p ( “ ” association ) + p (  ) p ( “ ” |  b ) b 0005 cluster p ( | d ) = 05*0000001+05*003 p ( d ) 05 … 0000001 topic 003 p ( “ text ” ) p (  ) p ( “ text ”  ) + p ( b ) p ( “ text ” | b ) choice p ( |  b ) 002 = 05*004+05*0000006 p ( b ) 05 0015 001 food 0003 … text 0000006 background ( topic ) b … 5 
idea mixture model mixture model “ ” “ text ” w text 004  mining 0035 association 003 cluster 0005 … 0000001 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) + ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 6 
generative model… w formally define follow generative model p ( w ) p ( d ) p ( w|d ) + p ( b ) p ( | b ) estimate model “ discover ” two topic + topic coverage p ( d ) 1 p ( b ) 1 7 
mixture two unigram language model • datum document • mixture model parameter = ( { p ( w|d ) } { p ( w|b ) } p ( b ) p ( d ) ) – two unigram lm d ( topic ) b ( background topic ) – mix weight ( topic choice ) p ( d ) p ( b ) 1 • likelihood function p ( |  )  i 1 p ( x |  )  i 1 [ p ( d ) p ( x | d )  p ( b ) p ( x | b ) ] | |  i 1 [ p ( d ) p ( w | d )  p ( b ) p ( w | b ) ] c ( w ) • ml estimate *  arg max  p ( |  ) subject i1 p ( w | d ) i1 p ( w | b ) 1 p ( d )  p ( b )  1 8 

1 
probabilistic topic model mixture model estimation text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
back factor background word text mining paper … text mining is… clustering… we… text 004  mining 0035 association 003 cluster 0005 … 0000001 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) + ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 3 
estimation one topic p ( | d ) adjust dto maximize p ( d| ) ( parameter know ) would ml estimate demote background word d … text mining is… clustering… we… text d mining association cluster … 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) + ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 4 
behavior mixture model = text likelihood p ( “ text ” ) p ( d ) p ( “ text ” d ) + p ( b ) p ( “ text ” | b ) = 05*p ( “ text ” d ) 05*01 text d p ( d ) 05 p ( “ ” ) = 05*p ( “ ” d ) 05*09 p ( d| ) p ( “ text ”  ) p ( “ ”  ) = [ 05*p ( “ text ” d ) + 05*01 ] x [ 05*p ( “ ” d ) + 05*09 ] p ( b ) 05 09 text 01 b set p ( “ text ” d ) & p ( “ text ” d ) maximize note p ( “ text ” d ) + p ( “ ” d ) 1 5 
“ collaboration ” “ competition ” d b p ( d| ) p ( “ text ”  ) p ( “ ”  ) = [ 05*p ( “ text ” d ) + 05*01 ] x [ 05*p ( “ ” d ) + 05*09 ] = note p ( “ text ” d ) + p ( “ ” d ) 1 text text d p ( d ) 05 𝒙 + 𝒚 = 𝒄𝒐𝒏𝒔𝒕𝒂𝒏𝒕 𝒙𝒚 reach maximum 𝒙 = 𝒚 p ( b ) 05 05*p ( “ text ” d ) + = 05*p ( “ ” d ) + 05*09  p ( “ text ” d ) 09 > > p ( “ ” d ) 01 09 text 01 b behavior 1 p ( w1|b ) > p ( w2|b ) p ( w1|d ) < p ( w2|d ) 6 
response datum frequency = text p ( d| ) = [ 05*p ( “ text ” d ) + 05*01 ] x [ 05*p ( “ ” d ) + 05*09 ]  p ( “ text ” d ) 09 > > ’ = text …the p ( “ ” d ) 01 p ( ’  ) = [ 05*p ( “ text ” d ) + 05*01 ] x [ 05*p ( “ ” d ) + 05*09 ] x [ 05*p ( “ ” d ) + 05*09 ] x [ 05*p ( “ ” d ) + 05*09 ] increase p ( b ) … x [ 05*p ( “ ” d ) + 05*09 ] ’ optimal solution p ( “ ” d ) > 01 p ( “ ” d ) < 01 behavior 2 high frequency word get higher p ( w|d ) 7 
summary • general behavior mixture model – every component model attempt assign high probability highly frequent word datum ( “ collaboratively maximize likelihood ” ) – different component model tend “ bet ” high probability different word ( avoid “ competition ” “ waste probability ” ) – probability choose component “ regulate ” competition component model • fix one component background word distribution ( ie background language model ) – help “ get rid background word ” component – example impose prior model parameter ( prior = one model must exactly background lm ) 8 

1 
probabilistic topic model mixture model estimation text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
back factor background word text mining paper … text mining is… clustering… we… text 004  mining 0035 association 003 cluster 0005 … 0000001 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) + ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 3 
estimation one topic p ( | d ) adjust dto maximize p ( d| ) ( parameter know ) would ml estimate demote background word d … text mining is… clustering… we… text d mining association cluster … 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) + ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 4 
behavior mixture model = text likelihood p ( “ text ” ) p ( d ) p ( “ text ” d ) + p ( b ) p ( “ text ” | b ) = 05*p ( “ text ” d ) 05*01 text d p ( d ) 05 p ( “ ” ) = 05*p ( “ ” d ) 05*09 p ( d| ) p ( “ text ”  ) p ( “ ”  ) = [ 05*p ( “ text ” d ) + 05*01 ] x [ 05*p ( “ ” d ) + 05*09 ] p ( b ) 05 09 text 01 b set p ( “ text ” d ) & p ( “ text ” d ) maximize note p ( “ text ” d ) + p ( “ ” d ) 1 5 
“ collaboration ” “ competition ” d b p ( d| ) p ( “ text ”  ) p ( “ ”  ) = [ 05*p ( “ text ” d ) + 05*01 ] x [ 05*p ( “ ” d ) + 05*09 ] = note p ( “ text ” d ) + p ( “ ” d ) 1 text text d p ( d ) 05 𝒙 + 𝒚 = 𝒄𝒐𝒏𝒔𝒕𝒂𝒏𝒕 𝒙𝒚 reach maximum 𝒙 = 𝒚 p ( b ) 05 05*p ( “ text ” d ) + = 05*p ( “ ” d ) + 05*09  p ( “ text ” d ) 09 > > p ( “ ” d ) 01 09 text 01 b behavior 1 p ( w1|b ) > p ( w2|b ) p ( w1|d ) < p ( w2|d ) 6 
response datum frequency = text p ( d| ) = [ 05*p ( “ text ” d ) + 05*01 ] x [ 05*p ( “ ” d ) + 05*09 ]  p ( “ text ” d ) 09 > > ’ = text …the p ( “ ” d ) 01 p ( ’  ) = [ 05*p ( “ text ” d ) + 05*01 ] x [ 05*p ( “ ” d ) + 05*09 ] x [ 05*p ( “ ” d ) + 05*09 ] x [ 05*p ( “ ” d ) + 05*09 ] increase p ( b ) … x [ 05*p ( “ ” d ) + 05*09 ] ’ optimal solution p ( “ ” d ) > 01 p ( “ ” d ) < 01 behavior 2 high frequency word get higher p ( w|d ) 7 
summary • general behavior mixture model – every component model attempt assign high probability highly frequent word datum ( “ collaboratively maximize likelihood ” ) – different component model tend “ bet ” high probability different word ( avoid “ competition ” “ waste probability ” ) – probability choose component “ regulate ” competition component model • fix one component background word distribution ( ie background language model ) – help “ get rid background word ” component – example impose prior model parameter ( prior = one model must exactly background lm ) 8 

university illinois urbana-champaign 
probabilistic topic model expectation-maximization ( em ) algorithm text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
estimation one topic p ( | d ) set dto maximize p ( d| ) ( parameter know ) … text mining is… clustering… we… text d mining association cluster … 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) + ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 3 
know word distribution… c ( w ) p ( w | d )  w v c ( w ) ’ … text mining is… clustering… we… p ( | b ) text d mining association cluster … 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) + ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 4 
give parameter infer distribution word from… “ text ” likely d b d ( z=0 ) p ( d ) p ( “ text ” d ) b ( z=1 ) p ( b ) p ( “ text ” b ) text 004 d mining 0035 association 003 p ( | d ) cluster 0005 … 0000001 p ( | b ) p ( z  0 | w  text )  p ( d ) p ( ` text | d ) p ( d ) p ( ` text | d )  p (  b ) p ( ` text |  b ) 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) p ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 5 
expectation-maximization ( em ) algorithm hide variable z  { 0 1 } z 1 paper 1 present 1 1 text 0 mining 0 algorithm 0 1 cluster 0 initialize p ( w|d ) random value iteratively improve used e-step & m-step stop likelihood ’ change p ( d ) p ( n ) ( w | d ) p ( z  0 | w )  p ( d ) p ( n ) ( w | d )  p ( b ) p ( w | b ) ( n ) e-step likely w d p ( n 1 ) c ( w ) p ( n ) ( z  0 | w ) ( w | d )  ( n ) c ( w ) p ( z  0 | w ) w v m-step 6 
em computation action p ( d ) p ( n ) ( w | d ) p ( z  0 | w )  e-step p ( d ) p ( n ) ( w | d )  p ( b ) p ( w | b ) ( n ) assume c ( w ) p ( z  0 | w ) ( n 1 ) p ( d ) p ( b ) = 05 p ( w |  )  m-step ( n ) w v c ( w ) p ( z  0 | w ) p ( w|b ) know ( n ) word # p ( w|b ) 4 05 paper 2 03 text 4 01 mining 2 01 log-likelihood iteration 1 p ( z=0|w ) p ( w| ) 033 025 045 025 071 025 071 025 1696 likelihood increase iteration 2 p ( z=0|w ) p ( w| ) 029 020 032 014 081 044 069 022 1613 iteration 3 p ( z=0|w ) p ( w| ) 026 018 025 010 093 050 069 022 1602 “ product ” also useful 7 
em hill-climb  converge local maximum likelihood p ( |  ) e-step = compute lower bound original likelihood lower bound likelihood function next guess p ( n 1 ) ( w | d ) current guess m-step = maximize lower bound p ( n ) ( w | d )  8 
summary • expectation-maximization ( em ) algorithm – general algorithm compute ml estimate mixture model – hill-climb converge local maximum ( depend initial point ) • e-step “ augment ” datum predict value useful hide variable • m-step exploit “ augment datum ” improve estimate parameter ( “ improve ” guarantee term likelihood ) • “ datum augmentation ” probabilistic  split count event probabilistically 9 

university illinois urbana-champaign 
probabilistic topic model expectation-maximization ( em ) algorithm text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
estimation one topic p ( | d ) set dto maximize p ( d| ) ( parameter know ) … text mining is… clustering… we… text d mining association cluster … 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) + ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 3 
know word distribution… c ( w ) p ( w | d )  w v c ( w ) ’ … text mining is… clustering… we… p ( | b ) text d mining association cluster … 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) + ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 4 
give parameter infer distribution word from… “ text ” likely d b d ( z=0 ) p ( d ) p ( “ text ” d ) b ( z=1 ) p ( b ) p ( “ text ” b ) text 004 d mining 0035 association 003 p ( | d ) cluster 0005 … 0000001 p ( | b ) p ( z  0 | w  text )  p ( d ) p ( ` text | d ) p ( d ) p ( ` text | d )  p (  b ) p ( ` text |  b ) 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) p ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 5 
expectation-maximization ( em ) algorithm hide variable z  { 0 1 } z 1 paper 1 present 1 1 text 0 mining 0 algorithm 0 1 cluster 0 initialize p ( w|d ) random value iteratively improve used e-step & m-step stop likelihood ’ change p ( d ) p ( n ) ( w | d ) p ( z  0 | w )  p ( d ) p ( n ) ( w | d )  p ( b ) p ( w | b ) ( n ) e-step likely w d p ( n 1 ) c ( w ) p ( n ) ( z  0 | w ) ( w | d )  ( n ) c ( w ) p ( z  0 | w ) w v m-step 6 
em computation action p ( d ) p ( n ) ( w | d ) p ( z  0 | w )  e-step p ( d ) p ( n ) ( w | d )  p ( b ) p ( w | b ) ( n ) assume c ( w ) p ( z  0 | w ) ( n 1 ) p ( d ) p ( b ) = 05 p ( w |  )  m-step ( n ) w v c ( w ) p ( z  0 | w ) p ( w|b ) know ( n ) word # p ( w|b ) 4 05 paper 2 03 text 4 01 mining 2 01 log-likelihood iteration 1 p ( z=0|w ) p ( w| ) 033 025 045 025 071 025 071 025 1696 likelihood increase iteration 2 p ( z=0|w ) p ( w| ) 029 020 032 014 081 044 069 022 1613 iteration 3 p ( z=0|w ) p ( w| ) 026 018 025 010 093 050 069 022 1602 “ product ” also useful 7 
em hill-climb  converge local maximum likelihood p ( |  ) e-step = compute lower bound original likelihood lower bound likelihood function next guess p ( n 1 ) ( w | d ) current guess m-step = maximize lower bound p ( n ) ( w | d )  8 
summary • expectation-maximization ( em ) algorithm – general algorithm compute ml estimate mixture model – hill-climb converge local maximum ( depend initial point ) • e-step “ augment ” datum predict value useful hide variable • m-step exploit “ augment datum ” improve estimate parameter ( “ improve ” guarantee term likelihood ) • “ datum augmentation ” probabilistic  split count event probabilistically 9 

university illinois urbana-champaign 
probabilistic topic model expectation-maximization ( em ) algorithm text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
estimation one topic p ( | d ) set dto maximize p ( d| ) ( parameter know ) … text mining is… clustering… we… text d mining association cluster … 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) + ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 3 
know word distribution… c ( w ) p ( w | d )  w v c ( w ) ’ … text mining is… clustering… we… p ( | b ) text d mining association cluster … 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) + ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 4 
give parameter infer distribution word from… “ text ” likely d b d ( z=0 ) p ( d ) p ( “ text ” d ) b ( z=1 ) p ( b ) p ( “ text ” b ) text 004 d mining 0035 association 003 p ( | d ) cluster 0005 … 0000001 p ( | b ) p ( z  0 | w  text )  p ( d ) p ( ` text | d ) p ( d ) p ( ` text | d )  p (  b ) p ( ` text |  b ) 003 b 002 0015 001 food 0003 … text 0000006 p ( d ) p ( b ) 1 p ( d ) 05 topic choice p ( b ) 05 5 
expectation-maximization ( em ) algorithm hide variable z  { 0 1 } z 1 paper 1 present 1 1 text 0 mining 0 algorithm 0 1 cluster 0 initialize p ( w|d ) random value iteratively improve used e-step & m-step stop likelihood ’ change p ( d ) p ( n ) ( w | d ) p ( z  0 | w )  p ( d ) p ( n ) ( w | d )  p ( b ) p ( w | b ) ( n ) e-step likely w d p ( n 1 ) c ( w ) p ( n ) ( z  0 | w ) ( w | d )  ( n ) c ( w ) p ( z  0 | w ) w v m-step 6 
em computation action p ( d ) p ( n ) ( w | d ) p ( z  0 | w )  e-step p ( d ) p ( n ) ( w | d )  p ( b ) p ( w | b ) ( n ) assume c ( w ) p ( z  0 | w ) ( n 1 ) p ( d ) p ( b ) = 05 p ( w |  )  m-step ( n ) w v c ( w ) p ( z  0 | w ) p ( w|b ) know ( n ) word # p ( w|b ) 4 05 paper 2 03 text 4 01 mining 2 01 log-likelihood iteration 1 p ( z=0|w ) p ( w| ) 033 025 045 025 071 025 071 025 1696 likelihood increase iteration 2 p ( z=0|w ) p ( w| ) 029 020 032 014 081 044 069 022 1613 iteration 3 p ( z=0|w ) p ( w| ) 026 018 025 010 093 050 069 022 1602 “ product ” also useful 7 
em hill-climb  converge local maximum likelihood p ( |  ) e-step = compute lower bound original likelihood lower bound likelihood function next guess p ( n 1 ) ( w | d ) current guess m-step = maximize lower bound p ( n ) ( w | d )  8 
summary • expectation-maximization ( em ) algorithm – general algorithm compute ml estimate mixture model – hill-climb converge local maximum ( depend initial point ) • e-step “ augment ” datum predict value useful hide variable • m-step exploit “ augment datum ” improve estimate parameter ( “ improve ” guarantee term likelihood ) • “ datum augmentation ” probabilistic  split count event probabilistically 9 

1 
probabilistic latent semantic analysis ( plsa ) text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
document sample mixed topic topic 1 topic 2 … topic k government 03 response 02 city 02 new 01 orlean 005 donate 01 relief 005 help 002 background b 004 003 blog article “ hurricane katrina ” [ criticism government response hurricane primarily consist criticism response approach storm aftermath specifically delay response ] [ flood new orlean … 80 % 13 million resident greater new orlean metropolitan area evacuate ] … [ seventy country pledge monetary donation assistance ] … many application possible “ decode ” topic text… 3 
mining multiple topic text input c k v output { 1 … k } { i1 … ik } doc 1 text datum 1 2 … sport 002 game 001 basketball 0005 football 0004 … travel 005 attraction 003 trip 001 … science 004 003 k scientist spaceship 0006 … 30 % 11 doc 2 21=0 % … doc n n1=0 % 12 % 12 22 n2 8 % 1k 2k nk 4 
generate text multiple topic p ( w ) =  k ( 1-b ) p ( 1 ) p ( w|1 ) p ( w|1 ) topic 1 p ( 1 ) d1 topic 2 city 02 new 01 orlean 005 p ( 2 ) d2 + ( 1-b ) p ( 2 ) p ( w|2 ) w + … + ( - b ) p ( k ) p ( w|k ) p ( w|2 ) … p ( w|k ) topic k + b p ( w|b ) p ( w|b ) 1 government 03 response 02 background b donate 01 relief 005 help 002 004 003   1 … p ( k ) d k - b topic choice p ( b ) = b 5 
probabilistic latent semantic analysis ( plsa ) percentage background word ( know ) coverage topic  j doc background lm ( know ) prob word w topic  j k pd ( w )  b p ( w |  b )  ( 1  b )   j p ( w |  j ) j 1 log p ( )   c ( w ) log [  wv log p ( c |  )  k b p ( w |  b )  ( 1  b )   j p ( w |  j ) ]   c ( w ) log [  c wv j 1 k b p ( w |  b )  ( 1  b )   j p ( w |  j ) ] j 1 unknown parameter = ( { d j } {  j } ) j=1 … k many unknown parameter total 6 
ml parameter estimation k pd ( w )  b p ( w |  b )  ( 1  b )   j p ( w |  j ) j 1 log p ( )   c ( w ) log [  wv log p ( c |  )  k b p ( w |  b )  ( 1  b )   j p ( w |  j ) ] j 1   c ( w ) log [  c wv constrain optimization k b p ( w |  b )  ( 1  b )   j p ( w |  j ) ]   arg max  p ( c |  ) * j  [ 1 k ] i 1 p ( w |  j )  1 j 1 d  c  j1 d j  1 k 7 
em algorithm plsa e-step hide variable ( topic indicator ) zd w  { b 1 2 … k } probability w doc generate topic  j p ( z w  j )  p ( z w  b )  use baye rule  ( dn j ) p ( n ) ( w |  j ) ( n ) ( n )   j1 j p ( w |  j ) k  b p ( w |  b )  b p ( w | b )  ( 1   b )  j1  ( dn j ) p ( n ) ( w |  j ) k probability w doc generate background  b 8 
em algorithm plsa m-step hide variable ( topic indicator ) zd w  { b 1 2 … k } ml estimate base “ allocate ” word count topic  j re-estimate probability doc cover topic  j  ( n 1 ) j    wv j p ( n 1 ) c ( w ) ( 1  p ( z w  b ) ) p ( z w  j ) wv c ( w ) ( 1  p ( z w  b ) ) p ( z w  j ) c ( w ) ( 1  p ( z  b ) ) p ( z  j )  ( w |  )    c ( w ) ( 1  p ( z  b ) ) p ( z  j ) dc w w j w v dc re-estimate probability word w topic  j w w 9 
computation em algorithm • initialize unknown parameter randomly • repeat likelihood converge k ( n ) ( n ) – e-step p ( z w  j )  d j p ( w |  j )  j1 p ( z w  j )  1 p ( z w  b )   b p ( w | b ) – m-step  ( n 1 ) j  wv c ( w ) ( 1  p ( z w  b ) ) p ( z w  j ) ’ normalizer one p ( n 1 ) ( w |  j )  dc c ( w ) ( 1  p ( z w  b ) ) p ( z w  j ) d  c  j1 d j  1 k j  [ 1 k ]  p ( w |  j )  1 wv general accumulate count normalize 10 
summary • plsa = mixture model k unigram lm ( k topic ) • add pre-determined background lm help discover discriminative topic • ml estimate “ discover ” topical knowledge text datum – k word distribution ( k topic ) – proportion topic document • output enable many application – cluster term docs ( treat topic cluster ) – associate topic different context ( eg time period location author source etc ) 11 

1 [ sound ] 
probabilistic latent semantic analysis ( plsa ) text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 compute maximum estimate used em algorithm e step introduce hide variable topic hide variable z topic indicator take two value specifically take k plus one value b note background locate denote k topic right e step recall augment datum predict value hide variable be go predict word whether word come one k plus one distribution 
document sample mixed topic topic 1 topic 2 … topic k government 03 response 02 city 02 new 01 orlean 005 donate 01 relief 005 help 002 background b 004 003 blog article “ hurricane katrina ” [ criticism government response hurricane primarily consist criticism response approach storm aftermath specifically delay response ] [ flood new orlean … 80 % 13 million resident greater new orlean metropolitan area evacuate ] … [ seventy country pledge monetary donation assistance ] … many application possible “ decode ” topic text… 3 equation allow us predict probability word w document generate topic zero sub j bottom one predict probability word generate background note use document index word whether word particular topic actually depend document see well s pi s pi s tie document document potentially different pi s right pi s affect prediction pi s depend document might give different guess word different document s desirable 
mining multiple topic text input c k v output { 1 … k } { i1 … ik } doc 1 text datum 1 2 … sport 002 game 001 basketball 0005 football 0004 … travel 005 attraction 003 trip 001 … science 004 003 k scientist spaceship 0006 … 30 % 11 doc 2 21=0 % … doc n n1=0 % 12 % 12 22 n2 8 % 1k 2k nk 4 case used baye s rule explain basically assess likelihood generate word division s normalize 
generate text multiple topic p ( w ) =  k ( 1-b ) p ( 1 ) p ( w|1 ) p ( w|1 ) topic 1 p ( 1 ) d1 topic 2 city 02 new 01 orlean 005 p ( 2 ) d2 + ( 1-b ) p ( 2 ) p ( w|2 ) w + … + ( - b ) p ( k ) p ( w|k ) p ( w|2 ) … p ( w|k ) topic k + b p ( w|b ) p ( w|b ) 1 government 03 response 02 background b donate 01 relief 005 help 002 004 003   1 … p ( k ) d k - b topic choice p ( b ) = b 5 step well may recall step take advantage infer z value split count collect right count re-estimate parameter 
probabilistic latent semantic analysis ( plsa ) percentage background word ( know ) coverage topic  j doc background lm ( know ) prob word w topic  j k pd ( w )  b p ( w |  b )  ( 1  b )   j p ( w |  j ) j 1 log p ( )   c ( w ) log [  wv log p ( c |  )  k b p ( w |  b )  ( 1  b )   j p ( w |  j ) ]   c ( w ) log [  c wv j 1 k b p ( w |  b )  ( 1  b )   j p ( w |  j ) ] j 1 unknown parameter = ( { d j } {  j } ) j=1 … k many unknown parameter total 6 case re-estimate coverage probability re-estimate base collect word document s count word document sum word be go look extent word belong 
ml parameter estimation k pd ( w )  b p ( w |  b )  ( 1  b )   j p ( w |  j ) j 1 log p ( )   c ( w ) log [  wv log p ( c |  )  k b p ( w |  b )  ( 1  b )   j p ( w |  j ) ] j 1   c ( w ) log [  c wv constrain optimization k b p ( w |  b )  ( 1  b )   j p ( w |  j ) ]   arg max  p ( c |  ) * j  [ 1 k ] i 1 p ( w |  j )  1 j 1 d  c  j1 d j  1 k 7 topic theta sub j part guess step tell us likely word actually theta sub j multiply together get discount count be locate topic theta sub j normalize topic get distribution topic indicate coverage similarly bottom one estimate probability word topic case used exact count see discount account ] tell us extend allocate word [ inaudible ] normalization different case interested word distribution simply normalize word different contrast normalize amount topic would useful take comparison two give us different distribution tell us improve parameter 
em algorithm plsa e-step hide variable ( topic indicator ) zd w  { b 1 2 … k } probability w doc generate topic  j p ( z w  j )  p ( z w  b )  use baye rule  ( dn j ) p ( n ) ( w |  j ) ( n ) ( n )   j1 j p ( w |  j ) k  b p ( w |  b )  b p ( w | b )  ( 1   b )  j1  ( dn j ) p ( n ) ( w |  j ) k probability w doc generate background  b 8 explain formula maximum estimate base allocate word count [ inaudible ] phenomena actually general phenomena em algorithms m-step general computer expect account event base e-step result count four particular normalize typically 
em algorithm plsa m-step hide variable ( topic indicator ) zd w  { b 1 2 … k } ml estimate base “ allocate ” word count topic  j re-estimate probability doc cover topic  j  ( n 1 ) j    wv j p ( n 1 ) c ( w ) ( 1  p ( z w  b ) ) p ( z w  j ) wv c ( w ) ( 1  p ( z w  b ) ) p ( z w  j ) c ( w ) ( 1  p ( z  b ) ) p ( z  j )  ( w |  )    c ( w ) ( 1  p ( z  b ) ) p ( z  j ) dc w w j w v dc re-estimate probability word w topic  j w w 9 term computation em algorithm actually keep accounting various event normalize think way also concise way present em algorithm actually help us better understand formula be go go detail algorithm first initialize unknown perimeter randomly right case interested coverage perimeter pi s award distribution [ inaudible ] randomly normalize initialization step repeat likelihood converge know whether likelihood converge compute likelihood step compare current likelihood previous likelihood nt change much be go say stop right step be go e-step m-step e-step be go augment datum predict hide variable case hide variable z sub w indicate whether word w topic background s topic topic look e-step formula essentially be actually normalize count sorry probability observe word distribution see basically prediction word topic zero sub j base probability select theta sub j word distribution generate word 
computation em algorithm • initialize unknown parameter randomly • repeat likelihood converge k ( n ) ( n ) – e-step p ( z w  j )  d j p ( w |  j )  j1 p ( z w  j )  1 p ( z w  b )   b p ( w | b ) – m-step  ( n 1 ) j  wv c ( w ) ( 1  p ( z w  b ) ) p ( z w  j ) ’ normalizer one p ( n 1 ) ( w |  j )  dc c ( w ) ( 1  p ( z w  b ) ) p ( z w  j ) d  c  j1 d j  1 k j  [ 1 k ]  p ( w |  j )  1 wv general accumulate count normalize 10 multiply probability observe word distribution say s proportional implementation em algorithm keep counter quantity end normalizes normalization topic would get probability m-step go collect allocate account topic split word among topic be go normalize different way obtain real estimate example normalize among topic get re-estimate pi coverage re-normalize base word would give us word distribution s useful think algorithm way implement use variable keep track quantity case normalize variable make distribution put constraint one intentionally leave exercise see s normalizer one s slightly different form s essentially one see one general envision em algorithms see accumulate count various count normalize 
summary • plsa = mixture model k unigram lm ( k topic ) • add pre-determined background lm help discover discriminative topic • ml estimate “ discover ” topical knowledge text datum – k word distribution ( k topic ) – proportion topic document • output enable many application – cluster term docs ( treat topic cluster ) – associate topic different context ( eg time period location author source etc ) 11 summarize introduce plsa model mixture model k unigram language model represent k topic also add pre-determined background language model help discover discriminative topic background language model help attract common term select maximum estimate cant discover topical knowledge text datum case plsa allow us discover two thing one k word distribution one represent topic proportion topic document detailed characterization coverage topic document enable lot photo analysis example aggregate document particular pan period assess coverage particular topic time period would allow us generate temporal chain topic also aggregate topic cover document associate particular author categorize topic written author etc addition also cluster term cluster document fact topic regard cluster already term cluster higher probability word regard belong one cluster represent topic similarly document cluster way assign document topic cluster s cover document remember pi s indicate extent topic cover document assign document topical cluster highest pi general many useful application technique [ music ] 

1 
latent dirichlet allocation ( lda ) text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
extension plsa • plsa prior knowledge  user-controlled plsa • plsa generative model  latent dirichlet allocation 3 
plsa prior knowledge • user may expectation topic analyze – expect see “ retrieval model ” topic ir – want see aspect “ battery ” “ memory ” opinion laptop • user may knowledge topic ( ) cover document – tag = topic  doc generate used topic corresponding tag assign document • incorporate knowledge prior plsa model 4 
maximum posteriori ( map ) estimate *  arg max p (  ) p ( datum |  )  • may use p (  ) encode kind preference constraint eg – p (  ) > 0 one topic precisely “ background ” p ( w|b ) – p (  ) > 0 particular doc d3=0 2 – p (  ) favor  topic assign high probability particular word • map estimate ( conjugate prior ) compute used similar em algorithm ml estimate smooth reflect prior preference 5 
em algorithm conjugate prior p ( | i ) p ( zd w  j )  p ( zd w    wv j p  k j 1  ( n ) j p ( n ) prior p ( w| ’ j ) ( w |  j ) battery 05 life 05 b p ( w |  b )  b )  k b p ( w |  b )  ( 1  b )  j 1  ( n ) j p ( n ) ( w |  j )  ( n j 1 )  ( n 1 )  ( n ) j p ( n ) ( w |  j ) ( w |  j c ( w ) ( 1  p ( z w  b ) ) p ( z w  j ) wv pseudo count w prior  ’ c ( w ) ( 1  p ( z w  b ) ) p ( z w  j )  c ( w ) ( 1  p ( z  b ) ) p ( z  )    c ( w ) ( 1  p ( z  b ) ) p ( z c w v c w w w =0 =+ j ) p ( w| ’ j ) w  j )  sum pseudo count may also set parameter constant ( include 0 ) need 6 
deficiency plsa • generative model – ’ compute probability new document – heuristic workaround possible though • many parameter  high complexity model – many local maxima – prone overfitting • necessarily problem text mining ( interested fitting “ training ” document ) 7 
latent dirichlet allocation ( lda ) • make plsa generative model impose dirichlet prior model parameter  – lda = bayesian version plsa – parameter regularize • achieve goal plsa text mining purpose – topic coverage topic word distribution infer used bayesian inference 8 8 
plsa  lda p ( w|1 ) p ( w|2 ) w topic 1 government 03 response 02 topic 2 city 02 new 01 orlean 005 … p ( w|k ) topic k  i  ( p ( w1 | i ) p ( w | i ) ) donate 01 relief 005 help 002 word distribution topic choice free plsa p ( 1 ) d1 p ( 2 ) d2 …   p ( d )  dirichlet (  )    ( 1  k ) i  0 p ( k ) d k   p ( i )  dirichlet (  )    ( 1  ) i  0  d  ( d 1 d k ) lda impose prior 9 
likelihood function plsa vs lda plsa k pd ( w | {  j } {  j } )    j p ( w |  j ) j 1 log p ( | {  j } {  j } )  log p ( c | {  j } {  j } )  lda pd ( w | {  j } {  j } )   log p ( |  {  j } )    log p ( c |   )  k  c ( w ) log [   wv j 1  log p ( | {  j c j p ( w |  j ) ] core assumption topic model } {  j } ) plsa component k   j p ( w |  j ) j 1 k   c ( w ) log [   wv j 1 j    p ( w |  j ) ] p (  |  ) d k   log p ( |  {  } ) p (  |   j ) d1 d k j  c j 1 add lda 10 
parameter estimation inference lda • parameter estimate used ml estimator     ˆ ( ˆ  )  argmax log p ( c |   )    many parameter lda vs plsa • however { j } { d j } must compute used posterior inference – computationally intractable – must resort approximate inference – many different inference method available 11 
summary probabilistic topic model • probabilistic topic model provide general principled way mining analyze topic text many application • basic task setup – input text datum – output k topic + proportion topic cover document • plsa basic topic model often adequate application • lda improve plsa impose prior – theoretically appeal – practically lda plsa perform similarly many task 12 
suggest reading • blei d 2012 “ probabilistic topic ” communication acm 55 ( 4 ) 77–84 doi 21338062133826 • qiaozhu mei xuehua shen chengxiang zhai “ automatic labele multinomial topic ” proceedings acm kdd 2007 pp 490-499 12811921281246 • yue lu qiaozhu mei chengxiang zhai investigate task performance probabilistic topic model empirical study plsa lda information retrieval 14 2 ( april 2011 ) s10791-010-9141-9 13 

1 
latent dirichlet allocation ( lda ) text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
extension plsa • plsa prior knowledge  user-controlled plsa • plsa generative model  latent dirichlet allocation 3 
plsa prior knowledge • user may expectation topic analyze – expect see “ retrieval model ” topic ir – want see aspect “ battery ” “ memory ” opinion laptop • user may knowledge topic ( ) cover document – tag = topic  doc generate used topic corresponding tag assign document • incorporate knowledge prior plsa model 4 
maximum posteriori ( map ) estimate *  arg max p (  ) p ( datum |  )  • may use p (  ) encode kind preference constraint eg – p (  ) > 0 one topic precisely “ background ” p ( w|b ) – p (  ) > 0 particular doc d3=0 2 – p (  ) favor  topic assign high probability particular word • map estimate ( conjugate prior ) compute used similar em algorithm ml estimate smooth reflect prior preference 5 
em algorithm conjugate prior p ( | i ) p ( zd w  j )  p ( zd w    wv j p  k j 1  ( n ) j p ( n ) prior p ( w| ’ j ) ( w |  j ) battery 05 life 05 b p ( w |  b )  b )  k b p ( w |  b )  ( 1  b )  j 1  ( n ) j p ( n ) ( w |  j )  ( n j 1 )  ( n 1 )  ( n ) j p ( n ) ( w |  j ) ( w |  j c ( w ) ( 1  p ( z w  b ) ) p ( z w  j ) wv pseudo count w prior  ’ c ( w ) ( 1  p ( z w  b ) ) p ( z w  j )  c ( w ) ( 1  p ( z  b ) ) p ( z  )    c ( w ) ( 1  p ( z  b ) ) p ( z c w v c w w w =0 =+ j ) p ( w| ’ j ) w  j )  sum pseudo count may also set parameter constant ( include 0 ) need 6 
deficiency plsa • generative model – ’ compute probability new document – heuristic workaround possible though • many parameter  high complexity model – many local maxima – prone overfitting • necessarily problem text mining ( interested fitting “ training ” document ) 7 
latent dirichlet allocation ( lda ) • make plsa generative model impose dirichlet prior model parameter  – lda = bayesian version plsa – parameter regularize • achieve goal plsa text mining purpose – topic coverage topic word distribution infer used bayesian inference 8 8 
plsa  lda p ( w|1 ) p ( w|2 ) w topic 1 government 03 response 02 topic 2 city 02 new 01 orlean 005 … p ( w|k ) topic k  i  ( p ( w1 | i ) p ( w | i ) ) donate 01 relief 005 help 002 word distribution topic choice free plsa p ( 1 ) d1 p ( 2 ) d2 …   p ( d )  dirichlet (  )    ( 1  k ) i  0 p ( k ) d k   p ( i )  dirichlet (  )    ( 1  ) i  0  d  ( d 1 d k ) lda impose prior 9 
likelihood function plsa vs lda plsa k pd ( w | {  j } {  j } )    j p ( w |  j ) j 1 log p ( | {  j } {  j } )  log p ( c | {  j } {  j } )  lda pd ( w | {  j } {  j } )   log p ( |  {  j } )    log p ( c |   )  k  c ( w ) log [   wv j 1  log p ( | {  j c j p ( w |  j ) ] core assumption topic model } {  j } ) plsa component k   j p ( w |  j ) j 1 k   c ( w ) log [   wv j 1 j    p ( w |  j ) ] p (  |  ) d k   log p ( |  {  } ) p (  |   j ) d1 d k j  c j 1 add lda 10 
parameter estimation inference lda • parameter estimate used ml estimator     ˆ ( ˆ  )  argmax log p ( c |   )    many parameter lda vs plsa • however { j } { d j } must compute used posterior inference – computationally intractable – must resort approximate inference – many different inference method available 11 
summary probabilistic topic model • probabilistic topic model provide general principled way mining analyze topic text many application • basic task setup – input text datum – output k topic + proportion topic cover document • plsa basic topic model often adequate application • lda improve plsa impose prior – theoretically appeal – practically lda plsa perform similarly many task 12 
suggest reading • blei d 2012 “ probabilistic topic ” communication acm 55 ( 4 ) 77–84 doi 21338062133826 • qiaozhu mei xuehua shen chengxiang zhai “ automatic labele multinomial topic ” proceedings acm kdd 2007 pp 490-499 12811921281246 • yue lu qiaozhu mei chengxiang zhai investigate task performance probabilistic topic model empirical study plsa lda information retrieval 14 2 ( april 2011 ) s10791-010-9141-9 13 

1 [ sound ] lecture first one text cluster 
text cluster motivation text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 lecture go talk text cluster important technique topic mining analysis particular lecture be go start basic question cluster 
overview • text cluster • text cluster • text cluster lecture – generative probabilistic model – approach • evaluate cluster result 3 text cluster interested text cluster follow lecture go talk text cluster evaluate cluster result text cluster well cluster actually general technique datum mining might learn course idea discover natural structure datum another word want group similar object together case object course text object example document term passage sentence website will go group similar text object together 
text cluster • • • • discover “ natural structure ” group similar object together object document term passage website … example well defined “ similar ” mean 4 let s see example well nt really see text object used shape denote object group together ask natural structure natural group look might agree group object base chip location two dimensional space get three cluster case may much agreement three cluster really depend perspective look object maybe also see thing different way might get different cluster will see another example ambiguity clearly main point problem actually well defined problem lie define similarity mean similar object 
“ cluster bias ” • two object similar depend look basis evaluation • “ car ” “ horse ” similar • user must define perspective ( ie “ bias ” ) assess similarity 5 problem clearly defined order well defined cluster problem problem general two object similar depend look example keep two word like car horse two word similar well depend look physical property car horse different look functionally car horse transportation tool sense may similar see really depend perspective look object ought make cluster problem well defined user must define perspective assess similarity call perspective cluster bias define cluster problem s important specify perspective similarity define similarity used group similar object otherwise similarity well defined one different way group object let s look example see object shape similar see first slide ask group object might feel previous slide example might think well steer group ship would give us cluster look like however might also feel well maybe object group base size would give us different way cluster datum look size look similarity size see clearly depend perspective will get different cluster result also clearly tell us order evaluate cluster without must use perspective without perspective s hard define best cluster result 
example text cluster • cluster document whole collection • term cluster define “ concept ” “ theme ” “ topic ” • cluster sentence select text segment larger text object ( eg text segment topic discover used topic model ) • cluster website ( text object multiple document ) • text cluster cluster generate hierarchy 6 many example text cluster setup example cluster document whole text collection case document unit cluster may able cluster term case term object cluster term used define concept theme topic fact s topic model see previous lecture give cluster term sense take term high probability word distribution another example cluster text segment example passage sentence segment extract former larger text object example might extract order text segment topic let s say used topic model have get text object cluster segment have get discover interesting cluster might also ripple subtopic case combine text cluster technique general see lot text mining accurate combine flexible way achieve goal sophisticated mining analysis text datum also cluster fairly large text object mean text object may contain lot document example might cluster website website actually compose multiple document similarly also cluster article written author example trigger article publish also one unit cluster way might group author together base whether be publish paper similar text cluster cluster generate hierarchy s general cluster text object different level 
text cluster • general useful text mining exploratory text analysis get sense overall content collection ( eg “ typical ” representative document collection ) link ( similar ) text object ( eg remove duplicate content ) create structure text datum ( eg browse ) as way induce additional feature ( ie cluster ) classification text object • example application – cluster search result – understand major complaint email customer 7 generally text cluster interesting well s s useful technique text mining particularly exploratory text analysis typical scenario get lot text datum let s say email message customer time period literature article etc hope get sense overall content connection example might interested get sense major topic typical representative document connection cluster help us achieve goal sometimes also want link similar text object together object might duplicate content example case technique help us remove redundancy remove duplicate document sometimes topic link together complete coverage topic may also used text cluster create structure text datum sometimes create hierarchy structure useful problem may also use text cluster induce additional feature represent text datum cluster document together treat cluster feature say document cluster feature value would one document cluster feature value zero help provide additional discrimination might used text classification discuss later general many application text cluster thought two specific one one cluster search result example [ inaudible ] search engine cluster result user see overall structure result return fall query query s ambiguous particularly useful cluster likely represent different sense ambiguous word another application understand major complaint customer base email right case cluster email message find major cluster understand major complaint [ music ] 

1 [ sound ] lecture generate probabilistic model text cluster 
text cluster generative probabilistic model ( part 1 ) text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 lecture be go continue discuss text cluster be go introduce generate probabilistic model 
overview • text cluster • text cluster • text cluster – generative probabilistic model – similarity-based approach • evaluate cluster result 3 way text cluster overall plan cover text cluster previous lecture talk text cluster text cluster interesting lecture be go talk text cluster general see slide two kind approach one generate probabilistic model topic lecture later will also discuss similarity-based approach 
topic mining revisit input c k v output { 1 … k } { i1 … ik } doc 1 text datum 1 2 … sport 002 game 001 basketball 0005 football 0004 … travel 005 attraction 003 trip 001 … science 004 003 k scientist spaceship 0006 … 30 % 11 doc 2 21=0 % … doc n n1=0 % 12 % 12 22 n2 8 % 1k 2k nk 4 talk generate model text cluster would useful revisit topic mining problem used topic model two problem similar slide see earlier lecture topic model show input text collection c number topic k vocabulary v hope generate output two thing one set topic denote theta s award distribution pi j probability document cover topic topic coverage s also visualize slide see get used topic model main difference text cluster problem document assume possibly cover multiple topic indeed general document cover one topic nonzero probability text cluster however allow document cover one topic assume one topic cluster 
one topic ( cluster ) per document input c k v output { 1 … k } { c1 … cn } ci  [ 1 k ] doc 1 text datum 1 sport 002 game 001 basketball 0005 football 0004 … 11=100 % 005 2 travel attraction 003 trip 001 … 12=0 science 004 003 k scientist spaceship 0006 … 1k=0 … doc 2 … 21=0 % 22=100 % 1k=0 doc n n1=100 % n2=0 nk=0 5 mean change problem definition slightly assume document generate used precisely one topic will definition cluster problem will hear output change longer detailed coverage distribution pi j instead be go cluster assignment decision ci ci decision document i c sub go take value 1 k indicate one k cluster basically tell us cluster illustrated longer multiple topic cover document precisely one topic although topic still uncertain 
mining one topic revisit output {  } input = { } v p ( w| ) text datum  ( 1 doc 1 topic ) text mining association database … query … doc 100 %  ( n docs n topic ) k < n  ( n docs k share topic ) cluster 6 also connection problem mining one topic discuss earlier s slide see hope estimate topic model distribution base precisely one document s assume document cover precisely one topic also consider variation problem example consider n document cover different topic s n document topic course case document independent topic also independent allow document share topic also assume go assume fewer topic number document document must share topic n document share k topic will precisely document cluster problem connection naturally think use probabilistically generative model solve problem text cluster 
generative model cluster input c k v output { 1 … k } { c1 … cn } ci  [ 1 k ] doc 1 text datum 1 sport 002 game 001 basketball 0005 football 0004 … 005 2 travel attraction 003 trip 001 … … 11=100 % 12=0 doc 2 … 21=0 % 22=100 % doc n n1=100 % n2=0 force every document generate science 004 ( instead k topic ) used one topic scientist k spaceship003 nk=0 1k=0 1k=0 0006 … 7 question generative model used cluster case design generative model hope generative model would adopt output hope generate structure hope model case cluster structure topic document cover one topic hope emb preference generative model think main difference problem topic model talk earlier see main requirement force every document generate precisely one topic instead k topic topic model 
generative topic model revisit ’ model used cluster text 004  mining 0035 1 association 003 cluster 0005 … “ ” 0000001 “ text ” w 003 2 002 0015 001 food 0003 … text 0000006 p ( 1 ) p ( 2 ) 1 p ( 1 ) 05 topic choice p ( 2 ) 05 8 let s revisit topic model detail detailed view two component mixture model k component look similar see generate document generate word independent generate word first make choice distribution decide use one probability p theta 1 probability choose distribution top first make decision regard distribution used generate word be go use distribution sample word note generative model decision distribution use word independent mean example can generate second distribution theta 2 whereas text likely generate first one top mean word document can generate general multiple distribution want say text cluster document cluster hope document generate precisely one topic mean need modify model well let s first think model used cluster say reason allow multiple topic contribute word document cause confusion be go know cluster document s importantly s violate assumption partition document cluster really one topic correspond one cluster document would document generate precisely one topic mean word document must generate precisely one distribution true topic model be see s used cluster ensure one distribution used generate word one document realize problem naturally design alternative mixture model cluster be see make decision regard distribution use generate document document can potentially generate k word distribution time make decision choose one topic 
mixture model document cluster difference topic model l p ( | 1 ) d=x1 x2 … xl p ( 1 ) 1 p ( 2 ) 1 l text 004  mining 0035 1 association 003 cluster 0005 … 0000001 003 2 002 p ( | 2 ) 0015 001 food 0003 … text 0000006 p ( 1 ) p ( 2 ) 1 p ( 1 ) 05 topic choice p ( 2 ) 05 9 be go stay regime generate word document mean make choice distribution generate first word be go go stay distribution generate word document word make choice basically make decision document state generate word similarly choose second distribution theta sub 2 see state one generate entire document d compare picture previous one see decision used particular distribution make document case document cluster case topic model make many decision number word document word make potentially different decision s key difference two model obviously also mixed model group together one box show model give us probability document inside model also switch choose different distribution nt observe s mixture model course main problem document cluster infer distribution used generate document would allow us recover cluster identity document useful think difference topic model also mentioned multiple time mainly two difference one choice used particular distribution make document cluster whereas topic model s make multiple time different word second word distribution go used regenerate word document case one distribution nt generate word document multiple distribution can used generate word document let s also think special case one probability choose particular distribution equal mean uncertainty stick one particular distribution case clearly see longer mixture model s uncertainty use precisely one distribution generate document be go back case estimate one order distribution base one document s connection discuss earlier see clearly case used generative model solve problem first look datum think design model design model next step write likelihood function be go look estimate parameter case s likelihood function s go similar see topic model also different still recall likelihood function look like realize general probability observe datum point mixture model go sum possibility generate datum case s go sum k topic every one user generate document inside sum still recall formula look like s go product two probability one probability choose distribution probability observe particular datapoint distribution 
likelihood function p ( ) = p ( )  p ( 1 ) p ( | 1 )  p ( 2 ) p ( | 2 )  p ( 1 ) i 1 p ( x | 1 ) p ( 2 ) i 1 p ( x | 2 ) l d=x1 x2 … xl l different topic model topic mod el p ( )  i 1 [ p ( 1 ) p ( x | 1 )  p ( 2 ) p ( x | 2 ) ] l 10 map kind formula problem see probability observe document basically sum case two different distribution simplified situation two cluster case see s sum two case case s indeed probability choose distribution either theta 1 theta probability multiply probability observe document particular distribution expand probability observe whole document see product observe word x sub i make assumption word generate independently probability whole document product probability word document form similar topic model s also useful think difference purpose also copy probability topic model two component see formula look similar many way similar also difference particular difference top see mixture model document cluster first take product take sum s corresponding assumption first make choice choose one distribution stay distribution will generate word s product inside sum sum correspond choice topic model see sum actually inside product s generate word independently s product outside generate word make decision regard distribution use sum word general mixture model estimate model used algorithm discuss later [ music ] 

1 [ sound ] lecture continue discussion generative probabilistic model text cluster 
text cluster generative probabilistic model ( part 2 ) text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 lecture go continue talk text cluster particularly generative probabilistic model 
likelihood function p ( ) = p ( )  p ( 1 ) p ( | 1 )  p ( 2 ) p ( | 2 )  p ( 1 ) i 1 p ( x | 1 ) p ( 2 ) i 1 p ( x | 2 ) l l d=x1 x2 … xl generalize include k cluster 3 slide see earlier written likelihood function document two distribution two component mixed model document cluster lecture be go generalize include k cluster look formula think question generalize will realize need add term like see add theta probability theta probability generate theta 
mixture model document cluster • datum collection document = { d1 … dn } • model mixture k unigram lm = ( { i } { p ( i ) } ) i [ 1 k ] – generate document first choose i accord p ( i ) generate word document used p ( w|i ) • likelihood p ( |  )  i 1 [ p ( i )  j1 p ( x j i ) ] k |  i 1 [ p ( i )  wv p ( w i ) c ( w ) ] k • maximum likelihood estimate *  arg max  p ( |  ) 4 precisely go use general presentation mixture model document cluster case would follow step used generate model first think datum case datum collection document end document denote sub talk model think model case design mixture k unigram language model s little bit different topic model similar parameter set theta s denote distribution corresponding k unigram language model p theta probability select k distribution generate document note although goal find cluster actually used general notion probability cluster see later allow us assign document cluster highest probability able generate document result also recover interesting property see later model basically would make follow assumption generation document first choose theta accord probability theta generate word document used distribution note s important use distribution word document different topic model likelihood function would like see take look formula used different notation second line equation go see notation change use unique word vocabulary product instead particular position document x subject w change notation change allow us show estimation formula easily see change also topic model presentation s basically still product probability word likelihood function talk parameter estimation simply use maximum likelihood estimator s standard way thing familiar 
cluster allocation parameter estimation • parameter mixture model = ( { i } { p ( i ) } ) i [ 1 k ] – i represent content cluster p ( | i ) – p ( i ) indicate size cluster – note unlike plsa p ( i ) ’ depend • cluster document belong = – likelihood assign cluster corresponding topic i likely used generate cd  arg max p ( | i ) – likelihood + prior p ( i ) ( bayesian ) favor large cluster cd  arg max p ( | i ) p ( i ) 5 s different model estimate parameter allocate cluster document well let s take look situation closely repeat parameter mixture model think get estimate model actually get information need cluster right theta example represent content cluster actually by-product help us summarize cluster look top term cluster word distribution tell us cluster p theta interpreted indicate size cluster tell us likely cluster would used generate document likely cluster used generate document assume larger cluster size note unlike plsa probability theta dependent d may recall topic choose document actually depend d mean document potentially different choice topic generic choice probability document course even particular document still infer topic likely generate document sense still document dependent probability cluster let s look key problem assign document cluster assign cluster document s computer c sub take one value range one k indicate cluster assign d first might think way use likelihood assign cluster corresponding topic theta likely used generate d mean be go choose one distribution give highest probability word see distribution content match [ inaudible ] intuitively make sense however approach consider size cluster also available us better way use likelihood together prior case prior p theta i together be go use base formula compute posterior probability theta give d choose theta base posterior probability would follow formula see bottom slide case be go choose theta large p theta mean large cluster also high probability generate d be go favor cluster s large also consistent document intuitively make sense chance document large cluster generally higher small cluster mean estimate parameter model easily solve problem document cluster next will discuss actually compute estimate model [ music ] 

1 
text cluster generative probabilistic model ( part 3 ) text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
compute ml estimate • datum collection document = { d1 … dn } • model mixture k unigram lm = ( { i } { p ( i ) } ) i [ 1 k ] – generate document first choose i accord p ( i ) generate word document used p ( w|i ) • likelihood p ( |  )  i 1 [ p ( i )  wv p ( w i ) c ( w ) ] k p ( c |  )   j1 p ( j |  ) n • maximum likelihood estimate *  arg max  p ( c |  ) 3 
em algorithm document cluster • initialization randomly set = ( { i } { p ( i ) } ) i [ 1 k ] • repeat likelihood p ( c| ) converge – e-step infer distribution used generate document hide variable zd  [ 1 k ] p ( n ) ( zd  | )  p ( n ) ( i )  wv p ( n ) ( w i ) c ( w )  k ( n ) p ( zd  | )  1 1 – m-step re-estimation parameter p ( n 1 ) ( i )   j1 p ( zd j  | j ) n ( n ) p ( n 1 ) ( w | i )   j1 c ( w j ) p ( n ) ( zd j  1 | j ) n ( n 1 ) p i1 ( i )  1 k ( n 1 ) p wv ( w | i )  1 i  [ 1 k ] 4 
example 2 cluster e-step random initialization p ( 1 ) p ( 2 ) = 05 p ( w|1 ) p ( w|2 ) text 05 01 mining 02 01 medical 02 075 health 01 005 c ( w ) document hide variable zd  { 1 2 } text 2 mining 2 medical 0 health 0 p ( 1 ) p ( ` text | 1 ) 2 p ( ` min ing | 1 ) 2 p ( z  1 | )  p ( 1 ) p ( ` text | 1 ) 2 p ( ` min ing | 1 ) 2  p ( 2 ) p ( ` text | 2 ) 2 p ( ` min ing | 2 ) 2 05 * 052 * 022 100   05 * 052 * 022  05 * 012 * 012 101 p ( z  2 | )  5 
normalization avoid underflow p ( w|1 ) p ( w|2 ) text 05 01 ( 05+01 ) 2 mining 02 01 ( 02+01 ) 2 medical 02 075 ( 02+075 ) 2 health 01 005 ( 01+005 ) 2 p ( w |  ) average p ( w|i ) possible normalizer p ( 1 ) p ( ` text | 1 ) 2 p ( ` min ing | 1 ) 2 p ( ` text |  ) 2 p ( ` min ing |  ) 2 p ( z  1 | )  p ( 1 ) p ( ` text | 1 ) 2 p ( ` min ing | 1 ) 2 p ( 2 ) p ( ` text | 2 ) 2 p ( ` min ing | 2 ) 2  p ( ` text |  ) 2 p ( ` min ing |  ) 2 p ( ` text |  ) 2 p ( ` min ing |  ) 2 6 
example 2 cluster ( cont ) e-step p ( zd=1|d ) c ( “ text ” ) d1 09 d1 2 c ( “ mining ” ) 3 d2 01 d2 1 2 d3 08 d3 4 3 p ( w|1 ) p ( w|2 ) text mining medical health m-step p ( 1 )   p ( 1 ) = p ( 2 ) = p ( zd1  1 | d1 )  p ( zd 2  1 | 2 )  p ( zd3  1 | 3 ) 3 09  01  08  06 3 p ( ` text | 1 )  c ( ` text d1 ) * p ( zd1  1 | d1 )   c ( ` text 3 ) * p ( zd3  1 | 3 )  2 * 09  1 * 01  4 * 08 p ( ` min ing | 1 )  3 * 09  2 * 01  3 * 08 p ( ` text | 1 )  p ( ` min ing | 1 )  p ( ` medical | 1 )  p ( ` health | 1 )  1 7 
summary generative model cluster • slight variation topic model used cluster document – cluster represent unigram lm p ( w|i )  term cluster – document generate first choose unigram lm generate word document used single lm – estimate model parameter give topic characterization cluster probabilistic assignment document cluster – “ hard ” cluster obtain force document cluster corresponding unigram lm likely used generate document • em algorithm used compute ml estimate – normalization often need avoid underflow 8 

1 
overview • text cluster • text cluster • text cluster – generative probabilistic model – similarity-based approach • evaluate cluster result 2 
similarity-based cluster general idea • explicitly define similarity function measure similarity two text object ( ie provide “ cluster bias ” ) • find optimal partition datum – maximize intra-group similarity – minimize inter-group similarity • two strategy obtain optimal cluster – progressively construct hierarchy cluster ( hierarchical cluster ) • bottom-up ( agglomerative ) gradually group similar object larger cluster • top-down ( divisive ) gradually partition datum smaller cluster – start initial tentative cluster iteratively improve ( “ flat ” cluster eg k-mean ) 3 
similarity-based cluster method • many general cluster method available • two representative method – hierarchical agglomerative cluster ( hac ) – k-mean 4 
agglomerative hierarchical cluster • give similarity function measure similarity two object • gradually group similar object together bottom-up fashion form hierarchy • stop stopping criterion meet • variation different way compute group similarity base individual object similarity 5 
similarity-induce structure 6 
compute group similarity three popular method give two group g1 g2  single-link algorithm ( g1 g2 ) = similarity closest pair  complete-link algorithm ( g1 g2 ) = similarity farthest pair  average-link algorithm ( g1 g2 ) = average similarity pair 7 
group similarity illustrated complete-link algorithm g2 g1 …… single-link algorithm average-link algorithm 8 
comparison single-link complete-link average-link • single-link – “ loose ” cluster – individual decision sensitive outlier • complete-link – “ tight ” cluster – individual decision sensitive outlier • average-link – “ ” – group decision insensitive outlier • one best depend need 9 
k-mean cluster • represent text object term vector assume similarity function defined two object • start k randomly select vector assume centroid k cluster ( initial tentative cluster )  initialization • assign every vector cluster whose centroid closest vector  e-step difference • re-compute centroid cluster base newly assign  m-step difference vector cluster • repeat process similarity-based objective function ( ie within cluster sum square ) converge ( local minimum ) similar cluster em mixture model 10 
summary cluster method • model base approach ( mixture model ) – – – – – used implicit similarity function ( model  cluster bias ) cluster structure “ build ” generative model complex generative model discover complex structure prior leverage customize cluster algorithm however easy way directly control similarity measure • similarity-based approach – allow direct flexible specification similarity – objective function optimized always clear • approach generate term cluster doc cluster 11 

1 [ music ] 
overview • text cluster • text cluster • text cluster – generative probabilistic model – similarity-based approach • evaluate cluster result 2 lecture evaluation text cluster far talk multiple way text cluster know method work best evaluation 
“ cluster bias ” • two object similar depend look • user must define perspective ( ie “ bias ” ) assess similarity basis evaluation 3 talk evaluation one must go back cluster bias introduce begin two object similar depend look must clearly specify perspective similarity without problem cluster well defined perspective also important evaluation look slide see two different way cluster shape ask question one best one better actually see s way answer question without know whether will like cluster base shape cluster base size s precisely perspective cluster bias crucial evaluation 
direct evaluation text cluster • question answer close system-generate cluster ideal cluster ( generate human ) – “ closeness ” assessed multiple perspective – “ closeness ” quantify – “ cluster bias ” impose human assessor • evaluation procedure – give test set human create ideal cluster result ( ie ideal partition text object “ gold standard ” ) – use system produce cluster test set – quantify similarity system-generate cluster gold standard cluster – similarity measure multiple perspective ( eg purity normalize mutual information f measure ) 4 general evaluate text cluster two way one direct evaluation indirect evaluation direct evaluation want answer follow question close system-generate cluster ideal cluster generate human closeness assessed multiple perspective help us characterize quality cluster result multiple angle sometimes desirable also want quantify closeness would allow us easily compare different measure base performance figure finally see case essentially inject cluster bias used human basically human would bring need desire cluster bias exactly well general procedure would look like give test set consist lot text object human create ideal cluster result be go ask human partition object create gold standard use judgment base need particular application generate think best cluster result would used compare system generate cluster test set ideally want system result human generate result general go would like quantify similarity system-generate cluster gold standard cluster similarity also measure multiple perspective give us various mesh quantitatively evaluate cluster cluster result commonly used measure include purity measure whether cluster similar object cluster gold standard normalize mutual information commonly used measure basically measure base identity cluster object system generally well predict cluster object gold standard vice versa mutual information capture correlation cluster label normalize mutual information often used quantify similarity evaluation purpose f measure another possible measure thorough discussion evaluation evaluation issue would beyond scope course have suggest read end take look know want discuss high level idea would allow think evaluation application second way evaluate text cluster indirect evaluation case question answer useful cluster result intend application course application specific question usefulness go depend specific application case cluster bias impose independent application well count best cluster result would dependent application 
indirect evaluation text cluster • question answer useful cluster result intend application – “ usefulness ” inevitably application specific – “ cluster bias ” impose intend application • evaluation procedure – create test set intend application quantify performance system application – choose baseline system compare – add cluster algorithm baseline system  “ cluster system ” – compare performance cluster system baseline term performance measure application 5 procedure wise also would create test set text object intend application quantify performance system case care contribution cluster application often baseline system compare can current system something hope add cluster improve baseline system can used different cluster method try experiment hope better idea word cluster case baseline system work add cluster algorithm baseline system produce cluster system compare performance cluster system baseline system term performance measure particular application case call indirect evaluation cluster s explicit assessment quality cluster rather s assess contribution cluster particular application 
summary text cluster • text cluster unsupervised general text mining technique – obtain overall picture text content ( explore text datum ) – discover interesting cluster structure text datum • many approach possible – strong cluster tend show matter method used – effectiveness method highly depend whether desire cluster bias capture appropriately ( either used right generative model right similarity function ) – decide optimal number cluster generally difficult problem method due unsupervised nature • evaluation cluster result do directly indirectly 6 summarize text cluster 
suggest read • man chris d prabhakar raghavan hinrich schütze introduction information retrieval cambridge cambridge university press 2007 ( chapter 16 ) 7 s useful unsupervised general text mining technique s particularly useful obtain overall picture text content often need explore text datum often first step deal lot text datum second application second kind application discover interesting cluster structure text datum structure meaningful many approach used form text cluster discuss model base approach narrative base approach general strong cluster tend show matter method used also effectiveness method highly depend whether desire cluster bias capture appropriately do either used right generate model model design appropriate cluster right similarity function expressly define bias decide optimal number customer difficult problem order cluster method be s unsupervised algorithm s training guide us select best number cluster sometimes may see method automatically determine number cluster general imply application cluster bias s specify without clearly define cluster bias s impossible say optimal number cluster important keep mind also say sometimes also use application determine number cluster example be cluster search result obviously nt want generate 100 cluster number dictate interface design situation might able use fitness datum assess whether have get good number cluster explain datum well vary number cluster watch well fit datum general add component mixed model fit datum better nt always set probability used new component zero ca nt general fit datum worse question add component would able significantly improve fitness datum used determine right number cluster finally evaluation cluster result kind do directly indirectly often would like order get good sense well method work s suggest read particularly useful better understand match calculate cluster general [ music ] 

1 [ sound ] lecture text categorization 
text categorization text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 lecture be go talk text categorization important technique text datum mining analytic relevant discovery various different kind knowledge show first s related topic mining analysis s analyze text datum base predefined topic secondly s also related opinion mining sentiment analysis discovery knowledge observer human sensor categorize author example base content article written right general categorize observer base content produce finally s also related text-based prediction often use text categorization technique predict variable real world remotely related text datum important technique text datum mining 
overview • text categorization • text categorization • text categorization lecture – generative probabilistic model – discriminative approach • evaluate categorization result 3 overall plan cover topic first be go talk text categorization be interested lecture be go talk text categorization evaluate categorization result 
text categorization • give follow – set predefined category possibly form hierarchy often – training set labele text object • task classify text object one category text object categorization system training datum ( know category ) sport business education categorization result sport business education … … science 4 problem text categorization defined follow be give set predefined category possibly form hierarchy often also set training example training set labele text object mean text object already enabled know category task classify text object one predefined category picture slide show happen text categorization lot text object processed categorization system system general assign category document show right categorization result often assume availability training example document tag know category example important help system learn pattern different category would help system know recognize category new text object see 
example text categorization • text object vary ( eg document passage collection text ) • category also vary – “ internal ” category characterize text object ( eg topical category sentiment category ) – “ external ” category characterize entity associate text object ( eg author attribution meaningful category associate text datum ) • example application – – – – – news categorization literature article categorization ( eg mesh annotation ) spam email filter sentiment categorization product reviews tweet automatic email rout author attribution 5 specific example text categorization fact many example first text object vary categorize document passage sentence collection text case cluster unit analyze vary lot create lot possibility secondly category also vary allocate general s two major kind category one internal category category categorize content text object example topic category sentiment category generally content text object throughout categorization content kind external category characterize entity associate text object example author entity associate content produce use content determine author written part example s call author attribution mininal category associate text datum long minimal connection entity text datum example might collect lot reviews restaurant lot reviews product text datum help us infer property product restaurant case treat categorization problem categorize restaurant categorize product base corresponding reviews example external category specific example application news categorization common start lot news agency would like assign predefined category categorize news generate everyday virtual article categorization important aspect example biomedical domain s mesh annotation mesh stand medical subject head ontology term characterize content literature article detail another example application spam email detection filter right often spam filter help us distinguish spam legitimate email clearly binary classification problem sentiment categorization product reviews tweet yet another kind application categorize compare positive negative positive negative neutral send category assign two text content another application automatic email rout sort might want automatically sort email different folder s one application text categorization folder category result another important kind application rout email right person handle helpdesk email messaging generally rout particular person handle different person tend handle different kind request many case person would manually assign message right person imagine ca nt able automatically text categorization system help rout request class file incoming request one category category actually correspond person handle request finally author attribution mentioned yet another application s another example used text actually infer property entity 
variant problem formulation • binary categorization two category – retrieval { relevant-doc non-relevant-doc } – spam filter { spam non-spam } – opinion { positive negative } • k-category categorization two category – topic categorization { sport science travel business … } – email rout { folder1 folder2 folder3 … } • hierarchical categorization category form hierarchy • joint categorization multiple related categorization task do joint manner binary categorization potentially support categorization 6 also many variant problem formulation first simplest case binary categorization two category many example like information retrieval search engine application one distinguish relevant document non-relevant document particular query spam filter distinguish spam non-spams also two category sometimes classification opinion two category positive negative general case would k-category categorization also many application like can two category topic categorization often example multiple topic email rout would another example may multiple folder route email right person handle multiple person classify case two kind category another variation hierarchical categorization category form hierarchy topical hierarchy common yet another variation joint categorization s multiple categorization task related hope kind join categorization leverage dependency task improve accuracy individual task among binary categorization fundamental part also s simple probably s actually used perform categorization task example k-category categorization task actually perform used binary categorization basically look category separately binary categorization problem whether object category meaning category hierarchical categorization also do progressively flat categorization level first categorize object let s say small number high-level category inside category categorize sub-category etc text categorization important well already show several application general several reason one text categorization help enrich text representation s achieve understand text datum be useful text analysis categorization text represent multiple level keyword condition s often used lot text process task also add category provide two level transition semantic category assign also directly indirectly useful application example semantic category can already useful attribution might directly useful another example semantic category facilitate aggregation text content another case application text categorization example want know overall opinion product can first categorize opinion individual view positive negative would allow us easy aggregate sentiment would tell us 70 % view positive 30 % negative etc without categorization much harder aggregate opinion provide concise way coding text sense base vocabulary sometimes may see application text categorization call text code encode control vocabulary second kind reason use text categorization infer property entity text category allow us infer property entity associate text datum mean use text categorization discover knowledge world 
text categorization • enrich text representation ( understand text ) – text represent multiple level ( keyword + category ) – semantic category assign directly indirectly useful application – semantic category facilitate aggregation text content ( eg aggregate negative opinion product ) • infer property entity associate text datum ( discovery knowledge world ) – long entity associate text datum always use text datum help categorize associate entity – eg discovery non-native speaker language prediction party affiliation base political speech 7 general long associate entity text datum always text datum help categorize corresponding entity s used single information network connect entity text datum obvious entity directly connect author also imagine s affiliation author s age thing actually connect text datum indirectly make connection make prediction value general way allow us use text mining text categorization discover knowledge world useful especially big text datum analytic often used text datum extra set datum extract human infer certain decision factor often together non-textual datum specifically text example also think example infer property entity example discovery non-native speaker language do categorize content speaker another example predict party affiliation politician base political speech example used text datum infer knowledge real world nature problem s defined s text categorization problem [ music ] 

1 
overview • text categorization • text categorization • text categorization – generative probabilistic model – discriminative approach • evaluate categorization result 2 lecture method text categorization lecture be go discuss text categorization first be many method text categorization method idea determine category base rule design carefully reflect domain knowledge category prediction problem example want topic categorization news article say well news article mention word like game sport three time be go say s sport thing like would allow us deterministically decide category document put 
categorization method manual • determine category base rule carefully design reflect domain knowledge categorization problem • work well – category well defined – category easily distinguish base surface feature text ( eg special vocabulary know occur particular category ) – sufficient domain knowledge available suggest many effective rule • problem – labor intensive  ’ scale well – ’ handle uncertainty rule rule may inconsistent  robust 3 • problem alleviat used machine learn strategy would work well follow condition hold first category must well defined allow person clearly decide category base clear rule certainly category half easy distinguish base surface feature text mean official feature like keyword punctuation whatever easily identify text datum example special vocabulary know occur particular category would effective easily use vocabulary padding vocabulary recognize category also sufficient knowledge design word be case effective domain sometimes however general several problem approach first s label intensive require lot manual work obviously ca nt kind categorization problem scratch different problem problem give rule need nt scale well secondly handle uncertainty rule often rule nt 100 % reliable take example look occurrence word text try decide topic s actually hard 100 % correct rule example say well game sport basketball sure s sport one also imagine type article mention cure may exactly sport marginally touch sport main topic can another topic different topic sport s one disadvantage approach finally rule maybe inconsistent would lead robustness specifically sometimes result categorization may different depend rule apply case face uncertainty also decide order apply rule combination result contradictory problem approach turn problem solve alleviate used machine learn 
categorization method “ automatic ” • use human expert – annotate datum set category label  training datum – provide set feature represent text object potentially provide “ clue ” category • use machine learn learn “ soft rule ” categorization training datum – figure feature useful separate different category – optimally combine feature minimize error categorization training datum – trained classifier apply new text object predict likely category ( human expert would assign ) 4 machine learn method automatic still put automatic quotation mark really completely automatic cause still require many work specifically use human expert help two way first human expert must annotate datum cell category label would tell computer document receive category call training datum secondly human expert also need provide set feature represent text object potentially provide clue category need provide basic feature computer look case tax natural choice would word used feature common choice start course sophisticated feature like phrase even part ancient tag even syntax structure human expert provide use machine run learn soft rule categorization training datum soft rule mean be go get decide category assign document s go use used rule deterministic might use something similar say match game sport many time s likely sport be go say exactly sure instead be go use probability weight combine much evidence learn process basically go figure feature useful separate different category s go also figure optimally combine feature minimize error categorization training datum training datum see important s basis learn trained classifier apply new text object predict likely category s simulate prediction human would assign text object human make judgement use machine learn text categorization also 
machine learn text categorization • general setup learn classifier f xy – input x = text object output = category – learn classifier function f xy f ( x ) y y give correct category xx ( “ correct ” base training datum ) • method – rely discriminative feature text object distinguish category – combine multiple feature weight manner – adjust weight feature minimize error training datum • different method tend vary – way measure error training datum ( may optimize different cost function ) – way combine feature ( eg linear vs non-linear ) 5 talk problem general set supervisement set learn classifier map value x map x text object category set category class phi take value x input would generate value output hope output right category x correct course judge base training datum s general goal machine learn problem supervised learn problem give example input output function s go figure function behave like base example try able compute value future x s see general method would rely discriminative feature text object distinguish different category s feature important provide human also combine multiple feature weight map weight optimized minimize error training datum learn process optimization problem objective function often tie error training datum different method tend vary way measure error training datum might optimize different objective function often also call loss function cost function also tend vary way combine feature linear combination example simple often used powerful nonlinear combination nonlinear model might complex training tradeoff well would lead different variation many variation learn method 
generative vs discriminative classifier • generative classifier ( learn datum “ look ” like category ) – attempt model p ( x ) = p ( ) p ( x|y ) compute p ( y|x ) base p ( x|y ) p ( ) used baye rule – objective function likelihood thus indirectly measure training error – eg naïve baye • discriminative classifier ( learn feature separate category ) – attempt model p ( y|x ) directly – objective function directly measure error categorization training datum – eg logistic regression support vector machine ( svm ) k-nearest neighbor ( knn ) 6 general distinguish two kind classifier high level one call generative classifier call discriminative classifier generative classifier try learn datum look like category attempt model joint distribution datum label x factor product distribution label joint probability sorry conditional probability x give s y first model distribution label model datum generate particular label estimate model compute conditional probability label give datum base probability datum give label label distribution used baye rule important thing conditional probability label used directly decide label likely approach objective function actually likelihood model datum generate indirectly capture training error model datum category accurately also classify accurately one example naïve baye classifier case kind approach call discriminative classify classify try learn feature separate category direct attack problem categorization separation class sorry problem discriminative classifier attempt model conditional probability label give datum point directly objective function tend directly measure error categorization training datum example include logistical regression support vector machine k-nearest neighbor cover classifier detail next lecture [ music ] 

1 
overview • text categorization • text categorization • text categorization – generative probabilistic model – discriminative approach • evaluate categorization result 2 
document cluster revisit cluster belong  i used generate d=x1x2…xl xiv cluster ( )  arg max p ( i | )  arg max p ( | i ) p ( i ) p ( w|1 )  arg max  j1 p ( x j | i ) p ( i ) sport 002 game 001 basketball 0005 football 0004 … p ( 1 ) l  arg max  wv p ( w i ) c ( w ) p ( | i ) p ( i ) p ( i | )  p ( ) p ( | i ) p ( i )  k j1 p ( |  j ) p (  j ) p ( i ) p ( w|2 ) travel 005 attraction 003 trip 001 … p ( 2 ) … p ( w|k ) science 004 scientist 003 spaceship 0006 … p ( k ) 3 
text categorization naïve baye classifier d=x1x2…xl xiv i represent category accurately then… make happen category ( )  arg max p ( i | )  arg max p ( | i ) p ( i )  arg max  wv p ( w i ) c ( w ) p ( i ) p ( w|1 ) p ( w|2 ) sport 002 game 001 basketball 0005 football 0004 … travel 005 attraction 003 trip 001 … p ( 1 ) p ( 2 ) … p ( w|k ) science 004 scientist 003 spaceship 0006 … category ( )  arg max log p ( i )  wv c ( w ) log p ( w | i ) p ( k ) 
learn training datum training document know category category 1 category 1 t1  { d11 d12 d1n1 } p ( w|1 ) = category 2 category 2 t2  { 21 22 2 n 2 } p ( w|2 ) = travel attraction trip … p ( 1 ) = p ( 2 ) = … category k tk  { k1 k 2 kn k } sport game basketball football … p ( w|k ) = category k science scientist spaceship … p ( k ) = 
estimate p ( w|i ) p ( i ) training document know category category 1 category 1 t1  { d11 d12 d1n1 } p ( w|1 ) = category 2 category 2 t2  { 21 22 2 n 2 } p ( w|2 ) = travel attraction trip … p ( 1 ) = p ( 2 ) = … category k tk  { k1 k 2 kn k } sport game basketball football … p ( w|k ) = category k science scientist spaceship … p ( k ) = 
naïve baye classifier p ( i ) = p ( w|i ) = category 1 category popular t1  { d11 d12 d1n1 } p ( i )  category 2 t2  { 21 22 2 n 2 } category k tk  { k1 k 2 kn k } p ( w | i )   ni  k ni j1 nj c ( w ij )   w v j1 | ti | ni j1 c ( w ij )  c ( w ti ) word frequent category constraint p ( i ) p ( w|i ) 
smooth naïve baye • smooth – address datum sparseness ( training datum small  zero prob ) – incorporate prior knowledge – achieve discriminative weighting ( ie idf weighting ) • p ( i )   ni   k j1 0 n j  k  p ( w|b ) background lm c ( w )  p ( w |  )  p ( w |  )    c ( w )   ni j1 ij b ni w v j1 ij 0 p ( w|b ) |  
anatomy naïve baye classifier two category 1 2 p ( 1 )  wv p ( w | 1 ) c ( w ) p ( 1 | ) score ( )  log  log p (  2 | ) p ( 2 )  wv p ( w | 2 ) c ( w ) p ( 1 ) p ( w | 1 )  log  wv c ( w ) log p (  2 ) p ( w |  2 ) category bias ( 0 ) ’ depend sum word ( feature { fi } ) weight word ( feature ) i feature value fi=c ( w )  ( f1 f 2 f ) f   generalize score ( )  0  i 1 f ii i   = logistic regression 9 

1 
overview • text categorization • text categorization • text categorization – generative probabilistic model – discriminative approach • evaluate categorization result 2 
anatomy naïve baye classifier two category 1 2 p ( 1 )  wv p ( w | 1 ) c ( w ) p ( 1 | ) score ( )  log  log p (  2 | ) p ( 2 )  wv p ( w | 2 ) c ( w ) p ( 1 ) p ( w | 1 )  log  wv c ( w ) log p (  2 ) p ( w |  2 ) category bias ( 0 ) ’ depend sum word ( feature { xi } ) weight word ( feature ) i feature value xi=c ( w )  ( x1 x 2 x ) x   generalize score ( )  0  i 1 x ii i   = logistic regression 3 
discriminative classifier 1 logistic regression predictor x  ( x1 x 2 x ) x   binary response variable  { 01 } 1 y 0 log modele p ( y|x ) directly category ( )  1 category ( )  2 allow many feature word p ( 1 | ) p (  1 | x ) p (  1 | x )  log  log  0  i 1 x ii p (  2 | ) p (  0 | x ) 1  p (  1 | x ) p (  1 | x )  e e p ( y=1|x ) 0  im1 x ii 0  im1 x ii i   10 1 x 4 
estimation parameter • training datum = { ( xi yi ) } i=12 … | • parameter   ( 0 1  m )  | • conditional likelihood p ( |  )  i1 p (  yi | x  x  ) yi 1 p (  1 | x )  e e yi 0 0  im1 x ii 0  im1 x ii • maximum likelihood estimate 1 p (  0 | x )  1 0  im1 x ii e *    arg max p ( |  ) compute many way ( eg newton ’ method ) 5 1 
discriminative classifier 2 k-nearest neighbor ( k-nn ) • find k example training set similar text object classify ( “ neighbor ” document ) • assign category common neighbor text object ( neighbor vote category ) • improve consider distance neighbor ( closer neighbor influence ) • regard way directly estimate conditional probability label give datum instance ie p ( y|x ) • need similarity function measure similarity two text object 6 
illustration k-nn classifier ( k=1 ) ( k=4 ) 7 
k-nn estimate p ( y|x ) assume p ( i|d ) locally smooth ie ’ region r p ( i|d ) = p ( i|r ) count ’ r category i estimate p ( i|r ) base know category region p ( i | r )  c ( i r ) r | total # docs r 8 

1 
discriminative classifier 3 support vector machine ( svm ) f ( x )  0  x category 1 • consider two category { 1 2 } f ( x )  0  x category 2 • use linear separator f ( x )  0  m x ii i   1 x2 0  1x1  2 x 2  0 0  1x1  2 x 2  0 assume 1 < 0 2 > 0 0  1x1  2 x 2  0 x1 2 
linear separator best x2   0  tx  0 t  0   x  0 x1 3 
best separator = maximize margin notation change w 0b x2 margin bias constant wtx+b=0 margin feature weight  w1     w2  w     wm  feature vector ( eg word count )  x1     x2  x     xm  x1 4 
support vector matter support vector x2 wtx+b=0 support vector x1 5 
linear svm classifier f ( x ) wtx+b parameter w b f ( x )  0  x category 1 f ( x )  0  x category 2 training datum = { ( xi yi ) } i=1 … | xi feature vector yi  { 1 1 } goal 1 correct labele training datum yi=1  wtxi+b 1 yi=-1  wtxi+b  1 goal 2 maximize margin large margin  small wtw constraint i yi ( wtxi+b ) 1 objective minimize  ( w ) wtw optimization problem quadratic programming linear constraint 6 
linear svm soft margin classifier f ( x ) wtx+b > 0 parameter w b add allow training error training datum = { ( xi yi ) } i=1 … | find w b i minimize  ( w ) w w+ci [ 1 | ] i subject i [ 1 | ] yi ( wtxi+b ) 1-i i0 c > 0 parameter control trade-off minimize error maximize margin optimization problem still quadratic programming linear constraint 7 
summary text categorization method • many method available clear winner – require effective feature representation ( need domain knowledge ) – useful combine multiple method particular problem • technique rely supervised machine learn thus apply text categorization problem – human annotate training datum design feature – computer optimize combination feature – good performance require 1 ) effective feature 2 ) plenty training datum – performance generally ( much ) affected effectiveness feature choice specific classifier 8 
summary text categorization method ( cont ) • design effective feature ( application-specific ) – analyze categorization problem exploit domain knowledge – perform error analysis obtain insight – leverage machine learn technique ( eg feature selection dimension reduction deep learn ) • obtain “ enough ” training example – low-quality ( “ pseudo ” ) training example may leverage – exploit unlabeled datum ( used semi-supervised learn technique ) – domain transfer learn ( “ borrow ” training example related problem ) 9 
suggest read man chris d prabhakar raghavan hinrich schütze introduction information retrieval cambridge cambridge university press 2007 ( chapter 13-15 ) 10 

1 [ sound ] lecture evaluation text categorization 
overview • text categorization • text categorization • text categorization – generative probabilistic model – discriminative approach • evaluate categorization result 2 have talk many different method text categorization know method work better particular application know best way solve problem understand know evaluate categorization result 
general evaluation methodology • human create test collection every document tag desire category ( “ ground truth ” ) • generate categorization result used system test collection • compare system categorization decision human-made categorization decision quantify similarity ( equivalently difference ) – higher similarity better result – similarity measure different perspective understand quality result detail ( eg category perform better ) – general different categorization mistake may different cost inevitably depend specific application okay consider cost variation relative comparison method 3 first general thought evaluation general evaluation kind empirical task categorization use methodology develop 1960s information retrieval researcher call cranfield evaluation methodology basic idea human create test correction already know every document tag desire category case search query document retrieve call ground truth ground truth test correction reuse collection test many different system compare different system also turn component system see s go happen basically provide way control experiment compare different method methodology virtually used task involve empirically defined problem case go compare system categorization result categorization ground truth create human be go compare system decision document get category category assign document human want quantify similarity decision equivalently measure difference system output desire ideal output generate human obviously highest similarity better result similarity can measure different way would lead different measure sometimes s desirable also match similarity different perspective better understand result detail example might also interested know category perform better category easy categorize etc general different categorization mistake however different cost specific application area might serious other ideally would like model difference read many paper categorization see nt generally instead use simplified measure s s often okay consider cost variation compare method interested know relative difference method s okay introduce bias long bias already particular method expect effective method perform better less effective one even though measure perfect 
classification accuracy ( percentage correct decision ) c1 d1 d2 d3 c3 … c2 ck ( + ) ( - ) n ( + ) ( - ) n ( + ) ( + ) n ( + ) n ( + ) ( + ) n ( + ) n ( + ) n ( + ) … dn … - human answer ( = correct - incorrect ) n system result ( y=ye n=no ) … classification accuracy = total number correct decision total number decision make  count ( (  ) )  count ( n (  ) ) kn 4 first measure will introduce call classification accuracy basic measure percentage correct decision see category denote c1 ck n document denote d1 n pair category document look situation see system say yes pair basically assign category document denote s system decision similarly look s decision also human assign category document plus sign mean human think assignment correct incorrect s minus will see combination ns yes no minus pluse four combination total two correct s ( + ) n ( - ) also two kind error measure classification accuracy simply count many decision correct normalize total number decision make know total number decision n multiply k number correct decision basically two kind one pluss n minus n put together count convenient measure give us one number characterize performance method higher better course 
problem classification accuracy • decision error serious other – may important get decision right document other – may important get decision right category other – eg spam filter miss legitimate email cost let spam go • problem imbalanced test set – skewer test set 98 % category 1 2 % category 2 – strong baseline put instance category 1  98 % accuracy 5 method also problem first treat decision equally reality decision error serious other example may important get decision right document other maybe important get decision right category other would call detailed evaluation result understand strand different method understand performance method detail per category per document basis one example show clearly decision error different cause spam filter can retrieve two category categorization problem miss legitimate email result one type error let spam come folder another type error two type error clearly different s important miss legitimate email s okay occasionally let spam email come inbox error first miss legitimate email high cost s serious mistake classification error classification accuracy address issue s also another problem imbalance test set imagine s skew test set instance category one 98 % instance category one 2 % category two case simple baseline accurately perform well baseline sign similar put instance major category get us 98 % accuracy case s go appear effective reality obviously good result general use classification accuracy measure want ensure cause balance one equal number instance example class minority category cause tend overlooked evaluation classification accuracy address problem course would like also evaluate result way different way say s beneficial look multiple perspective example look perspective document perspective base document question good decision document general case decision think four 
per-document evaluation c1 d1 d2 d3 c2 c3 … ( + ) ( - ) n ( + ) ( - ) n ( + ) ( + ) n ( + ) n ( + ) ( + ) human ( + ) n ( + ) n ( + ) n ( + ) system ( “ ” ) system ( “ n ” ) true positive false negative tp human ( - ) ck false positive fp fn true negative tn good decision di system say “ yes ” many correct tp precision  tp  fp recall  tp tp  fn doc category 6 combination possibility depend whether system say yes depend whether human say correct incorrect say yes four combination first human system say yes s true positive system say yes s positive system say yes s positive human confirm indeed correct become true positive system say yes human say s incorrect s false positive fp system say human say yes s false negative miss one assignment system human say s also correctly assume s true negative right measure better characterize performance used four number 
per-category evaluation c1 d1 d2 d3 c2 c3 … ( + ) ( - ) n ( + ) ( - ) n ( + ) ( + ) n ( + ) n ( + ) ( + ) human ( + ) n ( + ) n ( + ) n ( - ) system ( “ ” ) system ( “ n ” ) true positive false negative tp human ( - ) ck false positive fp fn true negative tn good decision ci system say “ yes ” many correct tp precision  tp  fp recall  tp tp  fn category assign docs category 7 two popular measure precision recall also propose information retrieval researcher 1960s evaluate search result become standard measure use everywhere system say yes ask question many correct s percent correct decision system say yes s call precision s true positive divide case system say yes positive measure call recall measure whether document category case s divide true positive true positive false negative case human say document category represent category get recall tell us whether system actually indeed assign category document give us detailed view document aggregate later be interested document tell us well document subset might interesting other example allow us analyze error detail well separate document certain characteristic other look error might see pattern kind document long document nt well shock document give insight input method 
combine precision recall f-measure f  1 1  r 2 pr f1  pr 2  2 1 1  2 1 05*p+05*r (  2  1 ) p * r  1  2p  r p p precision r recall  parameter ( often set 1 ) r system say “ ” category-doc pair 8 similarly look per-category evaluation case be go look good decision particular category previous case define precision recall would basically answer question different perspective system say yes many correct mean look category see document assign category indeed category right recall would tell us category actually assign document category s sometimes also useful combine precision recall one measure often do used f measure harmonic mean precision precision recall defined slide s also controlled parameter beta indicate whether precision important recall beta set 1 measure call f1 case take equal weight upon procedure recall f1 often used measure categorization case combine result always think best way combine case nt know thought can combine arithmetic mean right would still give us range value obviously s reason f1 popular s actually useful think difference think will see indeed difference undesirable property arithmatic basically obvious think case system say yes category document pair try compute precision recall case see would happen basically kind measure arithmetic mean go reasonable f1 minus one [ inaudible ] trade two value equal extreme case 0 one letter one f1 low mean would still reasonably high [ music ] 

1 
( macro ) average category c1 d1 d2 d3 … dn c2 c3 … ( + ) ( - ) n ( + ) ( - ) n ( + ) ( + ) n ( + ) n ( + ) ( + ) … ck n ( + ) n ( + ) n ( + ) … aggregate precision p1 p2 p3 … pk overall precision recall r1 r2 r3 … rk overall recall f1 f2 f3 … fk overall f score f-measure 2 
( macro ) average document c1 d1 d2 d3 c2 c3 … ( + ) ( - ) n ( + ) ( - ) n ( + ) ( + ) n ( + ) n ( + ) ( + ) ck n ( + ) n ( + ) n ( + ) … dn precision recall f-measure p1 r1 f1 p2 r2 f2 … … … pn rn fn aggregate overall precision overall recall overall f score 3 
micro-average precision recall c1 d1 d2 d3 c3 … c2 ( + ) ( - ) n ( + ) ( - ) n ( + ) ( + ) n ( + ) n ( + ) ( + ) ck n ( + ) n ( + ) n ( + ) first pool decision compute precision recall … dn … tp precision  tp  fp … system ( “ ” ) system ( “ n ” ) human ( + ) true positive ( tp ) false negative ( fn ) human ( - ) false positive ( fp ) true negative ( tn ) tp recall  tp  fn 4 
sometimes ranking appropriate • categorization result often pass human – editing ( eg correct system mistake news category ) – prioritize task ( eg rout email right person process ) • case evaluate result rank list system give score decision – eg discovery spam email (  rank email “ spam ” category ) – often appropriate frame problem ranking problem instead categorization problem ( eg ranking document search engine ) 5 
summary categorization evaluation • evaluation always important get right • measure must reflect intend use result particular application ( eg spam filter vs news categorization ) – consider result processed ( user ) – ideally associate different cost different decision error • commonly used measure relative comparison different method – accuracy precision recall f score – variation per-document per-category micro vs macro average • sometimes ranking may appropriate 6 
suggest read • man chris d prabhakar raghavan hinrich schütze introduction information retrieval cambridge cambridge university press 2007 ( chapter 13-15 ) • yang yiming evaluation statistical approach text categorization inf retr 1 1-2 ( may 1999 ) a1009982220290 7 

1 [ sound ] lecture opinion mining sentiment analysis 
opinion mining sentiment analysis motivation text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 cover motivation lecture be go start talk mining different kind knowledge namely knowledge observer human generate text datum particular be go talk opinion mining sentiment analysis 
objective vs subjective sensor video datum record output real world perceive ( perspective ) observed world express ( english ) text datum subjective opinion rich mine analyze opinion bury text 3 discuss earlier text datum regard datum generate human subjective sensor contrast device video recorder report s happen real world objective generate viewer datum example main difference test datum datum like video datum rich opinion content tend subjective s generate human actually unique advantage text datum compare datum office great opportunity understand observer mine text datum understand opinion understand person s preference person think something lecture follow lecture mainly mine analyze opinion bury lot text datum 
opinion objective statement factual statement ( prove wrong ) opinion  subjective statement describe person believe think something opinion target opinion holder depend culture background context 4 let s start concept opinion s easy formally define opinion mostly would define opinion subjective statement describe person believe think something highlighted quite word s s worth think little bit word help us better understand s opinion help us define opinion formally always need computation resolve problem opinion mining let s first look key word subjective contrast objective statement factual statement statement prove right wrong key differentiate factor opinion tend easy prove wrong right reflect person think something contrast objective statement usually prove wrong correct example might say computer screen battery s something check s either battery contrast think sentence laptop best battery laptop nice screen statement subjective s hard prove whether s wrong correct opinion subjective statement next let look keyword person indicate opinion holder talk opinion s opinion hold someone notice something target opinion opinion expressed something course believe think imply opinion depend culture background context general person might think different different context person different background may also think different way analysis show multiple element need include order characterize opinion 
opinion representation • basic opinion representation – opinion holder whose opinion – opinion target opinion – opinion content exactly opinion • enrich opinion representation – opinion context situation ( eg time location ) opinion expressed – opinion sentiment opinion tell us opinion holder ’ feel ( eg positive vs negative ) 5 s basic opinion representation like well include least three element right firstly specify s opinion holder whose opinion second must also specify target s opinion third course want opinion content exactly opinion identify get basic understand opinion already useful sometimes want understand want enrich opinion representation mean also want understand example context opinion situation opinion expressed example time expressed also would like person understand opinion sentiment understand opinion tell us opinion holder s feel example opinion positive negative perhaps opinion holder happy sad understand obvious beyond extract opinion content need analysis let s take simple example product review case actually expressed opinion holder expressed target obviously what opinion holder s reviewer also often clear what opinion target s product review example iphone review post usually ca nt information easier content course review text s general also easy obtain see product reviews fairly easy analyze term obtain basic opinion representation course want get information might know context example review written want know sentiment review positive additional understand course add value mining opinion see case task relatively easy s opinion holder opinion target already identify let s take look sentence news case implicit holder implicit target tasker general harder identify opinion holder s governor connecticut 
product review ( explicit holder target ) • basic opinion representation reviewer x – opinion holder whose opinion – opinion target opinion product iphone 6 – opinion content exactly opinion review text • enrich opinion representation – opinion context situation ( eg time location ) opinion expressed year = 2015 – opinion sentiment opinion tell us opinion holder ’ feel ( eg positive vs negative ) positive relatively easy mine analyze 6 also identify target one target hurricane sandy 
sentence news ( implicit holder target ) target holder “ … effort get resident wake pay attention hurricane sandy governor connecticut say sandy might bad worst hurricane ever hit new england - hurricane 1938… ” negative context target harder mine analyze need deeper nlp source blodget h ( 2012 october 28 ) hurricane sandy compare worst hurricane ever hit new england business insider business insider retrieve http hurricane-sandy-vs-hurricane-of-1938-2012-10 7 also another target mentioned hurricane s opinion well s negative sentiment s indicated word like bad worst also identify context new england case unlike playoff review element must extract used natural ram process technique task much harder need deeper natural language process example also suggest lot work easy do product reviews s indeed happened analyze assemble news still quite difficult s difficult analysis opinion product reviews 
variation opinion • opinion holder individual vs group • opinion target one entity group entity one attribute entity someone else ’ opinion etc • opinion content – surface variation one phrase paragraph whole article – emotion variation positive vs negative happy vs sad etc • opinion context – simple context different time location etc – complex context potentially include entire discourse context opinion 8 also interesting variation fact be go examine variation opinion systematically first let s think opinion holder holder can individual can group person sometimes opinion committee whole country person opinion target account vary lot one entity particular person particular product particular policy ect can group product can product company general can also specific one attribute though attribute entity example s battery iphone can someone else s opinion one person might comment another person s opinion etc see lot variation cause problem vary lot opinion content course also vary lot surface identify one-sentence opinion one-phrase opinion also longer text express opinion like whole article furthermore identify variation sentiment emotion damage s fee opinion holder distinguish positive versus negative mutual happy versus sad separate finally opinion context also vary simple context like different time different location can also complex context background topic discuss opinion expressed particular discourse context interpreted different way s expressed another context context [ inaudible ] entire discourse context opinion 
different kind opinion text datum observed “ ( believe ) love painting ” world observed opinion real world perceive report opinion opinion target express ( perspective ) author ’ opinion opinion holder “ ’ like phone ” infer opinion “ phone run battery 1 hour ” 9 computational perspective be mostly interested opinion extract text datum turn also differentiate distinguish different kind opinion text datum computation perspective 
task opinion mining set opinion representation text datum opinion holder opinion target opinion content opinion context opinion sentiment often element representation already know simplest opinion mining task ( ) 10 first observer might make comment opinion target observe word case author s opinion example nt like phone s opinion author contrast text might also report opinion other person can also make observation another person s opinion report opinion example believe love painting opinion really really expressed another person nt mean author love painting clearly two kind opinion need analyze different way sometimes product reviews see although mostly opinion false reviewer sometimes reviewer might mention opinion friend friend another complication may indirect opinion infer opinion obtain make inference s expressed text might necessarily look like opinion example one statement might phone run battery one hour way factual statement s either true false right even verify statement one also infer negative opinion quality battery phone feel opinion holder battery opinion holder clearly wish battery last longer interesting variation need pay attention extract opinion also reason indirect opinion s often also useful extract whatever person say product sometimes factual sentence like also useful practical viewpoint sometimes nt necessarily extract subject sentence instead sentence opinion useful understand person understand product commend task opinion mining defined take textualize input generate set opinion representation representation identify opinion holder target content context ideally also infer opinion sentiment comment context better understand opinion often element representation already know give good example case product will use opinion holder opinion target often expressly identify s turn one simplest opinion mining task s interesting think task might also simple case easily build application used opinion mining technique 
opinion mining • decision support – help consumer choose product service – help voter decide vote – help policy maker design new policy • understand person – help understand person ’ preference better serve ( eg optimize product search engine optimize recommender system ) – help advertising ( target advertising ) • “ voluntary survey ” ( human sensor aggregate opinion ) – – – – business intelligence market research data-driven social science research gain advantage prediction ( text-based prediction ) 11 talk opinion mining defined task let s also talk little bit opinion mining important s useful identify three major reason three broad reason first help decision support help us optimize decision often look person s opinion look read reviews order make decision like buy product used service also would interested other opinion decide vote example policy maker may also want know s opinion design new policy s one general kind application s broad course second application understand person also important example can help understand person s preference can help us better serve person example optimize product search engine optimize recommender system know person interested person think product also help advertising course target advertising know kind person tend like kind plot third kind application call voluntary survey important research used do survey manual survey question answer person need feel inform answer question directly related human sensor usually aggregate opinion lot human kind assess general opinion would useful business intelligence manufacturer want know product advantage other winning feature product winning feature competitive product market research understand consumer oppinion create useful directive data-driven social science research benefit text mining understand person s opinion aggregate lot opinion social medium lot popular information actually study question example study behavior person social medium social network regard voluntary survey do person general gain lot advantage prediction task leverage text datum extra datum problem use text base prediction technique help make prediction improve accuracy prediction [ music ] 

1 [ noise ] lecture sentiment classification 
sentiment classification know text datum opinion holder opinion target opinion content opinion context opinion sentiment 2 assume element opinion representation ready know task may sentiment classification show case suppose know s opinion holder s opinion target also know content context opinion mainly need decide opinion sentiment review case used sentiment classification understand opinion 
sentiment classification task definition • input opinionated text object • output sentiment label – polarity analysis eg category = { positive negative neutral } category = { 5 4 3 2 1 } – emotion analysis ( beyond polarity ) eg category = { happy sad fearful angry surprised disgusted } • special case text categorization  text categorization method used sentiment classification • improvement come – sophisticated feature appropriate sentiment tag – consideration order category ( eg ordinal regression ) 3 sentiment classification defined specifically follow input opinionated text object output typically sentiment label sentiment tag design two way one polarity analysis category positive negative neutral emotion analysis go beyond polarity characterize feel opinion holder case polarity analysis sometimes also numerical rating often see reviews web five might denote positive one maybe negative example general disk holder category characterize sentiment emotion analysis course also different way design category six frequently used category happy sad fearful angry surprised disgusted see task essentially classification task categorization task have see special case text categorization also mean textual categorization method used sentiment classification course accuracy may good sentiment classification require improvement regular text categorization technique simple text categorization technique particular need two kind improvement one use sophisticated feature may appropriate sentiment tag discuss moment consider order category especially polarity analysis s clear s order category independent s order among s useful consider order example can use ordinal regression s something will talk later 
commonly used text feature • character n-grams mixed different n ’ – general robust recognition error less discriminative word • word n-grams mixed different n ’ – unigram often effective sentiment analysis ( eg “ ’ good ” “ ’ good ” ) – long n-grams discriminative may cause overfitting • pos tag n-grams mixed n-gram word pos tag – eg “ adjective noun ” “ great noun ” 4 let s talk feature often useful text categorization text mining general especially also need sentiment analysis let s start simplest one character n-grams sequence character unit mixed different n s different length right general way robust way represent text datum can language pretty much also robust spelling error recognition error right misspell word one character representation actually would allow match word occur text correctly right misspell word correct form match contain common n-grams character course recommendation would discriminate word next word n-grams sequence word mix different n s unigram s actually often effective lot text process task s mostly word word design feature human communication often good enough many task s good sufficient sentiment analysis clearly example might see sentence like s good s good something else right case take good would suggest positive s good right s accurate take bigram good together s accurate longer n-grams generally discriminative be specific match say lot s accurate s unlikely ambiguous may cause overfitting unique feature machine oriented program easily pick feature training set rely unique feature distinguish category obviously kind classify one would generalize word future discriminative feature necessarily occur s problem overfitting s desirable also consider part speech tag n-grams part speech tag example adjective noun can form pair also mix n-grams word n-grams part speech tag example word great might follow noun can become feature hybrid feature can useful sentiment analysis 
commonly used text feature ( cont ) • word class – syntactic ( = pos tag ) – semantic concept eg ontology recognize entity – empirical word cluster ( eg cluster paradigmatically syntagmatically related word ) • frequent pattern text ( eg frequent word set collocation ) – discriminative word – may generalize better pure n-grams • parse tree-based ( eg frequent subtree path ) – even discriminative need avoid overfitting • pattern discovery algorithms useful feature construction 5 next also word class class syntactic like part speech tag can semantic might represent concept thesaurus ontology like wordnet recognize name entity like person place category used enrich presentation additional feature also learn word cluster parodically example have talk mining association word cluster paradigmatically related word syntaxmatically related word cluster feature supplement word base representation furthermore also frequent pattern syntax can frequent word set word form pattern necessarily occur together next will also location word occur closely together pattern provide discriminative feature word obviously may also generalize better regular n-grams frequent expect occur also test datum lot advantage might still face problem overfeed feature become complex problem general true parse tree-based feature use parse tree derive feature frequent subtree path even discriminate be also likely cause fitting general pattern discovery algorithm s useful feature construction allow us search large space possible feature complex word sometimes useful general natural language process important derive complex feature enrich text representation example simple sentence show long time ago another lecture word derive simple word n-grams representation character n-grams 
nlp enrich text representation complex feature dog chasing boy playground dog chasing boy playground det noun aux verb det noun prep noun phrase complex verb det noun “ great noun ” “ verb adv adj ” … noun phrase noun phrase prep phrase verb phrase verb phrase verb sentence dog animal chase boy person playground np location dog ( d1 ) boy ( b1 ) playground ( p1 ) chasing ( d1 b1 p1 ) “ great ” speech act = request 6 nlp enrich representation lot information part speech tag parse tree entity even speech act enrich information course generate lot feature complex feature like mixed gram word part speech tag even part parse tree 
feature construction text categorization • feature design affect categorization accuracy significantly • combination machine learn error analysis domain knowledge effective – domain knowledge  seed feature feature space – machine learn  feature selection feature learn – error analysis  feature validation • nlp enrich text representation  enrich feature space ( likely overfitting ) • optimize tradeoff exhaustivity specificity major goal high coverage ( frequent ) discriminative ( infrequent ) 7 general feature design actually affect categorization accuracy significantly s important part machine learn application general think would effective combine machine learn error analysis domain knowledge design feature first want use main knowledge understand problem design seed feature also define basic feature space lot possible feature machine learn program work machine apply select effective feature construct new feature s feature learn feature analyze human error analysis look categorization error analyze feature help recover error feature cause overfitting cause error lead feature validation revise feature set iterate might consider used different feature space nlp enrich text recognition say enrich feature space allow much larger space feature also many many feature useful lot task careful use lot category feature cause overfitting otherwise would training careful let overflow happen main challenge design feature common challenge optimize trade exhaustivity specificity trade turn difficult exhaustivity mean want feature actually high coverage lot document sense want feature frequent specifity require feature discriminative naturally infrequent feature tend discriminative really cause trade frequent versus infrequent feature s featured design usually odd s probably important part machine learn problem particularly case text categoration specifically senitment classification [ music ] 

1 [ noise ] lecture ordinal logistic regression sentiment analysis 
motivation rating prediction • input opinionated text document • output discrete rating r  { 1 2 … k } • used regular text categorization technique – ’ consider order dependency category – feature distinguish r=2 r=1 may distinguish r=k r=k-1 ( eg positive word generally suggest higher rating ) • solution add order classifier ( eg ordinal logistic regression ) 2 problem set typical sentiment classification problem specifically rating prediction opinionated text document input want generate output rating range 1 k s discrete rating categorization problem k category can use regular text categorization technique solve problem solution would consider order dependency category intuitively feature distinguish category 2 1 rather rating 2 1 may similar distinguish k k-1 example positive word generally suggest higher rating train categorization problem treat category independent would capture s solution well general order classify many different approach be go talk one call ordinal logistic regression 
logistic regression binary sentiment classification predictor x  ( x1 x 2 x ) x   binary response variable  { 01 } 1 y 0 log x positive x negative p (  1 | x ) p (  1 | x )  log  0  i 1 x ii p (  0 | x ) 1  p (  1 | x ) p (  1 | x )  e e i   0  im1 x ii 0  im1 x ii 1 3 let s first think use logistical regression binary sentiment categorization problem suppose want distinguish positive negative two category categorization problem predictor represent x feature feature together feature value real number representation text document two value binary response variable 0 1 1 mean x positive 0 mean x negative course standard two category categorization problem apply logistical regression may recall logistical regression assume log probability equal one assume linear function feature show would allow us also write probability equal one give x equation see bottom s logistical function see relate probability probability y=1 feature value course beta s parameter direct application logistical regression binary categorization multiple category multiple level well use binary logistical regression problem solve multus level rating prediction idea introduce multiple binary class file case ask class file predict whether rating j rating s lower j yj equal 1 mean rating j s 0 mean rating lower j basically want predict rating range 1-k first one classifier distinguish k versus other s classifier one be go another classifier distinguish k-1 rest s classifier end need classifier distinguish 2 altogether will k-1 classifier course also solve problem 
logistic regression multi-level rating 1 yj   0 rating k k-1 k-2 … 2 1 rating j rating lower j predictor x  ( x1 x 2 x ) x   rating r { 1 2 … k } p ( yj  1 | x ) p ( r  j | x ) log  log   j  i 1 x i ji p ( yj  0 | x ) 1  p ( r  j | x ) classifier 1 classifier 2  j  im1 x i ji e p ( r  j | x )  classifier k-1  ji   e  j  im1 x i ji 1 4 logistical regression program also straight forward see previous slide parameter classifier need different set parameter logistical regression classify index j correspond rating level also used j replace beta make notation consistent show ordinal logistical regression basically k minus one regular logistic regression classifier s set parameter approach rating follow trained k-1 logistic regression classifier separately course take new instance 
rating prediction multiple logistic regression classifier text object x  ( x1 x 2 x ) x   rating r { 1 2 … k } p ( rk|x ) > 05 training k-1 logistic regression classifier e p ( r  j | x )  e j=k k-1 … 2  j  im1 x i ji  j  im1 x i ji 1 yes r=k p ( rk-1|x ) > 05 yes … r=k-1 p ( r2|x ) > 05 r=1 yes r=2 5 invoke classifier sequentially make decision first let look classifier correspond level rating k classifier tell us whether object rating k probability accord logistical regression classifier larger point five be go say yes rating k s large twenty-five well mean rating s k right need invoke next classifier tell us whether s k minus one s least k minus one probability larger twenty-five will say well s k-1 say well mean rating would even k-1 be go keep invoke classifier hit end need decide whether s two one would help us solve problem right classifier would actually give us prediction rating 
problem k-1 independent classifier log p ( yj  1 | x ) p ( yj  0 | x )  log e p ( r  j | x )  rating k k-1 k-2 … 2 1 p ( r  j | x )   j  i 1 x i ji 1  p ( r  j | x ) classifier 1 classifier 2 classifier k-1 e  ji    j  im1 x i ji  j  im1 x i ji 1 many parameter total ( k-1 ) * ( m+1 ) k-1 classification problem dependent negative feature tend similar 6 range 1 k unfortunately strategy optimal way solve problem specifically two problem approach equation see first problem many parameter many parameter count many parameter exactly may interesting exercise might want pause video try figure solution many parameter classifier many classifier well see classifier n plus one parameter k minus one classifier together total number parameter k minus one multiply n plus one s lot lot parameter classifier lot parameter would general need lot datum actually help us training datum help us decide optimal parameter complex model s ideal second problem problem k minus 1 plus fife really independent problem actually dependent general word positive would make rating higher classifier classifier able take advantage fact idea ordinal logistical regression precisely key idea improvement k-1 independent logistical regression classifier idea tie beta parameter mean go assume beta parameter parameter indicated inference weight be go assume beta value - 1 parameter encode intuition positive word general would make higher rating likely intuitively assumption reasonable problem setup order category fact would allow us two positive benefit one s go reduce number family significantly allow us share training datum parameter similar equal training datum different classifier share help us set optimal value beta datum help us choose good beta value 
ordinal logistic regression key idea i 1 … j 3 … k = j-1i  share training datum rating k k-1 k-2 … 2 1 log p ( yj  1 | x ) p ( yj  0 | x ) classifier 1 classifier 2  log  reduce # parameter p ( r  j | x )   j  i 1 x ii 1  p ( r  j | x ) e p ( r  j | x )  e i    j  im1 x ii  j  im1 x ii 1 classifier k-1 many parameter total m+k-1 7 s consequence well formula would look similar see beta parameter one index correspond feature longer index correspond level rating mean tie together s one set better value classifier however classifier still distinct r value r parameter except s different course need predict different level rating r sub j different depend j different j different r value rest parameter beta s also ask question many parameter s interesting question think think moment see param far fewer parameter specifically plus k minus one beta value plus k minus one value let s look basically s basically main idea ordinal logistical regression let s see use method actually assign rating turn idea tie parameter beta value also end similar way make decision specifically criterium whether predictor probability least 05 equivalent whether score object larger equal negative author j show score function take linear combination feature divide beta value 
ordinal logistic regression rating prediction p ( r  j | x )  05  e e  j score ( x )  j score ( x ) 1  05  score ( x )   j score ( x )  i1 i x rating k k-1 k-2 … 2 1 r=k classifier 1 classifier 2 classifier k-1 k r=k-1 k-1 r=2 2 r=1 r=j score  [ j j+1 ) define 1= k+1=- 8 mean simply make decision rating look value score function see bracket fall see general decision rule thus score particular range value assign corresponding rating text object approach be go score object used feature trained parameter value score compare set trained alpha value see range score used range decide rating object get range alpha value correspond different level rating s way train alpha value tie level rating [ music ] 

1 [ music ] lecture latent aspect rating analysis opinion mining sentiment analysis 
opinion mining sentiment analysis latent aspect rating analysis text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 lecture be go continue discuss opinion mining sentiment analysis particular be go introduce latent aspect rating analysis allow us perform detailed analysis reviews overall rating 
motivation infer aspect rating hotel xyx reviewer 1 ` great location + spacious room = happy traveler stay weekend july walk everywhere enjoy comfy bed quiet hallway value room location service reviewer 2 value room location service ` terrific service gorgeous facility stay hotel wiht young daughter three night june 17-20 2010 absolutely love hotel room one nicest have ever stay … infer aspect weight value location service … value location service … 3 first motivation two reviews often see net hotel see overall rating case reviewer give five star course also reviews text look reviews s clear whether hotel good location service s also unclear reviewer like hotel want decompose overall rating rating different aspect value room location service decompose overall rating rating different aspect obtain detailed understand reviewer s opinionsabout hotel would also allow us rank hotel along different dimension value room general detailed understand reveal information user s preference reviewer s preference also understand better reviewer view hotel different perspective want infer aspect rating also want infer aspect weight reviewer may care value opposed service would case like s show left weight distribution see lot weight place value other care service therefore might place weight service value reason also important think five star value might still expensive reviewer care lot service right kind service price good reviewer might give five star reviewer really care value hotel five star likely would mean really cheap price order interpret rating different aspect accurately also need know aspect weight be combine together detailed understand opinion task get reviews overall rating input generate aspect rating compose aspect rating aspect rate output 
latent aspect rating analysis [ wang et al 10 ] • give set review article topic overall rating • output – major aspect comment reviews – rating aspect – relative weight place different aspect reviewer • many application – – – – – opinion-based entity ranking aspect-level opinion summarization reviewer preference analysis personalize recommendation product … 4 problem call latent aspect rating analysis task general give set review article topic overall rating hope generate three thing one major aspect comment reviews second rating aspect value room service third relative weight place different aspect reviewer task lot application enable lot application list later show result example opinion base entity ranking generate aspect-level opinion summary also analyze reviewer preference compare compare preference different hotel personalize recommendation product course question solve problem case advanced topic ’ time really cover technique detail ’ go give brisk basic introduction technique development problem first step ’ go talk solve problem two stage later ’ go also mention unify model take review overall rating input want first be go segment aspect be go pick word talk location word talk room condition etc would able obtain aspect segment particular be go obtain count word segment denote c sub w d do used seed word like location room price retrieve [ inaudible ] segment segment mine correlated word seed word would allow us segmented text segment discuss different aspect course later see also use [ inaudible ] model segmentation anyway s first stage obtain council word segment second stage call latent rating regression be go use word frequency different aspect predict overall rate predict happen two stage 
solve lara two stage rd aspect segmentation ci ( w ) “ friend stay hotel … hotel nice location amazing can walk almost anywhere … far room nicely appoint bed sooo comfortable even though bathroom door close way still pretty private … like best hotel staff soooo nice accommodate ” observed + latent rating regression ri ( ) i w i ( ) aspect segment term weight aspect rating aspect weight 00 location1 39 amazing1 02 38 01 walk1 02 far1 01 room1 17 nicely1 02 48 01 appointed1 39 comfortable1 21 nice1 12 06 accommod 1 58 17 smile1 12 friendliness1 06 attentiveness1 latent 5 first stage be go use [ inaudible ] weight word aspect predict aspect rating example discussion location see word like amazing mentioned many time high weight example increase aspect rating location another word like far act weight s mentioned many time decrease rating aspect rating assume weight combination word frequency weight sentiment weight word course sentimental weight might different different aspect aspect set term sentiment weight show s order beta sub w second stage second step be go assume overall rating simply weight combination aspect rating be go assume aspect weight [ inaudible ] sub used take weight average aspect rating denote r sub d be go assume overall rating simply weight average aspect rating set allow us predict overall rating base observable frequency left side see observed information r sub count right side see information range actually latent hope discover typical case generate model would emb interesting variable generate model be go set generation probability overall rating give observed word course adjust parameter value include beta rs alpha order maximize probability datum case conditional probability observed rating give document see case example pisa predict text datum be predict rating parameter course different see uncover parameter would nice r sub precise rating want get composer rating different aspect [ inaudible ] sub precisely aspect weight hope get byproduct also get beta factor [ inaudible ] factor sentiment weight word 
latent rating regression [ wang et al 10 ] • datum set review document overall rating = { ( rd ) } – pre-segment k aspect segment – ci ( w ) = count word w aspect segment ( zero w ’ occur ) • model predict rating base p ( rd d ) overall rating = weight average aspect rating    ( ) ~ n (   ) rd ~ n ( i 1 i ( ) ri ( )  ) k 2 ri ( )  wv ci ( w ) i w multivariate gaussian prior i w   aspect-specific sentiment w aspect rating = sum sentiment weight word aspect 6 formally datum modele set review document overall rating review document denote overall rating denote r sub d pre-segment turn k aspect segment be go use ci ( w ) denote count word w aspect segment i course s zero word nt occur segment model go predict rating base d be interested provisional problem r sub-d give d model set follow r sub-d assume two follow normal distribution nt mean denote actually await average aspect rating r sub show normal distribution variance datum square course assumption actual rating necessarily anything thing way always make assumption formal way model problem allow us compute interest quantity case aspect rating aspect weight aspect rating see [ inaudible ] assume weight sum weight weight [ inaudible ] weight say overall rating assume weight average aspect rating value r sub denote together vector depend token specific weight ’ go assume vector draw another multivariate gaussian distribution mean denote mu factor covariance metric sigma mean generate overall rating be go first draw set value multivariate gaussian prior distribution get value be go use weight average aspect rating mean use normal distribution generate overall rating aspect rating say sum sentiment weight word aspect note sentiment weight specific aspect beta index s aspect give us way model different segment word neither word might positive sentiment another aspect s also used see parameter beta sub w give us aspect-specific sentiment w obviously s one important parameter general see parameter beta value delta mu sigma 
latent rating regression ( cont ) • maximum likelihood estimate  – parameter   ( { i w }   2 ) – ml estimate *  arg max  dc p ( rd |  ) • aspect rating aspect ri ( )  wv ci ( w ) i w • aspect weight i ( ) weight aspect ci ( w ) 0 word occur aspect segment   2   ( ) *  arg max  ( ) p (  ( ) |   ) p ( rd | { i w }   ( ) ) maximum posteriori prior likelihood 7 next question estimate parameter collectively denote parameter lambda usual use maximum likelihood estimate give us setting parameter maximize observed rating condition respective reviews course would give us useful variable interested compute specifically estimate parameter easily compute aspect rating aspect sub d s simply take word occur segment take count multiply center weight word take sum course time would zero word occur s go take sum word vocabulary factor weight alpha sub well s part parameter right use compute case use maximum posteriori compute alpha value basically be go maximize product prior alpha accord assume multivariate gaussian distribution likelihood case likelihood rate probability generate observed overall rating give particular alpha value parameter see 
suggest read • [ wang et al 10 ] hongn wang yue lu chengxiang zhai latent aspect rating analysis review text datum rating regression approach proceedings acm kdd 2010 pp 783-792 2010 18358041835903 8 detail model read paper cite [ music ] 

1 [ sound ] lecture continue discussion latent aspect rating analysis earlier talk solve problem lara two stage first segmentation different aspect use latent regression model learn aspect rating later weight s also possible develop unify generative model solve problem model generational over-rate base text also model generation text natural solution would use topic model give entity assume aspect describe word distribution topic 
unify generative model lara [ wang et al 11 ] entity aspect location amazing walk anywhere room dirty appoint smelly terrible front-desk smile unhelpful review aspect rating aspect weight location room service excellent location walking distance tiananman square shopping street ’ best topic model part hotel room get really old bathroom nasty fixture fall lot crack everything look dirty ’ think worth price service disappointing part especially door man treat guest hospitality 086 004 010 2 use topic model model generation review text assume word review text draw distribution way assume generate model like prsa plug latent regression model use text predict overrate mean first predict aspect rating combine aspect weight predict overall rating would give us unify generate model model generation text overall ready condition text nt time discuss model detail many case part cause discuss cut edge topic s reference site find detail 
sample result 1 rating decomposition [ wang et al 10 ] • hotel overall rating different aspect rating ( 5 star hotel ground-truth parenthesis ) hotel value room location cleanliness hotel 1 42 ( 47 ) 38 ( 31 ) 40 ( 42 ) 41 ( 42 ) hotel 2 43 ( 40 ) 39 ( 33 ) 37 ( 31 ) 42 ( 47 ) hotel 3 37 ( 38 ) 44 ( 38 ) 41 ( 49 ) 45 ( 48 ) • reveal detailed opinion aspect level 3 be go show simple result get used kind generate model first s rating decomposition see decomposed rating three hotel overall rating look overall rating ca nt really tell much difference hotel decompose rating aspect rating see hotel higher rating dimension like value other might score better dimension like location give detailed opinion aspect level ground-truth show parenthesis also allow see whether prediction accurate s always accurate s mostly still reflect trend 
sample result 2 comparison reviewer [ wang et al 10 ] • per-reviewer analysis – different reviewer ’ rating hotel reviewer value room location cleanliness reviewer 1 37 ( 40 ) 35 ( 40 ) 37 ( 40 ) 58 ( 50 ) reviewer 2 50 ( 50 ) 30 ( 30 ) 50 ( 40 ) 35 ( 40 ) – reveal difference opinion different reviewer 4 second result compare different reviewer hotel table show decomposed rating two reviewer hotel high level overall rating look overall rating nt really get much information difference two reviewer decompose rating see clearly high score different dimension show model review difference opinion different reviewer detailed understand help us understand better reviewer also better feedback hotel 
sample result 3 aspect-specific sentiment lexicon [ wang et al 10 ] value room location cleanliness resort 2280 value 1964 excellent 1954 worth 1920 bad 2409 money 1102 terrible 1001 overprice 906 view 2805 comfortable 2315 modern 1582 quiet 1537 carpet 988 smell 883 dirty 785 stain 585 restaurant 2447 walk 1889 bus 1432 beach 1411 wall 1170 bad 540 road 290 website 167 clean 5535 smell 1438 linen 1425 maintain 1351 smelly 053 urine 043 filthy 042 dingy 038 learn sentimental information directly datum 5 something interesting sense byproduct problem formulation really design generate model component sentimental weight word different aspect see highly weight word versus negatively load weight word four dimension value room location cleanliness top word clearly make sense bottom word also make sense show approach also learn sentiment information directly datum kind lexicon useful general word like long let s say may different sentiment polarity different context say battery life laptop long s positive say reboot time laptop long s bad right even reviews product laptop word long ambiguous can mean positive can mean negative kind lexicon learn used kind generate model show whether word positive particular aspect clearly useful fact lexicon directly used tag reviews hotel tag comment hotel social medium like tweet s also interesting since almost completely unsupervised well assume reviews whose overall rating available allow us learn form potentially larger amount datum internet reach sentiment lexicon 
sample result 4 validate preference weight [ wang et al 10 ] top-10 reviewer highest x ratio ( emphasize “ value ” ) bot-10 reviewer lowest x ratio ( emphasize non-value aspect ) city avg price amsterdam 2416 san francisco 2613 florence 2721 group loc rm ser top-10 bot-10 top-10 bot-10 top-10 bot-10 1907 2708 2145 3211 2694 2989 2149 3339 2490 3111 2489 2934 2211 2362 2253 3114 2203 2926 higher 6 result validate preference word remember model infer wether reviewer care service price know whether infer weight correct pose difficult challenge evaluation show interesting way evaluate see price hotel different city price hotel favore different group reviewer top ten reviewer highest infer value aspect ratio example value versus location value versus room etcetera top ten reviewer highest ratio measure mean reviewer tend put lot weight value compare dimension mean really emphasize value bottom ten hand reviewer lowest ratio mean well mean reviewer put higher weight aspect value person care another dimension nt care much value sense least compare top ten group ratio computer base infer weight model see average price hotel favore top ten reviewer indeed much cheaper favore bottom ten provide indirect way validate infer weight mean weight random actually meaningful comparison average price three city actually see top ten tend average price whereas bottom half care lot thing like service room condition tend hotel higher price average 
application 1 rate aspect summarization aspect value location summary rating truly unique character great location reasonable price hotel max excellent choice recent three night stay seattle 31 overall negative experience however consider hotel industry much impress business lot room improvement 17 location short walk downtown pike place market make hotel good choice 37 visit big metropolitan city prepared hear little traffic outside 12 pay wireless day use complimentary internet business center behind lobby though 27 business service complaint daily charge internet access pretty much connect wireless street anymore 09 7 result build lot interesting application example direct application would generate rate aspect summary decomposition generate summary aspect positive sentence negative sentence aspect s informative original review overall rating review text 
application 2 discover consumer preference [ wang et al 2011 ] • amazon reviews guidance battery life accessory service file format volume video 8 result aspect s cover reviews rating mp3 reviews result show model discover interesting aspect comment low overall rating versus higher overall per rating care different aspect comment different aspect help us discover example consumer trend appreciate different feature product example one might discover trend person tend like larger screen cell phone light weight laptop etcetera knowledge useful 
application 3 user rating behavior analysis [ wang et al 10 ] value room location cleanliness service expensive hotel 5 star 3 star 0134 0148 0098 0162 0171 0081 0251 person like expensive hotel good service 0074 0163 0101 cheap hotel 5 star 1 star 0171 0093 0126 0121 0161 0116 0101 0082 0294 0049 person like cheap hotel good value 9 manufacturer design next generation product interesting result analyze user rating behavior see average weight along different dimension different group reviewer left side see weight viewer like expensive hotel give expensive hotel 5 star see average rate tend service suggest person like expensive hotel good service s surprising s also another way validate infer weight look right side look column 5 star reviewer like cheaper hotel give cheaper hotel five star expect put weight value s like cheaper hotel look like expensive hotel cheaper hotel will see tend weight condition room cleanness show used model infer information s hard obtain even read reviews even read reviews hard infer preference emphasis case text mining algorithms go beyond human review interesting pattern datum course useful compare different hotel compare opinion different consumer group different location course model general apply reviews overall rating useful technique support lot text mining application 
application 4 personalize ranking entity [ wang et al 10 ] query 09 value 01 other non-personalized personalize ( query-specific ) 10 finally result apply model personalize ranking recommendation entity infer reviewer weight different dimension allow user actually say care example query show 90 % weight value 10 % other mean care aspect care get cheaper hotel emphasis value dimension query use reviewer believe similar preference recommend hotel know well infer weight reviewer different aspect find reviewer whose weight precise course infer rate similar use reviewer recommend hotel call personalize rather query specific recommendation non-personalized recommendation show top see top result generally much higher price lower group s reviewer s care value dictate query tend really favor low price hotel yet another application technique show text mining understand user better handle user better solve user better 
summary opinion mining • important lot application • sentiment analysis do used text categorization technique – enrich feature representation – consideration order category • generative model powerful mining latent user preference • approach propose product reviews • opinion mining news social medium remain challenge 11 summarize discussion opinion mining general important topic lot application text sentiment analysis readily do used text categorization standard technique tend enough need enrich feature implementation also need consider order category will talk ordinal regression problem also assume generate model powerful mining latent user preference particular generative model mining latent regression emb interesting preference information send weight word model result learn useful information fitting model datum approach propose evaluate product reviews context opinion holder opinion target clear easy analyze course also lot practical application opinion mining news social medium also important s difficult analyze review datum mainly opinion holder opinion target interested call natural management process technique uncover accurately 
suggest read • bing liu sentiment analysis opinion mining morgan & claypool publisher 2012 • bo pang lillian lee opinion mining sentiment analysis foundation trend information retrieval 2 ( 1-2 ) pp 1–135 2008 • hongn wang yue lu chengxiang zhai latent aspect rating analysis review text datum rating regression approach proceedings acm kdd 2010 pp 783-792 18358041835903 • hongn wang yue lu chengxiang zhai latent aspect rating analysis without aspect keyword supervision proceedings acm kdd 2011 pp 20204082020505 12 suggest reading first two small book use topic find lot discussion variation problem technique propose solve problem next two paper generate model rating aspect rating analysis first one solve problem used two stage second one unify model topic model integrate regression model solve problem used unify model [ music ] 

1 [ sound ] lecture text-based prediction 
text-based prediction text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 lecture be go start talk mining different kind knowledge see slide namely be go use text datum infer value variable real world may directly related text remotely related text datum different content analysis topic mining directly characterize content text s also different opinion mining sentiment analysis still characterize mostly content focus subject content reflect know opinion holder provide limit review predict lecture follow lecture be go talk predict information world get sophisticated pattern text together kind datum 
big picture prediction datum mining loop predict value real world variable predictive model … change world real world multiple predictor ( feature ) sensor 1 … sensor k … non-text datum joint mining non-text text text datum 3 would useful first take look big picture prediction datum mining general call datum mining loop picture see right multiple sensor include human sensor report see real world form datum course datum form non-text datum text datum goal see predict value important real world variable matter us example someone s house condition weather etc variable would important might want act might want make decision base get datum predict value 
text-based prediction generate effective predictor text jointly mine text non-text datum real world sensor 1 … sensor k … non-text datum multiple predictor ( feature ) … joint mining non-text & text text datum 4 well general will first datum mining analysis datum general treat datum collect prediction problem set much interested joint mining non-text text datum combine datum together analysis generally multiple predictor interesting variable us call feature feature put predictive model actually predict value interesting variable allow us change world basically general process make prediction base datum include test datum s important emphasize human actually play important role process especially involvement text datum human first would involved mining datum would control generation feature would also help us understand text datum text datum create consume human human best consume interpret text datum course lot text datum machine help s need text datum mining sometimes machine see pattern lot datum human may see general human would play important role analyze text datum application next human also must involved predictive model build adjust testing particular lot domain knowledge problem prediction build predictive model next course predictive value variable human would involved take action change word make decision base particular value finally s interesting human can involved control sensor adjust sensor collect useful datum prediction s call datum mining loop perturb sensor will collect new datum useful datum obtain datum prediction datum generally help us improve predict accuracy loop human recognize additional datum need collect machine course help human identify datum collect next general want collect datum useful learn actually subarea machine learn call active learn identify datum point would helpful machine learn program label right general see loop datum acquisition datum analysis datum mining prediction value take action change word observe happen decide additional datum collect adjust sensor prediction arrow also note additional datum need acquire order improve accuracy prediction big picture actually general s reflect lot important application big datum s useful keep mind look text mining technique text mining perspective be interested text base prediction course sometimes text alone make prediction useful prediction human behavior human preference opinion general text datum put together non-text datum interesting question would first design effective predictor generate effective predictor text question address extent previous lecture talk kind feature design text datum also address extent talk knowledge mine text example topic mining useful generate pattern topic base indicator predictor fed predictive model topic intermediate recognition text would allow us design high level feature predictor useful prediction variable may also generate original text datum provide much better implementation problem serve effective predictor similarly similar analysis lead predictor well datum mining text mining algorithms used generate predictor question join mine text non-text datum together question address yet lecture follow lecture be go address problem generate much enrich feature prediction allow us review lot interesting knowledge world pattern generate text non-text datum sometimes already useful prediction put together many predictor really help improve prediction 
text-based prediction = unify view text mining analysis main task infer value real-world variable + non-text datum subtask mining content text datum observed world real world text datum + context perceive express ( perspective ) ( english ) subtask mining knowledge observer 5 basically see text-based prediction actually serve unify framework combine many text mining analysis technique include topic mining content mining technique segment analysis goal mainly evoke value real-world variable order achieve goal preparation subtask one subtask can mine content text datum like topic mining can mine knowledge observer sentiment analysis opinion help provide predictor prediction problem course also add non-text datum directly predict model non-text datum also help provide context text analyst improve topic mining opinion analysis improvement often lead effective predictor problem would enlarge space pattern opinion topic mine text will discuss later 
joint mining analysis text non-text datum • non-text datum help text mining – non-text datum provide context mining text datum – contextual text mining mining text context defined non-text datum ( see [ mei 2009 ] large body work ) • text datum help non-text datum mining – text datum help interpret pattern discover non-text datum – pattern annotation used text datum interpret pattern find non-text datum ( see [ mei et al 2006 ] detail ) 6 joint analysis text non-text datum actually understood two perspective one perspective non-text help testimony non-text datum provide context mining text datum provide way partition datum different way lead number type technique contextual type mining s mine text context defined non-text datum see reference large body work direction need highlight next lecture perspective text datum help non-text datum mining well text datum help interpret pattern discover non-text datum let s say discover frequent pattern non-text datum use text datum associate instance pattern occur well text datum associate instance pattern nt look give us two set text datum see s difference difference text datum interpretable text content easy digest difference might suggest meaning pattern find non-text datum help interpret pattern technique call pattern annotation see reference list detail 
suggest read • [ mei et al 2006 ] qiaozhu mei dong xin hong cheng jiawei han chengxiang zhai generate semantic annotation frequent pattern context analysis proceedings 12th acm sigkdd international conference knowledge discovery datum mining ( kdd 2006 ) acm new york ny usa 11504021150441 • [ mei 2009 ] qiaozhu mei contextual text mining thesis university illinois urbana-champaign 2009 http 14707 7 reference mentioned first reference pattern annotation second qiaozhu s dissertation contextual text mining contain large body work contextual text mining technique [ music ] 

1 [ sound ] lecture contextual text mining 
contextual text mining text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 contextual text mining related multiple kind knowledge mine text datum be show s related topic mining make topic associate context like time location similarly make opinion mining contextualize make opinion connect context s related text base prediction allow us combine non-text datum text datum derive sophisticated predictor 
contextual text mining motivation • text often rich context information – direct context ( meta-data ) time location author source … – indirect context ( additional datum related meta-data ) social network author author ’ age text source etc – related datum regard context • context used – partition text datum comparative analysis – provide meaning discover topic 3 prediction problem specifically interested contextual text mining well s first text often rich context information include direct context meta-data also indirect context direct context grow meta-data time location author source text datum be almost always available us indirect context refer additional datum related meta-data example office obtain additional context social network author author s age information general directly related text yet process connect can text datum source one text connect text well general related datum regard context can remove rate context s use text context used well context used partition text datum many interesting way almost allow us partition text datum way need important allow us interesting comparative analysis also general provide meaning discover topic 
context = partition text paper written 1998 1998 1999 …… …… paper “ text mining ” 2005 2006 paper written author us www sigir acl kdd sigmod enable discovery knowledge associate different context need 4 associate text context s illustration context regard interesting way partition text datum show research paper publish different year different venue different conference name list bottom like sigir acl etc text datum partition many interesting way context context include time conference venue perhaps include variable well let s see partition interesting way first treat paper separate unit case paper id paper context s independent also treat paper within 1998 one group possible availability time partition datum way would allow us compare topic example different year similarly partition datum base menu get sigir paper compare paper rest compare sigir paper kdd paper acl paper also partition datum obtain paper written author us course used additional context author would allow us compare subset another set paper written also see country obtain set paper text mining compare paper another topic note partitioning also intersected generate even complicate partition general enable discovery knowledge associate different context need particular compare different context often give us lot useful knowledge example compare topic time see trend topic compare topic different context also reveal difference 
many interesting question require contextual text mining • topic gain increase attention recently datum mining research ( time context ) • difference response person different region event ( location context ) • common research interest two researcher ( author context ) • difference research topic publish author usa outside ( author ’ affiliation location context ) • difference opinion topic expressed one social network another ( social network author topic context ) • topic news datum correlated sudden change stock price ( time series context ) • issue “ matter ” 2012 presidential election ( time series 5 context ) two context many interesting question require contextual text mining list specific one example topic get increase attention recently datum mining research answer question obviously need analyze text context time time context case difference response person different region event event broad answer question case course location context common research interest two researcher case author context difference research topic publish author usa outside case context would include author affiliation location go beyond author need look additional information connect author difference opinion topic expressed one social network another case social network author topic context topic news datum correlated sudden change stock price case use time series stock price context issue matter 2012 presidential campaign presidential election case time serve context see list go basically contextual text mining many application [ music ] 

1 
contextual text mining contextual probabilistic latent semantic analysis text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 
contextual probabilistic latent semantic analysis ( cplsa ) [ mei & zhai 06 ] • general idea – explicitly add interesting context variable generative model (  enable discovery contextualize topic ) – context influence coverage content variation topic • extension plsa – – – – – model conditional likelihood text give context assume context-dependent view topic assume context-dependent topic coverage em algorithm still used parameter estimation estimate parameter naturally contain context variable enable contextual text mining 3 
generation process cplsa choose topic theme view1 view2 view3 criticism government i document government 03draw word response togovernment hurricane response context primarily consist response time = july 2005 criticism response donate 01 …location total = shut-in texasoil donate relief 005 production author = xxxgulf help aid help 002 ofoccup mexico … = approximately sociologist 24 % annual age group = + orlean city 02 production shutnew … … gas production new 01 government donation new orlean texas seventy country pledge monetary donation assistance … orlean 005 july sociologist 2005 choose view 1 2 3 4 theme coverage texas july 2005 1 2 3 4 1 2 3 4 …… document choose coverage 4 
compare news article [ zhai et al 04 ] iraq war ( 30 article ) vs afghan war ( 26 article ) common theme indicate “ unite nation ” involved war cluster 1 common theme iraq theme afghan theme unite nation … 0042 004 n 003 weapon 0024 inspection 0023 … northern 004 alliance 004 kabul 003 taleban 0025 aid 002 … cluster 2 cluster 3 kill 0035 month 0032 death 0023 … troop 0016 hoon 0015 sanch 0012 … taleban 0026 rumsfeld 002 hotel 0012 front 0011 … … … … collection-specific theme indicate different role “ unite nation ” two war 5 
theme life cycle blog article “ hurricane katrina ” [ mei et al 06 ] oil price new orlean hurricane rita city 00634 orlean 00541 new 00342 louisiana 00235 flood 00227 evacuate 00211 storm 00177 … price 00772 oil 00643 gas 00454 increase 00210 product 00203 fuel 00188 company 00182 … 6 
spatial distribution topic “ government response ” blog article hurricane katrina [ mei et al 06 ] 7 
event impact analysis ir research [ mei & zhai 06 ] topic retrieval model term 01599 relevance 00752 weight 00660 feedback 00372 independence 00311 model 00310 frequent 00233 probabilistic 00188 document 00173 … vector concept extend model space boolean function feedback … xml email model collect judgment rank subtopic … 00514 00298 00297 00291 00236 00151 00123 00077 00678 00197 00191 00187 00102 00097 00079 sigir paper seminal paper [ croft & ponte 98 ] 1992 start probabilist trec00778 model logic ir boolean algebra estimate weight 8 … 00432 00404 00338 00281 00200 00119 00111 1998 model 01687 language 00753 estimate 00520 parameter 00281 distribution 00268 probable 00205 smooth 00198 markov 00137 likelihood 00059 … year 
suggest read • [ zhai et al 04 ] chengxiang zhai atulya velivelli bei yu crosscollection mixture model comparative text mining proceedings 10th acm sigkdd international conference knowledge discovery datum mining ( kdd 2004 ) acm new york ny usa 743-748 10140521014150 • [ mei & zhai 06 ] qiaozhu mei chengxiang zhai mixture model contextual text mining proceedings 12th acm sigkdd international conference knowledge discovery datum mining ( kdd 2006 ) acm new york ny usa 11504021150482 • [ mei et al 06 ] qiaozhu mei chao liu hang su chengxiang zhai 2006 probabilistic approach spatiotemporal theme pattern mining weblog proceedings 15th international conference world wide web ( www 2006 ) acm new york ny usa 533-542 11357771135857 9 

1 [ sound ] lecture mine text datum social network context 
contextual text mining mining topic social network context text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 lecture be go continue discuss contextual text mining particular be go look social network other context 
topic analysis network context • context text article form network eg – author research article may form collaboration network – author social medium content form social network – location associate text connect form geographic network • benefit joint analysis text network context – network impose constraint topic text ( author connect network tend write similar topic ) – text help characterize content associate subnetwork ( eg difference opinion expressed two subnetwork ) 3 first s motivation used network context analysis text context text article form network example author research article might form collaboration network author social medium content might form social network example twitter person might follow facebook person might claim friend other etc context connect content other similarly location associate text also connect form geographical network general imagine metadata text datum form kind network relation benefit jointly analyze text social network context network context general s use network impose constraint topic text example s reasonable assume author connect collaboration network tend write similar topic heuristic used guide us analyze topic text also help characterize content associate subnetwork say kind datum network text help example difference opinion expressed two subnetwork review type joint analysis 
network supervised topic modele general idea [ mei et al 08 ] • probabilistic topic modele optimization maximize likelihood *  arg max  p ( textdata |  ) • main idea network impose constraint model parameter  – text two adjacent node network tend cover similar topic – topic distribution smooth adjacent node – add network-induced regularizer likelihood objective function network generative model *  arg max  f ( p ( textdata |  ) r (  network ) ) way combine regularizer 4 briefly can use model call network supervised topic model slide be go give general idea next slide be go give detail general part course nt enough time cover frontier topic detail provide reference would allow read topic know detail still useful know general idea know know might able use general idea network supervised topic model follow let s start view regular topic model like lda sort optimization problem course case optimization objective function likelihood function often use maximum likelihood estimator obtain parameter parameter give us useful information want obtain text datum example topic want maximize probability test give parameter generally denote number main idea incorporating network think constraint impose base network general idea use network impose constraint model parameter lambda example text adjacent node network similar cover similar topic indeed many case tend cover similar topic may able smooth topic distribution graph network adjacent node similar topic distribution share common distribution topic slight variation topic distribution coverage technically simply add network use regularizer likelihood objective function show instead optimize probability test datum give parameter lambda be go optimize another function f function combine likelihood regularizer function call r regularizer define parameter lambda network tell us basically kind parameter prefer network constraint perspective easily see effect implement idea impose prior model parameter be necessary probabilistic model idea be go combine two one single objective function advantage idea s quite general top model generative model text nt plsa lea current topic model similarly network also network graph connect text object regularizer also regularizer flexible capture different heuristic want capture finally function f also vary many different way combine general idea actually quite quite powerful offer general approach combine different type datum single optimization framework general idea really apply problem paper reference particular instantiation call netplsa start case s instantiate plsa incorporate simple constraint impose network prior neighbor network must similar topic distribution must cover similar topic similar way s basically say english technically modify objective function let s define text actually see network graph g look formula actually recognize part fairly familiarly fairly familiar recognize part likelihood test give topic model 
instantiation netplsa [ mei et al 08 ] network-induced prior neighbor similar topic distribution modify objective function plsa log-likelihood text collection k ( c g )  ( 1   )  (   c ( w ) log  p (  j | ) p ( w |  j ) ) network graph 1    (  2 influence network constraint  j 1 w u v e k w ( u v )  ( p (  j | u )  p (  j | v ) ) 2 ) weight edge ( u v ) j 1 quantify difference topic coverage node u v 5 well look see part precisely plsa log-likelihood want maximize estimate parameter plsa alone second equation show additional constraint parameter particular will see s measure difference topic coverage node u node v two adjacent node network want distribution similar compute square difference want minimize difference note s negative sign front sum whole sum make possible find parameter maximize plsa log-likelihood mean parameter fit datum well also respect constraint network negative sign mentioned negative sign maximize object function will actually minimize statement term look picture will see result weight edge u v space network weight say well two node strong collaborator researcher two strong connection two person social network would weight mean would important be topic coverage similar s basically say finally see parameter lambda new parameter control influence network constraint see easily lambda set 0 go back standard plsa lambda set larger value let network influence estimate model see effect be go basically plsa be go also try make topic coverage two node strongly connect similar ensure coverage similar 
mining 4 topical community result plsa ’ uncover 4 community ( ir dm ml web ) topic 1 term 002 topic 2 peer 002 topic 3 visual 002 topic 4 interface 002 question 002 pattern 001 analog towards protein 001 mining 001 neuron 002 browse 002 training 001 cluster 001 vlsi 001 xml weighting 001 stream motion 001 generation 001 multiple frequent 001 chip 001 design 001 recognition 001 e 001 natural 001 engine 001 relation 001 page 001 cortex 001 service 001 library 001 gene 001 spike 001 social 001 001 001 002 002 001 6 several result paper slide show record result used plsa datum dblp datum bibliographic datum research article experiment used four community application ir information retrieval dm stand datum mining ml machinery web four community article hope see topic mining help us uncover four community assemble topic see generate plsa plsa unable generate four community correspond intuition reason mixed together many word share community s easy use four topic separate use topic perhaps coherent topic s interesting use netplsa network collaboration network case author used impose constraint case also use four topic ned pierre say give much meaningful topic 
mining 4 topical community result netplsa uncover 4 community well information topic 1 retrieval retrieval 013 information 005 document 003 query 003 text 003 search 003 evaluation 002 user 002 relevance 002 data2 mining topic mining 011 datum 006 discovery 003 machine topic 3 learn neural 006 learn 002 network 002 topicweb 4 web 005 service 003 semantic 003 databasis 002 rule 002 recognition 002 analog 001 service 003 peer 002 association 002 vlsi ontology 002 pattern 002 frequent 001 stream 001 neuron 001 gaussian 001 network 001 001 rdf 002 management 001 ontology 001 7 will see topic correspond well four community first information retrieval second datum mining third machine learn fourth web separation mostly influence network leverage collaboration network information essentially person form collaborate network would kind assume write similar topic s be go coherent topic listen text datum alone base occurrence wo nt get coherent topic even though topic model like plsa lda also able pick co-occurring word general topic generate represent word co-occur still generate coherent result netplsa show network contest useful similar model can also useful characterize content associate subnetwork collaboration 
text information network • general view text datum naturally “ life ” rich information network related datum • text datum associate – node network – edge network – path network – subnetwork –… • analysis text used entire network 8 general view text mining context network treat text live rich information network environment mean connect related datum together big network text datum associate lot structure network example text datum associate node network s basically discuss netplsa text datum associate age well path even subnetwork way represent text big environment context information powerful allow analyze datum information together general analysis text used entire network 
suggest read • [ mei et al 08 ] qiaozhu mei deng cai duo zhang chengxiang zhai topic modele network regularization proceedings 17th international conference world wide web ( www 2008 ) acm new york ny usa 13674971367512 9 information be related text datum s one suggest read paper netplsa find detail model make model [ music ] 

1 [ sound ] lecture used time series context potentially discover causal topic text 
contextual text mining mining causal topic time series supervision text-based prediction topic mining & analysis analysis observed world real world text datum perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process & text text representation word association mining analysis 2 lecture be go continue discuss contextual text mining particular be go look time series context analyze text potentially discover causal topic 
text mining understand time series might cause stock market crash … time sept 11 attack clue companion news stream dow jone industrial average [ source yahoo finance ] 3 usual start motivation case hope use text mining understand time series see dow jone industrial average stock price curf will see sudden drop right one would interested know might cause stock market crash well know background might able figure look time stamp datum help us think question get clue companion news stream lot news datum generate period might actually discover crash happened time september 11 attack s time sudden rise topic september 11 happened news article 
analysis presidential prediction market might cause sudden drop price candidate “ matter ” election tax cut … time clue companion news stream 4 s another scenario want analyze presidential election time series presidential prediction market example write trunk market would stock candidate believe one candidate win tend buy stock candidate cause price candidate increase s nice way actual survey person s opinion candidate suppose see something drop price one candidate might also want know might cause sudden drop social science study might interested know method election issue really matter person case look companion news stream ask question clue news stream might provide insight example might discover mention tax cut increase since point maybe s related drop price 
joint analysis text time series discover “ causal topic ” • input – time series – text datum produce similar time period ( text stream ) • output – topic whose coverage text stream strong correlation time series ( “ causal ” topic ) … tax cut gun control 5 case special case general problem joint analysis text time series datum discover causal topic input case time series plus text datum produce time period companion text stream different standard topic model text collection s see time series serve context output want generate topic whose coverage text stream strong correlation time series example whenever topic manage price tend go etc call topic causal topic course be strictly speaking causal topic never go able verify whether causal s true causal relationship s put causal quotation mark least correlate topic might potentially explain cause human certainly analyze topic understand issue better output would contain topic like topic modele hope topic regular topic topic certainly nt explain datum best text rather explain datum text meaning reprehend meaningful topic text cement also importantly correlated external hand series s give context understand solve problem let s first adjust solve problem reactive topic model example prsa apply text stream extension like cprsa contextual prsa discover topic correlation 
topic model apply text stream topic 1 government 03 response 02 city 02 topic 2 new 01 orlean 005 … donate 01 topic k relief 005 help 002 005 background k 004 003 … time 6 also discover coverage time one simple solution choose topic set strongest correlation external time series approach go good awareness picture topic discover prsa lda mean choice topic limit know model try maximize likelihood text datum topic tend major topic explain text datum well aand necessarily correlated time series even get best one correlated topic might still interesting causal perspective work site better approach propose approach call iterative causal topic modele idea iterative adjustment topic discover topic model used time series induce product s illustration work work take text stream input 
iterative causal topic modele [ kim et al 13 ] topic modele text stream sept 2001 oct 2001 … feedback prior causal topic topic 1 topic 2 topic 1 topic 2 topic 3 topic 4 topic 3 topic 4 split word topic 1 pos w1 + w3 + topic 1 neg w2 w4 - non-text time series zoom word level topic 1 w1 w2 w3 w4 w5 … + + - causal word 7 apply regular topic modele generate number topic let s say four topic show be go use external time series assess topic causally related correlated external time series something rank might think topic one topic four correlated topic two topic three can stop would like simple approach talk earlier get topic call causal topic also explain topic unlikely good general topic explain whole text connection necessary best topic correlated time series approach first zoom word level look word top rank word list topic let s say take topic 1 target examine know topic 1 correlated time series least best can get set topic far be go look word topic top word topic correlated time series must word highly correlated time series example might discover w1 w3 positively correlated time series w2 w4 negatively correlated topic s good mix word different correlation separate word go get red word indicate positive correlation w1 w3 be go also get another sub topic want represent negatively correlated word w2 w4 subtopic variation topic base correlation analysis topic still quite related original topic topic already deviate use time series information bias selection word sense well expect sense correlated time series original topic topic 1 mixed word separate two subtopic expect better coherent time series however may coherent mention idea go back topic model used prior guide topic modele s say ask topic model discover topic similar two subtopic cause bias toward correlate topic time series course apply topic model get another generation topic run base time series set highly correlated topic analyze component work topic try analyzeword level correlation get even correlated subtopic fed process prior drive topic model discovery 
heuristic optimization causality + coherence 8 whole process heuristic way optimize causality coherence s ultimate goal right see pure topic model good maximize topic coherence topic meaningful use causality test correlation measure might get set word strongly correlate time series may necessarily mean anything might cementric connect would extreme top ideal get causal topic s score high topic coherence also causal relation approach regard alternate way maximize sine engine apply topic model be maximize coherence decompose topic model word set word strong correlated time series select strongly correlated word time series push model back causal dimension make better causal score apply select word prior guide topic modele go back optimize coherence topic model ensure next generation topic coherent iterate be optimized way show picture 
measure causality ( correlation ) … topic i government 03 response 02 xt xt cause yt … external time series ( eg stock price ) time causality ( xt yt ) = correlation ( xt yt ) = yt granger causality test often useful [ seth 07 ] 9 think component nt see framework measure causality rest talk let s little bit discussion show let s say topic government response talk get coverage topic time time series x sub t also give time series represent external information s non text time series sub t s stock price question xt cause yt well word want match causality relation two maybe measure correlation two many measure use framework example pair correlation common use measure get consider time lag try capture causal relation used somewhat past datum used datum past try correlate datum point represent future example introduce lag hopefully capture causal relation even used correlation measure like person correlation common use measure causality granger causality test idea test actually quite simple basically be go regressive model use history information predict best can without information be go build model be go add history information x model see improve prediction y statistically significant difference say x causal inference otherwise would nt causal improvement prediction y hand difference insignificant would mean x really cause relation s basic idea nt time explain detail can read would read cite reference know measure s convenient used measure 
topic ny time correlated stock [ kim et al 13 ] june 2000 ~ 2011 aamrq ( american airline ) aapl ( apple ) russia russian putin europe european germany bush gore presidential polouse court judge airline airport air unite trade terrorism food food cheese net scott basketball tennis william open award gay boy moss minnesota chechnya paid notice st russia russian europe olympic game olympic ms oil ford price black fashion black computer technology software internet com web football giant jet japan japanese plane … topic bias toward time series 10 many application next let s look simple result generate approach datum new york time time period june 2000 december time series used stock price two company american airline apple goal see inject sum time series contest whether actually get topic wise time series imagine nt use input nt use context topic new york time discover prsa would general topic person talk news right major topic news event see topic indeed bias toward time series particularly look underline word american airline result see airline airport air unite trade terrorism etc clearly topic correlated external time series right side see topic clearly related apple right see computer technology software internet com web etc mean time series effectively serve context bias discovery topic another perspective result help us person talk case person person talk topic might correlated stock price topic serve start point person look issue will find true causal relation 
major topic 2000 presidential election [ kim et al 13 ] top three word significant topic ny time tax cut 1 screen pataki guiliani enthusiasm door symbolic oil energy price news w top pre al vice love tucker present partial abortion privatization court supreme abortion gun control nra text ny time ( may 2000 - 2000 ) time series iowa electronic market http issue know important 2000 presidential election 11 result analyze presidential election time series time series datum iowa electronic market s prediction market datum new york time may 2000 october s 2000 presidential campaign election see top three word significant topic new york time look topic indeed quite related campaign actually issue much related important issue presidential election mention text datum filter used article mention candidate name s subset news article different previous experiment result clearly show approach uncover important issue presidential election tax cut oil energy abortion gun control know important issue presidential election support literature political science also discuss wikipedia right basically result show approach effectively discover possibly causal topic base time series datum 
suggest read • [ kim et al 13 ] hyun duk kim malu castellano meichun hsu chengxiang zhai thomas rietz daniel diermeier mining causal topic text datum iterative topic modele time series feedback proceedings 22nd acm international conference information & knowledge management ( cikm 2013 ) acm new york ny usa 885-890 25055152505612 • [ seth 07 ] anil seth granger causality 2007 scholarpedia 2 ( 7 ) 1667 doi scholarpedia1667 12 two suggest reading one paper iterative topic modele time series feedback find detail approach work second one read granger casuality text 
summary text-based prediction • text-based prediction useful “ big datum ” application – infer new knowledge world – optimize decision make • text datum often combine non-text datum prediction – joint analysis text non-text necessary useful – non-text datum provide context mining text datum ( contextual text mining ) – text datum help interpret pattern discover non-text datum ( pattern annotation ) • active research topic many open challenge 13 end let s summarize discussion text-based prediction text-based prediction generally useful big datum application involve text help us inform new knowledge world knowledge go beyond s discuss text result also support optimize decision make wider spread application text datum often combine non-text datum prediction purpose prediction purpose generally would like combine non-text datum text datum together much cruel possible prediction result analysis text non-text necessary s also useful analyze text datum together non-text datum see help non-text datum provide context mining text datum discuss number technique contextual text mining hand text datum also help interpret pattern discover non-text datum call pattern annotation general active research topic new paper publish also many open challenge solve [ music ] 

1 lecture summary whole course 
topic cover course + non-text datum text-based prediction topic mining analysis observed world real world text datum + context perceive express ( perspective ) ( english ) opinion mining sentiment analysis natural language process text representation word association mining analysis 2 first let s revisit topic cover course begin talk natural language process enrich text representation talk mine knowledge language natural language used express s observe world text datum particular talk mine word association talk analyze topic text discover topic analyze regard knowledge observed world talk mine knowledge observer particularly talk mine opinion sentiment analysis finally talk text-based prediction predict value real world variable base text datum discuss also discuss role non-text datum contribute additional predictor prediction problem also provide context analyze text datum particular talk use context analyze topic key high-level take away message cost go go major topic point key take-away message remember first nlp text representation realize nlp always important text replication enrich text representation nlp better text representation enable accurate knowledge discovery discover deeper knowledge bury text however current estate art natural energy process still robust enough result robust text mining technology today tend base world [ inaudible ] tend rely lot statistical analysis have discuss course may recall have mostly used word base representation have rely lot statistical technique statistical learn technique particularly word-association mining analysis important point first introduce two concept two basic complementary relation word paradigmatic syntagmatic relation actually general relation element sequence take meaning element occur similar context sequence element tend co-occur relation might also meaningful sequence datum also talk lot test similarity discuss discover paradynamic similarity compare context word discover word share similar context point level talk represent text datum vector space model talk retrieval technique bm25 measure similarity text assign weight term tf-idf weighting et cetera part well-connected text retrieval technique relevant also next point co-occurrence analysis text introduce information theory concept entropy conditional entropy mutual information useful measure co-occurrence word also useful analyze kind datum useful example feature selection text categorization well another important concept good know talk topic mining analysis 
key high-level take-away message probabilistic topic model ( plsa lda ) generative model ml estimate em 38topic mining & analysis text cluster model vs similarity-based text categorization generative vs discriminative evaluation categorization observed world cluster andtext datum joint mining ofprediction text non-text text-based contextual plsa netplsa causal topic mining real world perceive ( perspective ) knowledge natural language express nlp  text representation  discovery process & text robust tm = word-base rep + statistical analysis representation ( english ) sentiment classification ordinal opinion miningregression & latent aspect rating analysisanalysis sentiment 3 syntagmatic relation 2paradigmatic word association 4 text similarity mining & analysisvector space bm25 co-occurrence analysis entropy mi 3 s introduce probabilistic topic model spend lot time explain basic topic model plsa detail basic understand lda theoretically opinion model enough time really go depth introduce lda practice plsa seem effective lda s simpler implement s also efficient part wilson video general concept would useful know one generative model general method modele text datum modele kind datum well talk maximum life erase datum em algorithm solve problem compute maximum estimator general technique tend useful scenario well talk text cluster text categorization two important build block text mining application system text cluster talk solve problem used slightly different mixture module probabilistic topic model also prefer view similarity base approach test cuss word categorization also talk two kind approach one generative classify rely base word infer condition probability category give text datum deeper will introduce use [ inaudible ] base detail practical use technique lot text capitalization task also introduce discriminative classifier particularly logistical regression nearest labor sbn also important popular useful text capitalization well part will also discuss evaluate result evaluation quite important match use nt really reflect volatility method would give mislead result important get variation right talk variation categorization detail lot specific measure talk sentiment analysis paradigm s introduce sentiment classification problem although s special case text recalculation talk extend improve text recalculation method used sophisticated feature would need sentiment analysis review common use complex feature text analysis also talk capture order category sentiment classification particular introduce ordinal logistical regression also talk latent aspect rating analysis unsupervised way used generative model understand review datum detail particular allow us understand compose rating reviewer different aspect topic give text reviews overall rating method allow even rating different aspect also allow us infer viewer layer weight aspect aspect important viewer reveal well enable lot interesting application finally discussion prediction mainly talk joint mining text non text datum important prediction particularly talk text datum help non-text datum vice versa case used non-text datum help text datum analysis talk contextual text mining introduce contextual plsa generalize generalized model plsa allow us incorporate context variable time location general way allow us reveal lot interesting topic pattern text datum also introduce net plsa case used social network network general text datum help analyze puppet finally talk used context mine potentially causal topic text layer way used text help interpret pattern discover lam text datum really discuss anything detail provide reference stress s important direction know want build practical text mining system understand interpret pattern quite important summary key take away message hope useful build text mining application start algorithms provide good basis read research paper know allowance organism invent new hour 
learn next • natural language process – foundation text-based application – nlp  deeper knowledge discovery • statistical machine learn – backbone technique nlp text analysis – key predictive modele “ big datum ” application • datum mining – general datum mining algorithms always apply text • information retrieval – essential system component text-based application ( human loop ) – technique useful text datum mining 4 know topic would suggest look area depth short period time course can touch basic concept basic principle text mining emphasize coverage practical algorithms cost cover algorithms many case omit discussion lot algorithms learn subject definitely learn natural language process foundation text base application nlp better additional text get deeper knowledge discover important second area look statistical machine learn technique backbone technique text analysis application also nlp lot nlp technique nowadays actually base supervised machinery important key also understand advance nlp technique naturally provide tool text analysis general particularly interesting area call deep learn attract lot attention recently also show promise many application area especially speech vision apply text datum well example recently work used deep learn segment analysis achieve better accuracy s one example [ inaudible ] technique nt able cover s also important area emerge status learn water bare technique learn better recognition word better recognition allow confuse similarity word see provide directly way discover paradigmatic relation word result person get far impressive s another promising technique time touch course whether new technique would lead practical useful technique work much better current technology still open question examine serious evaluation do yet example examine practical value word embedding word similarity basic evaluation nevertheless advanced technique surely make impact text mining future important know statistical learn also key predictive modele crucial many big datum application talk predictive modele component mostly regression categorization technique another reason statistical learn important also suggest learn datum mining s simply general datum mining algorithms always apply text datum regard special case general datum many application datum mining technique particular example pattern discovery would useful generate interesting feature test analysis reason information network mining technique also used analyze text information work good know order develop effective text analysis technique finally also recommend learn text retrieval information retrieval search engine especially important interested build practical text application system search end would essential system component text-based application s text datum create human consume human best position understand text datum s important human loop big text datum application particular help text mining system two way one effectively reduce datum size large collection small collection relevant text datum matter particular interpretation provide way annotate explain parent knowledge providence discover knowledge figure whether discovery really reliable need go back original text verify search engine important moreover technique information retrieval example bm25 vector space also useful text datum mining mention know text retrieval will see many technique used another technique s used indexing technique enable quick response search engine s query technique useful build efficient text mining system well 
main technique harness big text datum text retrieval + text mining text retrieval course search engine text mining text retrieval big datum bigtext text datum small relevant datum small relevant datum knowledge many application 5 finally want remind big picture harness big text datum show begin semester general deal big text application system need two kind text text retrieval text mining text retrieval explain help convert big text datum small amount relevant datum particular problem also help provide knowledge provenance help interpret pattern later text mining analyze relevant datum discover actionable knowledge directly useful decision make many task course cover text mining s companion course call text retrieval search engine cover text retrieval nt take course would useful take especially interested build text cach system take course give complete set practical skill build system [ inaudible ] would like thank take course hope learn useful knowledge skill test mining [ inaudible ] see discussion lot opportunity kind technique also lot open channel hope use learn build lot use application benefit society also join research community discover new technique text mining benefit thank [ music ] 

welcome video hi everyone excited see board welcome course 
welcome lesson • • like nlp much • goal goal course • overview topic • blend linguistic deep learn want start lesson informal discussion course brief introduction area natural language process know might feel little hand wavy introduction actually hope course know exactly everything mentioned lesson ready let us get start 
name anna… • finishing phd word embedding topic model • teach nlp ysda msu • hands-on experience − word similarity query understand ( google ) − text mining classification ( yandex ) − dl rl dialogue state tracking ( google ) big nice team create course • one anna andrey alexey sergey – read page course name anna big nice team create course sergey alexey andrey one anna prepare material background computer science machine learn be apply background natural language process different way know different activity like research teach industry give rather different perspective area example come industry soon realize paper academia useful particular setting like large scale implementation noisy datum specific need business probably need build simple solution would work nicely specific setting okay course 
course curious… • ’ inside dialogue agent • machine translation work • nlp end hype begin think would one word characterize audience thought would word curious course curious person want know inside application example differently used machine translation know work dialogue agent popular nowadays inside know popularity certain application can nt bet example dialogue agent much hype around easy distinguish beautiful word something really work practice hopefully one outcome course would ability distinguish hype something really work 
course understand ’ inside • in-depth theoretical insight • review state-of-the-art approach − research production gain intuition • give task approach work practice • home assignment python • project build conversational chat-bot course rather in-depth want go detail several method nlp give ability distinguish hype method okay also cover real state-of-the-art approach research production already say can rather different approach another goal s little bit contradict go in-depth would big picture area feel like really important expertise like give task approach would work certain case intuition try discuss many different setting task possible cover approach obviously talk listen read practice get hands-on experience prepare material home assignment python popular nlp task like text classification duplicate detection name entity recognition other experience hand also home task help build project course would conversational chat-bot 
course probably course • want practical tip care ’ inside • prior background machine learn quick test hear follow • sum rule product rule likelihood maximization • training vs inference loss function • recurrent neural network experience python ( tensorflow ) feel like really important also see course nlp big obviously course feed everyone s need feel like want know black box implementation stock together build solution probably course also think good idea take machine learn deep learn course first fill name formula example quick test know recurrent neural network hear likelihood maximization take moment see comfortable word see whether need take example deep learn course specialization first go course also expect experience python probably nt experience tensorflow maybe okay good moment try go tutorial course can good reason go actually tensorflow really nice tutorial think nt problem hope still frighten hope ready journey nlp want start journey survey main approach 

main approach nlp would say three main group method nlp 
main approach nlp rule-based method • regular expression • context-free grammar • … probabilistic modele machine learn • likelihood maximization • linear classifier • … deep learn • recurrent neural network • convolutional neural network • … one group would rule-based approach example regular expression would go group another one would traditional machine learn last one would deep learn recently gain lot popularity nlp video want go three approach example one particular task get flavor 
semantic slot fill cfg context-free grammar • show → show | want | see … • flight → ( ) flight | flight • origin → city • destination → city • city → boston | san francisco | denver | washington parse show flight origin destination departdate show fligth boston san francisco tuesday https 29pdf task can call semantic slot fill see query bottom slide say show flight boston san francisco tuesday sequence word want find slot slot would destination departure date something like fill slot use different approach slide context-free grammar rule-based approach context-free grammar show would rule produce word example see non terminal word show produce word show see something like word example origin non terminal produce city city non terminal produce specific city list context-free grammar use parse datum get sequence say non terminal create certain word advantage disadvantage approach well approach usually do manually write rule linguist come write obviously time consume also record approach would nice well write possible city many language native right positive thing though would precision approach usually rule-based approach high precision low recall 
semantic slot fill crf training corpus orig dest date show flight boston san francisco tuesday feature engineering • word capitalize • word list city name • previous word • previous slot • … another approach would build machine learn system first need training datum need corpus markup sequence word know certain phrase certain text right like origin destination date training datum need feature engineering need create feature like example word capitalize word occur list city something like 
semantic slot fill crf probabilistic graphical model • conditional random field ( crf ) feature p ( tags|word ) = parameter training p ( tags|word ) max ⇥ inference tags⇤ = argmax p ( tags|word ) need define model probabilistic model would example produce probability text give word different kind model explore lot course generally model would parameter depend feature generate parameter model trained need take train datum fit model datum maximize probability see parameter way fix parameter model able apply model test datum inference apply find probable text word fix parameter right call inference test deployment something like general framework right perimeter train apply model similar thing happen deep learn approach 
semantic slot fill lstm • • • • big training corpus feature generation define model training inference orig orig show flight boston dest dest dest date date san francisco tuesday also stage usually stage feature generation be feed sequence word neural network go detail neural network time go detail show idea feed word one hot encoder vector one non zero element correspond number word vocabulary lot zero feed vector complicate neural network complicate architecture lot parameter feed perimeter apply network test datum get text model 
deep learn vs traditional nlp oh fuck sake dl person leave language alone stop say solve yoav goldberg need study traditional nlp • perform good enough many task example sequence labele • allow us blind hype example word2vec distributional semantic • help improve dl model example word alignment prior machine translation https @ an-adversarial-review-of-adversarial-generation-of-natural-language-409ac3378bd7 deep learn method perform really nice many task nlp sometimes feel like forget traditional approach reason forget well first reason would traditional method perform really nice obligation example sequence labele probabilistic modele discuss week two will get really good performance another reason would idea deep learn method really similar something happen area example word2vec method actually even deep learn inspire neural network really similar idea distributional semantic method week three course discuss another reason would sometimes use knowledge traditional approach improve model base deep learn example word alignment machine translation attention haney s neural network similar see week four 
deep learn vs traditional nlp need study dl nlp • provide state-of-the-art performance many task example machine translation • research nlp happen example paper acl emnlp etc • look fancy everyone want know j way • study two approach parallel deep learn method indeed fancy lot research publication current conference look like area go future obviously need course well well think parallel every task will traditional deep learn approach study one one video next video see plan next week s 

brief overview next week 
week 1 text classification task • predict tag category • predict sentiment review • filter spam e-mail cat gray dark 
week 2 predict word sequence language model need chat-bot speech recognition machine translation summarization… predict tag word sequence • part-of-speech tag • name entity • semantic slot 
week 3 represent meaning word sentence text shall know word company keep ( firth 1957 ) • word embedding • sentence embedding • topic model dmity malkov https cartoon-word2vec-espresso-cappuccinohtml 
week 3 need • search question answer ranking • label propagation word similarity graph https graph-powered-machine-learning-at-googlehtml 
week 4 sequence sequence task • machine translation • summarization simplification • conversational chat-bot 
week 4 sequence sequence task • machine translation • summarization simplification • conversational chat-bot 
week 4 sequence sequence task • machine translation • summarization simplification • conversational chat-bot 
week 4 sequence sequence task • machine translation • summarization simplification • conversational chat-bot 
week 4 sequence sequence task • machine translation • summarization simplification • conversational chat-bot 
week 4 sequence sequence task • machine translation • summarization simplification • conversational chat-bot 
week 4 sequence sequence task • machine translation • summarization simplification • conversational chat-bot 
week 4 sequence sequence task • machine translation • summarization simplification • conversational chat-bot 
week 5 dialogue agent become popular • goal-oriented ( eg help call-center ) • conversational ( eg entertainment ) project build conversational chat-bot assist stackoverflow search 

linguistic knowledge nlp [ music ] video want remind nlp area mathematics also linguistic really important remember 
nlp pyramid pragmatic semantic syntax morphology natural language process pyramid first slide picture really popular many introduction nlp think also need briefly cover let us say give sentence different stage analysis sentence first stage call morphological stage would different form word example care part speech text care different case gender tense everything go single word sentence next stage syntactical analysis different relation word sentence example know object subject next stage know synthetic structure would semantic semantic meaning see go higher higher abstraction go symbol meaning pragmatic would highest level abstraction one reason cover build block many detail later course use nice log book implementation low level stage 
library tool • nltk − small useful dataset markup − preprocess tool tokenization normalization… − pre-train model pos-tag parsing… • stanford parser • spacy python cython library nlp • gensim python library text analysis eg word embedding topic modele • mallet java-based library eg classification sequence tag topic modele • … example morphological syntactical analysis might try used analytical library really convenient tool python please feel free investigate another thing want mention stanford parser parser synthetic analysis provide different option really lot different model build gensim mallet would high level abstraction example subclassification problem think semantic topic model word embedding representation discuss later week three 
linguistic knowledge • idea evaluation • external resource wordnet babelnet etc http synset word=nlp & lang=en & details=1 & orig=nlp another thing also come linguistic part area different type relation word linguist know really lot can type knowledge find extrinsic resource example wordnet resource tell example hierarchical relationship like fruit particular type fruit like peach apple orange relation would call hyponym hypernym also relationship like part whole example wheel car type relationship call meronyms type relationship find wordnet resource slide picture another resource babelnet babelnet resource multilingual find concept different language nice relation concept example type nop see part speech take test click test can see nearest neighbor space concept example see viterbi algorithm baum-welch algorithm somewhere close week two course will know indeed related task takeaway slide would remember extrinsic resource nicely used application example used 
linguistic knowledge + deep learn task question answer reasoning linguistic link co-reference ( red ) hypernyms ( green ) method dag-lstm dhingra et al linguistic knowledge memory recurrent neural network 2017 rather complicate task call reasoning say story natural language example mary get football go kitchen left ball okay story question story football answer question machine need somehow understand something right way build system would base deep learn might hear lstm network particular type recurrent neural network see sequential transition edge datum representation also edge red age tell coreference coreference another linguistic type relation word say like mary right substitute mary example football ball mentioned twice green think hypernym relationship briefly mentioned football particular type ball right know word relationship add additional edge datum structure call dag-lstm dynamic acyclic graph-lstm try utilize edge okay be go cover dag-lstm model want see way use linguistic knowledge need improve performance particular question answer task example 
syntax dependency tree obj sbj pmod detmod shot nmod elephant detmod pajama rest video want cover another example linguistic information used system syntax let us detail syntax represent usually kind tree see dependency tree say example word shot main word subject object elephant elephant modifier right dependency word usually obtain syntactic parser 
syntax constituency tree np vp vp v shot pp np p det n elephant np det n pajama another way represent syntax would so-called constituency tree see sentence bottom slide parse bottom top get hierarchical structure know elephant determinant noun respectively merge get noun phrase also merge verb short yet verb phrase merge another subtree get big verb phrase finally verb phrase plus noun phrase give whole sentence actually stop moment parse whole structure bottom top say enough know example picture particular subtree useful first call shallow parse used example name entity recognition name entity likely noun phrase altogether right new york city would nice noun phrase sentence help also help whole tree task 
sentiment analysis socher et al recursive deep model semantic compositionality sentiment treebank 2013 example task would sentiment analysis sentiment analysis treat reviews piece text try predict whether positive negative maybe neutral see pluse minuse zero stand sentiment sentence right try parse syntax get nice subtree see previous slide idea know sentiment particular word example know humor good try merge sentiment produce sentiment whole phrase okay intelligent humor good give good sentiment sentence get good sentiment result negative sentiment whole sentence rather advanced approach call recursive neural network dynamic acyclic graph neural network sometimes useful many practical case enough simple classification work rest week colleague discuss classification task example sentiment analysis many many detail [ music ] 

area ’ fun us hi name andre week focus text classification problem although method overview apply text regression well easier keep mind text classification problem example problem take sentiment analysis problem text review input output produce class sentiment example can two class like positive negative can fine grained like positive somewhat positive neutral somewhat negative negative forth example positive review follow ` hotel really beautiful nice helpful service front desk read understand positive review negative review ` problem get wi-fi work pool area occupied young party animal area nt fun us s easy us read text understand whether positive negative sentiment computer much difficult 
text preprocess will first start text preprocess 
text think text sequence • character • word • phrase name entity • sentence • paragraph • … first thing ask text think text sequence sequence different thing sequence character low level representation text think sequence word maybe high level feature like phrase like ` nt really like can phrase name entity like history museum museum history can like bigger chunk like sentence paragraph forth 
word seem natural think text sequence word • word meaningful sequence character find boundary word • english split sentence space punctuation input friend roman countryman lend ears output friend roman countryman lend ears • german compound word written without space − “ rechtsschutzversicherungsgesellschaften ” stand “ insurance company provide legal protection ” • japanese space − butyoucanstillreaditright let s start word let s denote word seem natural think text sequence word think word meaningful sequence character meaning usually like take english language example usually easy find boundary word english split sentence space punctuation left word let s look example friend roman countryman lend ears comma semicolon space split get word ready analysis like friend roman countryman forth can difficult german german compound word written without space longest word still use follow see slide actually stand insurance company provide legal protection analysis text can beneficial split compound word separate word every one actually make sense be written form nt space japanese language different story nt space person still read right even look example end slide actually read sentence english nt space s problem human 
tokenization tokenization process split input sequence so-called token • think token useful unit semantic process • word sentence paragraph etc example simple whitespace tokenizer • nltktokenizewhitespacetokenizer andrew ’ text ’ • problem “ ” “ ” different token meaning process splitting input text meaningful chunk call tokenization chunk actually call token think token useful unit semantic process word sentence paragraph anything else let s look example simple whitespacetokenizer split input sequence white space can space character visible actually find whitespacetokenizer python library nltk let s take example text say andrew s text nt split whitespace problem see different token left tokenization problem last token question mark actually meaning token without question mark try compare different token might desirable effect might want merge two token essentially meaning well text comma token simply text 
tokenization let ’ try also split punctuation • nltktokenizewordpuncttokenizer andrew ’ text ’ • problem “ ” “ ” “ ” meaningful come set rule • nltktokenizetreebankwordtokenizer andrew ’ text n ’ • “ ’ ” “ n ’ ” meaningful process let s try also split punctuation purpose tokenizer ready nltk library well time get something like problem thing apostrophe different token separate token well problem token actually nt much meaning nt make sense analyze single letter s make sense combine apostrophe previous word actually come set rule heuristic find treebanktokenizer actually used grammar rule english language make tokenization actually make sense analysis close perfect tokenization want english language andrew text different token apostrophe left untouched different token actually make much sense well n apostrophe t n apostrophe actually mean like negate last token 
python tokenization example http let s look python example import nltk bunch text instantiate tokenizer like whitespace tokenizer call tokenize list token use treebanktokenizer wordpuncttokenizer review previously s pretty easy tokenization python 
token normalization may want token different form word • wolf wolf à wolf • talk talk à talk stem • process remove replace suffix get root form word call stem • usually refer heuristic chop suffix lemmatization • usually refer thing properly use vocabulary morphological analysis • return base dictionary form word know lemma next thing might want token normalization may want token different form word like word wolf wolf actually thing right want merge token single one wolf different example like talk talk talk maybe s talk nt really care end word process normalize word call stem lemmatization stem process remove replace suffix get root form word call stem usually refer heuristic chop suffix replace another story lemmatization person talk lemmatization usually refer thing properly use vocabulary morphological analysis time return base dictionary form word know lemma 
stem example porter ’ stemmer • 5 heuristic phase word reduction apply sequentially • example phase 1 rule rule ss → ss ies → ss → ss → example caress → caress pony → poni caress → caress cat → cat • nltkstemporterstemmer • example − foot à foot cat à cat − wolf à wolv talk à talk • problem fail irregular form produce non-word let s see example work stem example well-known porter s stemmer like oldest stemmer english language five heuristic phase word reduction apply sequentially let show example phase one rule pretty simple rule think regular expression see combination character like ss replace ss strip es end may work word like caress s successfully reduce caress another rule replace ies i pony actually work way would get result valid word poni nt end end i problem actually work practice well-known stemmer find nltk library well let s see example might work foot produce foot nt know anything irregular form wolf produce wolv valid word still useful analysis cat become cat talk become talk problem obvious fail regular form produce non-word can much problem actually 
lemmatization example wordnet lemmatizer • used wordnet database lookup lemma • nltkstemwordnetlemmatizer • example − foot à foot cat à cat − wolf à wolf talk à talk • problem form reduce • takeaway need try stem lemmatization choose best task example lemmatization purpose use wordnet lemmatizer used wordnet database lookup lemma also find nltk library example follow time word foot actually successfully reduce normalize form foot database know word english language irregular form take wolf become wolf cat become cat talk become talk nothing change problem lemmatizer actually nt really use form noun might like normal form lemma can singular form noun verb different story might actually prevent merge token meaning takeaway follow need try stem lemmatization choose work best task 
python stem example let s look python example import nltk library take bunch text first thing need tokenize purpose let s use treebank tokenizer produce list token instantiate porter stemmer wordnet lemmatizer call stem lemmatize token text get result review previous slide pretty easy python nltk 
normalization normalize capital letter • us us à us ( pronoun ) • us us ( can pronoun country ) • use heuristic − lowercase begin sentence − lowercase word title − leave mid-sentence word • use machine learn retrieve true case à hard acronyms • eta eta eta à eta • write bunch regular expression à hard next normalize token bunch different problem let s review first problem capital letter us us written different form word pronounce safe reduce word us another story us us capital form can pronoun country need distinguish somehow problem remember always keep mind be text classification work let s say sentiment analysis easy imagine review written cap lock like capital letter us can mean actually us pronoun country tricky part use heuristic english language luckily lowercase begin sentence know every sentence start capital letter likely need lowercase also lowercase word see title english language title written form every word capitalize strip else leave mid-sentence word be capitalize somewhere inside sentence maybe mean name name entity leave go much harder way use machine learn retrieve true case scope lecture might harder problem original problem sentiment analysis another type normalization use token normalize acronyms like eta e eta written capital form thing acronym eta stand estimate time arrival person might frequently use reviews chat anywhere else actually write bunch regular expression capture different representation acronym will normalize pretty hard thing must think possible form advance acronyms want normalize 
summary • • • • • think text sequence token tokenization process extract token normalize token used stem lemmatization also normalize case acronyms next video transform extract token feature model let s summarize think text sequence token tokenization process extract token token like meaningful part meaningful chunk text can word sentence something bigger normalize token used either stem lemmatization actually try decide work best also normalize case acronyms bunch different thing next video transform extract token feature model 

transform token feature hi lecture transform token feature 
bag word ( bow ) let ’ count occurrence particular token text • motivation ’ look marker word like “ excellent ” “ disappoint ” • token feature column call text vectorization good movie like good movie 1 1 0 0 0 0 good movie 1 1 1 1 0 0 like 0 0 1 0 1 1 • problem − loose word order hence name “ bag word ” − counter normalize best way bag word let s count occurrence particular token text motivation follow be actually look marker word like excellent disappoint want detect word make decision base absence presence particular word might work let s take example three reviews like good movie good movie like let s take possible word token document token let s introduce new feature column correspond particular word pretty huge metric number translate text vector metric row metric let s take example good movie review word good present text put one column correspond word come word movie put one second column show word actually see text nt word rest zero really long vector sparse sense lot zero good movie four one rest zero forth process call text vectorization actually replace text huge vector number dimension vector correspond certain token database actually see problem first one lose word order actually shuffle word representation right stay s s call bag word s bag be order come order different problem counter normalize 
let ’ preserve order count token pair triplet etc • also know n-grams − 1-grams token − 2-grams token pair − … good movie movie 1 1 0 good movie … 0 … good movie 1 1 0 1 … like 0 0 1 0 … • problem − many feature let s solve two problem let s start preserve order actually easily come idea look token pair triplet different combination approach also call extract n-grams one gram stand token two gram stand token pair forth let s look might work three reviews nt column correspond token also column correspond let s say token pair good movie review translate vector one column corresponding token pair good movie movie good forth way preserve local word order hope help us analyze text better problem obvious though representation many feature let s say 100000 word database try take pair word actually come huge number exponentially grow number consecutive word want analyze problem 
remove n-grams let ’ remove n-grams feature base occurrence frequency document corpus overcome problem actually remove n-grams let s remove n-grams feature base occurrence frequency document corpus 
remove n-grams let ’ remove n-grams feature base occurrence frequency document corpus • high frequency n-grams − article preposition etc ( example ) − call stop-word ’ help us discriminate text à remove • low frequency n-grams − typos rare n-grams − ’ need either otherwise likely overfit • medium frequency n-grams − good n-grams actually see high frequency n-grams well low frequency n-grams show nt need n-grams high frequency take text take high frequency n-grams see almost document english language would article preposition stuff like be grammatical structure nt much meaning call stop-word wo nt help us discriminate text pretty easily remove another story low frequency n-grams look low frequency n-grams actually find typos person type mistake rare n-grams s usually see reviews bad model nt remove token likely overfeed would good feature future classifier see okay review typo like two reviews typo s pretty clear whether s positive negative learn independence actually nt really need last one medium frequency n-grams really good n-grams contain n-grams stop-word typos actually look 
’ lot medium frequency n-grams • prove useful look n-gram frequency corpus filter bad n-grams • use ranking medium frequency n-grams • idea n-gram smaller frequency discriminate capture specific issue review problem be lot medium frequency n-grams prove useful look n-gram frequency corpus filter bad n-grams use frequency ranking medium frequency n-grams maybe decide medium frequency n-gram better worse base frequency idea follow n-gram smaller frequency discriminate capture specific issue review let s say somebody happy wi-fi let s say say wi-fi break often n-gram wi-fi break frequent database corpus document actually highlight specific issue need look closer 
tf-idf term frequency ( tf ) • tf ( 𝑡 𝑑 ) – frequency term ( n-gram ) 𝑡 document 𝑑 • variant weighting scheme tf weight binary 0 1 raw count ft term frequency ft log normalization 1 + log ( ft ) https tf–idf x t0 2d ft0 utilize idea introduce notion first like term frequency denote tf frequency term t term n-gram token anything like document d different option count term frequency first easiest one binary actually take zero one base fact whether token absent text present different option take raw count many time have see term document let s denote f take term frequency actually look count term see document normalize counter sum one kind probability distribution token take f divide sum fs token document one useful scheme logarithmic normalization take logarithm count actually introduce logarithmic scale counter might help solve task better s term frequency use follow slide 
tf-idf inverse document frequency ( idf ) • 𝑁 = | – total number document corpus • 𝑑 ∈ 𝐷 𝑡 ∈ 𝑑 – number document term 𝑡 appear • idf 𝑡 𝐷 = log 4 5∈ 6 7∈ 5 tf-idf • tfidf 𝑡 𝑑 𝐷 = tf 𝑡 𝑑 ⋅ idf 𝑡 𝐷 • high weight tf-idf reach high term frequency ( give document ) low document frequency term whole collection document another thing inverse document frequency let denote capital n total number document corpus corpus capital set document let s look many document corpus contain specific term second line size set actually mean number document term appear think document frequency would take number document term appear divide total number document frequency term document want take inverse argument frequency swap ratio take logarithm thing call inverse document frequency logarithm n number document term appear used two thing idf term frequency actually come tf-idf value need term document corpus calculate work like follow take term frequency term document multiply inverse document frequency term document let s see actually make sense something like high weight tf-idf reach high term frequency give document low document frequency term whole collection document precisely idea want follow want find frequent issue reviews frequent whole data-set specific issue want highlight 
better bow • replace counter tf-idf • normalize result row-wise ( divide 𝐿𝟐 norm ) good movie movie … good movie 017 017 0 … good movie 017 017 0 … like 0 0 047 … let s see might work replace counter bag word representation tf-idf value also normalize result row-wise normalize row example divide l2 norm sum number go anyway actually get result counter real value let s look example good movie two gram appear two document collection pretty frequent two gram s value 017 actually lower 047 get 047 two gram actually two gram happened one review can specific issue want highlight want bigger value feature 
python tf-idf example let s look might work python python use scikit learn library import tf-idf vectorizer let remind vectorization mean replace text huge vector lot zero value zero precisely value correspond token see text let s take example five different text like small movie reviews instantiate tf-idf vectoriser useful argument pass like mean df stand minimum document frequency essentially cutoff threshold low frequency gram want throw away actually threshold maximum number document have see token do strip away stop word time scikit learn library actually bust argument ratio document real value number document have see last argument n-gram range actually tell tf-idf vectorizer n-grams used back word representation scenario take one gram two gram vectorized text get something like possible two gram one gram filter follow look reviews see happened also see real value matter actually tf-idf value row normalize norm one 
summary • • • • ’ make simple counter feature bag word manner add n-grams replace counter tf-idf value next video train first model top feature let s summarize have make actually simple counter feature bag word manner replace text huge vector counter add n-grams try preserve local order see actually improve quality text classification replace counter s tf-idf value usually give performance boost well next video train first model top feature 

first text classification model [ music ] video talk first text classification model top feature describe 
sentiment classification imdb movie reviews dataset • http • contain 25000 positive 25000 negative reviews • • • • • contain 30 reviews per movie least 7 star 10 à positive ( label = 1 ) 4 star 10 à negative ( label = 0 ) 50 test split evaluation accuracy let s continue sentiment classification actually take imdb movie reviews dataset download freely available contain 25000 positive 25000 negative reviews dataset appear actually look imdb website see person write reviews actually also provide number star one star ten star actually rate movie write review take reviews imdb website actually use dataset text classification text number star actually think star sentiment least seven star label positive sentiment four star mean bad movie particular person negative sentiment s get dataset sentiment classification free contain 30 reviews per movie make less bias particular movie dataset also provide 50 train test split future researcher use split reproduce result enhance model evaluation use accuracy actually happen number positive negative reviews dataset balanced term size class evaluate accuracy 
sentiment classification feature bag 1-grams tf-idf value • 25000 row 74849 column training • extremely sparse feature matrix – 998 % zero okay let s start first model let s take feature let s take bag 1-grams tf-idf value result matrix feature 25000 row 75000 column pretty huge feature matrix extremely sparse look many 0s see 998 % value matrix 0s actually apply restriction model use top feature 
sentiment classification model logistic regression • p ( = 1|x ) = ( w x ) • linear classification model • handle sparse datum • fast train • weight interpreted 𝜎 ( 𝑥 ) model usable feature logistic regression work like follow try predict probability review positive one give feature give model particular review feature use let remind vector tf-idf value actually find weight every feature bag force representation multiply value tf-idf value weight sum thing pass sigmoid activation function s get logistic regression model s actually linear classification model s good since linear handle sparse datum s really fast train s weight get training interpreted let s look sigmoid graph bottom slide linear combination close 0 mean sigmoid output probability review positive really nt know whether s positive negative linear combination argument sigmoid function start become positive go away zero see probability review positive actually grow really fast mean get weight feature positive weight likely correspond word positive take negative weight correspond word negative like disgusting awful okay logistic regression work feature interpret 
sentiment classification logistic regression bag 1-grams tf-idf • accuracy test set 885 % • let ’ look learnt weight vs top positive top negative let s train logistic regression bag 1-grams tf-idf value actually see accuracy test set 885 % huge jump random classifier output 50 % accuracy let s look learnt feature linear model interpreted look top positive weight see word great excellent perfect best wonderful s really cool model capture sentiment sentiment word know nothing english language know example provide take top negative way see word like worst awful bad waste bore forth word clearly negative sentiment model learnt example pretty cool 
better sentiment classification let ’ try add 2-grams • throw away n-grams see less 5 time • 25000 row 156821 column training let s try make model little bit better know let s introduce 2-grams model move throw away n-grams frequent see let s say less 5 time n-grams likely either typos like person nt say like actually nt make sense look feature likely overfeed want throw away introduce 2-grams threshold minimum frequency actually get number dimension feature matrix follow 25000 pretty huge matrix still use linear model work 
better sentiment classification logistic regression bag 12-grams tf-idf • accuracy test set 899 % ( 15 % ) • let ’ look learnt weight vs near top positive near top negative let s train logistical regression bag 1 2-grams tf-idf value actually observe accuracy test set bump 15 accuracy boost close 90 % accuracy let s look learnt weight look top positive weight see 2-grams actually used model look 2-grams like well worth better think 2-grams positive sentiment look contrary top negative weight see worst another 2-gram used model predict final sentiment might think okay nt make sense worst worst thing well well worth worth maybe 15 % improvement accuracy actually provide addition 2-grams model either believe actually increase performance 
make even better play around tokenization • special token like emoji ” ) ” “ ” help try normalize token • add stem lemmatization try different model • svm naïve baye … throw bow away use deep learn • https 151208183pdf • accuracy test set 2016 9214 % ( 25 % ) make even better play around tokenization reviews person use different stuff like emojis use smile written text usually use bunch exclamation mark lot exclamation mark actually look sequence look different token actually introduce model maybe get better sentiment classification like smile face better angry face use also try normalize token apply stem lemmatization try different model like svm naive baye model handle sparse feature another way throw bag word away use deep learn technique squeeze maximum accuracy dataset 2016 accuracy particular dataset close 92 % 25 % improvement best model get bag word 2-grams might seem like good improvement actually make sense task get lot money even 1 % improvement like ad click prediction anything like 
summary • bag word simple linear model actually work text • accuracy gain deep learn model mind blow sentiment classification • next video ’ look spam filter task let s summarize bag word simple linear model feature actually work add 2-grams do free get better model accuracy gain deep learn model mind-blow still might consider used deep learn technique solve problem sentiment classification next video look spam filter task another example task classification handled different way [ music ] 

spam filter task hi video will talk spam filter task 
mapping n-grams feature index dataset small store { n-gram à feature index } hash map huge dataset problem • let ’ say 1 tb text distribute 10 computer • need vectorize text • maintain { n-gram à feature index } mapping − may fit memory one machine − hard synchronize • easier way hashing { n-gram à hash ( n-gram ) % 220 } − collision work practice − sklearnfeature_extractiontexthashingvectorizer − implement vowpal wabbit library let remind want use bag-of-words representation every n-gram text actually need find feature index index column input value let s say tf idf value purpose need maintain correspondence n-gram feature index usually use hash map dictionary python let s assume datum set huge s become problem let s say one terabyte text distribute 10 computer need vectorize text need replace text vector tf idf value actually maintain correspondence n-gram feature index become problem 10 computer thing first problem hash map actually fit memory one machine mean need kind database store store correspondence old machine use database nt scale well another problem difficult synchronize hash map new n-gram appear introduce new index 10 machine parallel mean synchronize somehow say okay let s say be machine number one find new n-gram be take next free index hash map add correspondence hash map particular n-gram converted feature index machine well machine somehow know first machine introduce new feature index become problem actually lead bad scaling workload easier way actually throw away hash map replace hashing mean take n-gram take hash value n-gram take value modulo two 20 two 22 huge number hash actually function convert input number give different string output different number string sometimes output value know collision hash function collision practice later see take take hash value modulo two high rise high power collision neglected actually hashing vectorizer implement scikit-learn s call hashing vectorizer obviously s also implement vowpal wabbit library later overview 
spam filter huge task spam filter proprietary dataset • https 09022206pdf • 04 million user • 32 million letter • 40 million unique word let ’ say map token index used hash function 𝝓 • 𝜙 𝑥 = hash 𝑥 % + • 𝑏 = 22 4 million feature • huge improvement 40 million feature • turn ’ hurt quality model okay let s take spam filter task might guess huge task even medium mail server person send lot email million user terabyte text need analyze actually paper archive actually introduce proprietary datum set spam filter half million user three million letter 40 million unique word see letter let s say map token index used hash function fi work like follow take token x hash take value modulo two b b equally 22 four million feature actually huge improvement 40 million feature originally somehow map 40 million feature four million feature thank fact hash collision unlikely lot actually train model top formula feature still get pretty decent result 
hashing example • • • • • • 𝜙 𝜙 𝜙 𝝓 𝝓 𝜙 𝑔𝑜𝑜𝑑 = 0 𝑚𝑜𝑣𝑖𝑒 = 1 𝑛𝑜𝑡 = 2 𝒂 𝟑 𝒅𝒊𝒅 = 𝟑 𝑙𝑖𝑘𝑒 = 4 𝐡𝐚𝐬𝐡 𝒔 = 𝒔 𝟎 + 𝒔 𝟏 𝒑𝟏 + ⋯ + 𝒔 𝒏 𝒑𝒏 𝑠 – string 𝑝 – fix prime number 𝑠 [ 𝑖 ] – character code hash collision 0 1 2 3 4 good movie 1 1 0 0 0 good movie 1 1 1 1 0 like 0 0 1 1 1 let s look example hashing vectorizer work first let introduce hash function arbitrary string string s take first character code string actually number zero two 55 example take next character multiply fix prime number take third character multiply b two forth actually obtain integer number hash value string course string hash value know collision practice see lot let s see might particular datum set three reviews good movie good movie nt like take possible token let s pass hashing function phi take pretty small b actually get follow zero one two three three do hash value fine like hash value four vectorization work column instead different token different hash value number zero four let s see good movie vectorize look hash value good zero add one column corresponding value take next word movie hash well get value one input one column correspond hash value proceed reviews actually look pretty similar bag-of-words instead token hash value okay let s try train linear model top feature 
trillion feature hashing personalize token trick • 𝜙m 𝑡𝑜𝑘𝑒𝑛 = hash 𝑡𝑜𝑘𝑒𝑛 % + • 𝜙n 𝑡𝑜𝑘𝑒𝑛 = hash 𝑢 + ” _ ” + 𝑡𝑜𝑘𝑒𝑛 % + • obtain 16 trillion pair ( user word ) still + feature text document ( email ) https 09022206pdf bag word bag word ( personalize ) neu votre apotheke … neu user123_neu votre user123_votre apotheke user123_apotheke … 0 ( x ) + 1 0 1 0 0 1 0 1 0 … u ( x ) first let s look thing actually propose way squeeze number feature originally bag-of-words manner 40 million feature hash four million feature actually control number feature output adjust b parameter find power two actually mean introduce lot token lot feature trillion feature hash still fix number two b feature analyze let s see might work phi_zero old hashing trick use take hash value token take value modulo two b another thing actually use personalize hashing mean want feature say particular user you particular token see user token email actually mean want learn personalize preference user spam non-spam e-mail take user add underscore token hash new token take value modelo two b new feature actually datum set take pair user word actually 16 trillion pair s possible look feature bag-of-words representation take 16 terabyte datum s feasible take hash feature take modulo two b fix number feature pipeline text document extract token add personalize token add prefix let s say user one three underscore token have see user hash token get sparse vector size two b okay 
experimental result • 𝑏 = 22 perform like linear model original token • observe personalize token give huge improvement miss-rate let s see whether hashing actually hurt performance graph see three different model first one denote black color baseline actually model trained original token without hashing bag-of-words manner trained linear model top bag-of-words blue one actually hash version replace tfidfvectorizer let s say hashingvectorizer smaller number feature see start value b let s say 22 actually nt lose anything term quality take b equally 18 lose quality value huge okay s pretty okay use hashing lot hash value another thing s red curve correspond personalize hashing model introduce personalize token hash used linear model well see somehow give significant boost miss-rate actually axis miss-rate want make low possible 
personalize feature work personalize feature capture “ local ” user-specific preference • user might consider newsletter spam majority person fine work new user number email training okay let s understand personalize feature actually work answer pretty obvious be personalize actually capture local user-specific preference let s say user might consider newsletter spam mean see word s frequently used newsletter particular user would spam well rest person like majority newsletter can okay add personalize feature actually understand make letter spam letter particular user work new user let s look different user different number email training let s say take user 64 email training mean low miss-rate know really well spam letter less less example actually start hurt quality model less example spam letter one surprising thing even user nt letter training set higher quality baseline model happen user nothing change nt add user-specific token actually expect nothing change get pretty close baseline actually perform superior baseline let s find 
personalize feature work turn learn better “ global ” preference personalize feature learn ” local ” user preference • think universal definition spam number email training actually think follow way turn learn better global preference feature correspond personalize preference local use preference let s take example person hate newsletter can small number person person actually use personalize feature learn person hate newsletter majority person newsletter fine mean personalize feature linear model learn okay look personalize feature particular person hate spam okay hate newsletter okay rest use feature contain word see newsletter like newsletter person rest learn better model actually happen practice s describe happen 
size matter need huge dataset • turn learn better model used simple linear classifier ad click prediction • https 11104198pdf • trillion feature billion training example • datum sampling hurt model sampling rate another thing deal huge dataset take one terabyte datum ca nt take like thousand email train classifier turn learn better model used simple linear classifier simple feature datum learn better classifier well see ad click prediction paper archive appropriatory dataset well trillion feature billion training example person actually show sample dataset let s say take one percent sample 10 percent sample huge terabyte dataset actually hurt model hurt model term area roc curve see hurt sampling rate take may think difference third digit auroc actually make sense may think okay much need bother one terabyte dataset talk ad click prediction mean improvement ad click prediction actually lead million dollar revenue person actually want squeeze maximum model 
vowpal wabbit • • • • popular machine learn library training linear model used feature hashing internally lot feature really fast scale well https wiki last want overview vowpal wabbit library well-known machine learn library used training linear model used feature hashing describe previously internally lot different feature s really fast scale well s wonderful input library give raw text convert tokenize text white space take hash value token use hash value internally hash vectorization also say want pass feature already know hash value also say like 13 colon real value number mean column correspond hash value 13 value 
summary • • • • ’ take look application feature hashing personalize feature nice trick linear model bag word scale well production next video ’ take look text classification problem used deep learn okay let s summarize have take look different application particularly spam filter used feature hashing thank hashing hash lot feature trillion feature actually add personalize feature really nice trick boost performance model linear model bag-of-words scale well production well-known thing s actually overview likely implement linear model baseline work somewhere corporation anywhere else next video take look text classification problem used deep learn technique 

neural network text hi video apply neural network text 
text think text sequence • character • word • phrase name entity • sentence • paragraph • … let s first remember text think sequence character word anything else video continue think text sequence word token 
bag word way ( sparse ) 100k column good movie like 0 0 1 0 0 0 good 1 0 0 0 0 0 movie 0 1 0 0 0 0 let s remember bag word work every word forever distinct word dataset feature column actually effectively vectorize word one-hot-encoded vector huge vector zero one non-zero value column corresponding particular word example good movie vectorized independently set actually real world problem like hundred thousand column get bag word representation 
bag word way ( sparse ) 100k column good movie 0 0 1 like 0 0 0 0 0 0 0 0 0 0 0 0 + good 1 0 0 + movie 0 1 0 = good movie 1 1 1 bag word representation sum sparse one-hot-encoded vector actually see sum value vector come bag word vectorization correspond good movie can good think bag word representation sum sparse one-hot-encoded vector corresponding particular word 
neural way ( dense ) 300 column 𝑥𝟏 𝑥𝟐 𝑥𝟑 07 04 05 good 02 01 01 movie 05 04 01 example word2vec embedding word2vec property word similar context tend collinear vector okay let s move neural network way opposite sparse way have see bag word neural network usually like dense representation mean replace word dense vector much shorter 300 value real value item vector example vector word2vec embedding pretrain embedding do unsupervised manner actually dive detail word2vec next two week know right word2vec vector nice property word similar context term neighboring word tend vector collinear actually point roughly direction nice property use okay replace word dense vector 300 real value next come feature descriptor whole text actually use manner used bag word 
neural way ( dense ) 300 column 𝑥𝟏 𝑥𝟐 𝑥𝟑 07 04 05 + good movie 02 01 01 + 05 04 01 = good movie 04 07 07 sum word2vec vector good text descriptor already example word2vec embedding dig sum vector representation base word2vec embedding whole text like good movie s word2vec vector actually work practice give great baseline descriptor baseline feature classifier actually work pretty well 
better way 1d convolution word embedding cat 07 04 05 sit 02 01 01 05 04 01 make 2-grams dog 06 03 05 rest 03 01 02 05 04 01 http another approach neural network embedding let s look two example sentence ` cat sit ` dog rest word take row actually represent word2vec embedding length let s say want apply neural network somehow let s first think follow thing make use 2-grams used representation bag word representation particular 2-gram different column long sparse factor possible 2-grams nt word2vec embedding token pair actually word2vec embedding particular word analyze 2-grams 
better way 1d convolution word embedding cat 07 04 05 sit 02 01 01 05 04 01 dog 06 03 05 rest 03 01 02 05 04 01 convolutional filter 06 04 05 02 01 02 better bow • convolution provide high activation 2-grams certain meaning http 09 actually turn look pair embedding vector think slide window green border first two word take word embedding want take value want analyze somehow neural network purpose actually use convolutional filter size number take value pretty close value correspond ` cat sit mean convolve filter 2-gram ` cat sit high activation convolutional filter similar word embedding pair word okay know analyze 2-grams text convolve word vector near better bag word bag word manner particular 2-gram different column come lot convolutional filter learn representation 2-grams will able analyze 2-grams well 
better way 1d convolution word embedding cat 07 04 05 sit 02 01 01 05 04 01 dog 06 03 05 rest 03 01 02 05 04 01 convolutional filter 06 04 05 02 01 02 high activation 2-grams meaning “ animal sit ” • convolution provide high activation 2-grams certain meaning • word2vec vector similar word similar term cosine distance ( similar dot product ) http 09 084 better turn used good property word2vec embedding follow similar word similar term context see similar word similar term cosine distance cosine distance similar dot product product actually convolution be mean take different sentence like ` dog rest actually find cat dog similar representation word2vec be see context like dog run away dog eat homework replace dog cat would frequent sentence well convolutional filter better take n-gram dog rest thank fact value pretty similar value 2-gram cat sit mean convolve convolutional filter high activation value well turn good embedding vector used convolution actually look high-level meaning two gram s cat sit dog rest cat rest dog sit actually animal sit meaning 2-gram learn convolutional filter pretty cool have do neural column possible 2-grams need look pair word embedding learn convolutional filter learn meaningful feature 
1d convolution • extend 3-grams 4-grams etc • one filter enough need track many n-grams • call 1d slide window one direction cat 07 04 05 sit 02 01 01 05 04 01 01 08 03 05 03 02 01 okay see easily extend three-gram three-gram n-gram contrary bag-of-words representation feature metric wo nt explode feature metric actually fix change size filter convolution pretty easy operation also see like convolutional neural network one filter enough need track many n-grams need track many different meaning two three gram s need lot convolutional filter filter call 1d convolution actually slide window one direction contrary let s say image slide window two direction let s see slide window actually work input sequence cat sit word word back representation slide window size three let s add padding size output size input let s convolve first patch get metric let s say get 01 
1d convolution • extend 3-grams 4-grams etc • one filter enough need track many n-grams • call 1d slide window one direction cat 07 04 05 01 sit 02 01 01 03 05 04 01 01 08 03 05 03 02 03 minus_02 07 minus_04 
1d convolution • extend 3-grams 4-grams etc • one filter enough need track many n-grams • call 1d slide window one direction cat 07 04 05 01 sit 02 01 01 03 05 04 01 02 01 08 03 05 03 02 
1d convolution • extend 3-grams 4-grams etc • one filter enough need track many n-grams • call 1d slide window one direction cat 07 04 05 01 sit 02 01 01 03 05 04 01 02 01 08 03 07 05 03 02 actually see slide window one direction direction actually time think sequence word happen time word occur time axis okay number bad property number output equal number input mean variable length symptom variable number feature nt want nt know let s assume like bag-of-words manner actually lose order word mean nt really care have see two-gram meaning animal sit actually try find convolutional filter nt care occur begin sentence end thing care whether combination actually text assume actually take maximum activation get convolutional filter go whole text take value result convolution 
1d convolution • extend 3-grams 4-grams etc • one filter enough need track many n-grams • call 1d slide window one direction cat 07 04 05 01 sit 02 01 01 05 04 01 01 08 03 03 02 vector 07 05 03 02 04 actually call maximum pule time like image maximum pule apply time have do be take input sequence have propose take convolutional window size three number variable embedding convolve filter slide one direction take maximum activation output okay let s come final model final architecture might look like use filter size three four five capture information three four five gram n-gram learn 100 filter mean effectively 300 output let s look image input sequence let s say red window correspond convolutional filter maximum activation 07 output filter size green let s say two-gram convolve throughout whole sentence maximum value have see minus_07 add output way used different filter different size 300 output okay vector vector actually kind embedding input sequence have propose way convert input sequence vector fix size next obvious thing apply dense layer actually apply multi-layer positron top 300 feature train task want either classification regression anything else okay let s compare quality model bag-of-words approach classical actually link paper have do experiment customer reviews dataset compare model naive buyer top one two gram classical model give 863 accuracy use propose 1d convolution architecture mlp top feature get whopping 38 bump accuracy give almost 90 percent accuracy pretty cool apply neural network propose emb word use lot unsupervised text learn embedding actually propose analyze two-gram three-gram used convolution pretty fast operation work even faster bag-of-words work better 
1d convolution • extend 3-grams 4-grams etc • one filter enough need track many n-grams • call 1d slide window one direction cat 07 04 05 sit 02 01 01 05 04 01 07 01 08 03 05 03 02 maximum pool time pretty cool okay let s summarize average pre-train word2vec embedding text split text token token take embedding vector sum baseline model actually work pretty well another approach little bit better use 1d convolution describe way train neural network end end input sequence result want predict use back propagation train convolution train specific feature neural network need classify sentence next video continue apply convolution text 
let ’ train many filter final architecture • 345-gram window 100 filter • mlp top 300 feature quality comparison customer reviews ( cr ) • naïve baye top 12-grams – 863 % accuracy • 1d convolution mlp – 896 % ( 38 % ) accuracy cat 07 04 05 sit 02 01 01 05 04 01 01 05 08 03 03 02 http collobert11apdf 07 maximum 03 value … filter 07 300 output https 14085882pdf 
summary • average pre-train word2vec vector text • better 1d convolution learn complex feature • next video ’ continue apply convolution text 

go deeper text hi video will go deeper text 
text think text sequence • character • word • phrase name entity • sentence • paragraph • … let remind think text sequence word phrase sentence like video will think text sequence character 
text sequence character one-hot encode character length 70 _ c _ r u n _ 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 … 0 … … … … 0 … … … 1 … 1 1 1 1 … 0 0 0 … 0 … … … … 0 let ’ start character n-grams treat text sequence character let take example phrase cat run underscore mean white space easy talkanize text character emb character vector length let s say 70 one hot-encode manner alphabet huge special character number huge well one hot-encode vector sparse long okay next number look like sequence number let s start character n-grams seem need like process word set character n-grams still make sense 
1d convolution character one-hot encode character length 70 filter # 1 _ c _ r u n _ 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 … 0 … … … … 0 … … … 1 … 1 1 1 1 … 0 0 0 … 0 … … … … 0 04 1d convolution mean take c character use padding white space left take convolution get result move window get another value 
1d convolution character one-hot encode character length 70 filter # 1 _ c _ r u n _ 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 … 0 … … … … 0 … … … 1 … 1 1 1 1 … 0 0 0 … 0 … … … … 0 04 08 way end sequence 
1d convolution character one-hot encode character length 70 filter # 1 _ c _ r u n _ 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 … 0 … … … … 0 … … … 1 … 1 1 1 1 … 0 0 0 … 0 … … … … 0 04 08 05 01 03 02 07 01 get value well 1d convolution slide window one direction time okay different kernel different convolutional kernel 
1d convolution character one-hot encode character length 70 _ c _ r u n _ 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 … 0 … … … … 0 … … … 1 … 1 1 1 1 … 0 0 0 … 0 … … … … 0 filter # 1 04 08 05 01 03 02 07 01 filter # 2 05 
1d convolution character one-hot encode character length 70 _ c _ r u n _ 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 … 0 … … … … 0 … … … 1 … 1 1 1 1 … 0 0 0 … 0 … … … … 0 filter # 1 04 08 05 01 03 02 07 01 filter # 2 05 01 04 02 03 09 01 08 different value 
1d convolution character one-hot encode character length 70 _ c _ r u n _ 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 … 0 … … … … 0 … … … 1 … 1 1 1 1 … 0 0 0 … 0 … … … … 0 filter # 1 04 08 05 01 03 02 07 01 filter # 2 05 01 04 02 03 09 01 08 filter # 3 04 07 03 07 05 05 09 04 1024 filter ’ next let ’ add pool take 1000 kernel will 1000 filter result s next remember convolutional network usually add convolution follow pool convolution pool forth let s add pool 
max pool filter # 1 04 08 05 01 03 02 07 01 filter # 2 05 01 04 02 03 09 01 08 filter # 3 04 07 03 07 05 05 09 04 08 let s see pool apply take let remind work filter level take value neighboring value take maximum time s 08 
max pool filter # 1 04 08 05 01 03 02 07 01 filter # 2 05 01 04 02 03 09 01 08 filter # 3 04 07 03 07 05 05 09 04 08 05 move window stride two 
max pool filter # 1 04 08 05 01 03 02 07 01 filter # 2 05 01 04 02 03 09 01 08 filter # 3 04 07 03 07 05 05 09 04 08 05 03 07 take maximum value well end filter 
max pool filter # 1 04 08 05 01 03 02 07 01 filter # 2 05 01 04 02 03 09 01 08 filter # 3 04 07 03 07 05 05 09 04 08 05 03 07 pool output 05 04 09 08 07 07 05 09 provide little bit position invariance character n-grams pool output need pool actually introduce little bit position invariance character n-grams character n-gram slide like one character left right high chance thank pool activation pool output stay 
repeat 1d convolution + pool 08 05 03 07 pool output 05 04 09 08 07 07 05 09 another filter # 1 04 08 04 09 another filter # 2 09 08 06 05 okay remember continue apply convolution follow pool forth let s take previous pool output let s apply 1d convolution well get filter output work value 
repeat 1d convolution + pool 08 05 03 07 pool output 05 04 09 08 07 07 05 09 another filter # 1 04 08 04 09 another filter # 2 09 08 06 05 another pool output 08 09 09 06 next add pool pool work take neighboring two value take maximum move slide window green window stride two different value s apply convolution pool notice length like feature representation actually decrease mean receptive field actually increase look character input make decision activation deep level 
repeat 1d convolution + pool 08 05 03 07 pool output 05 04 09 08 07 07 05 09 another filter # 1 04 08 04 09 another filter # 2 09 08 06 05 another pool output 08 09 09 06 repeat 6 time continue actually six time s get final architecture 
final architecture • let ’ take first 1014 character text • apply 1d convolution + max pool 6 time − kernel width 7 7 3 3 3 3 − filter step 1024 • 𝟏𝟎𝟐𝟒 × 𝟑𝟒 matrix feature • apply mlp task https 150901626pdf final architecture look like take first 1000 character text certain dataset make sense nt make sense like read whole text maybe 1000 character enough apply 1d convolution plus max pool 6 time use follow kernel width 77 rest 3s use 1000 filter step mean apply procedure 6 time get matrix feature size 1000 feature actually apply multi-layer perceptron task regression classification law like 
experimental dataset categorization sentiment analysis smaller bigger https 150901626pdf let s see work experimental dataset dataset either categorization like news dataset sentiment analysis like yelp reviews amazon reviews two category dataset first one red one smaller dataset contain 600000 training sample bigger dataset contain million sample let s compare model two type dataset 
experimental result error test set classical model error test set deep model deep model work better large dataset https 150901626pdf first table see contain error test set classical model classical model like bag worth bag worth tfidf linear model top replace token n-grams thing see small dataset red border see error least use n-grams tfidf time tell us small training set make sense use classical approach dataset grow million example maybe learn deeper representation second table contain error test set dataset see lstm convolutional architecture overview see architecture actually beat lsdm huge dataset gain sometimes huge actually surprising see deep approach work significantly better classical approach let s say amazon reviews last column have get degree error roughly 8 % like 5 % pretty cool learn have learn follow deep model work better large dataset make sense make huge architecture huge dataset 
summary • use convolutional network top character ( call learn scratch ) • work best large dataset beat classical approach ( like bow ) • sometimes even beat lstm work word level okay let summarize use convolutional network top word also top character tweak text sequence character call learn scratch literature work best large dataset beat classical approach like bag word surprisingly sometimes even beat lstm work word level have do come character level learn deeper representation nt tell system model word work better pretty cool video conclude first week wish good luck follow week [ music ] 

n-gram language model [ sound ] hi everyone welcome week two nlp course week core nlp task go speak language model first model work sequence word example part-of-speech tag named-entity recognition task build block nlp application be useful first thing s first let s start language model 
language modele house rat … malt ’ probability next word p ( house | ) = imagine see begin sentence like would continue probably human know sound nice sound nice intuition know well written book see text s obvious build similar intuition computer well try try estimate probability next word give previous word first need datum 
toy corpus house jack build malt lay house jack build rat eat malt lay house jack build cat kill rat eat malt lay house jack build p ( house | ) = let us get toy corpus nice toy corpus house jack build let us try use estimate probability house give four interesting fragment 
toy corpus house jack build malt lay house jack build rat eat malt lay house jack build cat kill rat eat malt lay house jack build p ( house | ) = one exactly need 
toy corpus house jack build malt lay house jack build rat eat malt lay house jack build cat kill rat eat malt lay house jack build p ( house | ) = house 
toy corpus house jack build malt lay house jack build rat eat malt lay house jack build cat kill rat eat malt lay house jack build с ( house ) p ( house | ) = с ( … ) 1 = 4 mean probability one 1 c denote count count house piece text piece text n-grams n-gram sequence n word speak 4-grams 
toy corpus house jack build malt lay house jack build rat eat malt lay house jack build cat kill rat eat malt lay house jack build с ( house ) p ( house | ) = с ( … ) 4-grams 1 = 4 also speak unigram bigram trigram etc try choose best n speak later bigram imagine happen bigram example estimate probability jack give build 
toy corpus house jack build malt lay house jack build rat eat malt lay house jack build cat kill rat eat malt lay house jack build p ( jack | ) = с ( jack ) с ( that… ) bigram 4 = 10 okay count different bigram like jack lay etc say four jack mean probability 4 divide s next count probability estimate datum well need 
need lm • suggestion messenger • spelling correction • machine translation • speech recognition • handwriting recognition • smart reply researchgoogleblogcom use actually need everywhere begin let s discuss smart reply technology technology google get email try suggest automatic reply example suggest say thank happen well text generation right language model speak later many many detail week four also application like machine translation speech recognition application try generate text datum mean want evaluate probability text probability long sequence 
language modele house rat … malt ’ probability whole sequence p ( house ) = like evaluate probability house probability long long sequence 100 word well complicate maybe whole sequence never occur datum count something need somehow deal small piece sequence right 
let ’ math predict probability sequence word w = ( w1 w2 w3 wk ) let s math understand deal small piece sequence sequence keyword would like estimate probability 
let ’ math predict probability sequence word w = ( w1 w2 w3 wk ) • chain rule p ( w ) = p ( w1 ) p ( w2 w1 ) p ( wk w1 wk 1 ) apply chain rule mean take probability first word condition next word word s already better last term s still kind complicate prefix condition long get rid 
let ’ math predict probability sequence word w = ( w1 w2 w3 wk ) • chain rule p ( w ) = p ( w1 ) p ( w2 w1 ) p ( wk w1 wk • 1 ) markov assumption p ( wi w1 wi 1 ) = p ( wi wi n+1 wi 1 ) yes [ laugh ] actually markov assumption say care history forget take last n term condition correct last n-1 term introduce assumption everything text connect definitely helpful us chance estimate probability 
bigram language model ’ get n = 2 p ( w ) = p ( w1 ) p ( w2 w1 ) p ( wk wk 1 ) happen n = 2 bigram model recognize already know estimate small probability right-hand side mean solve task 
bigram language model ’ get n = 2 p ( w ) = p ( w1 ) p ( w2 w1 ) p ( wk wk 1 ) toy corpus malt lay house jack build p ( house ) = p ( ) p ( | ) p ( | ) p ( | ) 
bigram language model ’ get n = 2 p ( w ) = p ( w1 ) p ( w2 w1 ) p ( wk wk 1 ) toy corpus malt lay house jack build 12 1 1 2 p ( house ) = p ( ) p ( | ) p ( | ) p ( | ) toy corpus estimate probability s get clear hope want think everything nice do well see least two problem be go describe try fix actually s super easy fix first let s look first word probability first word first word toy corpus never malt something else well maybe use maybe spread probability among possible word vocabulary stick likely first spot 
bigram language model ’ get n = 2 p ( w ) = p ( w1 ) p ( w2 w1 ) p ( wk wk 1 ) p ( w1 start ) toy corpus malt lay house jack build 2 1 1 2 p ( house ) = p ( ) p ( | ) p ( | ) p ( | ) let us condition first word fake start token let s add fake start token begin get probability = 2 first place right either 
bigram language model ’ get n = 2 p ( w ) = p ( w1 ) p ( w2 w1 ) p ( wk wk p ( w1 start ) 1 ) helpful else actually another problem think probability normalize across different sequence different length 
bigram language model ’ get n = 2 p ( w ) = p ( w1 ) p ( w2 w1 ) p ( wk wk 1 ) p ( w1 start ) ’ normalize separately sequence length p ( ) + p ( ) = 10 p ( ) + p ( ) + … + p ( build build ) = 10 … well s good probability short sequence sequence length 1 = 1 sequence length 2 also sum probability = s want want one distribution sequence first show fix show fix like 
bigram language model ’ get n = 2 p ( w ) = p ( w1 ) p ( w2 w1 ) p ( wk wk p ( w1 start ) 1 ) p ( end wk ) ’ normalize separately sequence length p ( ) + p ( ) = 10 p ( ) + p ( ) + … + p ( build build ) = 10 … discuss help let us add fake token end begin let us probability end token give last term okay easy help imagine generative process imagine model generate next word be go show example generate different sequence hopefully see sequence fall one big probability mass spread probability mass among 
let ’ check model _ dog _ _ dog cat tiger _ _ cat dog cat _ p ( cat dog cat ) = toy corpus want evaluate probability untouched probability mass yet 
let ’ check model _ dog _ _ dog cat tiger _ _ cat dog cat _ p ( cat dog cat ) = p ( cat | _ ) dog cat let s cut go dog cat 
let ’ check model _ dog _ _ dog cat tiger _ _ cat dog cat _ p ( cat dog cat ) = p ( cat | _ ) dog cat let s go cat 
let ’ check model _ dog _ _ dog cat tiger _ _ cat dog cat _ p ( cat dog cat ) = p ( cat | _ ) p ( dog | cat ) dog cat tiger cat dog cat _ decide whether want go tiger dog want terminate super important model option terminate s can without fake end token place thing can go wrong add fake token 
let ’ check model _ dog _ _ dog cat tiger _ _ cat dog cat _ p ( cat dog cat ) = p ( cat | _ ) p ( dog | cat ) dog cat tiger cat dog cat _ okay decide pick something 
let ’ check model _ dog _ _ dog cat tiger _ _ cat dog cat _ p ( cat dog cat ) = p ( cat | _ ) p ( dog | cat ) p ( cat | dog ) dog cat tiger cat dog cat cat _ cat _ 
let ’ check model _ dog _ _ dog cat tiger _ _ cat dog cat _ p ( cat dog cat ) = p ( cat | _ ) p ( dog | cat ) p ( cat | dog ) dog cat tiger cat dog cat cat _ cat _ split pick split pick 
let ’ check model _ dog _ _ dog cat tiger _ _ cat dog cat _ p ( cat dog cat ) = p ( cat | _ ) p ( dog | cat ) p ( cat | dog ) p ( _ | cat ) dog cat tiger cat dog cat tiger cat _ cat dog cat dog cat dog cat _ cat _ exactly probability sequence want evaluate s important imagine also split area different part fit area right sequence different length altogether give probability mass equal 1 mean correctly normalize probability 
let ’ check model _ dog _ _ dog cat tiger _ _ cat dog cat _ p ( cat dog cat ) = p ( cat | _ ) p ( dog | cat ) p ( cat | dog ) p ( _ | cat ) dog cat tiger cat dog cat tiger cat _ cat dog cat dog cat dog cat _ cat _ congratulation summarize can introduce bigram language model split factorize probability two term can learn evaluate term datum see two formula bottom slide let moment see normalize count n-grams either think count n-1 gram think count n-1 gram + different continuation possible option sum possible option okay hope can see really count next video continue study detail train model test model problem 
resume bigram language model define model p ( w ) = k+1 i=1 p ( wi wi 1 ) estimate probability c ( wi 1 wi ) c ( wi 1 wi ) p ( wi wi 1 ) = p = c ( wi 1 ) wi c ( wi 1 wi ) ’ count thank [ sound ] 

model surprised real text 
train n-gram model w = ( w1 w2 w3 wk ) bigram language model p ( w ) = k+1 i=1 p ( wi wi 1 ) 
train n-gram model w = ( w1 w2 w3 wk ) bigram language model p ( w ) = k+1 i=1 n-gram language model p ( w ) = p ( wi wi ( wi k+1 i=1 p ( wi wii 1 ) n+1 wi 1 ) 1 n+1 ) 
train n-gram model log-likelihood maximization log p ( wtrain ) = n 1 x i=1 log p ( wi wii 1 n+1 ) max estimate parameter c ( w ) c ( w n+1 1 p ( wi wii n+1 ) = p = c ( w ) c ( w n+1 wi n+1 ) 1 n+1 ) n length train corpus ( word concatenate ) 
generate shakespeare unigram swallow confess hear save trail ay device rote life every enter severally let hill late speak leg less first enter bigram mean sir confess sort trim captain dost stand forth thy canopy forsooth palpable hit king henry live king follow hath get rest send scold nature bankrupt first gentleman jurafsky & martin https slp4pdf 
generate shakespeare 3-grams sweet prince falstaff shall die harry monmouth ’ grave shall forbid brand renown make empty ’ cry indeed duke good friend fly rid news price therefore sadness part say ’ tis do 4-grams king henry go seek traitor gloucester exeunt watch great banquet serv ’ tell indeed short long marry ’ tis noble lepidus say lover swear performance wont keep oblige faith jurafsky & martin https slp4pdf 
model better best n might depend much datum • bigram might enough • 7-gram might never occur extrinsic evaluation • quality downstream task machine translation speech recognition spelling correction… intrinsic evaluation • hold-out ( text ) perplexity 
evaluate model test set likelihood l = p ( wtest ) = perplexity p = p ( wtest ) n 1 1 n i=1 p ( wi wii p = n 1 n+1 ) 1 p ( wtest ) n length test corpus ( word concatenate ) lower perplexity better 
out-of-vocabulary word toy train corpus house jack build toy test corpus malt 
out-of-vocabulary word toy train corpus house jack build toy test corpus malt ’ perplexity bigram lm 
out-of-vocabulary word toy train corpus house jack build toy test corpus malt ’ perplexity bigram lm c ( malt ) p ( malt|the ) = 0 c ( ) 
out-of-vocabulary word toy train corpus house jack build toy test corpus malt ’ perplexity bigram lm c ( malt ) p ( malt|the ) = 0 c ( ) p ( wtest ) = 0 
out-of-vocabulary word toy train corpus house jack build toy test corpus malt ’ perplexity bigram lm c ( malt ) p ( malt|the ) = 0 c ( ) p ( wtest ) = 0 p = inf 
out-of-vocabulary word toy train corpus house jack build toy test corpus malt ’ perplexity bigram lm c ( malt ) p ( malt|the ) = 0 c ( ) p ( wtest ) = 0 p = inf 
fix simple idea • build vocabulary ( eg word frequency ) • substitute oov word < unk > ( train test ) • compute count usual token • profit 
ok oov word toy train corpus house jack build toy test corpus jack ’ perplexity bigram lm 
ok oov word toy train corpus house jack build toy test corpus jack ’ perplexity bigram lm c ( jack ) p ( jack | ) = 0 c ( ) 
ok oov word toy train corpus house jack build toy test corpus jack ’ perplexity bigram lm c ( jack ) p ( jack | ) = 0 c ( ) p ( wtest ) = 0 
ok oov word toy train corpus house jack build toy test corpus jack ’ perplexity bigram lm c ( jack ) p ( jack | ) = 0 c ( ) p ( wtest ) = 0 p = inf 
ok oov word toy train corpus house jack build toy test corpus jack ’ perplexity bigram lm c ( jack ) p ( jack | ) = 0 c ( ) p ( wtest ) = 0 p = inf 

see new n-grams 
zero probability test datum toy train corpus house jack build toy test corpus jack ’ perplexity bigram lm c ( jack ) p ( jack | ) = 0 c ( ) p ( wtest ) = 0 p = inf hey remember motivation previous video language really variative mean train model train datum likely whether apply model test datum get zero example bigram occur test datum zero can probably substitute one well actually reason way get correct probability distribution normalize one 
laplacian smooth idea • pull probability frequent bigram infrequent one • add 1 count ( add-one smooth ) 1 p̂ ( wi wii n+1 ) = c ( wii n+1 ) + 1 1 c ( wii n+1 ) v • tune parameter ( add-k smooth ) 1 p̂ ( wi wii n+1 ) = c ( wii n+1 ) + k 1 c ( wii n+1 ) vk instead another simple thing add one account even zero add v number word vocabulary denominator way get correct probability distribution zero do idea simple need somehow pull probability mass frequent n-grams infrequent one actually idea smooth technique rest video discover best way pull probability mass frequent n-grams infrequent n-grams one rather simple approach well would add one k tune constant used test datum call add-k smooth approach sometimes call laplacian smooth may easiest popular smooth 
katz backoff problem • longer n-grams better datum always enough idea • try longer n-gram back shorter need p̂ ( wi wii 1 n+1 ) = ( 1 p̃ ( wi wii n+1 ) 1 ↵ ( wii n+1 ) p̂ ( wi wii 1 n+2 ) c ( wii n+1 ) > 0 otherwise let us try see something complicate sometimes would like use longer n-grams would nice use might enough datum idea try use longer n-grams first enough datum estimate count become greedy go shorter n-grams katz backoff implementation idea let us start example five gram language model counter greater zero awesome go s greater zero let us greedy go full gram language model counter greater zero go else go trigram language model simple question alpha also tilde near b branch reason still need care probability 
katz backoff problem • longer n-grams better datum always enough idea • try longer n-gram back shorter need p̂ ( wi wii 1 n+1 ) = ( 1 p̃ ( wi wii n+1 ) 1 ↵ ( wii n+1 ) p̂ ( wi wii 1 n+2 ) c ( wii n+1 ) > 0 otherwise p̃ ↵ choose ensure normalization alpha constant discount make sure probability sequence sum one model idea implement different way interpolation smooth say let us mixture n-gram model different end example unigram bigram trigram language model weight lambda coefficient lambda coefficient sum one still get normal probability find lambda well tune used test development set afraid get fit optionally lambda also depend context sophisticated scheme okay great two method left 
interpolation smooth idea • let us mixture several n-gram model • example trigram model p̂ ( wi wi 1 + 2 2 wi 1 ) + 3 = 1 p ( wi wi 2 wi 1 ) + 2 p ( wi wi 1 ) 1 • weight optimized test ( dev ) set • optionally also depend context + 3 p ( wi ) 
absolute discounting idea • let ’ compare count bigram train test set experiment ( church gale 1991 ) • subtract 075 get good estimate test count train bigram count test bigram count 2 125 3 224 4 323 5 421 6 523 7 621 8 721 https 4pdf first one call absolute discounting recap motivation method video pull probability mass frequent n-grams infrequent n-grams extent pull probability mass answer question give nice experiment hold let us stick bigram let us see count number bigram training datum count average number bigram test datum number really correlated see be subtract 075 train datum count get good estimate test datum little bit magical property property language try use 
absolute discounting idea • let ’ compare count bigram train test set experiment ( church gale 1991 ) • subtract 075 get good estimate test count c ( wi p̂ ( wi wi 1 ) = p 1 wi ) + ( wi x c ( wi 1 x ) 1 ) p ( wi ) way use let us subtract 075 maybe tune used test datum subtract count model probability frequent n-grams pull mass 075 extent pull give probability infrequent term used unigram distribution right hand side will see weight make sure normalization fine unigram distribution maybe something better unigram distribution idea kneser-ney smooth 
kneser-ney smooth idea • unigram distribution capture word frequency • need capture diversity context word p̂ ( w ) x c ( x w ) > | malt … kong • probably popular smooth technique https 4pdf let us see example malt kong word kong might even popular word malt thing occur bigram hong kong word kong variative term different context go prefer word continue phrase opposite word malt popular go nicely different context idea formalized formula top slide let us probability word proportional many different context go word take absolute discounting model instead unigram distribution nice distribution get kneser-ney smooth 
resume smooth technique • • • • • add-one ( add-k ) smooth katz backoff interpolation smooth absolute discounting kneser-ney smooth n-gram model + kneser-ney smooth strong baseline language modele awesome cover several smooth technique simple like add-one smooth really advanced technique like kneser-ney smooth actually kneser-ney smooth really strong baseline language modele next lesson also cover neural language model see easy beat baseline 

probable tag 
motivation output sentence generate different sequence hide state det noun verb noun bear like honey det verb noun adj bear like honey 
decode hmm p ( x ) = t=1 p ( yt yt 1 ) p ( xt yt ) transition output probability probability decode problem probable sequence hide state = argmaxy p ( y|x ) = argmaxy p ( x ) solve problem efficiently used dynamic programming 
viterbi decode let qt probable sequence hide state length finish state generate o1 ot letqt probability sequence qt compute dynamically qt = max qt 0 1 s0 · p ( s|s0 ) · p ( ot s ) transition output probability probability qt determine remember argmax 
example hmm transition probability suppose follow pos tag adj noun verb consider transition probability tag \ adj noun verb adj 04 04 02 noun 02 04 04 verb 01 06 03 note sum probability row equal 1 
example hmm transition probability suppose follow pos tag adj noun verb consider transition probability tag \ adj noun verb adj 04 04 02 noun 02 04 04 verb 01 06 03 note sum probability row equal 1 let initial state probability equal ( 3 ) 
example hmm output probability consider follow output probability vocabulary tag\word bear fly honey like sweet adj 02 01 01 01 01 04 noun 01 02 02 02 02 01 verb 01 02 02 01 03 01 probability row also sum 1 
probability hide state “ like ” adj q=025 adj noun q=05 noun verb q=025 verb bear like honey 
possible transition adj state adj q=025 noun q=05 verb q=025 bear adj 04 02 01 noun 01 verb like honey 
best transition adj adj q=025 noun q=05 verb q=025 bear adj 04 02 01 noun 01 verb like honey 
probability adj state give “ like ” adj q=025 noun q=05 04 adj q=025 · 04 · 01 noun 01 verb q=025 verb bear like honey 
possible transition noun state adj q=025 04 adj q=025 · 04 · 01 noun q=05 04 noun verb q=025 bear 06 verb like 02 honey 
best transition noun adj q=025 04 adj q=025 · 04 · 01 noun q=05 04 noun verb q=025 bear 06 verb like 02 honey 
probability noun state give “ like ” adj q=025 · 04 · 01 adj q=025 noun q=05 04 noun q=05 · 04 · 02 verb q=025 verb bear like 02 honey 
possible transition verb state adj q=025 noun q=05 verb q=025 02 adj q=025 · 04 · 01 04 noun q=05 · 04 · 02 verb 03 03 bear like honey 
best transition verb adj q=025 noun q=05 verb q=025 02 adj q=025 · 04 · 01 04 noun q=05 · 04 · 02 verb 03 03 bear like honey 
probability verb state give “ like ” adj q=025 adj q=025 · 04 · 01 noun q=05 noun q=05 · 04 · 02 verb q=025 04 verb q=05 · 04 · 03 03 bear like honey 
probability hide state “ like ” adj q=025 adj q=025 · 04 · 01 noun q=05 noun q=05 · 04 · 02 verb q=025 verb q=05 · 04 · 03 bear like honey 
remember best transition adj q=025 adj q=025 · 04 · 01 noun q=05 noun q=05 · 04 · 02 verb q=025 verb q=05 · 04 · 03 bear like honey 
backtrace adj noun adj adj adj q=0082 noun noun noun q=0735 verb verb verb verb q=0184 bear like honey 
backtrace adj noun adj adj adj q=0082 noun noun noun q=0735 verb verb verb verb q=0184 bear like honey 
backtrace adj noun adj adj adj q=0082 noun noun noun q=0735 verb verb verb verb q=0184 bear like honey 
backtrace adj noun adj adj adj q=0082 noun noun noun q=0735 verb verb verb verb q=0184 bear like honey 
backtrace adj noun adj adj adj q=0082 noun noun noun q=0735 verb verb verb verb q=0184 bear like honey 
viterbi algorithm input observation length state-graph length n output best-path state 1 n p ( s|s0 ) · p ( o1 s ) 0 backpointer [ 1 ] q [ 1 ] time step state q [ ] n max q [ 0 1 backpointer [ ] 0 argmaxn q [ ] 0 1 2 1 n 1 s0 ] · p ( s|s0 ) · p ( ot s ) argmaxn s0 1 q [ return backtrace path backpointer 1 s0 ] · p ( s|s0 ) [ ] 

model name entity recognition 
hide markov model p ( x ) = t=1 generative model 𝑦 𝑥 p ( yt yt 1 ) p ( xt yt ) 
maximum entropy markov model p ( y|x ) = t=1 discriminative model 𝑦 𝑥 p ( yt yt 1 xt ) 
maximum entropy markov model p ( yt yt 1 xt ) = 1 zt ( yt 1 xt ) normalization constant exp k x ✓k fk ( yt yt 1 xt ) k=1 weight feature 
conditional random field ( linear chain ) 1 p ( y|x ) = exp z ( x ) t=1 k x ✓k fk ( yt yt k=1 1 xt ) undirected graph f ( yt yt 1 ) f ( yt xt ) 
conditional random field ( general form ) 1 p ( y|x ) = z ( x ) a=1 ( ya xa ) arbitrary factor 
black-box implementation + https mallet http grmm http crfsuite http factorie http wwwfactoriecc http crftut-fntpdf 
feature engineering label-observation feature • f ( yt yt 1 xt ) = [ yt = ] gm ( xt ) • f ( yt yt 1 xt ) = [ yt = ] [ yt 1 = y0 ] 0 • f ( yt yt 1 xt ) = [ yt = ] [ yt 1 = ] gm ( xt ) 
observation function example 𝑤5 = 𝑣 ∀𝑣 ∈ part-of-speech tag 𝑤5 𝑗 ∀ tag 𝑗 𝑤5 phrase syntactic type 𝑗 ∀ tag 𝑗 capitalize 𝑤5 match [ a-z ] [ a-z ] + allcaps 𝑤5 match [ a-z ] + endsindot 𝑤5 match [ ] + \ 𝑤5 match dash 𝑤5 appear list stop word 𝑤5 appear list capital http crftut-fntpdf 
dependency input trick pretend current input xt contain current word wt also wt 1 wt+1 build observation function well model discriminative use whole input [ yt = ] [ yt 1 = y0 ] [ yt = ] gm ( xt ) 
resume lesson probabilistic graphical model • hide markov model ( generative direct ) • maximum entropy markov model ( discriminative direct ) • conditional random field ( discriminative undirected ) task • training – fit parameter ( baum-welch hmm ) • decode – find probable tag ( viterbi hmm ) practice • feature engineering + black-box implementation 

neural language model hi week already learnt traditional nlp method task language modele part speech tag named-entity recognition lesson go cover task neural network neural network strong technique give state art performance kind task please stay lesson 
recap language modele day weather good … terrible time 4-gram language model p ( day | good ) = c ( good day ) c ( good ) recap language modele task predict next word give previous word know example 4-gram language model count n-grams normalize 
curse dimensionality imagine see follow many time • good day however see follow • great day happen ( even smooth ) v | 42 good 127 great let us take closer look let us discuss important problem imagine datum similar word datum like good great current model treat word separate item us separate index vocabulary let us say term neural language model one-hot encode mean encode word long long vector vocabulary size zero vector one non-zero element correspond index word encode nice imagine see ` good day lot time datum never see ` great day can understand good great similar can probably estimate good probability ` great day even though never see say okay maybe ` great day behave exactly way ` good day be similar read word independently want realize really huge problem language really variative another example let us say lot breed dog never assume breed dog datum maybe dog datum know somehow similar know particular type dog occur datum transfer knowledge dog great 
generalize better • learn distribute representation word • express probability sequence term distribute representation learn parameter good great dog c v ⇥m – matrix distribute word representation well call distribute representation exactly fix problem go represent word low-dimensional vector dimension something like 300 maybe 1000 vector dense go learn vector importantly hope similar word similar vector example good great similar dog similar ask remember notation bottom slide c matrix build vector representation row correspond word go define probabilistic model datum used distribute representation go learn lot parameter include distribute representation 
probabilistic neural language model v th output p ( wi = v|context ) softmax tanh c ( wi n+1 ) c ( wi 2 ) c ( wi 1 ) matrix c index wi n+1 index wi 2 index wi 1 yoshua bengio réjean ducharme pascal vincent christian jauvin neural probabilistic language model jmlr 2003 model try actually famous model 2003 bengio model one first neural probabilistic language model slide maybe understandable yo s okay want get idea big picture word bottom feed neural network first encode c matrix computation occur long vector top slide vector many element word vocabulary every element correspond probability certain word model let us go detail let us see formula bottom middle top part neural network 
probabilistic neural language model exp ( ywi ) p ( wi wi n+1 wi 1 ) = p exp ( yw ) w2v = b + w x + u tanh ( + hx ) x = [ c ( wi n+1 ) c ( wi ) ] 1 look scary nt nt scared break 
probabilistic neural language model exp ( ywi ) softmax p ( wi wi n+1 wi 1 ) = p exp ( yw ) component w2v = b + w x + u tanh ( + hx ) x = [ c ( wi n+1 ) c ( wi ) ] 1 last thing neural network softmax apply component vector vector long size vocabulary mean get probability normalize word vocabulary s need happen middle neural network 
probabilistic neural language model exp ( ywi ) softmax p ( wi wi n+1 wi 1 ) = p exp ( yw ) component w2v = b + w x + u tanh ( + hx ) feed-forward nn ton parameter x = [ c ( wi n+1 ) c ( wi ) ] 1 huge computation lot parameter actually every letter line parameter either matrix vector letter parameter x x 
probabilistic neural language model exp ( ywi ) softmax p ( wi wi n+1 wi 1 ) = p exp ( yw ) component w2v = b + w x + u tanh ( + hx ) feed-forward nn ton parameter x = [ c ( wi n+1 ) c ( wi ) ] 1 distribute representation context word x representation context remember c matrix distribute representation word take representation word context concatenate get x bottom top time get context representation feed neural network compute normalize get probability check understand everything s always good try understand dimension matrix example dimension w matrix 
’ over-complicated… = b + w x + u tanh ( + hx ) ⇤ ( n 1 ) c ( wi = b + w × x + c ( wi v | v | v | ) n+1 1 ) well write like see want get result formula dimension size vocabulary dimension x well x concatenation dimensional representation n minus 1 word context multiply n minus go see dimension w matrix neural network great kind over-complicated see non-linearities really time-consuming compute next slide model simpler 
log-bilinear language model • much less parameter non-linear activation • measure similarity word context exp ( r̂t rwi + bwi ) p ( wi wi n+1 wi 1 ) = p exp ( r̂t rw + bw ) w2v representation word rwi = c ( wi ) representation context r̂ = n x1 wk c ( wi ) k k=1 andriy mnih geoffrey hinton three new graphical model statistical language model proceedings 24th international conference machine learn ( icml 07 ) let s try understand one call log-bilinear language model maybe nt look like something simpler let us figure happen still softmax still produce probability value normalize bias term b important important part multiplication word representation context representation let s figure word representation easy s row c matrix context representation still get row c matrix represent individual word context multiply wk matrix matrix different different position context s actually nice model bag-of-words model try capture somehow word go target word influence probability way word somewhere far away history get word representation context representation dot product compute similarity normalize similarity model intuitive predict word similar context great feedforward neural network language modele next video recurrent neural network see 

label - lstms help 
recap recurrent neural network • extremely popular architecture sequential datum hi = f ( w hi 1 + v xi + b ) yi = u hi + b̃ y1 u y2 yk h1 w h2 hk v x1 x2 xk 
rnn language model • predict next word base previous context architecture • use current state output • apply linear layer top • softmax get probability good mikolov karafiát burget cernocký khudanpur recurrent neural network base language model interspeech 2010 
train cross-entropy loss ( one position ) log p ( wi ) = x [ w = wi ] log p ( w ) w2v day one non-zero p ( w ) • target word wi • output probability p ( w ) good 
use generate language idea • feed previous output next input • take argmax step ( greedily ) use beam search 
use generate language idea • feed previous output next input • take argmax step ( greedily ) use beam search < eos > 
use generate language idea • feed previous output next input • take argmax step ( greedily ) use beam search < eos > 
use generate language idea • feed previous output next input • take argmax step ( greedily ) use beam search good < eos > 
use generate language idea • feed previous output next input • take argmax step ( greedily ) use beam search good day < eos > good 
rnn language model • rnn-lm lower perplexity word error rate 5gram model knesser-ney smooth • experiment hold wall street journal corpus • later experiment char-level rnns effective mikolov karafiát burget cernocký khudanpur recurrent neural network base language model interspeech 2010 
character-level rnn shakespeare example andrej karpathy http 
cook language model • use lstms grus gradient clipping https • start one layer stack 3-4 use skip connection • use dropout regularization zaremba sutskever vinyal recurrent neural network regularization 2014 • look tf tutorial work model https recurrent • tune learn rate schedule sgd use adam • explore state-of-the-art improvement • july 2017 state art evaluation neural language model • august 2017 regularize optimize lstm language model 
sequence tag task • • • • part-of-speech tag name entity recognition semantic role label … bio-notation • b – begin – inside – outside book table 3 domino ’ pizza 
sequence tag task • • • • part-of-speech tag name entity recognition semantic role label … bio-notation • b – begin – inside – outside b_act book i_act i_act b_num_per table 3 b_loc i_loc domino ’ pizza 
bi-directional lstm • universal approach sequence tag • stack several layer + add linear layer top • trained cross-entropy loss come position b_act i_act i_act b_num_per book table 3 b_loc i_loc domino ’ pizza 

honey vs bee bumblebee [ music ] hey everyone be welcome week three course week semantic go understand get meaning word document piece text go represent meaning vector way similar word similar vector similar document similar vector need well example need search let s say want ranking example keyword candidate rank compute kind similarity query candidate get top similar result actually numerous application technique example also think ontology learn mean sometimes need represent hierarchical structure area need know concept example concept example might want know nt know plumber fix tap faucet need know tap faucet similar word present concept also do distributional semantic go cover right 
word similarity • first order co-occurrence syntagmatic associate relatedness ( bee honey ) • second order co-occurence paradigmatic parallel similarity ( bee bumblebee ) bee bumblebee schutze h & pedersen j ( 1993 ) vector model syntagmatic paradigmatic relatedness make sense word proceedings conference pp 104-113 oxford england cosine similarity okay example want understand bee bumblebee similar get let us start count word co-occurrence decide interested word co-occur small slide window example window size ten word co-occur say plus 1 counter get green counter slide way understand bee honey related call syntagmatic associate often co-occur together context however get back example understand tap faucet similar s need need get know second order co-occurrence mean two word would co-occur similar word context example compute long sparse vector bee cell popular neighbor word also count vector bumblebee compute similarity two vector way understand bee bumblebee interchangeably used language mean similar right be usually call paradigmatic parallel type co-occurrence usually need let us get little bit detail first compute green count 
distributional hypothesis “ shall know word company keep ” － firth 1957 • use slide window fix size • compute word co-occurrence nuv okay already say compute word co-occurrence bias popular word vocabulary like stop word rather noisy estimate right need help penalize popular word 
distributional hypothesis “ shall know word company keep ” － firth 1957 • use slide window fix size • compute word co-occurrence nuv • better pointwise mutual information p ( u v ) nuv n p = log = log p ( u ) p ( v ) nu nv one way would pointwise mutual information say put individual count word denominator way understand whether two word randomly co-occurrent look first formula see numerator joint probability word denominator joint probability case two random variable independent right word independent can say two probability factorized case independent word get 1 fraction case dependent word occur much together get something intuition pmi whether word randomly co-occur really related see problem measure well actually see count logarithm apply bad feel go 0 somewhere 
distributional hypothesis “ shall know word company keep ” － firth 1957 • use slide window fix size • compute word co-occurrence nuv • better pointwise mutual information p ( u v ) nuv n p = log = log p ( u ) p ( v ) nu nv • even better positive pointwise mutual information pp = max ( 0 p ) indeed word never co-occur together word co-occur really rare low number logarithm good idea say let us take maximum pmi way get rid minus infinity value get nice positive pointwise mutual information measure usually used idea go measure actually would distributional hypothesis say know word company keep meaning word somehow defined context word 
problem • first order co-occurrence syntagmatic associate relatedness ( bee honey ) • second order co-occurence paradigmatic parallel similarity ( bee bumblebee ) bee bumblebee schutze h & pedersen j ( 1993 ) vector model syntagmatic paradigmatic relatedness make sense word proceedings conference pp 104-113 oxford england cosine similarity let s get back nice slide know compute green value problem well want measure cosine similarity long sparse vector maybe s good idea long noisy sparse let us try dimensionality reduction 
vector space model semantic • input word-word co-occurrence ( count pmi … ) • method dimensionality reduction ( svd … ) • output similarity vector representation word v u w pmi x w w ⇡ ⇥ ⇥ w turnay pd pantel p frequency meaning vector space model semantic 2010 matrix left stack row see previous slide filled value describe like pmi factorize two matrix dimension would k low dimensional factorization example k would 300 something like lot different option factorization get later need know go compare row v matrix instead original sparse row x matrix way get measure whether word similar output model far look word occur word slide window context would word slide window however complicate notion context 
context amod prep nsubj dobj amod nsubj prep_with dobj omer levy yoav goldber dependency-based word embedding acl-2014 pobj example syntax parse know syntactic dependency word see word co-occur another word type relationship right example australian co-occur scientist modifier model say context word plus type relationship 
context • c vocabulary context eg dependency • usually context word form slide window • w = c x symmetric matrix v u ✓v c u pmi x w c ⇡ ⇥ w ⇥ case square matrix matrix word context context vocabulary word class modifier unit okay actually better idea syntax really help understand important local context random co-occurrence near meaningful however usually forget speak word word co-occurrence matrix still sometimes say word context general model can [ music ] 

factorization [ music ] hey previous video can see build model distributional semantic need kind matrix factorization go cover different way perform matrix factorization 
singular value decomposition ( svd ) x ⌃ u = ⇥ vt ⇥ eigenvector eigenvector x x xx square root eigenvalue x x many detail let us start approach base singular value decomposition fact linear algebra say every matrix factorized three matrix left right matrix contain call left right eigenvector metric middle diagonal value diagonal related eigenvalue importantly value diagonal sort decrease order many well number value diagonal number non-zero eigenvalue x transpose x related rank matrix awesome something know use task 
truncate svd keep first k component x ⇡ uk x̂k = uk ⌃k vkt ⇥ ⌃k ⇥ vkt idea follow value sort keep first k component probably keep blue region every matrix get nice approximation original x matrix sound rather hand-wavey right nice approximation actually accurate answer question 
truncate svd keep first k component x ⇡ uk x̂k = uk ⌃k vkt ⇥ ⌃k ⇥ vkt ’ best approximation rank k term frobenius norm x v ux u n x x̂||f = ( xij i=1 j=1 x̂ij ) 2 always blue part best approximation rank k x matric term loss written bottom slide let us look loss square loss okay see square corresponding element two matrix pair square root hope sound familiar will take away ease truncate svd get us best approximation matrix accord loss remember actually previous video go factorize matrix two matrix three matrix okay 
use option 1 = uk ⌃k ⇥ = vkt option 2 = uk p ⌃k ⇥ = p ⌃k vkt well use heuristic actually one idea would say okay let us take first second matrix put phi matrix last one put theta another option would say diagonal matrix honestly split two matrix phi theta apply square root say one square root go left another square root go right okay see svd provide way build phi theta matrix 
vector space model semantic • input word-word co-occurrence ( count pmi … ) • method dimensionality reduction ( svd … ) • output similarity vector representation word w v u w ✓v u pmi x w ⇡ ⇥ ⇥ w turnay pd pantel p frequency meaning vector space model semantic 2010 let us summarize realize go build model distributional semantic mean word concurrence matrix filled pmi value concurrence value go represent factorization phi theta matrix can see svd provide us actually see phi u vector theta v vector would embedding vector interested also multiply phi u theta v inner product get value equal let s say pmi left-hand side matrix right want remember notation phi theta matrix phi u theta v vector matrix way think correspond way think every element like pmi somehow equal dot product phi u theta v okay awesome far used square loss svd deal square loss perfect maybe maybe better 
weight square loss glove fill x withlog nuv x x u2w v2w f ( nuv ) ( h u ✓v try another objective + bu + b0v 2 log nuv ) pennington et al glove global vector word representation 2014 min 0 u ✓v bu bv next model call global vector try better nt scared let us break see still square loss weight f function provide weight will see f function increase word concurrence higher particular element matrix important approximate exactly okay question green line f function stop increase point go constant well might remember stop word important start moment word get bigger weight somehow noisy awesome let us look bracket see green part red part red part regional matrix used word concurrence bmi value logarithm okay green part usually would inner product phi u theta v would correspond matrix factorization task s almost b term bias term actually important say well maybe chew well freedom model important train model case svd exact recipe linear algebra build three matrix loss want minimize loss want minimize well try stochastic gradient descent case treat every element matrix example take element perform one step gradient descent update parameter take another element proceed way finally get global vector model trained will obtain phi u theta v vector vector used word embedding actually model rather recent nice way provide word embedding please keep mind let us forget matrix factorization slide let us think would approach build word embedding 
word prediction skip-gram model predict context word give focus word p ( wi h wi+h wi ) = hkh k6=0 p ( wi+k wi ) model probability softmax exp h u ✓v p ( u|v ) = p 0 u0 2w exp h u ✓v still two matrix parameter another approach think task would language modele recap language model probability word give word case case call skip-gram model go produce probability context give focus word example context word come window fix size assume window represent bag word s will go product probability one word give another word would produce probability see formula bottom slide might recognize softmax okay mean indeed normalize want correct probability distribution vocabulary insight inner product phi u theta v inner product correspond similarity two vector take similarity normalize used softmax get probability 
train model log-likelihood maximization = x x nuv log p ( u|v ) u2w v2w word co-occurrence method • sgd online word pair corpus problem • softmax vocabulary slow okay model call skip-gram important unfortunately rather slow softmax computation slow especially big vocabulary actually say let us reformulate whole procedure like 
skip-gram negative sampling ( sgn ) instead predict word another word predict “ yes ” “ ” word pair x x u2w v2w nuv log ( h u ✓v ) + k ev̄ log ( h u ✓v̄ ) max u ✓v • use positive example datum v co-occur u • sample negative example k random v̄ vocabulary train sgd find two matrix parameter ( usual ) see green red part slide green part correspond positive example positive example pair word u v co-occur together take datum right pair word co-occur want predict yes pair word co-occur sample randomly want predict model prediction model sigmoid function will see sigmoid function apply inner product give us probability whether yes okay maybe somehow scared mathematical expectation see actually take mathematical expectation though write theoretical way sample k correspond number sample sample k word vocabulary every give u word use sample term forget expectation get model well build embedding word need normalize anything size vocabulary nice model skip-gram negative sampling efficient used lot 
sgn implicit matrix factorization sgn objective maximize h shift pointwise mutual information nuv n spmi = log nu nv u ✓v equal log k ✓v u x ⇡ ⇥ levy goldberg neural word embedding implicit matrix factorization 2014 ⇥ final slide somehow understand skip-gram negative sampling model still related matrix factorization approach discuss take derivative loss say would best value inner product understand best value inner product shift pmi value like will see pmi value minus logarithm k k number negative sample nice fact publish recent paper say even though skip-gram negative sampling model think matrix however still interpret implicit matrix factorization shift pmi value usual two matrix phi theta rather important fact say recent model still similar previous model like svd decomposition pmi value thing different shift pmi loss okay mathematical part method next video see use model [ music ] 

( evaluate ) [ music ] hey previous video necessary background see inside word2vec doc2vec two model rather famous see use task 
word2vec two architecture • cbow ( continuous bag-of-words ) p ( wi wi h wi+h ) • continuous skip-gram p ( wi h wi+h wi ) two way avoid softmax • negative sampling • hierarchical softmax open-source fast okay let us get start word2vec software package several different variance one variant would continuous bag-of-words mean try predict one word give context word another option would vice versa predict context word give word one call skip-gram softmax computation usually slow produce probability effective way avoid one way would negative sampling might remember previous video already discuss skip-gram negative sampling model one architecture word2vec program open source find code okay use model 
evaluation word similarity test similar word similar vector • linguist know lot “ similar ” • use human judgement word pair • compare spearman ’ correlation two list tiger tiger 1000 tiger tiger medium radio 742 medium radio … tiger cat 737 tiger cat … train car 631 train car … … … … … co ( u v ) one task would produce meaningful similarity word remember can build word embedding sum vector represent meaning word apply cosine similarity vector get measure similarity word test model see actually similarity measure good somehow meaningful well actually complicate question use human judgement see datum set provide linguist look like first table slide example say tiger tiger super similar medium radio also similar extent rank list word pair similarity produce similarity model table right compare two rank list let s say spearman s correlation see whether model somehow agree assessor obviously used human judgement always best way would better use extrinsic evaluation example can build ranking system apply word similarity compute quality ranking system use evaluate model okay anyways let us come next task 
evaluation word analogy • cognitive science well know relational similarity ( vs attributional similarity ) • b b ( man woman king ) co ( b + a0 x ) max x gentner d structure-mapping theoretical framework analogy cognitive science 1983 mikolov et al linguistic regularity continuous space word representation 2013 next task rather appeal hear look vector word example vector king apply arithmetic operation vector like king minus man plus woman get vector closest word vector queen see somehow understand relation word understand man woman related way king queen think analogy like example moscow minus russia plus france equal peru something like say equal mean cosine similarity get maximum target worth task become famous recent paper however start lot cognition science call relational similarity contrary similarity discuss moment call attributional similarity evaluate word analogy task usually rely human judgment dataset say man woman relate king queen many many example try predict last word compute accuracy prediction awesome let us see different model perform two task 
word similarity task performance • word similarity task count-based method ( ppmi svd ) perform par predictive method ( glove sgn ) win width window co-occurrence collection levy et al improve distributional similarity lesson learn word embedding 2015 let us try remember every model first row pmi compute pmi value word long sparse vector pmi word embedding second apply svd pmi matrix get somehow dense low dimensional vector skip-gram negative sampling module discuss lot previous video remember glove well glove also cover previous video measure factorization respect weight square loss might remember green f function increase point go constant overwhelmed frequent word okay four method different way perform matrix factorization maybe implicit matrix factorization obtain word embedding see actually perform really similar different column correspond different dataset word pair see bold best value somehow spread around table old method like svd much worse recent method like skip-gram negative sampling 
word analogy task performance • word analogy task solve 70 % average accuracy add way analogy solve discuss mull modification levy et al improve distributional similarity lesson learn word embedding 2015 haven word analogy task also two datum set one google another microsoft research one take away would quality nice google say 70 % accuray mean 70 % guess right word correctly example guess king minus man plus woman equal queen awesome actually will see problem next video 
paragraph2vec aka doc2vec reason bee know make honey context focus word context dm ( distribute memory ) p ( wi wi h wi+h ) dbow ( distribute bag word ) p ( wi h wi+h d ) okay let us come paragraph2vec doc2vec actually two name model paragraph2vec name go paper doc2vec name go gensim library implement remember word2vec two architecture say produce context give focus word vice versa focus word give context also document id treat document way treat word id fix vocabulary document build embedding document two architecture first architecture dm stand provide probability focus word give everything dbow architecture stand provide probability context give document last one somehow similar skip-gram model right instead focus word condition document use model well use provide document similarity apply example ranking test document similarity provide model good 
evaluation document similarity test similar document similar vector • arxiv triplet paper similar paper b dissimilar paper c • measure accuracy guess dissimilar paper http 12065743 http 0403258 http 14080189 http 12090268 http 13077598 http 0504051 http 9908436 http 9707019 http 11123014 http 11112905 http 11091922 http 13032538 http 0112013 http 9704013 http 14084595 http 07093419 http 0611134 http 09020616 http 9609148 http 9710009 http 0508060 andrew dai cristopher olah quoc le document embedding paragraph vector corr 2015 well need test set set release way paper bottom slide provide triplet archive paper 
evaluation document similarity http 12065743 http 0403258 http 14080189 http 12090268 http 13077598 http 0504051 http 9908436 http 9707019 http 11123014 http 11112905 http 11091922 http 13032538 http 0112013 http 9704013 http 14084595 http 07093419 http 0611134 http 09020616 http 9609148 http 9710009 http 0508060 paper another paper 
evaluation document similarity http 12065743 http 0403258 http 14080189 http 12090268 http 13077598 http 0504051 http 9908436 http 9707019 http 11123014 http 11112905 http 11091922 http 13032538 http 0112013 http 9704013 http 14084595 http 07093419 http 0611134 http 09020616 http 9609148 http 9710009 http 0508060 
evaluation document similarity know similar third paper dissimilar task predict one dissimilar one model model provide good estimate document similarity will compute accuracy prediction task okay 
resume method • word2vec sgn cbow … • doc2vec dbow dm … • python library https evaluation • word similarity analogy • document similarity • interpretability component • geometry embedding space count-based predictive approach different want sum everything cover model call word2vec doc2vec actually even model rather implementation different architecture find example gensim library play different way use model every usage model need dataset evaluate whether usage good whether provide word similarity document similarity good enough way evaluate model would see whether component vector interpretable way look geometry space embedding might complicate go detail way maybe also need one takeaway really need understood count-based method like svd apply pmi metric different predictive base method word2vec magic behind next video actually see problem behind thank [ music ] 

king – man + woman = queen hey previous video see school work nicely lot different task however video raise doubt see especially world analogy task everything smooth 
magical property word2vec reason bee know make honey context focus word context learn word vector predict context ( vice-versa ) obtain vector solve word analogy • king – man + woman = queen • moscow – russia + france = paris demo recap word2vec model trained unsupervised manner mean model see let s say wikipedia obtain word vector word vector obtain nice property example take vector king will subtract vector man add vector woman get vector closest word one queen awesome right look like model can understand meaning language even though datum explicitly well let us look closer detail closest word perform 
closer look analogy task co ( b + a0 x ) max0 x2 { b } king – man + woman = king roger et al ( many ) problem analogical reasoning word vector 2017 see arithmetic expression maximize cosine similarity result expression candidate space exclude three candidate search say source let s say king man woman participate search well know rather important trick usually omitted description word2vec model however let us see would happen look honestly perform maximization whole space picture show would closest neighbor arithmetic expression case search color show ratio name left correspond different category analogy important let us look last one call encyclopedia example king fall one see king minus man plus woman get vector case close b vector king also case close prime vector woman never b prime vector target queen vector see actually let s say 90 % 80 % different analogy find vector close b vector instead target b prime vector well know somehow ruin little bot picture word2vec understand language want dig little bit deeper exclude prime b vector actually find b prime vector exclude end b vector 
closest neighbor b good enough plural noun category google test set 70 % accuracy take closest neighbor vector b b 0 a0 b linzen issue evaluate semantic space used word analogy 2016 think picture shed light thing shift vector prime minus seem close plus woman minus man close mean employ try find closest neighbor well closest neighbor actually b b exclude next closest neighbor indeed b prime say okay king exclude queen find okay maybe use much simple method mean nearest neighbor b apply arithmetic operation well person try say one particular category analogy plural category apple apple orange orange strategy take closest neighbor b result 70 % accuracy see really high accuracy similar can see world2vec back previous video dumb approach 
good accuracy b b close bucket base similarity b b another visualization idea come recent paper say let us split word analogy example several bracket example analogy example b b prime vector similar go right example b b prime vector also similar go left blue bar slide show accuracy wold2vec every bucket easily see blue bar high right low left mean word2vec work really nice analogy b b prime similar work poorly complicate task similar let us see complicate task 
bat dataset inflectional morphology • student student strong stronger follow follow … derivational morphology • bake baker edit editable home homeless … lexicographic semantic • hypernyms meronyms peach fruit sea water player team … • antonyms synonym clean dirty cry scream … encyclopedic semantic • animal cat kitten dog bark … • geography athen greece peru spanish … • person lincoln president lincoln american … • blood red actor actress … let us study type analogy cover diagram actually four main type analogy example find actor actress bottom line kind thing king queen example much first morphological example inflectional morphology mean change form word like orange orange apple apple derivational morphology also change part speech like bake baker play player lot different semantical analogy example hypernyms would example peach fruit cucumber vegetable many example nice one color like blood red sky blue many different option easy build dataset need linguistic expertise anyways look word2vec perform different analogy compare word2vec simple baseline baseline would take closest neighbor one query word 
performance category finley et al analogy reveal word vector compositionality 2017 go line correspond analogy example example one line can correspond apple apple orange orange left point every line performance baseline right point every line performance word2vec mean horizontal line show word2vec better base line line high slope mean word2vec good job see inflectional morphology easier task derivational morphology line horizontal 
gender county cherry-pick finley et al analogy reveal word vector compositionality 2017 happen semantic analogy well nice picture thing left different type analogy horizontal slow push mean word2vec nt work two line red line high slope two example gender like man woman king queen actor actress picture right name entity three red line country capital example really popular world2vec description example moscow russia paris france know famous example kind one actually work word2vec mean one look thing generally worst random different task 
resume • word2vec work fine word similarity • many question word analogy • careful hype okay takeaway insight would careful hype see around always nice dig detail like relation perform would happen little bit different task see whether provide good bad solution look like word2vec work nicely word similarity task example application need understand tap faucet really similar place one category word2vec choice nt blind nt think somehow solve language provide solution word analogy task case work sometimes always nice question research okay next video will talk extension technique like word2vec see current state approach open source implementation use application stay tune get practical advice model build case [ sound ] 

sentence embedding hey cover lot way build word embedding know word mean sometimes need representation sentence s obvious far get case need go sub-word level example might language rich morphology would nice somehow use morphology model actually linguistic really helpful see couple example right 
morphology help problem • language rich morphology • low frequency word oov word use morphology improve word embedding i vulic et al morph-fitting fine-tuning word vector space simple language-specific rule 2017 let us start morphology example english say mature immature relevant irrelevant know prefix change meaning word opposite one antonyms hand understand suffix change semantic word lot example break breaker still similar concept sense idea window propose paper bottom slide follow let us try put word important morphological change together space opposite let us try embedding word prefix change meaning word completely let us put far possible see try put word closer word far away many way like let s say would regularization model loss loss make sure additional constraint okay idea nice sometimes nt linguist tell us morphological pattern language 
fasttext represent word bag character n-grams eg n = 3 gwhere wh whe ere _ _ model word vector sum sub-word vector sgn sim ( u v ) = h fasttext u ✓v sim ( u v ) = x g2gv code pre-train embedding https p bojanowsky et al enrich word vector subword information 2016 h u ✓g well case try brute-force approach would go character n-grams fasttext model propose facebook research really famous good implementation play idea follow let us represent word set character n-grams also let us put word set well example character n-grams size three will example slide usually several n-value like n three six n-grams different length set set represent word use well remember skip-gram negative sampling similarity two word can represent product word word represent vector set vector well sum say sum character n-grams every character n-gram represent vector awesome think idea great work well language rich morphology fasttext model provide nice way represent sub-word union need go another level represent sentence idea build sentence embedding 
sent2vec first idea • average pre-train word vector ( word2vec glove etc ) • maybe use tf-idf weight average sent2vec • learn sentence embedding sum sub-sentence unit 1 x sim ( u ) = h gs | g2gs u ✓g gs set word n-grams sentence code embedding https sent2vec pagliardini et al unsupervised learn sentence embedding used compositional n-gram feature 2017 idea summarize slide simple idea would take pre-train vector let s say word2vec model average obtain embedding sentence well might also weight average example tf-idf weight know might nice approach pre-train vector trained objective might suit well task another idea would somehow represent sentence sum sub-sentence unit let s closer look first go represent similarity word sentence training datum word occur sentence positive pair word occur sentence negative example word occur sentence assume similar model similarity sum sub-union union word n-grams bag word n-grams represent sentence awesome see model similar fasttext model instead character n-grams represent word word n-grams represent sentence also another minor difference average sum one divide size set important see different level language similar idea build general approach level 
starspace general framework entity ( eg sentence ) feature ( eg word ) lot ’ application • text classification eg sentiment analysis • ranking eg ranking web document give query • collaborative filtering-based recommendation • content-based recommendation • embedding graph eg freebase • learn word sentence document embedding code tutorial starspace wu et al starspace emb thing 2017 attempt build general approach find recent paper call starspace idea entity represent feature example word represent character n-grams sentence represent word n-grams go example think recommendation system user represent bag item like example bag movie will learn emb user movie space another example would document classification problem document bag word label example sentiment label rather simple entity represent singleton feature label application try learn produce correct label document will say similarity measure document label high label find supervised datum document low vice-versa build model get embedding label document word space read application github page want cover detail one application 
starspace mode 3 ( sentence embedding ) use case learn pairwise similarity collection similar object eg sentence similarity datum format line collection similar sentence sent1_word1 sent1_word2 … < tab > sent2_word1 sent2_word2 … training • sentence represent bag feature ( word n-grams ) embed predict sentence similarity • similar sentence pair take collection • dissimilar sentence pair sample random sentence embedding let s say supervised datum similar sentence know group sentence duplicate similar let us put one line file let us tab sentence let us space word format feed datum starspace say need train embedding word sentence happen next well similar sentence good source positive example take two sentence line use positive example model similar sentence sample random example take sentence one line random sentence another line say negative example train kind word2vec model sense obtain embedding entity awesome 
deep learn popular option • recurrent neural network ( sequence model ) • convolutional neural network ( much faster ) • recursive neural network ( use hierarchical structure ) linguistic structure back • morphology help build word embedding • recursive neural network tree-lstms dag-lstms etc use syntax span annotation co-reference links… last thing want cover deep learn approach build sentence representation actually everything point rather shallow network speak deep learn can three main trend one trend would obviously recurrent neural network popular nlp another would convolutional neural network actually much faster recurrent neural network seem like super promising approach third one would recursive neural network so-called tree-lstms dynamic acyclic graph dag-lstm kind model use syntax language build hierarchical representation rather awesome approach much time cover need know syntax help us build representation sentence take-away slide would linguistic help us example morphology syntax many many task 
skip-thought vector • predict next previous sentence text • rnn encoder-decoder architecture thought vector h1 h2 input h3 next phrase text < eos > v s1 s2 s3 s4 next phrase text phrase < eos > kiro et al skip-thought vector 2015 https skip-thought last architecture want cover video call skip-thought vector base recurrent neural network idea follow sentence want predict next sentence encode sentence recurrent neural network get hide representation call thought vector try generate next sentence language model already know neural language model conditional neural language model condition thought vector great thing thought vector go represent meaning sentence used embedding actually architecture call encoder-decoder architecture many many detail next week nt realize detail one slide nt worry cover many many detail 

text collection 
text topic 
formal task give • collection text bag-of-words nwd count word w document find • probability word topic wt = p ( w|t ) • probability topic document ✓td = p ( t|d ) 
formal task give • collection text bag-of-words nwd count word w document find • probability word topic definition topic wt = p ( w|t ) • probability topic document ✓td = p ( t|d ) 
need exploration navigation large text collection 
need • social network analysis • dialogue manager chat-bot 
need topic model provide hide semantic representation text many application • categorization classification text • document segmentation summarization • news flow aggregation analysis • recommender system • image caption • bioinformatic ( genome annotation ) • exploratory search • … 
generative model text probabilistic latent semantic analysis ( plsa ) p ( w|d ) = x t2t notation • w – word • – document • – topic p ( w|t ) p ( t|d ) = x t2t p ( w|t ) p ( t|d ) 
generative model text probabilistic latent semantic analysis ( plsa ) p ( w|d ) = x p ( w|t ) p ( t|d ) = t2t law total probability x p ( w ) = p ( w|t ) p ( ) t2t notation • w – word • – document • – topic x t2t p ( w|t ) p ( t|d ) 
generative model text probabilistic latent semantic analysis ( plsa ) p ( w|d ) = x t2t p ( w|t ) p ( t|d ) = x p ( w|t ) p ( t|d ) t2t assumption law total probability x conditional independence p ( w ) = p ( w|t ) p ( ) p ( w|t ) = p ( w|t ) t2t notation • w – word • – document • – topic 
generative model text topic document david blei probabilistic topic model 2012 topic proportion assignment 
matrix way think probabilistic latent semantic analysis p ( w|d ) = x p ( w|t ) p ( t|d ) = t2t x wt ✓td t2t w p ( w|d ) w ≈ w φ × θ 

train plsa hey let us understand train plsa model 
would train model probabilistic latent semantic analysis p ( w|d ) = x p ( w|t ) p ( t|d ) = t2t x t2t parameter model • wt • ✓td – probability word w topic – probability topic document wt ✓td recap topic model predict word document mixture topic parameter model two kind probability distribution phi parameter stand probability word topic theta parameter stand probability topic document probabilistic model datum datum train model estimate parameter likelihood maximization something always help us 
would train model log-likelihood optimization log p ( ) d2d xx w2d ndw log p ( w|d ) ndw max ⇥ x wt ✓td t2t d2d w2d max ⇥ give non-negativity normalization constraint wt ✓td 0 0 x w2w wt 1 x t2t ✓td = 1 top line slide log-likelihood model need maximize respect parameter let us modification formula first let us apply logarithm sum logarithm instead logarithm product let us get rid probability document probability document depend parameter even know model pair forget care probability word document substitute sum topic model say great s want maximize likelihood need remember constraint parameter probability s need non-negative need normalize 
would train model log-likelihood optimization log p ( ) d2d xx w2d ndw log p ( w|d ) ndw max ⇥ x wt ✓td t2t d2d w2d max ⇥ give non-negativity normalization constraint wt ✓td 0 0 x w2w wt 1 x t2t ✓td = 1 notice term need maximize nice logarithm sum something ugly really clear maximize fortunately em-algorithm can hear algorithm course specialization want come algorithm intuitively 
plain text pooh rub nose say nt thought brighten say rain already heffalump would look sky wonder would clear would nt see deep pit half-way down… let us start datum go train model plain text everything 
plain text pooh rub nose say nt thought brighten say rain already heffalump would look sky wonder would clear would nt see deep pit half-way down… topic document topic proportion assignment let us remember know generative model assume every word text one topic generate decide reach next let us pretend moment one slide know topic 
know topic assignments… pooh rub nose say nt thought brighten say rain already heffalump would look sky wonder would clear would nt see deep pit half-way down… let us pretend know word sky rain clear go sub topic number 22 s know assignment would calculate probability word topic know four word topic 
know topic assignments… pooh rub nose say nt thought brighten say rain already heffalump would look sky wonder would clear would nt see deep pit half-way down… would count nwt 1 p ( w = sky|t ) = p = 4 w nwt want calculate probability sky let s say say ` well like one word four word probability one divide four nwt denote count many time certain word connect certain topic imagine would evaluate probability topic document colorful case 
know topic assignments… pooh rub nose say nt thought brighten say rain already heffalump would look sky wonder would clear would nt see deep pit half-way down… would count nwt 1 p ( w = sky|t ) = p = 4 w nwt ntd 4 p ( = t|d ) = p = 54 ntd well s know four word red topic 54 word document s probability example 
plain text pooh rub nose say nt thought brighten say rain already heffalump would look sky wonder would clear would nt see deep pit half-way down… well unfortunately life like know colorful topic assignment plain text s problem somehow estimate assignment somehow estimate probability color every word 
plain text pooh rub nose say nt thought brighten say rain already heffalump would look sky wonder would clear would nt see deep pit half-way down… idea let ’ estimate topic assignment probability p ( w t|d ) p ( w|t ) p ( t|d ) p ( t|d w ) = = p ( w|d ) p ( w|d ) baye rule product rule yes baye rule help us say need probability topic word document apply baye rule product rule understand advise forget formula everything clear apply two rule get estimate probability 
put everything together em-algorithm e-step p ( w|t ) p ( t|d ) wt ✓td p ( t|d w ) = p p ( w|d ) s2t w ✓sd m-step wt ✓td nwt p w nwt ntd p ntd nwt = x ndw p ( t|d w ) ntd = x w ndw p ( t|d w ) hide variable probability topic s time put everything together em-algorithm two step e-step m-step step estimate probability hide variable discuss m-step update parameter discuss simple case know topic assignment exactly know exactly bit complicate compute nwt count many time word connect topic s still doable take word take count word weight probability know e-step s get estimate nwt int counter anymore flow variable still meaning still intuition em-algorithm super powerful technique used time model observable datum hide variable formula need want understand build topic model need repeat e-step m-step iteratively scan datum compute probability topic used current parameter update parameter used current probability topic repeat iterative process converge hopefully get nice topic model trained 

zoo topic model hey know basic topic model call plsa know train topic model world application solve topic modele 
martha ballard ’ diary • diary daily entry course 27 year • topic modele help analyze • reveal topic ( probable word ) • garden garden work clear bean corn warm plant matter cucumber potato plant • church meeting attend afternoon reverend worship foren mr famely st lecture discoarst administer • death day yesterday inform morn year death ye hear expire expire weak dead • shopping butter sugar carry candle wheat store flower want start nice application diary martha ballard big diary writing 27 year s rather complicate person read diary analyze person decide apply topic modele see topic reveal diary example topic see top probable word remember phi metric stand probability word topic exactly word highest probability actually see topic rather intuitively interpretable something garden potato work garden something shopping like sugar flour something else look top word name topic s nice 
martha ballard ’ diary • diary daily entry course 27 year • topic modele help analyze • topic develop time garden ( average year ) emotion ( 1785-1812 ) http s nicer look topic change time example garden topic popular summer diary s popular winter make perfect sense right another topic emotion high probability period life emotional event example one moment high probability correspond moment get husband prison somebody else die something else happened historian say ok interpretable understand topic high probability feel flexible apply topic many application 
latent dirichlet allocation dirichlet prior dir ( | ) = q ( w = ( 0 ) ( wt ) w2w w ) w w wt 1 ✓d = ( ✓td ) t2t 0 = x w > 0 w • inference • variational baye • gibbs sampling • output • posterior probability parameter ( also dirichlet ) asuncion a well m smyth p teh y w smooth inference topic model 2009 need little bit math first model call latent dirichlet allocation guess popular topic model ever propose 2003 david blei actually paper topic model cite work know different plsa model everything say ok still phi theta parameter go dirichlet prior dirichlet distribution rather ugly form need memorize always google important thing say parameter fix value distribution s output model also go distribution parameter two matrix value distribution call posterior distribution also dirichlet hyperparameter course specialization devote bayesian method can learn lot way estimate model train name way one way would variational baye another way would gibbs sampling lot complicate math go detail right instead be go show main path develop new topic model 
bayesian method graphical model ali daud juanzi li lizhu zhou faqir muhammad knowledge discovery direct probabilistic topic model survey 2010 usually person use probabilistic graphical model bayesian inference provide new topic model say ok parameter prior connect way person draw nice picture happen model let us go math detail instead let us look model apply 
hierarchical topic model d blei et al hierarchical topic model nest chinese restaurant process nips-2003 well one extension lda model would hierarchical topic model imagine want topic build hierarchy example topic speech recognition would subtopic topic algorithms see root topic general lexis actually surprising unfortunately general lexis always something see high probability especially root topic model try distill topic say well maybe separate topic stop word nt want see main topic also play 
dynamic topic model david blei probabilistic topic model 2012 another important extension topic model dynamic topic model model say topic evolve time keyword topic one year change year 
dynamic topic model topic detection analysis news flow jianwen zhang yangqiu song changshui zhang shixia liu evolutionary hierarchical dirichlet process multiple correlated time-vary kdd-2010 see probability topic change example news flow know topic bank-related stuff super popular month popular later 
multilingual topic model english corpus italian corpus i vulic w de smet j tang moen probabilistic topic modele multilingual setting short overview methodology application nips-2012 ok one extension multilingual topic model topic something really dependent language mathematics exist everywhere right express different term english italian russian language model capture intuition topic every language expressed different term usually train model parallel datum two wikipedia article topic let s better say particular concept know topic article similar expressed different term s okay 
additive regularization topic model combine extension one model plsa l = x x ndw log d2d w2w artm l + n x i=1 x wt ✓td t2t ⌧i ri ( ✓ ) max ⇥ example regularizer – diversity topic ri ( ) = xx wt w t6=s w k vorontsov a potapenko additive regularization topic model 2015 max ⇥ cover extension topic model believe much literature one natural question might whether way combine requirement one topic model might different approach one approach develop nlp lab call additive regularization topic model idea super simple likelihood plsa model let us additional regularizer let us add likelihood coefficient need formalize requirement regularizer tune tau coefficient say example need better hierarchy rather better dynamic model provide one example regularizer look like imagine need different topic model would great different topic possible try maximize negative pairwise correlation topic exactly written bottom formula pair topic try make different possible 
regularize em-algorithm e-step p ( w|t ) p ( t|d ) wt ✓td p ( t|d w ) = p p ( w|d ) s2t w ✓sd m-step ✓x @ r ndw p ( t|d w ) + wt wt = norm w2w @ wt d2d ✓x ◆ @ r ✓td = norm ndw p ( t|d w ) + ✓td t2t @ ✓td w2d ◆ train model well still use em algorithm e-step hold exactly plsa topic model m-step change slightly thing new green derivative regularizer parameter need add term get maximum likelihood estimation parameter m-step pretty straightforward formalize criterium take derivative can build model 
multimodal topic model k vorontsov et al bigartm open source library regularize multimodal topic modele large collection 2015 show one example many application need model word text additional modality mean metadata user maybe author paper time stamp category many thing go document word build somehow model 
multi-artm incorporate token additional modality plsa = x x ndw log d2d w2w x wt ✓td max wt ✓td max t2t ⇥ multi-artm x m2m x x d2d w2w ndw log x t2t • topic characterize several probability distribution • parameter still trained em-algorithm ⇥ actually use absolutely intuition let us instead one likelihood weight likelihood let us likelihood every modality let us weigh modality coefficient every modality actually different vocabulary treat token author modality separate vocabulary every topic distribution word distribution author well five modality every topic represent five distinct distribution one cool thing multimodal topic model represent entity hide space topic way somehow unify information model example find probable topic word probable topic time stamp let s say compare time stamp word say what similar word day 
inter-modality similarity 2015-12-18 star war release 2016-02-29 oscar 2015-05-09 victory day jedi sith fett anakin chewbacca film series hamill prequel awaken boyega statuette award nomination linklater oscar birdman win criticism director lubezki great anniversary normandy parade demonstration vladimir celebration concentration auschwitz photograph potapenko popov vorontsov interpretable probabilistic embedding bridge gap topic model neural network 2017 example exactly corpus time stamp document model topic word time stamp get know closest word time stamp correspond oscar date would oscar birdman word really related date way emb different modality one space somehow find way build similarity 
library topic modele • bigartm open-source library additive regularization topic model bigartmorg • gensim library text analysis python gensim • mallet library text analysis java malletcsumassedu • vowpal wabbit fast implementation online lda would action want build topic model well probably need library bigartm library implementation last approach mentioned gensim mallet implement online lda topic model gensim build python mallet build java vowpal wabbit implementation online lda topic model know super fast maybe s also good idea check 
word visualization j chuang c d man j heer – termite visualization technique assess textual topic model 2012 finally word visualization topic model never get large collection easy represent output model probability distribution way person understand example visualize phi metric word topic s metric see group word correspond every certain topic together see blue topic term one social network 
380 way visualize textvislnuse actually visualization topic model whole world website contain 380 way visualize topic model want end video ask explore maybe moment get know topic model build different colorful representation datum 

introduction machine translation [ music ] hi everyone week sequence sequence task lot nlp one obvious example would machine translation sequence word one language input want produce sequence word language output think example example summarization also sequence sequence task think machine translation one language monolingual machine translation well cover example end week let us start statistical machine translation neural machine translation see actually technique super similar approach example see alignment word alignment need statistical machine translation see attention mechanism neural network kind similar meaning task 
machine translation okay let us begin think need tell machine translation important know would better start two question two question actually skip lot course course two important question speak one question datum another question evaluation get real task life nlp task usually model plane usually datum evaluation fancy neuro-architecture good datum nt settle evaluation procedure be go good result 
parallel datum parallel corpus • europarl • movie subtitle • translate news book • wikipedia ( comparable ) • http lot ’ problem datum • noisy • specific domain • rare language pair • align enough first datum well kind datum need machine translation need parallel corpus need text one language need translation another language come source think well one source well maybe obvious one good source european parliament proceedings text several language maybe 20 language exact translation one statement nice use domain would movie subtitle translate many language nice something useful still useful would book translation wikipedia article example wikipedia guarantee text two language something similar example vague translation topic least call corpus comparable parallel opus website nice overview many source please check want discuss something nice problem datum actually lot problem datum kind problem happen machine translation well first usually datum come specific domain imagine movie subtitle want train system scientific paper translation s go work right need close domain need know transfer knowledge one domain another domain something think decent amount datum language pair like english french english german probably rare language pair really lot datum s huge problem also noisy enough datum align well alignment mean need know correspondence sentence even better correspondence word sentence luxury usually least huge amount datum okay think s clear datum second thing evaluation 
evaluation • compare two arbitrary translation • low agreement rate even reviewer • bleu score – popular automatic technique well say parallel datum nt split train test test set compare correct translation produce system well know translation wrong occur reference know language relative every translator would different translation mean system produce something different nt mean yet wrong well nice answer question mean problem yes one thing multiple reference let s say five reference compare system output thing careful compare definitely exact match right something intelligent be go show blue score know popular measure machine translation try somehow softly measure whether system output somehow similar reference translation 
evaluation • compare two arbitrary translation • low agreement rate even reviewer • bleu score – popular automatic technique reference e-mail send tuesday system output letter send tuesday okay let show example reference translation output system try compare well remember nice tool call engram compute unigram bigram trigram idea use well first try compute precision mean look system output six word six unigram compute many actually occur reference 
evaluation • compare two arbitrary translation • low agreement rate even reviewer • bleu score – popular automatic technique reference e-mail send tuesday system output letter send tuesday 1-grams 4 6 unigram precision core 4 tell would bigram score well bigram score 3 5 
evaluation • compare two arbitrary translation • low agreement rate even reviewer • bleu score – popular automatic technique reference e-mail send tuesday system output letter send tuesday 1-grams 4 6 2-grams 3 5 5 bigram system output 3 send send tuesday occur reference 
evaluation • compare two arbitrary translation • low agreement rate even reviewer • bleu score – popular automatic technique reference e-mail send tuesday system output letter send tuesday 1-grams 4 6 2-grams 3 5 3-grams 2 4 4-grams 1 3 proceed compute 3-grams score 4-grams score s good maybe average measure well can one problem well imagine system try super precise good system output super short sentence right be sure union gram occur output output punish penalty model brevity score 
evaluation • compare two arbitrary translation • low agreement rate even reviewer • bleu score – popular automatic technique reference e-mail send tuesday system output letter send tuesday 1-grams 4 6 2-grams 3 5 3-grams 2 4 4-grams 1 3 brevity min ( 1 5 ) brevity penalty say divide length output length reference system output two short sentence get know compute bleu score value 
evaluation • compare two arbitrary translation • low agreement rate even reviewer • bleu score – popular automatic technique reference e-mail send tuesday system output letter send tuesday 1-grams 4 6 r 2-grams 3 5 3 2 1 4 4 bleu = 1 · · · · 3-grams 2 4 6 5 4 3 4-grams 1 3 brevity min ( 1 5 ) like average root average union gram bigram 3-gram 4-gram s course multiply average brevity 
mandatory slide interlingual semantic transfer generation analysis syntactic transfer direct source henry s thompson https target okay let us speak system actually work kind mandatory slide machine translation kind tutorial machine translation decide exception show idea like source sentence want translate get target sentence first thing direct transfer translate source sentence word word get target sentence well maybe s super good right ever study foreign language know dictionary translation every word usually get nice coherent translation probably would better go synthetic level syntax analysis transfer generate target sentence know look like syntactic level even better can try go semantic layer first analyze source sentence understand meaning part sentence somehow transfer meaning language generate good syntactic structure good meaning dream like best thing can ever think would interlingual interlingual mean n ice representation whole source sentence enough generate whole target sentence actually still dream still dream translator kind system sound appeal neural translation system somehow mechanism resemble show couple slide 
roller-coaster machine translation 1954 georgetown ibm experiment russian english • claim mt would solve within 3-5 year 1966 alpac report • conclude mt expensive ineffective okay want show brief history area like area machine translation bright dark period 1954 great expectation ibm experiment translate 60 sentence russian english say s easy solve machine translation task completely three five year try work work lot many year conclude actually s easy say well machine translation expensive automatic machine translation system better focus tool help human translator provide good quality translation know great expectation disappointment make area silent 
two main paradigm statistical machine translation ( smt ) • 1988 – word-base model ( ibm model ) • 2003 – phrase-based model ( philip koehn ) • 2006 – google translate ( mosis next year ) neural machine translation ( nmt ) • 2013 – first paper pure nmt • 2015 – nmt enter share task ( wmt iwslt ) • 2016 – launch production company 1988 ibm researcher propose word-base machine translation system machine translation system rather simple cover kind video next video system kind first work system machine translation nice next important step phrase base machine translation system propose philip koehn probably person mean statistical machine translation definitely know google translate right maybe nt hear mosis mosis system allow researcher build machine translation system allow train model compare nice tool researcher make available extent obviously important step neural machine translation amazing fast neural machine translation system can go research paper production usually big gap two thing case two three year amazing idea propose can implement launch many company 2016 neutral machine translation might wonder wmt workshop machine translation kind annual competition annual event share task mean compare system nice venue compare different system different researcher company see trait machine translation happen every year usually person research area keep eye nice thing 
zero-shot translation english google neural machine translation english japanese japanese korean korean https zero-shot-translation-with-googleshtml slide intralingual promise show google neural machine translation work actually lot hype around maybe even much still idea train system pair language example english japanese japanese english english korean pair train encoder decoder architecture mean encoder encode sentence hide representation decoder take hide representation decodes target sentence nice thing take encoder let s say japanese decoder korean take somehow work nicely even though system never see japanese korean translation see zero-shot translation never see japanese korean build nice encoder nice decoder stack get path seem like hide representation kind universal language pair well completely true least promising result [ music ] 

say english receive french today cover one main idea statistical machine translation 
main equation • give french ( foreign ) sentence f • find english translation e e⇤ = argmax p ( e|f ) e2e 1993 brown et al “ mathematics statistical machine translation ” imagine sentence let s say french foreign language want translation english well try compute probability english sentence give french sentence want maximize probability take sentence give maximum probability right sound intuitively 
main equation • give french ( foreign ) sentence f • find english translation e p ( f e ) p ( e ) e = argmax p ( e|f ) = argmax = p ( f ) e2e e2e ⇤ 1993 brown et al “ mathematics statistical machine translation ” let us apply base rule let us say instead compute probability e give f would better compute probability f give e multiply probability english sentence also normalize denominator idea simplify formula well actually denominator nt depend english sentence 
main equation • give french ( foreign ) sentence f • find english translation e p ( f e ) p ( e ) e = argmax p ( e|f ) = argmax = p ( f ) e2e e2e ⇤ = argmax p ( e ) p ( f e ) e2e 1993 brown et al “ mathematics statistical machine translation ” mean get rid okay formula question easier like original formula 
easier deal e⇤ = argmax p ( e ) p ( f e ) e2e language model translation model • p ( e ) model fluency translation • p ( f e ) model adequacy translation • argmax search problem implement decoder slide go explain two model decouple complicate problem two simple problem one problem language modele actually know lot produce meaningful probability sentence word problem translation model model nt think coherent sentence think good translation e f end something related source sentence two model language adequacy translation argmax perform search space find sentence english give best probability one interpretation 
noisy chanel noisy channel channel source p ( e ) letter send tuesday p ( f e ) channel output le lettre été envoyé le mardi noisy channel super popular idea definitely need know actually super simple source sentence probability source sentence go noisy channel noisy channel represent conditional probability get output give input channel output obtain french sentence let s say source sentence spoilt channel obtain french rest video model two probability probability sentence probability translation give sentence 
language model p ( e ) p ( e ) = p ( e1 ) p ( e2 e1 ) p ( ek e1 ek n-gram model neural network good day < eos > good 1 ) okay first language model know lot cover week two one slide recap need compute probability sentence word apply chain rule know factorize probability next word give previous history use markov assumption end n-gram language model use neural language model lstm produce next word need previous word 
translation model p ( f|e ) p ( f e ) = p ( f1 f2 fj e1 e2 ei ) f ( foreign ) крику много а шерсти мало e ( english ) great cry little wool translation model well easy imagine sequence word one language need produce probability sequence word language example foreign language like russian english language two sentence produce probability well obvious let us start word level understand something level separate word sentence okay 
translation model p ( f|e ) can learn translation probability separate word vf 01 01 02 04 01 08 02 02 03 wool 05 02 07 09 p ( fj ei ) 01 01 translation table probability russian word give english word normalize right row matrix normalize one translation learn look dictionary build somehow 
translation model p ( f|e ) build probability whole sentence p ( f e ) = magic factorization p ( fj ei ) okay s doable build probability whole sentence give separate probability 
translation model p ( f|e ) build probability whole sentence p ( f e ) = magic factorization p ( fj ei ) reordering крику много а шерсти мало great cry little wool need word alignment problem reordering language like 
word alignment one-to-many many-to-one аппетит приходит во время еды appetite come eating word disappear appear nowhere у каждой пули свое назначение every bullet billet even worse one many many one correspondence example word appetit correspond appetite word correspond two russian word [ foreign ] mean need model build alignment another example would word appear disappear example article auxiliary word happen one language ca nt vanish language unique word alignment model topic fall next video 

word alignment model 
word alignment “ english language word order put » - yoda 
word alignment task give corpus ( e f ) sentence pair • english source e = ( e1 e2 ei ) • foreign target f = ( f1 f2 fj ) predict • alignment e f e appetite come eating f аппетит приходит во время еды 
recap baye ’ rule e⇤ = argmax p ( e ) p ( f e ) e2e language model translation model • p ( e ) model fluency translation • p ( f e ) model adequacy translation • argmax search problem implement decoder 
word alignment matrix appetite come eating 
word alignment matrix appetite come eating j target word allow one source 
a1 = 2 word alignment matrix appetite come eating j target word allow one source 
a1 = 2 a2 = 3 word alignment matrix appetite come eating j target word allow one source 
a1 = 2 a2 = 3 a3 = 4 word alignment matrix appetite come eating j target word allow one source 
a1 = 2 a2 = 3 a3 = 4 a4 = 4 word alignment matrix appetite come eating j target word allow one source 
a1 = 2 a2 = 3 a3 = 4 a4 = 4 a5 = 5 word alignment matrix appetite come eating j target word allow one source 
sketch learn algorithm probabilistic model ( generative story ) give e model generation f p ( f a|e ⇥ ) = creative step • parametrize model • complicate unrealistic 
sketch learn algorithm probabilistic model ( generative story ) give e model generation f p ( f a|e ⇥ ) = observable variable creative step • parametrize model • complicate unrealistic 
sketch learn algorithm probabilistic model ( generative story ) give e model generation f p ( f a|e ⇥ ) = hide variable observable variable creative step • parametrize model • complicate unrealistic 
sketch learn algorithm probabilistic model ( generative story ) give e model generation f p ( f a|e ⇥ ) = hide variable parameter observable variable creative step • parametrize model • complicate unrealistic 
sketch learn algorithm likelihood maximization incomplete datum p ( f e ⇥ ) = x p ( f a|e ⇥ ) max ⇥ 
sketch learn algorithm likelihood maximization incomplete datum p ( f e ⇥ ) = x p ( f a|e ⇥ ) max ⇥ em-algorithm rescue iterative process • e-step estimate posterior probability alignment • m-step update ⇥ – parameter model 
generative story p ( f a|e ) = p ( j|e ) j j=1 j 1 j 1 p ( aj a1 f1 j e ) ⇥ choose length foreign sentence 
generative story p ( f a|e ) = p ( j|e ) j j=1 j 1 j 1 p ( aj a1 f1 j e ) ⇥ choose length foreign sentence choose alignment word ( give lot thing ) 
generative story p ( f a|e ) = p ( j|e ) j j=1 j 1 j 1 p ( aj a1 f1 j e ) ⇥ ⇥p ( fj aj aj1 1 f1j 1 j e ) choose length foreign sentence choose alignment word ( give lot thing ) choose word ( give lot thing ) 
ibm model 1 p ( f a|e ) = p ( j|e ) j j=1 p ( aj ) p ( fj aj e ) uniform prior ` translation table ( fj eaj ) + model simple many parameter – alignment prior depend word position 
translation table vf 01 01 02 04 01 08 02 02 03 wool 05 02 07 09 p ( fj ei ) 01 01 
ibm model 2 p ( f a|e ) = p ( j|e ) j j=1 p ( aj j j ) p ( fj aj e ) position-based prior ( aj j j ) translation table ( fj eaj ) + alignment depend position-based prior – quite lot parameter alignment 
position-based prior • pair length sentence • ⇥ j matrix probability j dyer et al simple fast effective reparameterization ibm model 2 2013 
re-parametrization dyer et al 2013 • know ’ go diagonal – let ’ model diagonal • much less parameter easier train small datum j dyer et al simple fast effective reparameterization ibm model 2 2013 
hmm prior p ( f a|e ) = j j=1 p ( aj aj transition probability ( aj aj 1 j ) 1 j ) p ( fj aj e ) translation table ( fj eaj ) e cat grey dark f в темноте все кошки серы 
resume • ibm model – first work system mt • lot ’ problem model 1 2 • deal spurious word • control fertility • … • many-to-many alignment • phrase-based machine translation 

encoder-decoder architecture [ sound ] hey everyone be go discuss important technique neural network go speak encoder-decoder architecture attention mechanism cover example neural machine translation mostly propose machine translation originally apply many many task example think summarization simplification text sequence sequence chatbot many many other 
sequence sequence cat gray decoder encoder все кошки серы v let us start general idea architecture sequence input would want get sequence output example can two sequence different language right encoder task encoder build hide representation input sentence hide way get green hide vector try encode whole meaning input sentence sometimes vector also call thought vector encode thought sentence encoder task decode thought vector context vector output representation example sequence word language type encoder can well one obvious type would current neural network actually option aware also convolutional neural network fast nice also encode meaning sentence can also hierarchical structure example recursive neural network try use syntax language build representation hierarchically bottom top understand sentence way 
sequence sequence cat gray < eos > h1 h2 h3 v s1 s2 s3 s4 все кошки серы < eos > cat gray ilya sutskever oriol vinyal quoc v le sequence sequence learn neural network 2014 okay first example sequence sequence architecture model propose 2014 rather simple say lcm module rnn module encode input sentence end sentence token point point understand state thought vector context vector need decode start moment decode conditional language model be already familiar language model neural network condition context vector green vector okay language model usually fit output previous state input next state generate next word one one 
sequence sequence все кошки серы < eos > cat gray < eos > cat gray let us go deeper stack several layer lstm model straightforwardly like let us move forward speak little bit different variant architecture 
sequence sequence h1 все h2 cat s1 s2 s3 h3 v gray < eos > s4 кошки серы < eos > cho et al learn phrase representation used rnn encoder-decoder statistical machine translation 2014 s5 one problem previous architecture green context letter forgotten feed input first state decoder likely forget come end output sentence would better feed every moment architecture exactly say every stage decoder three kind error go first error previous state error context vector current input output previous state 
sequence sequence p ( y1 yj x1 xi ) = j j=1 p ( yj v y1 yj 1 ) • encoder map source sequence hide vector rnn hi = f ( hi 1 xi ) v = hi • decoder perform language modele give vector rnn sj = g ( sj 1 [ yj 1 v ] ) • prediction ( simplest way ) p ( yj v y1 yj 1 ) = sof tmax ( u sj + b ) okay let us go detail formula sequence modele task conditional need produce probability one sequence give another sequence factorize used chain rule also importantly see x variable need anymore encode v vector v vector obtain last hide state encoder encoder recurrent neural network decoder also recurrent neural network however input right see concatenate current input v vector mean use kind information three error transition get prediction model well easiest way soft mark right decoder rnn hide state rnn call sj apply linear layer softmax get probability current word give everything 
hide representation good… ilya sutskever oriol vinyal quoc v le sequence sequence learn neural network 2014 awesome let us try see whether v vector somehow meaningful one way say okay let s say three dimensional hide vector let us dimensional reduction example ts & e pca let us plot two dimension see vector see representation sentence close s nice model capture active passive voice nt actually matter meaning sentence example see sentence give card give card close space okay even though representation nice still bottleneck 
… still bottleneck cat gray decoder encoder v все кошки серы bottleneck think avoid avoid go attention mechanism topic next video [ sound ] 

attention mechanism 
attention mechanism decoder yj yj+1 … sj 1 sj sj+1 … ↵ 1j j ↵2 attention encoder 1 ↵ ij ↵3j yj h1 h2 h3 x1 x2 x3 … hi xi bahdanau et al - neural machine translation jointly learn align translate 2015 
attention mechanism • encoder state weight obtain representation relevant decoder state vj = x ↵ij hi i=1 • weight learnt find relevant encoder position ↵ij = pi exp ( sim ( hi sj i0 1 1 ) ) exp ( sim ( hi0 sj 1 ) ) 
compute attention weight • additive attention sim ( hi sj ) = wt tanh ( wh hi + w sj ) • multiplicative attention sim ( hi sj ) = hti w sj • dot product also work sim ( hi sj ) = hti sj 
put together p ( y1 yj x1 xi ) = j j=1 p ( yj vj y1 yj 1 ) • still encoder-decoder architecture rnns hi = f ( hi 1 xi ) sj = g ( sj 1 [ yj 1 vj ] ) • source representation differ position j decoder 
help long sentence nmt attention bahdanau et al neural machine translation jointly learn align translate 2015 
example attention ( alignment ) bahdanau et al neural machine translation jointly learn align translate 2015 
attention similar human • human save time attention save time read ( ie look relevant part sentence ) • machine waste time compute attention weight model carefully examine position thus waste even time 
local attention find relevant position ajin source • monotonic alignment aj = j • predictive alignment aj = · ( bt tanh ( w sj ) ) attend position within window [ aj h aj + h ] • compute score usual • probably multiply gaussian center aj luong et al effective approach attention-based neural machine translation 2015 
global vs local attention system perplexity bleu global ( location ) 64 193 global ( dot ) 61 205 global ( mult ) 61 195 local-m ( dot ) > 70 x local-m ( mult ) 62 204 local-p ( dot ) 66 196 local-p ( mult ) 59 209 luong et al effective approach attention-based neural machine translation 2015 
global vs local attention system perplexity bleu w sj global ( location ) 64 193 hti sj global ( dot ) 61 205 global ( mult ) 61 195 local-m ( dot ) > 70 x local-m ( mult ) 62 204 local-p ( dot ) 66 196 local-p ( mult ) 59 209 hti w sj luong et al effective approach attention-based neural machine translation 2015 

conversational chat-bot [ music ] hey video go discuss apply encode decode attention architecture hot topic nowadays chat-bot 
mean chat-bot goal-oriented • narrow domain • specific question task • example call center • model usually retrieval-based chit-chat • general conversation • human-like experience • example entertain bot • model generative first let us understand mean chat-bot hot topic every one mean something slightly different first goal-oriented bot bot nice conversation us goal-oriented bot usually speak narrow domain specific task fulfill example bot call center bank help customer need worth serve specific answer specific question customer mean usually bot base retrieval approach example can database information will solve answer user entertain bot bot conversation us usually call chit chat bot model would generative 
model pro con retrieval-based model generative model • use repository predefined response • generate new response scratch • grammatical mistake • make mistake ( especially longer sentence ) • unable handle unseen case • impression ’ talk human mean generative mean generate new response compare retrieval-based model would get predefined response rank repository would pattern would use pattern get specific reply pro con two model obviously generate model freedom generate whatever want also mistake complicate build model video will speak generative model conversational chat-bot bot goal assist user specific need cover next week 
sequence sequence • lstms encoder decoder • attention ( least reversed input ) • beam search best reply chat-bot ’ reply decoder encoder incoming message okay let us recap can encoder encode incoming message bot decoder would generate bot s response example lstms encoder decoder would also want attention one alternative attention would reversed input actually simple thing study attention shall say maybe need reverse input sentence thought vector sentence somehow meaningful understand let us cover one technical detail need understand build system kind 
padding eos end sentence pad filler go start decode unk unknown word vocabulary q fine q [ pad pad pad pad pad pad “ ” “ ” “ ” “ ” ] [ go “ ” “ ” “ fine ” “ ” eos pad pad pad pad ] detail set effect sequence different length okay question example need somehow pad question fix length need pack batch send batch neural network suggest technical implementation detail implement static recurrent neural network need pad sequence pad token end sequence can see end sequence absolutely meaningful right pad pad pad try build thought vector base maybe nice s reverse everything word end sequence encode decoder get answer also pad news 
bucketing bucketing solve problem high variance length sentence putt bucket different size bucket length interval ( 0 5 ] [ 5 10 ] ( 10 20 ] … q fine q [ pad “ ” “ ” “ ” “ ” ] [ go “ ” “ ” “ fine ” “ ” eos pad pad pad pad ] one idea would bucketing mean let us group sentence bucket base length example sentence length less five would go first bucket pad length five approach give us opportunity many pad token adaptable length base maximum length bucket important thing put bucket different batch make sure recurrent neural network get bucket different length one batch 
trained movie subtitle human purpose live machine live forever human purpose existence machine find happen get planet earth human machine be middle nowhere work problem though • unrealistic topic • dramatic response oriol vinyal quoc le neural conversational model 2015 https okay rest video be go show chat-bot work also discuss work will see analysis problem idea fix see human tool machine talk trained movie subtitle rather impressive purpose live machine say live forever okay sound good also notice dependent datum set use train model try use model let see assistant bank maybe s good idea response dramatic topic might unrealistic important understand specific property outcome base domain datum 
trained call machine hi human hi machine can please let know operate system used human linux machine ok issue system performance nasser tool human machine window machine human used linux machine let take remote machine check issue push properly human ok oriol vinyal quoc le neural conversational model 2015 https want use call let us train call meaningful lexis inconsistent chat-bot say operate system machine user say linux turn machine say window machine user answer nice bot nt remember happen 
context conversation • rnn keep track intent human hi machine may help today human computer respond machine oh sorry hear get error message human yes machine recent change make computer human update computer machine see window 7 human linux machine ok used affected computer right human yao et al attention intention neural network conversation model 2015 conversation try fix paper say would need track intent context conversation separate recurrent neural network somehow memorize bot topic already cover need ask operate system machine let s say see problem example dialogue 
coherent personality • 4-layer seq2seq trained 25 million twitter snippet human live machine live los angele human city live machine live madrid human country live machine england • persona-based model human machine ’ england human city live machine live london li et al persona-based neural conversation model 2016 another important problem bot personality try ask bot live bot say live los angele sound okay ask bot get response trained datum question answer bot know consistency one idea would build persona-based model mean need memorize bot personality train iton coherent piece dialogue different person build knowledge person ask country city still get coherent response 
diversity response always popular response • love • ’ know top-3 response ( much choice ) • tomorrow • wan na get together tomorrow • suggest meet tomorrow kannan et al smart reply automate response suggestion email kdd 2016 another problem diversity response smart reply technology google say help answer gmail automatically example see email let us meet discuss want get propose response model would propose tomorrow tomorrow suggest tomorrow diversity user pick one thing also another problem would two popular response come email enough diversity love even email colleague good chat-boat cope idea may track 
intent cluster kannan et al smart reply automate response suggestion email kdd 2016 one idea would intent cluster response example small supervised datum type response example label time friday something like actually graph different response similarity response build bearing distributional semantic model labele node graph supervised datum want propagate knowledge label graph technique call label propagation graph expander library implement technique main idea try propagate label response way close response get close label way label already know supervision stay awesome method different idea something cluster pick one example every cluster suggest user 
google smart reply kannan et al smart reply automate response suggestion email kdd 2016 get nice query top generate response see tuesday wednesday day see diversity s want 
still human demo cherry-pick conversation look like ’ meaningful conversation actually try quickly go rail andrew ng http http well even though bot try meaningful conversation see still many problem easy understand speaking bot human s actually careful hype realize well indeed promising good opportunity future current model still human [ music ] 

one-size fit 
sequence sequence • machine translation • summarization • text simplification • language code • chit-chat bot • question answer • listen attend spell speech recognition • show attend tell image caption generation • … 
sequence sequence • machine translation • summarization • text simplification • language code • chit-chat bot • question answer • listen attend spell speech recognition • show attend tell image caption generation • … 
summarization document summary 
summarization original text alice bob take train visit zoo see baby giraffe lion flock colorful tropical bird extractive summary alice bob visit zoo see flock bird abstractive summary alice bob visit zoo see animal bird https text-summarization-with-tensorflowhtml 
sequence sequence summary document decoder encoder full text document 
google research blog dataset annotate english gigaword – 10 mln document ldc2012t21 model sequence sequence attention + beam search code open-source tf implementation textsum result 
google research blog input article 1st sentence metro-goldwyn-mayer report third-quarter net loss dlrs 16 million due mainly effect accounting rule adopt year model-written headline mgm report 16 million net loss higher revenue start july 1 island hainan curb spread province hainan southern china disease implement strict market access control incoming livestock animal product prevent possible spread epidemic disease https text-summarization-with-tensorflowhtml 
encoder-decoder framework • machine translation • summarization • text simplification • language code • chit-chat bot • question answer • listen attend spell speech recognition • show attend tell image caption generation • … 
simplification text simplification – reduce lexical syntactical complexity text a normal isolde arrive side tristan die name lip simple isolde arrive side tristan die speaking name b normal alfonso perez munoz usually refer alfonso former spanish footballer striker position simple alfonso perez former spanish football player c normal endemic type species especially likely develop island geographical isolation simple endemic type likely develop island isolate coster et al simple english wikipedia new text simplification task 2011 
operation simplify text splitting source text deletion reordering paraphrase lexical substitution syntactic transformation xu et al optimize statistical machine translation text simplification 2016 tong wang et al text simplification used neural machine translation aaai-16 
rule-based approach paraphrase • synchronous context-free grammar ( scfg ) rule • uppercase indicate non-terminal symbol • paraphrase database http xu et al optimize statistical machine translation text simplification 2016 
simplification encoder-decoder framework – yes network might learn copy content… force simplify reinforcement learn used weak supervision • action output next word yj • policy p ( yj x y1 yj 1 ) • reward adequacy + fluency + simplicity reward come whole sequence generate zhang lapata sentence simplification deep reinforcement learn 2017 
simplification y1 y2 y3 y4 decoder get action encoder update policy simplicity relevance reinforce x1 x2 x3 zhang lapata sentence simplification deep reinforcement learn 2017 fluency 
measure simplicity input retain reference delete system human reference input unchanged system reference input system output input correctly delete system replace content reference xu et al optimize statistical machine translation text simplification 2016 potentially incorrect system output 
measure simplicity sari ( system reference input ) – arithmetic average n-gram precision recall • addition • copy • deletion example precision addition precision = p g2o [ g p g2o 2 ( \ i¯ \ r ) ] ¯ [ g 2 ( \ ) ] xu et al optimize statistical machine translation text simplification 2016 
sari example input 95 species currently accept ref-1 95 species currently know ref-2 95 species accept ref-3 95 species accept output-1 95 get output-2 95 species agree output-3 95 species currently agree xu et al optimize statistical machine translation text simplification 2016 02683 07594 05890 
compare bleu input 95 species currently accept ref-1 95 species currently know ref-2 95 species accept ref-3 95 species accept output-1 95 get output-2 95 species agree output-3 95 species currently agree bleu distinguish output 2 3 xu et al optimize statistical machine translation text simplification 2016 01562 06435 06435 

pointer-generator network hey video be go cover one nice paper summarization recent paper chris man group nice tell us one hand use encoder-decoder architecture work somehow hand think little bit improve lot improvement base pointer network also useful tool aware also sometimes rather hand-wavy explanation architecture picture sometimes good go detail see actual formula s want precise video end video able understand detail architecture 
seq2seq + attention recap first usually encoder example bidirectional lstm attention mechanism 
seq2seq + attention mean produce probability tell us important moment input sentence see arrow right slide idea arrow mean come 
seq2seq + attention well attention mechanism important moment encoder base current moment decoder definitely yellow part decoder current state decoder tell us compute attention complete scheme say use attention mechanism generate distribution vocabulary awesome recap encoder-decoder attention architecture let us see work sentence 
seq2seq + attention try get summary 
original text ( truncate ) lago nigeria ( cnn ) day winning nigeria ’ presidency muhammadu buhari tell cnn ’ christiane amanpour plan aggressively fight corruption long plague nigeria go root nation ’ unrest buhari say ’ “ rapidly give attention ” curb violence northeast part nigeria terrorist group boko haram operate cooperate neighboring nation chad cameroon niger say administration confident able thwart criminal other contribute nigeria ’ instability first time nigeria ’ history opposition defeat ruling party democratic election buhari defeat incumbent goodluck jonathan 2 million vote accord nigeria ’ independent national electoral commission win come long history military rule coup botch attempt democracy africa ’ populous nation summary would like first see unk token vocabulary big enough also problem paragraph try improve one problem model abstractive model generate lot nt know sometimes better copy something input 
seq2seq + attention unk unk say administration confident able destabilize nigeria ’ economy unk say administration confident able thwart criminal nigerians say country long nigeria nigeria ’ economy next architecture tell us let us closer look formula see improve model first attention distribution remember notation remember h well h encoder state decoder state use compute attention weight apply softmax get probability use probability weigh encoder state get v_j v_j context vector specific position j decoder use see video use compute next state decoder model go little bit simple way decoder normal rnn model take state rnn model s_j concatenate v_j use produce probability outcome concatenate apply transformation softmax get probability word vocabulary improve model 
closer look formula attention distribution ( source position ) eji = wt tanh ( wh hi + w sj + battn ) pj = sof tmax ( ej ) vocabulary distribution ( generative model ) vj = x pji hi pvocab = sof tmax ( v 0 ( v [ sj vj ] + b ) + b0 ) would want copy distribution distribution tell us sometimes nice copy something input well attention distribution already probability different moment input sum word example see two time input sequence let us say probability equal sum two way will get distribution word occur input 
closer look formula copy distribution ( word source ) pcopy ( w ) = x pji xi w soon possible attention distribution copy distribution final thing mixture two distribution one copy distribution tell word input good another distribution generative model discuss 
pointer-generator network copy distribution vocabulary distribution little bit formula weigh two distribution weigh probability p generation also sum function every thing green slide parameter learn parameter learn produce probability weigh two kind distribution weighting coefficient depend everything context vector v_j decoder state s_j current input decoder apply transformation everything sigmoid get probability training objective model would usual cross-entropy loss final distribution try predict word need predict similar likelihood maximization 
closer look formula final distribution pf inal = pgen pvocab + ( 1 pgen ) pcopy pgen = ( wvt vj + wst sj + wxt yj 1 training loss = j x 1 log pf inal ( yj ) j j=1 + bgen ) need optimize subjective 
pointer-generator network whole architecture encoder attention yellow decoder two kind distribution weigh together get final distribution top 
pointer-gen muhammadu buhari say plan aggressively fight corruption northeast part nigeria say ’ “ rapidly give attention ” curb violence northeast part nigeria say administration confident able thwart criminal let us see work call pointer-generation model two piece generative model pointer network part copy phrase input would call pointer network see good learn extract piece text one drawback see model repeat sentence piece sentence 
coverage mechanism coverage vector j c = j 1 x p j0 j 0 0 modify attention eji = wt tanh ( wh hi + w sj + wc cji + battn ) coverage loss covlossj = x min ( pji cji ) need one trick trick call coverage mechanism remember attention probability know much attention give every distinct piece input let us accumulate every step go sum attention distribution coverage vector coverage vector know certain piece attend already many time compute attention well compute attention would also need take account coverage vector difference one term coverage vector multiply parameter green usual enough also need put loss apart loss one term loss call coverage loss idea minimize minimum attention probability coverage vector take moment understand imagine want attend moment already attend lot minimum high want minimize s small attention probability moment opposite moment low coverage value safe try high attention weight minimum still low coverage value loss high loss motivate attend place nt attend lot yet 
model avoid repetition let us see whether model work nice whether coverage trick help us avoid repetition compute ratio duplicate produce outcome also compute ratio human reference summary see okay duplicate unigram okay duplicate sentence green level really low zero model coverage red one nt know duplicate lot three-gram four-gram sentence blue one nt duplicate really nice 
become extractive however another problem summary become really extractive mean generate new sentence extract input try compare reference summary let us compute ratio n-grams novel see reference summary rather high bar model coverage mechanism sufficiently lower level model without coverage mechanism 
pointer-gen + coverage muhammadu buhari say plan aggressively fight corruption long plague nigeria say administration confident able thwart criminal win come long history military rule coup botch attempt democracy africa ’ populous nation case coverage spoil model little bit real example summary generate pointer-generator network plus coverage actually let us see somebody say plan something 
original text ( truncate ) lago nigeria ( cnn ) day winning nigeria ’ presidency muhammadu buhari tell cnn ’ christiane amanpour plan aggressively fight corruption long plague nigeria go root nation ’ unrest buhari say ’ “ rapidly give attention ” curb violence northeast part nigeria terrorist group boko haram operate cooperate neighboring nation chad cameroon niger say administration confident able thwart criminal other contribute nigeria ’ instability first time nigeria ’ history opposition defeat ruling party democratic election buhari defeat incumbent goodluck jonathan 2 million vote accord nigeria ’ independent national electoral commission win come long history military rule coup botch attempt democracy africa ’ populous nation original text see exactly sentence somehow link link say otherwise extractive model extract three important sentence 
comparison model abstractive model ( nallapati et al 2016 ) extractive model ( nallapati et al 2017 ) lead-3 baseline seq2seq + attention pointer-generator pointer-generator + coverage rouge score 1 2 l 3546 1330 3265 396 162 353 4034 1770 3657 3133 1181 2883 3644 1566 3342 3953 1728 3638 want show quantitative comparison different approach rouge score automatic measure summarization think something bleu summarization instead machine translation see pointer-generator network perform better vanilla seq2seq plus attention coverage mechanism improve system even however model good compare baseline one competitive baseline would take first three sentence text simple extractive baseline idea improve mean something get straightforward approach contrary model attention coverage idea improve even future everybody hope neural system able improve absolutely obvious year able beat baseline 

task-oriented dialog system 
task-oriented dialog system talk personal assistant • apple siri • google assistant • microsoft cortana • amazon alexa • … solve task • set reminder • find photo pet • find good restaurant • send message • … 
task-oriented dialog system write chat bot • book ticket • order food • contest parking ticket • track expense • … 
utterance speech automatic speech recognition text utterance text 
intent classification • user want • predefined scenario user try execute © apple siri intent navtimeclosest 
’ many intent • need classify give correct answer • classification task measure accuracy © apple siri intent navdirectionsclosest 
one example • time assistant need additional information initiate dialog © apple siri intent navdirection 
form fill approach dialog management • think intent form user need fill • intent set field ( slot ) must filled execute request • example navdirection intent − @ slot default current geolocation − @ slot require • need slot tagger extract slot utterance 
slot tag • example − user show way history museum − slot tagger show way @ { history museum } • token classification task bio scheme − b correspond word begin slot − correspond word inside slot ( exclude begin ) − correspond word outside slot • example bio scheme tag token show way history museum tag b-to i-to 
slot tag • train sequence tag task bio scheme • slot consider correct range type correct • 𝐑𝐞𝐜𝐚𝐥𝐥 = # 𝐜𝐨𝐫𝐫𝐞𝐜𝐭 𝐬𝐥𝐨𝐭𝐬 𝐟𝐨𝐮𝐧𝐝 # 𝐭𝐫𝐮𝐞 𝐬𝐥𝐨𝐭𝐬 • 𝐏𝐫𝐞𝐜𝐢𝐬𝐢𝐨𝐧 # 𝐜𝐨𝐫𝐫𝐞𝐜𝐭 𝐬𝐥𝐨𝐭𝐬 𝐟𝐨𝐮𝐧𝐝 = # 𝐟𝐨𝐮𝐧𝐝 𝐬𝐥𝐨𝐭𝐬 • evaluate slot tagger 𝐅𝟏 = 𝐏𝐫𝐞𝐜𝐢𝐬𝐢𝐨𝐧 × 𝐑𝐞𝐜𝐚𝐥𝐥 𝟐 × 𝐏𝐫𝐞𝐜𝐢𝐬𝐢𝐨𝐧 7 𝐑𝐞𝐜𝐚𝐥𝐥 
form fill dialog manager ( single turn ) • user give direction san francisco − intent classifier navdirection − slot tagger @ { san francisco } − dialog manager slot filled ’ route • agent ( assistant ) ’ route google map 
form fill dialog manager ( multi-turn ) • user give direction los angele − intent classifier navdirection − slot tagger @ { los angele } − dialog manager require slot miss • agent ( assistant ) want go • user san francisco − intent classifier navdirection − slot tagger @ { san francisco } google − dialog manager okay ’ route map • agent ( assistant ) ’ route 
form fill dialog manager ( multi-turn ) • user give direction los angele − intent classifier navdirection − slot tagger @ { los angele } − dialog manager require slot miss • agent ( assistant ) want go • user san francisco − intent classifier navdirection − slot tagger @ { san francisco } google − dialog manager okay ’ route map • agent ( assistant ) ’ route 
form fill dialog manager ( multi-turn ) • user give direction los angele − intent classifier navdirection − slot tagger @ { los angele } − dialog manager require slot miss • agent ( assistant ) want go • user san francisco need context − intent classifier navdirection − slot tagger @ { san francisco } google − dialog manager okay ’ route map • agent ( assistant ) ’ route 
track context ( easy way ) • intent classifier slot tagger need context ( happened ) • let ’ add simple feature − previous utterance intent categorical feature − slot filled far binary feature possible slot • improve slot tagger f1 05 % • reduce intent classifier error 67 % • better way memory network http contextualpdf 
track form switch • user give direction los angele − intent classifier navdirection − slot tagger @ { los angele } − dialog manager require slot miss • agent ( assistant ) want go • user forget let ’ eat sushi first yelp − intent classifier navfind − slot tagger @ category { sushi } − dialog manager okay let ’ start new form find sushi • agent ( assistant ) okay nearby sushi place 
track form switch • user give direction los angele − intent classifier navdirection − slot tagger @ { los angele } − dialog manager require slot miss • agent ( assistant ) want go • user forget let ’ eat sushi first yelp − intent classifier navfind − slot tagger @ category { sushi } − dialog manager okay let ’ start new form find sushi • agent ( assistant ) okay nearby sushi place 
task-oriented dialog system overview speech automatic speech recognition text backend action young 2000 text natural language understand speech natural language generation text dialog manager intent slot dialog state tracking dialog policy 
summary • ’ overview task-oriented dialog system form fill • evaluate accuracy intent classifier f1-measure slot tagger • next video ’ take closer look intent classifier slot tagger 

intent classifier slot tagger hi video talk intent classifier slot tagger depth 
intent classifier • − model bow n-grams tf-idf − rnn ( lstm gru … ) − cnn ( 1d convolution ) • cnns perform better dataset task essentially key phrase recognition task sentiment detection dataset https 170201923pdf let s start intent classifier use model bag-of-words n-grams tf-idf use classical approach text mining use recurrent architecture use lstm cell gru cell also use convolutional network use 1d convolution overview week one study actually show cnns perform better dataset task essentially key phrase recognition task happen sentiment detection dataset example make sense try rnn cnn classical approach baseline choose work best 
slot tagger • − handcraft rule like regular expression − crf − rnn seq2seq − cnn seq2seq − seq2seq attention come slot tagger bit difficult task use handcraft rule like regular expression say example take starbucks know something happen phrase take definitely like two slot slot intent approach nt scale natural language huge variation express thing make sense something datum drive use conditional random field rather classical approach use rnn sequence-to-sequence model encoder decoder funny fact still use convolutional network sequence-to-sequence task well add attention model sequence-to-sequence model next slide want overview convolutional sequence-to-sequence model - gain popularity work faster sometimes even beat rnn task 
cnn sequence gate linear unit stack 6 layer kernel size 5 result input field 25 element https 161208083pdf https 170503122pdf okay let s see convolutional network used model sequence let s say input sequence bedding-bedding start sequence three german watt actually want let s say want solve task language modele see new token need predict token come next usually use recurrent architecture let s see use convolution let s say generate next token - actually care last three token sequence see assume use convolution aggregate information last three token blue triangle actually get filter output let s take half filter add second half pass sigmoid activation function take element multiplication two half actually get get gate linear unit add non-linear part become non-linear actually look context predict hide state let s say next token use convolution triangle actually convolutional filter slide across sequence use weight learn filter work every iteration sequence pretty similar rnn way actually nt hide state need change actually look context intermediate representation see actually look three last token good maybe need look like last 10 token rnn like lstm cell actually long short-term memory okay know convolutional neural network know increase input receptive field actually stack convolutional layer let s stack six layer kernel size five actually result input field 25 element experiment show 25 element receptive field might enough model sequence 
cnn sequence result • sometimes beat lstm language modele • … machine translation https 161208083pdf https 170503122pdf let s see cnns work sequence office provide result language modele dataset wikitext-103 see cnn architecture actually beat lstm lower perplexity actually run faster go little bit later another example machine translation dataset english french let s say metric call bleu higher metric better see convolutional sequence-to-sequence actually beat lstm well pretty surprising 
cnn sequence speed benefit • work faster rnn − training process time step parallel − testing encoder − testing get higher throughput thank convolution optimization gpus translation generation speed testing https 170503122pdf good thing cnns speed benefit compare rnn problem rnn hide state change state iteration calculation parallel every step depend actually overcome convolutional network training process time step parallel apply convolutional filter time step independent parallel testing let s say sequence-to-sequence manner encoder actually dependence previous output use input token apply convolution get hide state parallel testing one thing one good thing gpus highly optimized convolution get higher throughput thank used convolution instead rnns actually see table show model base lstm model base convolutional sequence-to-sequence see convolutional model actually provide better score term translation quality also work 10 time faster pretty good thing real-world system like let s say facebook need translate post want need translate fast order implement machine translation production environment maybe cnn good choice way paper folk facebook 
cnn sequence encoder look like • bi-directional encoder easy • work parallel time step https 170503122pdf let s look one thing know sequence-to-sequence task actually want encoder bi-directional look sequence left right right left good thing convolution actually make convolutional filter symmetric look context left right time easy make bi-directional encoder cnns still work parallel dependence hide state apply multiplication parallel 
atis dataset • • • • • airline travel information system collect 90s 4978 context independent utterance 17 intent 127 slot label state-of-the-art 179 % intent error 959 slot f1 http 1352pdf https slt10pdf move let remind actually review intent classifier slot tagger move need dataset use overview let s take atis dataset s airline travel information system collect back 90s roughly 5000 context independent utterance important mean actually one turn dialogue nt need like fancy dialogue manager 17 intent 127 slot label like location location departure time forth utterance like show flight seattle san diego tomorrow state-of-the-art task follow 17 intent error 959 slot f1 
joint training intent classifier slot tagger • analyze sequence • learn representation suitable task • result supervision higher quality http 1352pdf pretty cool another thing actually learn intent classifier slot tagger jointly nt need train like two separate task train supertask actually learn representation suitable task time provide supervision training get higher quality result 
joint training intent classifier slot tagger • encoder-decoder architecture joint intent detection slot fill • encoder bi-directional lstm • align input ( ℎ right ) attention ( 𝑐 right ) http 1352pdf let s see joint model might work still sequence-to-sequence model time use let s say bi-directional encoder last hide state use decode slot tag time use decode intent train end-to-end two task get higher quality notice decoder hide state encoder post call align input also c-vector attention 
attention decoder decoder state 𝑠 attention 𝑐 ( 𝑔 feed forward network ) attention weight ( darker higher ) predict slot label last word “ noon ” 𝛼 % http 1352pdf 𝑇 let s see attention work decoder let say time step e output new decoder hide state se actually function previous hide state blue previous output red hide state encoder vector attention let s see attention work vector attention ci actually weight sum hide vector encoder need come weight vector actually train system learn weight way make sense give attention weight vector coefficient use define weight particular vector encoder modeled forward network used previous decoder hide state state encoder need figure whether need state encoder also see example attention distribution predict label last word see predict label like departure time model look phrase like city city name something like 
joint training loss • final training loss sum loss intent slot log ( log-loss ) green – intent loss blue – slot loss iteration okay also see two loss decrease training training use two loss use sum see green loss intent blue one slot see intent loss actually saturate nt change blue slot blue curve continue decrease model continue train harder task intent classification 
joint training result • better performance atis dataset training independent training slot fill independent training intent detection joint training slot fill intent detection • work faster two separate model http 1352pdf slot f1 intent % error 9578 - - 202 9587 157 okay let s look joint training result 80s dataset trained slot fill independently slot f1 957 train intent detection classifier independently intent two percent train two task jointly used architecture overview actually get higher slot f1 lower intent error good thing also joint model work faster use mobile phone embed system one encoder reuse information two task 
summary • ’ overview different option intent classifier slot tagger training • person start use cnn sequence modele sometimes get better result rnn • joint training beneficial term speed performance • next video ’ take look context utilization nlu ( intent classifier slot tagger ) okay let s summarize overview view different option intent classifier slot tagger start classical approach go way deep approach person start use cnns sequence modele sometimes get better result rnn pretty surprising fact also use joint training beneficial term speed performance slot tagger intent classifier next video take look context utilization nlu intent classifier slot tagger 

utilize context nlu hi video will talk context utilization nlu let remind need context 
need context handle multi-turn dialog • user give direction los angele − intent classifier navdirection − slot tagger @ { los angele } − dialog manager require slot miss • agent ( assistant ) want go • user san francisco need context − intent classifier navdirection − slot tagger @ { san francisco } google − dialog manager okay ’ route map • agent ( assistant ) ’ route dialect like user say ` give direction la understand need miss slot ask ` want go user say ` san francisco next utterance would nice intent classifier slot tagger can use previous context can understand san francisco actually @ slot wait intent nt change context 
let ’ store previous utterance “ memory ” https is16_contextualslupdf proper way call memory network let s see might work history utterance let s call x s utterance pass special rnn encode memory vector take two utterance pass rnn memory vector dense vector like neural network like okay encode utterance memory let s see use memory 
knowledge relevant new utterance https is16_contextualslupdf new utterance come utterance c lower left corner actually encode vector size memory use special rnn call rnn input orange ` u vector actually actually representation current utterance need need match current utterance utterance memory use dark product representation utterance actually give us apply soft mark actually knowledge attention distribution know knowledge previous knowledge relevant current utterance actually take memory vector take weight attention distribution final vector weight sum edit representation utterance orange vector pass fully connect layer get final vector ` knowledge encode current utterance knowledge vector vector actually accumulate context dialect actually use rnn tag let s say 
tag current utterance knowledge • add knowledge representation final rnn tagger https is16_contextualslupdf let s say implement knowledge vector tag rnn edit input every step rnn tagger memory vector nt change train end end might better quality use context 
track context ( memory network ) chen et al 2016 • encode previous utterance store “ memory ” dense vector • use attention mechanism retrieve relevant prior knowledge conversation https is16_contextualslupdf okay overview whole architecture historical utterance use special rnn turn memory vector use attention mechanism new utterance come actually know prior knowledge relevant us current stage use information rnn tagger give us slot tag sequence 
track context ( memory network ) • evaluation result slot tagger − multi-turn dataset − f1-measure model first turn turn overall rnn tagger wo context 558 457 474 memory network 732 657 671 let s see actually work evaluate slot tagger multi-turn datum set dialect along actually measure f1 f1-measure let s compare rnn tagger without context memory network architecture see model perform better first turn also consecutive turn well overall give significant improvement f1 score like 47 compare 6 7 
summary • make nlu context-aware memory network • next video ’ take look lexicon utilization nlu ( eg list music artist ) let summarize make nlu context-aware memory network previous week previous video actually overview simple manner memory network seem right approach next video take look lexicon utilization nlu think lexicon let s say list music artist already know knowledge base let s try use intent classifier slot tagger 

utilize lexicon nlu video talk lexicon utilization nlu 
want utilize lexicon • • • • let ’ take atis dataset finite set city training model work new city list city use • another example • imagine need fill slot “ music artist ” • music artist database like musicbrainzorg • use want utilize lexicon let s take atis dataset example problem dataset finite set city training thing nt know whether model work new city testing good fact list city like wikipedia source actually use somehow help model detect new city another example imagine need fill slot like ` music artist music artist database like musicbrainzorg actually download parse use nlu 
let ’ add lexicon feature input word • let ’ match every n-gram input text entry lexicon take san francisco • match successful n-gram match prefix postfix entry least half length entry match “ san ” à “ san antonio ” “ san ” à “ san francisco ” “ san francisco ” à “ san francisco ” • multiple overlapping match • prefer exact match partial • prefer longer match shorter • prefer earlier match sentence later https 151108308v4pdf use let s add lexicon feature input word overview approach paper see lower left corner let s match every n-gram input text entry lexicon let s take n-grams ` take ` ` san ` san francisco possible one let s match lexicon dictionary let s say city say match successful n-gram match either prefix postfix entry dictionary least half length entry nt lot spurious match let s see match might san might match san antonio san francisco san francisco n-gram match san francisco entry will get match need decide one best overlapping match mean one word used different n-grams need decide one better prefer follow order first prefer exact match partial word san used san francisco exact match preferable let s say match san san antonio also prefer longer match shorter prefer earlier match sequence later three rule actually give us unique distribution word non-overlapping match lexicon let s see use information lexicon match information model 
match encode use bioe coding ( begin inside outside end single ) • b – token match begin entity • b – two token match prefix • e – two token match postfix • – match single token entity • … example 4 lexicon dictionary b e later encode one-hot vector https 151108308v4pdf use so-called bioe coding stand begin inside outside end single mark token b token match begin entity use b token match prefix use e two token match postfix token middle token end entity use match single token match entity let s see example coding four lexicon dictionary location miscellaneous organization person certain utterance like ` hayao tada commander japanese north china area army see match person lexicon give us b e know entity also full match ` north china area army match organisation lexicon encode like b e e actually full match even nt entity lexicon let s say north china history museum let s say nt know country area army entity two entity actually postfix second one prefix first match still give us bioe encode pretty cool make new entity nt see okay next use letter later encode one hot encode vector 
add feature model hayao tada … … word embedding lexicon feature feature ’ lexicon feature forward lstm backward lstm output layer tag score best tag https 151108308v4pdf let s see add lexicon information module let s say utterance ` see painting picasso word embedding every token word embedding actually add lexicon information follow way remember table previous slide let s take two first word let s take column correspond word let s use one hot encode decode bioe letter number use vector concatenate embedding vector word use input b directional lstm let s say thing predict tag slot tagger like pretty easy approach emb lexicon information model 
lexicon help conll-2003 name entity recognition task yes let s see work bench-mark dataset name entity recognition see add lexicon actually improve precision recall f1 measure little bit like one percent something like seem work seem helpful implement lexicon feature real world dialogue system 
training detail • sample lexicon dictionary model learn context entity well lexicon feature • procedure help detect unknown entity testing • augment dataset replace slot value value lexicon take san francisco take washington let s look training detail sample lexicon dictionary model learn lexicon feature also context word let s say say ` take san francisco mean word come phrase ` take likely two-slot want model learn feature well real world see entity vocabulary lexicon feature work sampling procedure actually give ability detect unknown entity testing pretty cool approach lexicon dictionary also augment datum set replace slot value value lexicon let s say ` take san francisco become ` take washington easily replace san francisco s slot value washington lexicon dictionary 
summary • add lexicon feature improve nlu • next video ’ take look dialog manager ( dm ) let summarize add lexicon feature improve nlu help detect entity user mention unknown long entity like ` south china area army detected next video take look dialogue manager 

dialog manager ( state tracking ) 
dialog manager • state tracker ( require hand-crafted state ) − query external database knowledge base − track evolve state dialog − construct state estimation • policy learner − take state estimation input choose dialog action https 170307055pdf 
state tracking policy learn policy learn state tracking ( dst ) © gašić 
dstc 2 dataset • dialog state tracking challenge collect 2013 • human-computer dialog ( find restaurant cambridge ) − 3324 telephone-based dialog person recruit used amazon mechanical turk − dialog system used mdp pomdp tracking dialog state hand-crafted policy policy learnt used reinforcement learn • labele procedure − utterance transcription used amazon mechanical turk − annotation heuristic − checked & correct hand http handbookpdf 
dstc 2 dataset • dialog state − goal distribution value informable slot − method distribution method name constraint alternative finished − request slot probability requestable slot request user system inform • user dialog act inform request negate confirm … − part town à request ( area ) • method infer act goal − inform ( food=chinese ) à ” constraint ” 
dstc 2 dialog excerpt utterance goal ’ look expensive restaurant venetian food food=venetian pricerange=expensive method byconstraint request slot [ ] utterance goal one thai food food=thai pricerange=expensive method byconstraint request slot [ ] utterance goal address food=thai pricerange=expensive method byconstraint request slot [ addr ] 
dstc 2 result • best result competition − goal 65 % correct combination − method 97 % correct − request slot 95 % correct 
rule-based state tracking • train good nlu ( intent slot ) • make simple hand-crafted rule dialog state change 
neural belief tracker system output user utterance candidate pair ( ) would like indian food farsi food … food indian food persian food czech context representation [ ] utterance representation [ r ] candidate representation [ c ] gate mechanism context modele [ ] semantic decode [ ] binary decision make [ ] joint dst https 160603777pdf 
utterance representation relu max pool word vector https 160603777pdf unigram filter bigram filter trigram filter summary n-gram representation final utterance representation 
neural belief tracker result dst model dstc2 woz 20 goal request goal request delexicalisation-based model 691 957 708 871 delexicalisation-based model + semantic dictionary * 957 * 876 neural belief tracker nbt-dnn * 964 * * neural belief tracker nbt-cnn * 965 * * https 160603777pdf 
frame dataset • collect 2016 • human-human goal-oriented dataset − 12 participant 20 day 1369 dialogue − two human talk via slack chat find vacation september 1st september 8th havana stuttgart $ 700 date flexible available end conversation access database + package compose hotel round-trip flight provide help via chat interface user wizard https 170400057pdf 
frame dataset • introduce new task call frame tracking extend state tracking set several state track simultaneously • dataset user compare result corresponding different constraint go back-and-forth result dialogue excerpt active frame annotation 
frame dataset • ’ annotate follow − dialogue act slot type slot value reference frame utterance − id currently active frame • example inform ( category=25 ) 25 star offer ( = [ 6 ] seat=business price=100227 ) $ 100227 business class ticket san francisco 
summary • • • • • ’ overview state tracker dm ’ discuss dataset dm training state tracking do hand rule good nlu better neural network approach next video ’ talk dialog policy dm 

dialog manager ( policy learner ) hi video talk policy learner dialogue manager 
state tracking policy learn policy learn state tracking ( dst ) © gašić okay let remind policy learn dialogue progress time every turn every observation user somehow update state dialogue state record responsible certain state actually make action need figure policy tell us certain state action must something sell user 
dialog policy • dialog state à agent act • policy execution example inform ( = “ 780 market st ” ) nearest one 780 market st request ( location ) delivery address let s look dialog policy actually actually mapping dialog state agent act imagine conversation user collect information internal state tell us user essentially want need take action continue dialog need mapping dialog state agent act dialog policy essentially let s look policy execution example system might inform user location 780 market street user hear follow ` nearest one 780 market street another example system might request location user user see ` delivery address train model give us act dialog state hand craft rule 
simple approach hand craft rule • nlu state tracker • come hand craft rule policy favorite okay let s look simple approach hand craft rule nlu state tracker come hand craft rule policy state tracker state remember dialog state tracking challenge dataset actually contain part state request slot use information understand next whether need tell user value particular slot search database something else pretty easy come hand craft rule policy 
optimize dialog policy ml supervised learn • train imitate observed action expert • often require large amount expert-labele datum • even large amount training datum part dialogue state space may well-cover training datum reinforcement learn • give reward signal agent optimize dialogue policy interaction user • rl require many sample environment make learn scratch real user impractical • ’ need simulate user rl https 170307055pdf turn make better machine learn two way optimize dialog policy machine learn first one supervised learn set train imitate observed action expert human-human interaction one expert use observation try imitate action expert often require large amount expert label datum know pretty expensive collect datum use crowd sourcing platform like amazon mechanical turk even large amount training datum part dialog state space may well cover training datum system blind different approach call reinforcement learn huge field scope like honorable mention give reward signal agent optimize dialog policy interaction user reinforcement learn require many sample environment make learn scratch real user impractical waste time expert s need simulate user base supervised datum reinforcement learn huge field gain popularity dialog policy optimization 
joint nlu dm system action j+1 dm policy https 161200913pdf let s look supervised approach might work example another model joint nlu dialog management policy optimization see actually four utterance utterance get user far pass nlu give us intent slot tag also take hide vector hide representation phrase nlu use consecutive lstm actually come idea system action actually execute have get several utterance nlu result lstm read utterance latent space nlu actually decide next pretty cool nt need dialog state tracking nt state state replace state lstm latent variable like 300 let s say state become hand craft become real value vector pretty cool actually learn classifier top lstm output us probability next system action let s see actually work 
joint nlu dm result model dm nlu baseline ( crf + svms ) 77 331 pipeline-blstm 120 364 joint model 228 374 frame level accuracy dstc 4 ( count whole frame parse correct ) https 161200913pdf look result three model compare first one baseline classical approach problem conditional random field slot tag svm action classification see frame level accuracy mean need accurate everything current frame every utterance see accuracy dialog manager pretty bad nlu s okay another model pipeline-blstm actually nlu training separately bidirectional lstm dialog policy optimization top model model trained separately see third option two model nlu bidirectional lstm blue previous slide actually train end end jointly increase dialog manager accuracy huge margin actually improve nlu well see effect joint training still continue happen 
summary • dialog policy do hand craft rule • supervised way • reinforcement learn way okay look dialog policy do hand craft rule good nlu good state tracker do supervised way learn datum learn jointly nlu way need state tracker example reinforcement learn way story different course 

final remark [ music ] hi video want overview do week 
task-oriented dialog system overview speech automatic speech recognition text backend action young 2000 text natural language understand speech natural language generation text dialog manager intent slot dialog state tracking dialog policy overview so-called task-oriented dialog system dialog system look like follow get speech user convert text used asr get text like chat bot come natural language understand give us intent slot natural language magic box call dialog manager actually two thing track dialog state learn dialog policy do user actually want dialog manager query backend like google map yelp cast say something user need convert text dialogue manager speech natural language generation red box part system nt overview take lot time actually work without system take user input text need asr output response user text well nt need natural language generation sometimes nt need backend action solve user s task overview detail natural language understand dialog manager 
nlu dm • • • • • • train slot tagger intent classifier nlu separately jointly… train nlu dm separately jointly… use hand-crafted rule sometimes ( eg dialog policy ) learn datum actually work better let remind train slot tagger intent classifier basically nlu train separately jointly jointly yield better result train nlu dialogue manager separately jointly give better result well use hand-crafted rule sometimes example dialog policy state tracking learn datum actually work better time 
evaluation • nlu − turn-level metric intent accuracy slot f1 • dm − turn-level metric state tracking accuracy … − dialog-level metric task success rate reward … let remind evaluate nlu dialog manager nlu use turn-level metric like intent accuracy slot f1 dialogue manager two kind metric first turn-level metric mean every turn dialogue track let s say state accuracy policy accuracy be dialog-level metric like success rate whether dialog solve problem user reward get solve problem user reward can number turn want minimize turn solve task user faster 
importance nlu https 170307055pdf actually question nlu dialogue manager train separately want understand error nlu affect final quality dialog manager left vertical axis success rate right axis average number turn dialogue three color legend blue one nt nlu error green one 10 % error nlu red one 20 % error nlu see happen huge error nlu success rate task actually decrease number turn need solve task success actually increase take time user solve task chance solve task lower nlu actually consist intent classifier slot tagger let s see one important 
important slot intent let s look happen change intent error rate look like effect quality success rate dialogue much dialogue become much longer look like intent error important slot tag see 
important slot intent look like need concentrate slot tagger introduce amount error slot tag actually decrease success rate dialogue dramatically seem slot tag error actually main problem success rate look like need concentrate slot tagger give insight want train joint model loss intent loss slot tag actually come weight intuition nt follow seem like slot tag loss bigger weight important success whole dialogue 
summary • ’ week • good luck final project let summarize overview test-oriented dialogue system look like overview in-depth nlu component dialog manager component basic knowledge need build task-oriented dialog system s week wish good luck final project [ music ] 

