cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide6,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide8,0.994615957,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.150818021,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide6,0.945002857,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.146570276,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.111028622,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.109584059,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,0.261085339,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.095287609,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.08427601,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.078612246
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide6,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide8,0.994615957,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.150818021,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide6,0.945002857,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.146570276,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.111028622,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.109584059,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide13,0.261085339,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.095287609,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.08427601,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.078612246
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide8,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide6,0.994615768,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.153138433,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide8,0.945000998,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.1459942,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.112991244,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.110853458,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,0.26090872,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.096436714,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.085314643,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.079343167
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide8,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide6,0.994615768,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.153138433,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide8,0.945000998,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.1459942,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.112991244,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.110853458,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide13,0.26090872,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.096436714,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.085314643,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.079343167
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide4,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide2,0.994393431,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.044039496,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide3,0.206752059,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide7,0.025173387,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.024661586,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide8,0.040968788,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide5,0.04347196,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide9,0.022107723,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide5,0.021912524,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.021624955
cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide2,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide4,0.99439306,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.045432253,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide13,0.134599176,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide6,0.113484537,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.025132866,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide7,0.024822543,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide9,0.02209821,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide5,0.021927856,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.021622954,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide8,0.021369424
cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.993354426,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.993354426,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.987759204,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.969849492,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.862416287,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.856688185,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.831877442,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.826647019,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.823633472,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.822976476
cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.993353998,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.985172811,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.972192519,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.942192578,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.862160186,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.835945479,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.834348282,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.829841747,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.827330751,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.826685757
cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.993353998,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.985172811,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.972192519,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.942192578,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.862160186,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.835945479,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.834348282,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.829841747,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.827330751,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.826685757
cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide1,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.993182261,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.560394491,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.545125709,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.508741862,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.478655276,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.472939224,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.465593418,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.465561673,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.393580598,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.316774452
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.992998041,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.98135858,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide3,0.989169674,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide5,0.953407901,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.964565932,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.972560428,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,0.942366893,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide14,0.958091681,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.974274075,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.976531922
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.992993395,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.979724174,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.992291023,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide3,0.991734976,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.990245475,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide9,0.901210339,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.987431622,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,0.990022019,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide8,0.974721875,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.972157105
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.992792291,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.742204074,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.720743839,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.696850382,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.674591324,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.651115266,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.615820046,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide15,0.985726388,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide14,0.988596765,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.991263528
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.992792291,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.742204074,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.720743839,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.696850382,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.674591324,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.651115266,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.615820046,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide15,0.985726388,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide14,0.988596765,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.991263528
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.99278172,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.747658515,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.710824344,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.693230035,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.691268251,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.672912746,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.639674875,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide7,0.947692206,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide6,0.953990978,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide9,0.955294299
cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.992493743,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.992493743,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.991189616,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.991189616,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.991189616,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.974997365,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.965320012,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.961700717,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.956369121,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.944985403
cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.992493743,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.992493743,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.991189616,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.991189616,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.991189616,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.974997365,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.965320012,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.961700717,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.956369121,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.944985403
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.992302058,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.978006313,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide3,0.988741517,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide14,0.952354657,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.966615713,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.968610119,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide5,0.939956574,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.973120528,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.975192715,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.970010302
cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide1,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide1,0.99220392,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide1,0.988394205,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide1,0.963774933,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide1,0.963774933,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide1,0.944948802,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.847675177,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.847675177,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.845046648,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.834653657,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide1,0.833024028
cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide1,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide1,0.99220392,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide1,0.988394205,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide1,0.963774933,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide1,0.963774933,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide1,0.944948802,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.847675177,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.847675177,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.845046648,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.834653657,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide1,0.833024028
cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.992194793,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.992194793,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.991332629,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.991332629,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.991332629,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.973260016,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.969122022,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.960204612,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.954781498,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.944901919
cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.992194793,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.992194793,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.991332629,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.991332629,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.991332629,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.973260016,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.969122022,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.960204612,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.954781498,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.944901919
cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide1,0.992022568,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.648175386,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.609820103,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.542550035,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.530151223,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.527452523,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.521225842,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.512849517,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.458126647,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide5,0.791846143
cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide1,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide1,0.991919326,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide1,0.991919326,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide1,0.990271595,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide1,0.960551934,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide1,0.960551934,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.835381505,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.835381505,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.832545069,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.821325112,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide1,0.819932405
cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.991765658,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.9885599,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.960174514,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.960174514,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.960150494,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.960150494,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.956529184,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.956529184,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.956529184,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.92407378
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide3,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.991740378,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.979916031,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide3,0.987493622,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide5,0.940362561,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.971622275,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.959704668,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.975666628,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.977286574,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide14,0.95010193,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.972834092
cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.991461345,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.991461345,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.990230916,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.990230916,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.965142439,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.962461177,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.953828899,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.947003362,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.944922814,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.944922814
cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.991461345,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.991461345,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.990230916,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.990230916,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.965142439,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.962461177,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.953828899,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.947003362,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.944922814,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.944922814
cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.991461345,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.991461345,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.990230916,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.990230916,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.965142439,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.962461177,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.953828899,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.947003362,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.944922814,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.944922814
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.991260582,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.773197176,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.758176539,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.729370034,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.709592601,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.693696537,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide16,0.676469122,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide7,0.947816844,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide6,0.954060577,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide5,0.962414556
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.991260582,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.773197176,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.758176539,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.729370034,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.709592601,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.693696537,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide16,0.676469122,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide7,0.947816844,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide6,0.954060577,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide5,0.962414556
cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.991060255,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.986883675,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.986883675,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide1,0.980624801,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.966340269,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide1,0.964659809,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.956304019,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.895735165,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.773020177,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.748752167
cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.990936038,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.987217866,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.914545567,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.914545567,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.911957601,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.911957601,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.89528234,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.89528234,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.89528234,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.892427411
cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.990932278,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.990751763,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.924642785,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.924642785,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.923483628,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.923483628,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.910874259,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.910874259,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.910874259,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.877871637
cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.990867707,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.986689856,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.986689856,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide1,0.978209376,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.973240074,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.972577627,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide1,0.965328134,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.905297514,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.764522585,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.743528197
cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide2,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide6,0.990763571,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide3,0.271194353,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.247538356,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.217687292,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.216641463,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.185307235,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.162819482,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.159605085,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.18324736,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.131965973
cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide6,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide2,0.990727052,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.175252413,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.122600626,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.122566235,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.114272836,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.113082965,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.098159841,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.094415724,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.091255163,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.081123383
cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide1,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide1,0.99037352,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide1,0.988085368,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide1,0.988085368,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide1,0.954526167,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide1,0.954526167,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.849176199,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.849176199,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.84588281,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.834069432,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide1,0.833561954
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.990249123,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.982558936,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide3,0.984737017,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide5,0.942963844,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.97671747,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.977907579,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.979834512,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,0.93004156,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide14,0.948641262,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.981034741
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.990036798,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.68449843,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.674234267,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.640214767,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.60747659,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.59014403,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.565265201,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide15,0.98826628,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide14,0.985256492,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.988186056
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.990026667,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.98474915,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide3,0.984428006,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide3,0.977911469,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide5,0.941927635,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.979489331,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.983100789,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.980520821,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.983310421,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide14,0.947791704
cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.989833755,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide5,0.884426905,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.574147035,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide2,0.545458579,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,0.253394655,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide4,0.454137418,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide3,0.239148904,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide3,0.128242737,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.104037509,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,0.101361201
cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide3,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide2,0.989394152,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide2,0.989394152,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide2,0.989394152,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide3,0.945318261,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.398937432,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide5,0.185070974,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide5,0.185070974,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,0.166877555,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.684544103,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.155884539
cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide3,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide2,0.989394152,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide2,0.989394152,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide2,0.989394152,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide3,0.945318261,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.398937432,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide5,0.185070974,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide5,0.185070974,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,0.166877555,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.684544103,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.155884539
cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide2,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide3,0.989383436,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide3,0.989383436,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide2,0.944495005,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide2,0.944495005,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.441035969,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide5,0.183179189,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide5,0.183179189,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.771350394,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.164318938,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.771350394
cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide2,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide3,0.989383436,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide3,0.989383436,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide2,0.944495005,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide2,0.944495005,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.441035969,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide5,0.183179189,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide5,0.183179189,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.771350394,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.164318938,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.771350394
cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide2,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide3,0.989383436,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide3,0.989383436,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide2,0.944495005,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide2,0.944495005,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.441035969,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide5,0.183179189,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide5,0.183179189,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.771350394,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.164318938,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.771350394
cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.989240916,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.98919828,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.97560635,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide1,0.972166903,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide1,0.963904957,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.95883093,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.939933427,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.922617936,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.864467232,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.85806956
cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.989240916,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.98919828,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.97560635,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide1,0.972166903,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide1,0.963904957,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.95883093,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.939933427,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.922617936,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.864467232,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.85806956
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide3,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.989169726,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.965836727,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.988732448,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide9,0.882691376,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide3,0.9874922,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.98473486,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,0.984424263,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.980094679,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide8,0.963398344,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.962613465
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide14,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.988593649,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.791679667,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.750891908,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.693447007,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.673169364,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,0.665624504,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide16,0.649193769,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide7,0.933140865,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide5,0.95012961,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide6,0.940949496
cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.988462262,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide5,0.79395532,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.565156621,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide2,0.492325703,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide4,0.27260281,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide4,0.42243392,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.123382569,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide3,0.116763259,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.097179952,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide3,0.186816982
cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide1,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,0.988389047,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.988233555,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.97249184,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,0.935889646,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.755430707,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.697208722,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.651257306,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.594108161,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,0.3095767,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide2,0.279075795
cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,0.988389047,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.988233555,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.97249184,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide1,0.935889646,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.755430707,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.697208722,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.651257306,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.594108161,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,0.3095767,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide2,0.279075795
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide15,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide3,0.988265217,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.755176497,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.713441153,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.65572014,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.640126584,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,0.630943031,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide7,0.92469305,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide9,0.935641403,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide5,0.942463949,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide6,0.932916162
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.987984458,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.960183405,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.2571728,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.253425062,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.227628104,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.348396364,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.219128072,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.212928962,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide6,0.198154707,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide4,0.192268932
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.98798088,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.974580532,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.243180505,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.20930545,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.207121998,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.199598519,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.19916658,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide6,0.186869317,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide4,0.18003405,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.136898309
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.987436546,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.984344307,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide3,0.980105011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide3,0.977014959,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide5,0.940780858,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.979791842,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.983107875,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.980923405,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,0.93041814,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.983392694
cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide1,0.987393159,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.979060184,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide1,0.978450674,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.830995952,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.82100739,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.815645364,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.815645364,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.807367223,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.805797872,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.793421385
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide0,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide0,0.987142393,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide5,0.687728029,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide1,0.987142393,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide4,0.987142393,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide3,0.922001082,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide2,0.922001082,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide1,0.050685264,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.050683816,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.050237517,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.048421408
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide0,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide0,0.987126466,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide1,0.048335779,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide1,0.958306325,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide2,0.941395763,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide3,0.941395763,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.045545713,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.045545713,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.045545713,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.044566466,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.042929165
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide0,0.987126466,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide1,0.048335779,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide1,0.958306325,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide2,0.941395763,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide3,0.941395763,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.045545713,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.045545713,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.045545713,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.044566466,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.042929165
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide0,0.987126466,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide1,0.048335779,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide1,0.958306325,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide2,0.941395763,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide3,0.941395763,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.045545713,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.045545713,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.045545713,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.044566466,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.042929165
cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide8,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,0.987040573,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.193696187,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.315240657,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.110828915,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.093407475,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.079232056,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.074923706,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.074923706,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide10,0.066207187,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,0.063319797
cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide1,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.986617255,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.973010194,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide1,0.969742239,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.792210871,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.783491592,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.767074092,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.764137443,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.764137443,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.758533417,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.732597052
cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.986442989,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.981220198,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.967672623,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,0.695035636,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide1,0.695035636,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.599199489,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,0.580260096,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.440658637,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,0.271989326,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide2,0.230325832
cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide8,0.986351071,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.209654585,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.151612501,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide3,0.346823031,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.129348249,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide6,0.406885829,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.102701204,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.095322479,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.095322479,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.094784684
cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide4,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,0.986084676,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.571827615,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.571827615,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.414465629,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.374501961,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.302737303,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.206036905,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide2,0.177450167,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide2,0.177450167,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide2,0.177450167
cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide5,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,0.985906615,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.552297405,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.552297405,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide1,0.443884946,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.211215768,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide2,0.142282523,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.128162007,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.108589185,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.102330209,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide7,0.151733581
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.985354842,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,0.97646146,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,0.923500196,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.980755303,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.98174082,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.983671305,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.972550003,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide3,0.979998883,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.984086661,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide8,0.963009139
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,0.985347304,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,0.984780459,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide9,0.960386875,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.982595787,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.963169975,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.980753138,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide8,0.968009746,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide8,0.971472896,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide6,0.965116308,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.984379093
cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.985328489,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.982237672,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.982237672,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.931053747,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.80115398,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.716339138,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.693266403,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide1,0.653365055,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.644643731,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide1,0.644241018
cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide1,0.985114195,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,0.985114195,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.982329849,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.97926547,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.811946788,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.725080848,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.701375024,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.641421505,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,0.521558249,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide2,0.439077619
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,0.984079219,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.983421824,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.960475688,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.980765722,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide9,0.957227828,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.981065972,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide8,0.964970777,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide8,0.968279263,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide6,0.962299041,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide3,0.977318734
cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide5,0.984031763,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.350812993,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.350812993,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.44448942,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.181947058,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.222961083,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide2,0.152466136,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.144165863,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.130938116,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.114929952
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,0.983664522,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.983137996,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide9,0.956335466,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.958532107,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.979696263,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.979869862,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide8,0.964240017,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide3,0.975709552,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide6,0.958318182,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,0.983129026
cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.983181331,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.807300648,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.359431135,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,0.962594269,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide7,0.194238692,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.167581652,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.152244451,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,0.145628026,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.116961905,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide2,0.106625595
cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.983122049,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.769875569,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.21707575,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.148790473,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,0.129454874,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide7,0.128014151,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide3,0.118934473,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.107681576,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide5,0.086711709,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.084213357
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,0.982938338,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.616846182,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.550728593,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.521112589,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.503196524,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.482209166,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide15,0.968622428,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.981231792,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.982938338,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide14,0.973672101
cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.981962427,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.977193614,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.973744855,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,0.784927135,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.621336404,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,0.606596332,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide1,0.606596332,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.588615866,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,0.463512407,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide2,0.404542559
cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide2,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide2,0.981903971,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.225898079,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide2,0.223918883,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide7,0.150892756,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.129535138,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.117719327,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.117719327,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.096320042,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.085038676,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.079565343
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,0.981740858,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.980972241,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.956838022,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.977067833,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide9,0.948060871,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.9779673,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide6,0.956557636,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,0.980572505,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide8,0.958246055,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide3,0.972918374
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide17,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.981522919,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide0,0.204002001,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.189992266,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide2,0.169930688,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.098241436,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide2,0.055794084,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.050557532,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide0,0.160039191,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.070675228,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.044509808
cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide4,0.981292009,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.365789243,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.365789243,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.293945681,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.253305952,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.226801808,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide3,0.165611364,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide3,0.165611364,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.165409688,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide2,0.153517725
cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide1,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide1,0.981248709,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide1,0.968848047,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.952535568,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.94823334,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.842289271,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.726001117,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.696077598,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.696077598,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.693504411,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.692333944
cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide1,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide1,0.981245564,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide1,0.974457663,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.962618767,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.958765609,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.856658834,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.741451671,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.741451671,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.740037626,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.736779946,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.732097556
cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.981228214,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.979834005,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.962349755,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.687314724,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,0.6746002,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide1,0.625731818,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,0.625731818,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.520516933,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,0.393896863,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide2,0.344421809
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide10,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.981053376,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,0.967788741,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,0.920190796,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.974086247,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide3,0.979734889,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.975771211,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.978649599,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide1,0.978981781,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide2,0.978713773,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.960679787
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,0.980759426,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.979848261,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.956198811,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.975370985,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide9,0.944217925,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.976786248,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide6,0.953082829,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,0.979548994,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide3,0.971718552,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide8,0.954403552
cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide1,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.980749326,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide1,0.974289692,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.971352211,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.901701506,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.886837814,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.883450845,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.883450845,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.882318366,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.882318366,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.881504788
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.980732038,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,0.967123779,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,0.943616204,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.975290992,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.9769995,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide5,0.943665334,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide3,0.977800861,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.97965574,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.980715601,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.98846245
cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,0.98003267,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide1,0.98003267,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,0.976184046,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.968696804,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.695295007,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.609424624,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.575566989,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.537825624,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,0.288994668,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide2,0.281629768
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide3,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,0.980011128,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,0.977982471,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide9,0.971822817,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.977870934,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.942124821,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide8,0.974375127,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.977089477,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide7,0.957244185,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.968163756,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide6,0.956506291
cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide2,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide2,0.979851257,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.569050522,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.378668899,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide2,0.373321996,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.355951473,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.303032947,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.266371865,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.266371865,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide3,0.492559018,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.258009829
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide8,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide1,0.979029622,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.938681378,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.867535832,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide2,0.979006833,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide3,0.974332505,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.967961061,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.964903295,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.964081554,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.957923888,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.954030031
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide1,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide8,0.979029331,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.976371733,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.933933854,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide9,0.976850656,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,0.945663951,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.972884752,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide6,0.965370127,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,0.974267695,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide7,0.965969665,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide8,0.95162013
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide2,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide8,0.979006542,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.976182812,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.93321362,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide9,0.97668515,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,0.945324241,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.972606438,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide6,0.96529737,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,0.97392629,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide7,0.965807293,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide8,0.951411408
cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.978977821,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.952713418,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.535286906,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.504903977,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.481555283,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide1,0.42932706,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.323145376,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.305514337,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.302573882,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.250564686
cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.978915786,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.970728483,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.501473061,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.482843125,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide1,0.44643719,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.441124645,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.425968935,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.317999441,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide1,0.494404294,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.297378633
cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.978418118,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.798776538,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.515339948,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.515339948,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.472464651,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.428106928,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.428106928,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.416854237,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.416854237,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.379594122
cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.978193414,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide1,0.973578002,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide1,0.967149854,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.768748021,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.766467212,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.762481712,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.762481712,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.739236181,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.721924168,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.72053281
cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide1,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.977952356,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.975719585,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.952574732,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.952574732,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.920877594,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.915971182,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide1,0.913782381,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.827723705,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.483972149,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.474952829
cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.977619224,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.806790844,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.638240657,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.546187842,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.546187842,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.500447829,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.466491422,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide5,0.442404109,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.414548481,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.414548481
language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide17,0.976952093,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.290215543,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide2,0.25840626,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide0,0.580160129,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.182156632,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide1,0.134767713,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide2,0.312461023,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide2,0.102011067,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.079793825,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide1,0.259949835
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide3,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide9,0.97693048,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.112357897,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide10,0.942363213,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide6,0.104047022,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.241292008,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.534599839,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.099582116,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.093246893,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.088182625,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide7,0.088028495
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide9,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide3,0.976891966,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide6,0.153027265,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.140202048,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide10,0.114438741,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.113840901,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.112759844,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.100343884,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.09885512,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.095231334,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.09374699
cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.976854889,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.902432861,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.864020953,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.852184092,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.851041874,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.849702078,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.849702078,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.827432871,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.823175515,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.823175515
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.976851009,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.936201923,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.897090614,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.891910003,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.886308814,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.864441817,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.864441817,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.862443441,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.862103772,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.852146868
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide9,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide1,0.976836266,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,0.911185848,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,0.874971986,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide3,0.97173937,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide2,0.976670659,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.960246338,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.956007956,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.957056998,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.947442245,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.94352191
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide2,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide3,0.976785593,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.086731185,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide2,0.348844136,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.059030692,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide9,0.046251107,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide4,0.204126033,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide7,0.081181007,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide2,0.036968272,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.22421968,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.032838696
cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide1,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.975225683,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.966944796,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.966944796,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.965735715,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.964324473,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.943339029,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide1,0.922419979,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.83423061,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.578137181,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.554868622
cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide1,0.975134936,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.973544312,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.971575287,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.971575287,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.965968583,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.964767874,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide1,0.928412692,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.866406091,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide1,0.652253624,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.613153825
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide8,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.97475061,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.971410223,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide14,0.968866313,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,0.924390255,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.965343738,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.958396411,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.949337087,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide3,0.963417284,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.968234166,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide12,0.943798061
cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,0.974609912,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.96410026,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,0.96181208,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide1,0.96181208,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.602900371,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.572920662,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.535338538,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.472975224,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,0.394238467,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide5,0.510462778
cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide1,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide1,0.974471582,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide1,0.968852007,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.937073488,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.932884177,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.771367141,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.725538591,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.697228153,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.675602573,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.672267452,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.672267452
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.974307022,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.958578001,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.569184011,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.510169952,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.498729543,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.490302029,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.455753696,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.362426146,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.349868063,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.318633119
cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide3,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide4,0.973650602,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide3,0.94578323,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.081378942,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide0,0.068157992,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.087445444,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.183556897,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.036810579,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide2,0.029097201,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.026050809,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide5,0.082692716
cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide3,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide4,0.973650602,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide3,0.94578323,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.081378942,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide0,0.068157992,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,0.087445444,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.183556897,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.036810579,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide2,0.029097201,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.026050809,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide5,0.082692716
cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.973104843,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.965582471,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.961681354,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,0.647514436,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.574794443,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.539720244,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide1,0.537062185,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,0.537062185,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,0.456517702,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide2,0.361972599
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.972517778,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.136414251,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.134414374,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.134414374,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,0.108175984,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide16,0.405852059,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide5,0.106174506,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.105870377,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide17,0.405852059,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.103514859
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.972513845,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.215597216,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.961644312,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide8,0.952134018,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.215597216,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.205560153,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.149547727,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.13128034,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.123150124,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.118403105
cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.972133929,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.972133929,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.971368736,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.971368736,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.964906891,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.964906891,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.964906891,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.959945297,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,0.935955253,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.900448293
cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.972068931,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.963632372,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.954508036,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.949950099,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.949950099,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide1,0.941458699,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide1,0.922988791,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.854207958,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.661035305,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide1,0.62617186
cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide4,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide3,0.971620749,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide3,0.971620749,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.11510425,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide1,0.079095341,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide8,0.068883643,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.060022792,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.046476502,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide2,0.044597877,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.074197435,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.041287011
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.971566816,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.955496935,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.229728547,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.190138478,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide6,0.162863772,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.155780101,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide4,0.143900942,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.126295521,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.125621755,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.206833622
cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide3,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide2,0.970825999,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.225916378,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.121467815,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.049108519,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide9,0.049053909,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.210479527,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.046257114,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide2,0.045707079,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.044871256,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.040058529
cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.970803742,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.944087644,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,0.160250433,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide7,0.137488312,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide2,0.430042295,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.100088297,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.088319666,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.088319666,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,0.072860368,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide2,0.063295542
cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.970803742,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.944087644,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,0.160250433,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide7,0.137488312,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide2,0.430042295,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.100088297,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.088319666,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.088319666,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,0.072860368,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide2,0.063295542
cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.97077056,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.97077056,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.968243887,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.941223008,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.874437499,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.823776143,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.819859986,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.817040511,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.814699435,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.813487902
cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide10,0.97005508,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.793006971,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.545378253,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide4,0.334255369,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide5,0.676398487,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.236301807,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.230573102,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide9,0.415400087,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.211335268,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,0.606525886
cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.969743251,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.969743251,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.964731172,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.964731172,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.962700881,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.962700881,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.962700881,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.920758304,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.920753421,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.90773475
cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.969458844,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.52276474,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide2,0.503843977,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.164057491,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide17,0.119305938,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.262862774,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.347249016,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.071201219,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.065188867,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.251871063
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide14,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide8,0.968963684,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.949978615,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,0.882719316,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.952519043,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.958242726,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.943060947,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide3,0.950328211,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.947234736,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.94888377,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide5,0.948040395
cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide1,0.968438639,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.967786728,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide1,0.959822936,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide1,0.94678075,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.914683501,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.862390528,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.857977098,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.857977098,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.852282225,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.832336612
cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide10,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.968077924,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.805723281,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.374504548,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.62991408,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.200900162,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.186318815,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.17177607,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide6,0.167057369,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.156063841,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.12421165
cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide7,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide7,0.967336157,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.382697487,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.751599676,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.161306765,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.161306765,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.161306765,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.150943405,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.130153089,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.130153089,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.079277743
cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.96707143,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide1,0.964377518,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide1,0.95511666,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide1,0.941886187,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.915618474,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.910653794,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.859470047,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.85362685,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.85362685,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.848342917
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.966730465,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.963251729,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.971665263,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.960577895,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.964702415,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.956262921,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide8,0.949567289,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.956911543,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide3,0.959884236,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.95863738
cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.966577323,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide2,0.421253705,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.328233673,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.148896522,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide17,0.143040794,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide3,0.072303075,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide6,0.215840874,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.071023702,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.28635345,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide2,0.063491927
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide8,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.966248274,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.635042731,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.550253068,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.513159204,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.502678439,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.479747995,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide15,0.948814673,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.963957135,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,0.963957135,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide14,0.955561815
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide7,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide1,0.965981906,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.932421239,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.882190587,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide2,0.965819591,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.965279508,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.962180904,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide3,0.957191145,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.95775199,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.95533606,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.951503128
cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide4,0.965978299,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.880161867,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.756634431,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.745326441,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.66642692,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.584107592,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.554911972,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.554911972,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.511623609,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.497035257
cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide10,0.965828908,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,0.941984532,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide12,0.553867418,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide12,0.553867418,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.551686668,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide6,0.369496479,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.322952883,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide7,0.109390128,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.073460939,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.051472711
cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide10,0.965828908,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,0.941984532,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide12,0.553867418,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide12,0.553867418,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.551686668,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide6,0.369496479,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.322952883,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide7,0.109390128,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.073460939,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.051472711
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide13,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.965745518,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.150386453,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.126751153,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.108451081,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide0,0.103204376,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide2,0.041905135,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.051689709,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.059630104,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide12,0.994561107,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.046615105
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide6,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide1,0.965397596,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.930748555,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.881731487,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide2,0.965324894,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.965091199,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.962259905,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide3,0.956470664,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.956258003,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.958170754,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.952736661
cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.965317731,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.964362943,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,0.962975334,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.956431281,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.956431281,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.95635822,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.95635822,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.9519728,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.9519728,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.9519728
cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.965307455,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide1,0.911953638,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide1,0.911953638,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.894118095,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.882090381,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.882090381,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide1,0.869671589,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide6,0.864955578,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.861440997,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.856484514
cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.964634812,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.964634812,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide19,0.164474072,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide2,0.501852395,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.141084286,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.117870899,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide2,0.501852395,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide7,0.110950044,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.09320414,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.089813391
cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide4,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide5,0.964194671,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide4,0.946025494,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.043101607,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.041090054,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.041090054,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide8,0.038937649,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.035599626,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.024592618,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide2,0.022573582,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,0.022027063
cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide4,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide5,0.964194671,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide4,0.946025494,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.043101607,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.041090054,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.041090054,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide8,0.038937649,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.035599626,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.024592618,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide2,0.022573582,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,0.022027063
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide1,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide1,0.96375733,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide1,0.96375733,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide1,0.961866178,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide1,0.955634168,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide1,0.94447816,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.866591342,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.866591342,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.855830267,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide1,0.846986235,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide1,0.846986235
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide1,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide1,0.96375733,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide1,0.96375733,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide1,0.961866178,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide1,0.955634168,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide1,0.94447816,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.866591342,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.866591342,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.855830267,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide1,0.846986235,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide1,0.846986235
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide5,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.962340105,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.517805458,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.501231266,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.470148053,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.446147034,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide0,0.411102379,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide15,0.942417935,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.961756349,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide14,0.950044174,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,0.962340105
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide11,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.961651027,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.139601401,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.118396163,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.113780205,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide0,0.100318201,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide2,0.039654072,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.050895245,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.054068629,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide13,0.994560543,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.045245822
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide12,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.961651027,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.139601401,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.118396163,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.113780205,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide0,0.100318201,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide2,0.039654072,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.050895245,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.054068629,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide13,0.994560543,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.045245822
cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide4,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.961625824,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.716783188,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.678519825,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.664761214,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide1,0.632707554,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.539296098,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.468212028,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.450695359,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,0.446761522,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.416868849
cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide1,0.961536915,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.915196453,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.908292345,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.86919656,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.864601635,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.857576809,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.83710322,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.83710322,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.830151689,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.776645105
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.960783045,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.153775223,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.153775223,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.11941093,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.110524437,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide17,0.33646643,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.101818843,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide1,0.094554511,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide16,0.33646643,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,0.086191121
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide4,0.960734018,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide3,0.22652226,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.165374832,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide4,0.112733441,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide2,0.096811232,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.250911749,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide23,0.077830904,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide7,0.068917215,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide5,0.056066238,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide4,0.094964384
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide1,0.960728345,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.325395586,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.197440992,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide3,0.221816901,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide6,0.159926585,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide4,0.127053982,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.107509148,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide23,0.099361645,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide4,0.135022444,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide11,0.229329694
cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.959450161,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.938927577,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.912265107,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.910067388,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.89117367,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.863082208,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.858293103,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.856693319,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.801713178,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.800742995
cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.959212033,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.716683292,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide3,0.232886682,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide4,0.141092805,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.105530511,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide3,0.103038316,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide8,0.102691801,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide5,0.10037576,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.099621156,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide6,0.094269968
cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide1,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.959198681,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.949296153,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,0.8891284,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.844597402,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.824315415,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.751873381,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.746905302,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide1,0.719909461,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide1,0.719909461,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.699451854
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide12,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.959149699,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.955428746,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide5,0.989407187,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide8,0.944023949,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide1,0.949337515,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide2,0.949082043,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.951321849,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide3,0.949239578,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.951334418,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.94632161
cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide6,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide6,0.959131705,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.940145903,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.926914093,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.926914093,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.924655258,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.924655258,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.919834317,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.919834317,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.919834317,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.888918312
cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide2,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide2,0.958861979,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide3,0.265982682,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.119012006,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.09596714,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.09596714,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide6,0.120613732,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide3,0.072475804,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.042708383,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.041441253,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.041150353
cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.958076744,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.93436937,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide1,0.886385192,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.82092338,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide6,0.789716797,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.751416042,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.744446738,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.744234507,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.744234507,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.727102342
cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,0.957484051,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.807720695,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.589443147,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide4,0.496348377,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.390262021,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.373798725,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide10,0.799564506,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.278328674,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.276338454,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide5,0.613186781
cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.957175372,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.763400281,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.36744408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide14,0.224175846,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.465583596,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.218036619,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.204679783,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.188946208,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.188573657,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.179561959
cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide7,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide7,0.957102465,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.264231941,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.873252485,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.192122375,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.192122375,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.192122375,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.173439568,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.105570416,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.098034738,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.098034738
cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide2,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.957022701,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.391477059,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.387338125,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.246785933,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.246785933,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide8,0.147361132,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.123340865,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide3,0.120103239,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide11,0.105039219,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.085302087
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide1,0.956932982,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.211065839,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide0,0.956932982,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide4,0.956932982,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide5,0.644842765,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide3,0.825709515,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.17156368,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide2,0.825709515,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.140849299,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide6,0.126594835
cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide5,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide4,0.956725527,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide4,0.956725527,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.066336319,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.060972737,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide3,0.07888195,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.057388813,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide4,0.051276689,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.047641913,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.047192123,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.045139877
cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.95594463,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.627398634,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.322960774,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide8,0.786264138,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.292096033,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,0.371853393,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.252384518,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide2,0.213571795,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.199092571,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.166460696
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide9,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.955563969,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.566513895,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.484399486,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.449762447,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.437157292,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.421135005,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide16,0.392217399,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide15,0.936025435,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.952967086,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,0.952967086
cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide1,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.955166235,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide1,0.931982802,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.785082969,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.678297959,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.669863931,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.663552258,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.661172665,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.649404918,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.640551125,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.640551125
cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide2,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide2,0.955098325,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide3,0.246415368,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.152242538,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.150915637,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.139760416,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide3,0.130574874,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.119578478,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.109907563,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.106361647,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.080415179
cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide5,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.954533758,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.548024526,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.548024526,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.540150438,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.522588521,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.518657695,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.477078124,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.474264395,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.466368587,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide4,0.464531441
cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide6,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide6,0.954295819,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.943948123,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.823639583,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,0.792421457,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.679503861,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.656994389,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.656994389,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.634079948,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.634079948,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide1,0.62781849
cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide1,0.954255501,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.837092743,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.804275191,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,0.734063198,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.698939638,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.68888559,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.688301274,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide1,0.667059129,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide1,0.667059129,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.639858743
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide6,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.953973486,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.580439512,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.477801773,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.45218347,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide0,0.446895719,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,0.435624357,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide15,0.932872854,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.953937528,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,0.953973486,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide14,0.940854608
cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide2,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide4,0.953877014,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide2,0.097276455,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.073400443,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.067575506,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.065636518,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.060935329,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,0.213987338,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.054474288,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.051477051,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.050202131
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide13,0.953621802,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.197292682,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide12,0.948561191,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide11,0.948561191,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.141862708,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide10,0.837554683,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide14,0.218284252,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.121999328,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide16,0.388894216,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.137705685
cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide10,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,0.95358511,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,0.95358511,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.515687995,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide12,0.363744728,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide12,0.363744728,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide6,0.196308203,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide7,0.195573721,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.140759629,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.075846465,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.074244383
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide5,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.953444761,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.942553706,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.891796668,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.943718699,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.940783608,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.939965759,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide2,0.93199592,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.942998348,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide1,0.932242617,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide3,0.940414688
cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.95095542,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.945437482,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.899503151,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.885290758,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.855497328,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.798571975,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.786255755,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.780673573,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide1,0.738253114,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.714169779
cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.950331687,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.932512025,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.866008735,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.850072733,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.827565935,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.775323438,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.765730176,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.748150544,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.734940069,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide1,0.733677999
cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.949616002,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.747281564,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.508659118,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.313561282,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.269909179,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide7,0.24836929,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide14,0.206992158,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.198037721,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.192960269,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.469420243
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.949545315,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.565247063,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.565247063,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide2,0.426746844,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide0,0.28223864,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide28,0.175558253,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide13,0.129936683,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.104904689,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.052732602,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.098993005
cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.948809577,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.52299002,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide8,0.438598808,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.421898077,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide11,0.410533984,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide12,0.23170826,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide12,0.23170826,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide6,0.204138284,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide12,0.144572337,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide12,0.144572337
cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.948189115,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.936242146,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.914768441,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.912497157,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide1,0.909512184,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.902616687,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide1,0.877009229,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.865887789,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide1,0.864197287,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.8533637
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.947925886,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.546381456,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.453325596,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.420929681,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide0,0.409982815,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,0.407799646,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide15,0.924944349,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.947847353,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,0.947925886,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide14,0.933296993
cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide11,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide11,0.947781554,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.392975925,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.343410826,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.343410826,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide7,0.333111096,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,0.174706621,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.174706621,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide4,0.284121352,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.164920003,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.164920003
cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide11,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide11,0.947781554,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.392975925,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.343410826,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.343410826,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide7,0.333111096,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,0.174706621,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.174706621,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide4,0.284121352,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.164920003,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.164920003
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.947500671,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.67148708,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.487655234,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.465851325,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.458227614,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.443657632,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,0.418875143,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.412932081,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.412932081,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.403795624
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.947500671,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.67148708,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.487655234,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.465851325,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.458227614,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.443657632,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,0.418875143,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.412932081,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.412932081,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.403795624
cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide8,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide8,0.947253987,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide8,0.947253987,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.459769619,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.412768564,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.412768564,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.283089231,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.241753995,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.241753995,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.216364232,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.192845787
cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide8,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide8,0.947253987,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide8,0.947253987,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.459769619,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.412768564,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.412768564,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.283089231,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.241753995,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.241753995,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.216364232,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.192845787
cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide8,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide8,0.947253987,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide8,0.947253987,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.459769619,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.412768564,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.412768564,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.283089231,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.241753995,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.241753995,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.216364232,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.192845787
cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide5,0.947115614,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.69206095,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.679162381,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.654341743,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.627849643,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.454503126,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.410845165,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.406177514,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.362944226,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.347718953
cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.947054732,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.789863507,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide2,0.781909864,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide2,0.781909864,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,0.332692584,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,0.147540336,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.127144443,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.66230686,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.66230686,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.947054732
cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.947054732,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.789863507,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide2,0.781909864,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide2,0.781909864,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,0.332692584,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,0.147540336,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.127144443,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.66230686,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.66230686,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.947054732
cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.94700909,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide9,0.660677902,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.379325198,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.379325198,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide12,0.310537045,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.307351699,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.271113618,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide6,0.252954445,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide10,0.191702016,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.17799269
cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,0.94700909,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide9,0.660677902,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.379325198,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.379325198,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide12,0.310537045,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.307351699,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.271113618,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide6,0.252954445,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide10,0.191702016,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.17799269
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide8,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.946846883,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.157131278,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.157131278,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide1,0.101739995,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.098129603,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide15,0.360313922,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.092913656,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide1,0.088522476,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide16,0.321579405,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.084401998
cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.946831037,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.501127732,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.430865935,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,0.430865935,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.374090857,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.373872507,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.373872507,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide11,0.363823916,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide11,0.363823916,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.362717862
cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.946831037,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.501127732,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.430865935,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,0.430865935,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.374090857,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.373872507,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.373872507,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide11,0.363823916,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide11,0.363823916,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.362717862
cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.946764907,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.222218265,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide0,0.177811279,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.147380105,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.131042851,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.127061398,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.127061398,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.152166815,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.12502095,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide0,0.112769796
cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.946764907,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.222218265,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide0,0.177811279,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.147380105,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.131042851,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.127061398,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.127061398,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.152166815,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.12502095,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide0,0.112769796
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide7,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide7,0.946756315,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.058699717,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide9,0.860961323,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.05140483,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.046043432,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.043618835,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide6,0.074146329,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide5,0.038282149,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide8,0.03735235,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide6,0.036748059
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide7,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide7,0.946756315,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.058699717,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide9,0.860961323,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.05140483,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.046043432,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.043618835,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide6,0.074146329,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide5,0.038282149,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide8,0.03735235,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide6,0.036748059
cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.946609184,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.640108727,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.640108727,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.567998866,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.516269782,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.465159788,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.463910456,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.455988302,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.340508345,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.304398203
cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.946609184,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.640108727,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.640108727,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.567998866,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.516269782,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.465159788,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.463910456,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.455988302,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.340508345,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.304398203
cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.946491941,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.779166677,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.619413214,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.619413214,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.548462424,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.430967944,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.40369304,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.37352098,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.373374666,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide8,0.297856523
cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.946491941,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.779166677,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.619413214,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.619413214,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.548462424,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.430967944,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.40369304,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.37352098,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.373374666,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide8,0.297856523
cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide6,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide6,0.946490688,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide1,0.058383293,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide8,0.04701372,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.041969808,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.039624166,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide0,0.039082957,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.038789179,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide4,0.037667307,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.037556542,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide3,0.087048357
cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide6,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide6,0.946490688,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide1,0.058383293,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide8,0.04701372,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.041969808,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.039624166,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide0,0.039082957,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.038789179,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide4,0.037667307,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.037556542,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide3,0.087048357
cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide3,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide3,0.946419836,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.20107026,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.147103099,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.140284782,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.127173815,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide2,0.217634053,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.120195529,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide0,0.106221612,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.179375548,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.093531211
cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide3,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide3,0.946419836,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.20107026,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.147103099,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.140284782,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.127173815,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide2,0.217634053,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.120195529,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide0,0.106221612,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.179375548,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.093531211
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide9,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide9,0.946404294,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.167032599,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide7,0.858126979,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,0.102897086,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.101656577,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.100417884,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.100029453,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.084198498,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.070262095,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide5,0.068152113
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide9,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide9,0.946404294,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.167032599,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide7,0.858126979,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,0.102897086,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.101656577,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.100417884,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.100029453,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.084198498,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.070262095,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide5,0.068152113
cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.946307352,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.585495403,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.538687948,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.499294771,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.438023788,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.438023788,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.433129538,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.433129538,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.416429991,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide0,0.389535921
cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.946307352,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.585495403,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.538687948,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.499294771,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.438023788,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.438023788,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.433129538,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.433129538,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.416429991,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide0,0.389535921
cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.946274189,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.705631351,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide0,0.3628564,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.305436186,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.336368156,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide1,0.456308978,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide1,0.132494595,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.192886942,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide2,0.235480116,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.222973381
cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.946274189,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.705631351,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide0,0.3628564,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.305436186,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.336368156,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide1,0.456308978,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide1,0.132494595,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.192886942,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide2,0.235480116,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.222973381
cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide4,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide4,0.946127727,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.143911882,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide5,0.604252649,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide17,0.046930458,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,0.044469757,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide7,0.527856108,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.029757144,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.02925654,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide3,0.066545637,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide6,0.027996833
cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide4,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide4,0.946127727,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.143911882,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide5,0.604252649,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide17,0.046930458,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,0.044469757,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide7,0.527856108,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.029757144,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.02925654,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide3,0.066545637,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide6,0.027996833
cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.946109715,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.906154636,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.268140125,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.261490374,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide5,0.441602608,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.219530421,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.219530421,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.197805997,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.197805997,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.197805997
cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.946109715,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.906154636,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.268140125,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.261490374,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide5,0.441602608,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.219530421,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.219530421,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.197805997,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.197805997,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.197805997
cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide12,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide12,0.946074214,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,0.596366868,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,0.596366868,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide10,0.397488811,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.301785616,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide8,0.212367676,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.179578665,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.158346003,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.150580766,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.140599408
cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide12,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide12,0.946074214,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,0.596366868,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,0.596366868,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide10,0.397488811,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.301785616,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide8,0.212367676,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.179578665,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.158346003,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.150580766,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.140599408
cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.945852959,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.651641198,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide8,0.503887805,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.454225879,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide11,0.445620678,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide6,0.177351968,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide12,0.16824235,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide12,0.16824235,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.124885982,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide12,0.118573751
cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.945849929,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide2,0.864381426,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.843242599,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.843242599,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.843242599,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.218323016,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide5,0.698283686,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.218323016,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide4,0.78007212,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.830414397
cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.945849929,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide2,0.864381426,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.843242599,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.843242599,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.843242599,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.218323016,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide5,0.698283686,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.218323016,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide4,0.78007212,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.830414397
cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide10,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide10,0.945673231,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide1,0.340231142,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide3,0.221859086,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide1,0.220791162,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.304966546,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide0,0.104818865,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.081328811,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.081328811,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.139085911,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide5,0.105525968
cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide10,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide10,0.945673231,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide1,0.340231142,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide3,0.221859086,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide1,0.220791162,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.304966546,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide0,0.104818865,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.081328811,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.081328811,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.139085911,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide5,0.105525968
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide1,0.945642201,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.943479703,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide9,0.875008837,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide8,0.924420178,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.942320135,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide2,0.945302363,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.929809766,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.923993094,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.93462741,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.930040817
cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide2,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide2,0.945641776,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.944163394,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.456806466,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.093292333,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.092868503,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.593947135,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.073087046,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.052730235,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.049836616,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide1,0.040196943
cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide2,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide2,0.945641776,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.944163394,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.456806466,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.093292333,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.092868503,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.593947135,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.073087046,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.052730235,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.049836616,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide1,0.040196943
cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.945540719,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide8,0.265048343,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.217086336,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.14306214,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.136299729,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.124160263,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.124160263,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.07873605,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.078680148,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide7,0.064845877
cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.945540719,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide8,0.265048343,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.217086336,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.14306214,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.136299729,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.124160263,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.124160263,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.07873605,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.078680148,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide7,0.064845877
cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide3,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide3,0.945396398,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.183543253,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.162708126,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.156316796,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.145353003,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide1,0.14407189,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,0.142559364,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.139969096,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide1,0.139532108,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide1,0.139532108
cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide3,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide3,0.945396398,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.183543253,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.162708126,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.156316796,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.145353003,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide1,0.14407189,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,0.142559364,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.139969096,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide1,0.139532108,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide1,0.139532108
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide11,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide11,0.945250431,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide5,0.773527531,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide10,0.90124388,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,0.110466425,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide7,0.080721409,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.692150675,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide6,0.208881589,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide6,0.022022469,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide3,0.021704553,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide14,0.039776409
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide11,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide11,0.945250431,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide5,0.773527531,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide10,0.90124388,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,0.110466425,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide7,0.080721409,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.692150675,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide6,0.208881589,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide6,0.022022469,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide3,0.021704553,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide14,0.039776409
cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide9,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide9,0.945229206,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,0.061010336,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide8,0.346328588,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.058522702,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide2,0.05139114,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide7,0.047520482,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.039395843,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.039395843,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.038769879,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.038769879
cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide9,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide9,0.945229206,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,0.061010336,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide8,0.346328588,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.058522702,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide2,0.05139114,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide7,0.047520482,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.039395843,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.039395843,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.038769879,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.038769879
cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.945187939,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide8,0.24593514,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.218187069,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.147275115,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.134623256,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.134623256,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.123928645,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.077958964,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.077097479,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide2,0.065205864
cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.945187939,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide8,0.24593514,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.218187069,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.147275115,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.134623256,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.134623256,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.123928645,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.077958964,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.077097479,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide2,0.065205864
cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide2,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide2,0.945122118,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.895805473,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,0.839262532,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.785205898,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.785205898,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.143463065,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.679377596,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,0.130232374,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide2,0.945122118,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide4,0.033282599
cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide2,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide2,0.945122118,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.895805473,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,0.839262532,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.785205898,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.785205898,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.143463065,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.679377596,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,0.130232374,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide4,0.033282599,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide15,0.040320843
cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.944967766,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.869690669,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.690592147,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.690592147,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.656532442,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,0.284072076,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.263865197,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide2,0.708183787,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide13,0.181716988,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.944967766
cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.944967766,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.869690669,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.690592147,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.690592147,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.656532442,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,0.284072076,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.263865197,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide2,0.708183787,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide13,0.181716988,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.944967766
cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.944931135,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.944931135,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.875055008,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.83929149,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.83929149,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.189934273,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide2,0.788566103,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.189934273,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide2,0.788566103,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.728137192
cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.944931135,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.944931135,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.875055008,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.83929149,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.83929149,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.189934273,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide2,0.788566103,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.189934273,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide2,0.788566103,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.728137192
cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.944931135,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.944931135,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.875055008,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.83929149,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.83929149,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.189934273,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide2,0.788566103,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.189934273,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide2,0.788566103,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.728137192
cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide5,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide5,0.944824173,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.82544229,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.356447738,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide3,0.168941379,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.388853593,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide3,0.168941379,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide2,0.163908265,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide2,0.163908265,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide2,0.163908265,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.159696469
cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide5,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide5,0.944824173,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.82544229,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.356447738,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide3,0.168941379,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.388853593,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide3,0.168941379,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide2,0.163908265,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide2,0.163908265,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide2,0.163908265,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.159696469
cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide5,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide5,0.944660549,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.423797027,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide4,0.690741105,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide8,0.40585899,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide7,0.929559175,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide6,0.797957277,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide3,0.153669569,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,0.091633479,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.063719002,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.063719002
cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide5,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide5,0.944660549,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.423797027,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide4,0.690741105,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide8,0.40585899,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide7,0.929559175,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide6,0.797957277,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide3,0.153669569,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,0.091633479,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.063719002,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.063719002
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide14,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide14,0.944595776,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.096893753,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,0.864764529,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.074718504,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.072895363,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide10,0.071150079,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide10,0.071150079,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide6,0.067967457,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide3,0.066769703,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide6,0.153682345
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide14,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide14,0.944595776,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.096893753,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide13,0.864764529,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.074718504,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.072895363,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide10,0.071150079,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide10,0.071150079,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide6,0.067967457,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide3,0.066769703,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide6,0.153682345
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide13,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,0.944575408,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.176950999,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide14,0.874412503,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.15458295,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.137102989,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide6,0.278593559,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide6,0.101559888,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide3,0.06895238,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide5,0.134974213,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.068348174
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide13,0.944575408,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.176950999,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide14,0.874412503,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.15458295,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.137102989,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide6,0.278593559,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide6,0.101559888,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide3,0.06895238,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide5,0.134974213,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.068348174
cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.944500272,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide4,0.531066186,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,0.387296423,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.372619754,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide15,0.211250631,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.289891694,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.21879508,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.11331933,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide3,0.15632673,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide2,0.160832563
cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.944500272,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide4,0.531066186,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,0.387296423,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.372619754,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide15,0.211250631,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.289891694,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.21879508,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.11331933,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide3,0.15632673,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide2,0.160832563
cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide12,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide12,0.944489426,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide8,0.380509895,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.318229032,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.300947157,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.27518085,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.235215787,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.215777319,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.213920547,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,0.211740507,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide11,0.202287703
cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide12,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide12,0.944489426,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide8,0.380509895,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.318229032,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.300947157,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.27518085,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.235215787,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.215777319,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.213920547,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,0.211740507,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide11,0.202287703
cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide7,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide7,0.944418568,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide7,0.944418568,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.139450591,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,0.119228987,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.114070939,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide6,0.087329781,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.074159162,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.073703933,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.070459219,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.068807755
cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide7,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide7,0.944418568,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide7,0.944418568,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.139450591,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,0.119228987,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.114070939,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide6,0.087329781,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.074159162,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.073703933,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.070459219,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.068807755
cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide7,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide7,0.944418568,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide7,0.944418568,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.139450591,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,0.119228987,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.114070939,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide6,0.087329781,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.074159162,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.073703933,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.070459219,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.068807755
cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide1,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide1,0.944412167,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.909495132,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.851774689,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.851774689,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.848783472,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.838044807,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide1,0.836479834,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide1,0.836479834,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.834492468,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide6,0.829160213
cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide1,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide1,0.944412167,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.909495132,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.851774689,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.851774689,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.848783472,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.838044807,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide1,0.836479834,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide1,0.836479834,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.834492468,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide6,0.829160213
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.944207572,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.367595943,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,0.126261002,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide5,0.375245775,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide2,0.221010542,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.120524057,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide4,0.106035056,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.105527338,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide3,0.082199481,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.14841861
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.944207572,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.367595943,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,0.126261002,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide5,0.375245775,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide2,0.221010542,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.120524057,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide4,0.106035056,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.105527338,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide3,0.082199481,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.14841861
cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide6,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide6,0.944160363,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.210798215,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.210798215,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.210798215,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide4,0.113659806,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.390384421,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.074387445,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide4,0.534356434,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.047052405,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.04454074
cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide6,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide6,0.944160363,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.210798215,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.210798215,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.210798215,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide4,0.113659806,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.390384421,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.074387445,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide4,0.534356434,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.047052405,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.04454074
cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide1,0.944127081,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.921292169,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.921292169,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.920577157,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.920577157,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.915324466,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.915324466,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.915324466,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide1,0.908272571,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide1,0.908272571
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide5,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide5,0.944029036,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.205616568,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide3,0.111425672,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.373016603,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide4,0.100365974,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,0.148361186,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,0.090812936,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide1,0.114360661,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide4,0.063329899,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.045479133
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide5,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide5,0.944029036,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.205616568,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide3,0.111425672,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.373016603,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide4,0.100365974,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide13,0.148361186,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,0.090812936,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide1,0.114360661,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide4,0.063329899,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.045479133
cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.944027952,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide5,0.528477576,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,0.351292599,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.330479494,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide10,0.248878143,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.209825612,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.194432984,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.273752195,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.158270533,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.389305835
cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.944027952,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide5,0.528477576,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,0.351292599,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.330479494,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide10,0.248878143,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.209825612,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.194432984,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.273752195,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.158270533,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.389305835
cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide1,0.944000389,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.892358064,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.880998097,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.867268275,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.838973407,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.838973407,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.815810917,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide1,0.784741878,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.779727649,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.743633751
cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide1,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide1,0.943824164,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.911975277,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.911087324,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.878633304,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.876915816,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.876915816,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide6,0.864184415,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.860038344,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.860038344,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide1,0.847041273
cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide1,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide1,0.943824164,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.911975277,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.911087324,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.878633304,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.876915816,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.876915816,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide6,0.864184415,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.860038344,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.860038344,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide1,0.847041273
cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide8,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide8,0.943815268,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,0.1250022,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide9,0.381130242,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide5,0.345488594,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.075975631,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.075975631,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.068144452,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.068144452,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide3,0.044457359,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,0.03927672
cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide8,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide8,0.943815268,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,0.1250022,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide9,0.381130242,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide5,0.345488594,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.075975631,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.075975631,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.068144452,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.068144452,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide3,0.044457359,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,0.03927672
cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.943564727,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.92669887,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.882074352,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide1,0.876870886,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide1,0.876870886,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide6,0.874374564,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide1,0.873447611,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.867120564,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.866773871,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide1,0.866587091
cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.943564727,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.92669887,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.882074352,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide1,0.876870886,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide1,0.876870886,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide6,0.874374564,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide1,0.873447611,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.867120564,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.866773871,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide1,0.866587091
cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.94301222,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.32036178,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.32036178,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.32036178,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide4,0.153423329,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide4,0.67659835,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.099143111,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide6,0.358786274,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.070569151,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.05647512
cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.94301222,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.32036178,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.32036178,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.32036178,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide4,0.153423329,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide4,0.67659835,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.099143111,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide6,0.358786274,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.070569151,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.05647512
cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide6,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide6,0.942849269,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide6,0.942849269,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.158639092,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide9,0.138187754,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.054090285,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.049421772,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.041543674,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.041109298,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.039010263,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.039010263
cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide6,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide6,0.942849269,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide6,0.942849269,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.158639092,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide9,0.138187754,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.054090285,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.049421772,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.041543674,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.041109298,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.039010263,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.039010263
cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide6,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide6,0.942849269,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide6,0.942849269,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.158639092,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide9,0.138187754,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.054090285,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.049421772,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.041543674,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.041109298,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.039010263,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.039010263
cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.942807794,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.942807794,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.548057776,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.395462729,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.374628215,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.359221842,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide13,0.285988959,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.259880635,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.236953318,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.222076677
cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.942807794,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.942807794,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.548057776,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.395462729,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.374628215,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.359221842,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide13,0.285988959,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.259880635,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.236953318,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.222076677
cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.942807794,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.942807794,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.548057776,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.395462729,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.374628215,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.359221842,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide13,0.285988959,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.259880635,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.236953318,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.222076677
cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide1,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.942611914,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.941345452,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide3,0.444434188,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.412198435,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.369976061,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.289956157,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.14229159,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.133385143,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.133385143,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide2,0.328569435
cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.942482923,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.479367932,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.464936043,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.340170568,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide7,0.13265777,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.11063443,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide28,0.090205726,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide11,0.083460411,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide2,0.062208322,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.061625963
cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.942482923,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.479367932,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.464936043,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.340170568,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide7,0.13265777,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.11063443,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide28,0.090205726,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide11,0.083460411,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide2,0.062208322,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.061625963
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide10,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide3,0.942363213,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.112357897,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide6,0.104047022,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.099582116,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.093246893,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.088182625,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide7,0.088028495,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.083501304,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.082914008,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.082332048
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide12,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide12,0.942218675,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.84552308,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.169136419,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide6,0.1488679,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.11676943,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide5,0.058102046,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.045210498,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,0.041624329,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide5,0.035611759,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.034964412
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide12,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide12,0.942218675,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.84552308,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.169136419,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide6,0.1488679,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.11676943,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide5,0.058102046,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.045210498,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,0.041624329,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide5,0.035611759,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.034964412
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide10,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide10,0.942077523,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.903233984,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,0.472372407,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide11,0.883627166,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide6,0.082276657,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide4,0.072881211,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide5,0.068277527,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide6,0.135589182,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide5,0.284124295,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.035273304
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide10,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide10,0.942077523,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.903233984,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,0.472372407,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide11,0.883627166,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide6,0.082276657,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide4,0.072881211,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide5,0.068277527,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide6,0.135589182,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide5,0.284124295,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.035273304
cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.942045253,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.173433367,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide1,0.12600987,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.121716971,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.121716971,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.093640715,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.132789108,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.082168674,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.119193874,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.062116707
cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.942045253,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.173433367,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide1,0.12600987,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.121716971,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.121716971,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.093640715,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.132789108,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.082168674,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.119193874,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.062116707
cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.941958055,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.941958055,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide4,0.769924987,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.620603919,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.620603919,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide4,0.518589498,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.152549507,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.152549507,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.768950259,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide4,0.518589498
cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.941958055,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.941958055,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide4,0.769924987,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.620603919,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.620603919,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide4,0.518589498,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.152549507,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.152549507,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.768950259,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide4,0.518589498
cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.941958055,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.941958055,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide4,0.769924987,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.620603919,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.620603919,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide4,0.518589498,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.152549507,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.152549507,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.768950259,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide4,0.518589498
cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.941351562,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide2,0.328734422,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide7,0.087531548,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide10,0.17114924,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide3,0.115587217,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide2,0.051708156,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide5,0.080125628,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide0,0.187365951,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,0.043126108,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.034546739
cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.941351562,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide2,0.328734422,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide7,0.087531548,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide10,0.17114924,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide3,0.115587217,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide2,0.051708156,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide5,0.080125628,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide0,0.187365951,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,0.043126108,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.034546739
cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide3,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide3,0.9411855,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide5,0.148866127,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide7,0.052432131,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.042345025,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide7,0.097601907,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.107392446,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.02324998,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.02324998,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.017769243,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.017769243
cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide3,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide3,0.9411855,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide5,0.148866127,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide7,0.052432131,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.042345025,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide7,0.097601907,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.107392446,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.02324998,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.02324998,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.017769243,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.017769243
cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide4,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide2,0.94109706,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide2,0.09519252,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.071166354,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.06354563,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.059743015,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.057471659,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.056984573,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide1,0.052826795,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.1816462,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.0459263
cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide4,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide4,0.940201107,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.550525511,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.550525511,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.550525511,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide5,0.493078906,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.691523132,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide6,0.519658856,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.173235719,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.154950335,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.13405246
cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide4,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide4,0.940201107,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.550525511,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.550525511,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.550525511,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide5,0.493078906,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.691523132,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide6,0.519658856,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.173235719,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.154950335,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.13405246
cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide1,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide1,0.940050893,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.577834792,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide7,0.88006462,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.290665676,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.263328165,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide4,0.503295912,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide1,0.24918305,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.245963644,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.239358197,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.478199363
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.940013995,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide26,0.186458316,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.061582657,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.061582657,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide2,0.122291799,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.036620984,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.028320308,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide5,0.040477103,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide3,0.025849235,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide2,0.025681741
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.940013995,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide26,0.186458316,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.061582657,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.061582657,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide2,0.122291799,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.036620984,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.028320308,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide5,0.040477103,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide3,0.025849235,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide2,0.025681741
cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide1,0.939950343,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.635825071,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.481965052,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.43280187,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.423112845,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.329001567,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.329001567,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.224478343,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.625627515,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.224478343
cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide6,0.939491379,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide6,0.871485816,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.75531971,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.574995425,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,0.570944206,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.487013151,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.487013151,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.47076798,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.448983578,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.448983578
cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,0.938479144,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.69859309,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.515797649,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.491650427,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.490304812,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.468659007,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.468659007,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.39014157,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide6,0.354085963,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide6,0.350440328
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide2,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide2,0.93820003,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.587202272,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.483612506,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.244110365,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide28,0.104258517,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,0.081661017,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.078971187,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.517100343,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.111800247,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.044861781
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide2,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide2,0.93820003,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.587202272,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.483612506,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.244110365,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide28,0.104258517,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,0.081661017,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.078971187,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.517100343,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.111800247,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.044861781
cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.938079275,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.602556991,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.494374361,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.478320766,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide6,0.47594587,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.431538246,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.418990182,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.418990182,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide5,0.404384062,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.397294246
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide0,0.938073864,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.484340141,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide1,0.938073864,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide4,0.938073864,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.304143984,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide5,0.605647466,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide3,0.78788499,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide2,0.78788499,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.175004916,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.175004916
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide0,0.938073864,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.484340141,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide1,0.938073864,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide4,0.938073864,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.304143984,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide5,0.605647466,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide3,0.78788499,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide2,0.78788499,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.175004916,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.175004916
cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide6,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide6,0.937977883,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.741604934,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide5,0.816289676,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide7,0.890637669,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide3,0.030849425,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide14,0.027981028,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide4,0.29907188,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide14,0.027981028,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.022531245,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.022531245
cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide6,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide6,0.937977883,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.741604934,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide5,0.816289676,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide7,0.890637669,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide3,0.030849425,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide14,0.027981028,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide4,0.29907188,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide14,0.027981028,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.022531245,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.022531245
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.937850649,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide11,0.395303701,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.230912757,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.207174889,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.192927988,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.063024855,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide5,0.134132188,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide0,0.062747659,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.050736558,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide4,0.067628201
cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide7,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide7,0.937366832,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.528318176,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide4,0.643590276,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide5,0.936923982,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide6,0.890556675,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide3,0.17068975,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.021275413,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.021275413,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.020618216,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.019535616
cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide7,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide7,0.937366832,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.528318176,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide4,0.643590276,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide5,0.936923982,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide6,0.890556675,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide3,0.17068975,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.021275413,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.021275413,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.020618216,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.019535616
cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide2,0.937280071,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide2,0.937280071,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.218189544,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.174643326,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.147135911,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.145653257,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.118919634,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.095434789,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.137870774,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.089235341
cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide2,0.936249527,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.417596225,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.177562631,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.177562631,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide5,0.161068625,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.132187751,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.132187751,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.104766424,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.102960687,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.08735506
cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide2,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide2,0.935712055,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide0,0.65350402,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.327492667,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide3,0.305251574,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide0,0.098813801,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.242319054,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.06649564,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.058829051,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide7,0.056083826,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide6,0.097313961
cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide2,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide2,0.935712055,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide0,0.65350402,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.327492667,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide3,0.305251574,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide0,0.098813801,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.242319054,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.06649564,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.058829051,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide7,0.056083826,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide6,0.097313961
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.934897001,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide21,0.344029221,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.644621114,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.088028232,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.072088904,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.072088904,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.056108172,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide28,0.317768405,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.056108172,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.049125091
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.934897001,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide21,0.344029221,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.644621114,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.088028232,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.072088904,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.072088904,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.056108172,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide28,0.317768405,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.056108172,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.049125091
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide7,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.934897001,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide21,0.344029221,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.644621114,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.088028232,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.072088904,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.072088904,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.056108172,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide28,0.317768405,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.056108172,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.049125091
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide7,0.934897001,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide21,0.344029221,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.934897001,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,0.934897001,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,0.545962743,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.088028232,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.072088904,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.072088904,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide2,0.398028168,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.056108172
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide0,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide0,0.934034046,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.732051381,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,0.62272846,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.587787759,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.582422758,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide16,0.562302699,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide0,0.61284239,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide7,0.586280423,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.674427789,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide13,0.707093857
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide0,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide0,0.934034046,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.732051381,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide11,0.62272846,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.587787759,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.582422758,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide16,0.562302699,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide0,0.61284239,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide7,0.586280423,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide2,0.556746022,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide13,0.707093857
cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.933869437,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.723095119,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.690478663,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.586431671,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide6,0.533829273,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.498277769,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.48790206,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.471627261,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.397960626,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide4,0.853914988
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide16,0.932490643,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide3,0.242385611,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide23,0.920970541,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide17,0.924244194,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide20,0.9204281,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.170666909,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide21,0.920627257,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide19,0.920627257,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide24,0.920661995,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide25,0.920661995
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide16,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.932487536,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.090157317,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.921513073,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.921513073,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.899424804,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.867190429,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide16,0.935746352,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.173565633,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.084632797,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.083626722
cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide1,0.932278741,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.601435682,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.501172909,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.477566679,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide3,0.393151868,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.339445324,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.272568967,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.239297718,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.210374173,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.162331507
cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide8,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.932006608,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.867520432,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide11,0.652117266,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.559261768,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.473736525,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide12,0.367675143,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide12,0.367675143,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide12,0.219390137,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide12,0.219390137,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide6,0.084625677
cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.931397558,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.635099578,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.446893821,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.438743302,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.43506295,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.432329036,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.376028723,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide3,0.32878639,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.266137081,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.254081795
cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.931168857,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.858618602,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide1,0.856737884,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.853484363,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.827584923,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.827584923,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.820989,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.818937114,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.800576785,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.77532831
cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide6,0.930165544,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,0.316995999,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,0.316995999,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.201288333,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide10,0.146526445,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide11,0.132638844,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.094157745,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide6,0.090799231,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.085478756,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.081814187
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.928975153,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.528661534,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.528661534,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide2,0.427564751,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.170803827,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.158986971,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide28,0.157309424,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.150302179,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.097381687,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.097381687
cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide8,0.926852135,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.822706758,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide11,0.529470891,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.495575175,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.445477944,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide12,0.286916327,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide12,0.286916327,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide12,0.148144077,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide12,0.148144077,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide6,0.11500744
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide17,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.924260805,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.090245166,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.912339344,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.912339344,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.890777549,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.854509246,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide17,0.936542692,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.173011477,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.084591228,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide13,0.100873209
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide0,0.922113975,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.060395202,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide1,0.835443025,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide3,0.802256421,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide2,0.802256421,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.032756998,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.032668654,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.030824119,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.030824119,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.030824119
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide0,0.922113975,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.060395202,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide1,0.835443025,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide3,0.802256421,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide2,0.802256421,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.032756998,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.032668654,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.030824119,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.030824119,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.030824119
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide6,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.921682781,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide23,0.38233944,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.286122667,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.115257432,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide34,0.275012101,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide21,0.382269834,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.332832101,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide20,0.370922102,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide27,0.271755654,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide22,0.377568182
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide16,0.921502658,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide3,0.179021197,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide17,0.912302203,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide23,0.907234035,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide21,0.906970311,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide19,0.906970311,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide11,0.185559767,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide20,0.906223202,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide25,0.906582386,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide24,0.906582386
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide16,0.921502658,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide3,0.179021197,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide17,0.912302203,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide23,0.907234035,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide21,0.906970311,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide19,0.906970311,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide11,0.185559767,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide20,0.906223202,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide25,0.906582386,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide24,0.906582386
cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide3,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide3,0.921267112,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide2,0.480012135,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.312064293,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.280212659,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,0.277490314,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.272396748,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide2,0.521542914,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.250702184,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide1,0.238372392,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.214182129
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide23,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.920990581,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.084242737,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.907290583,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.907290583,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.893307031,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.844741174,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,0.083041864,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.169421276,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.085215376,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.169421276
cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.920892933,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.350292363,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide7,0.901249806,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.251885905,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,0.422878606,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.217136586,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.217136586,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide9,0.442813586,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.14280767,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.14280767
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide19,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.920693163,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.085075383,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.90707918,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.90707918,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.892198573,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.844612653,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide19,0.937123602,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.168293373,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.084387931,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.082426785
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide21,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.920693163,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.085075383,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.90707918,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.90707918,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.892198573,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.844612653,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide19,0.937123602,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.168293373,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.084387931,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.082426785
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide24,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.920607134,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide15,0.085488526,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.89237747,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.906548011,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.906548011,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.843425252,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.168437424,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.086836498,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.168437424,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.086836498
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide25,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.920607134,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide15,0.085488526,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.89237747,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.906548011,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.906548011,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.843425252,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.168437424,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.086836498,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.168437424,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.086836498
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide2,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide1,0.920564457,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide17,0.023187585,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide0,0.017443471,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide12,0.014817007,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide7,0.038175087,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide8,0.014580826,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide13,0.014106162,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.013081945,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide0,0.047838701,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide4,0.047147145
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide20,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.920492535,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide15,0.084243633,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.894225784,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.906327721,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.906327721,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.842544717,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.168048898,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.086003858,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.168048898,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.086003858
cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide1,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.920084347,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide1,0.578847784,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide1,0.578847784,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide1,0.578847784,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.558282459,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.558282459,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.529985443,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.529985443,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.510845762,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.469197119
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide5,cluster-analysis##02_module-1##01_lesson-1-.txt##slide2,0.919840112,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide3,0.87931333,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide5,0.806044858,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide5,0.793703053,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide4,0.179678147,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.048219603,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.043400188,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.036473706,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.035098051,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.033697916
cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide9,0.919300746,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.43018146,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.43018146,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.43018146,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.276945974,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.163627609,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.162361079,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.146431747,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.122053789,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide6,0.19316172
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide26,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.918838734,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.083699644,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.888332638,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.904476376,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.904476376,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.840834731,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.165488433,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,0.08718299,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.165488433,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.083699644
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide18,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.918558863,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.08601652,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.90520374,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.90520374,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.886398359,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.843031813,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.08044082,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.165555809,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.080968007,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide13,0.098368081
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide6,0.917863735,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide22,0.260457426,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide7,0.779704709,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide1,0.36779854,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide8,0.764130632,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.067487332,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide4,0.678643548,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.034409574,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide15,0.702567596,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide24,0.034041206
cluster-analysis##02_module-1##01_lesson-1-.txt##slide2,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide5,0.916970048,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide3,0.910766095,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide5,0.760885413,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide5,0.645941279,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide4,0.2716775,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.067446191,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.065606264,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.064193788,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide6,0.059378399,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.057630176
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide18,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.914265833,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.472082968,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,0.278297536,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.522881208,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.268277365,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.219874516,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.196075751,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.17649733,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide7,0.167274327,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide12,0.154861256
cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.914009686,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,0.837227397,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.758198768,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.700580205,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.639642892,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.606870547,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide9,0.405851396,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide6,0.336689661,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.332901572,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide12,0.224190399
cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide6,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.913653459,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,0.421151257,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,0.421151257,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide6,0.239653724,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide10,0.232677132,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide11,0.21912402,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.199184,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.198720366,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.174211779,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.140898215
cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.91343461,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.394878757,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.34632021,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.34632021,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide7,0.727244932,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.34632021,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.254497415,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.194144352,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.153033789,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.124714576
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide13,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.912958139,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.910664017,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.882373231,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.886369505,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.912100143,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide1,0.899544347,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.899427979,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide2,0.898458555,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide3,0.896491155,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.898037154
cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide3,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide3,0.912945062,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide2,0.546445675,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.485191579,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,0.448544098,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.445189334,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.430353971,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide1,0.416645803,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide1,0.375835061,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.375777432,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.375712579
cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.912131536,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,0.644737023,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.567527227,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.545274644,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.503087108,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide6,0.329712952,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.316325184,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.280438693,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide12,0.187595543,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide12,0.187595543
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide22,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.91212526,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.085272548,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.897710678,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.897710678,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.881133034,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.83211294,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide22,0.937579298,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.168121654,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide4,0.08283039,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide13,0.101282521
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide28,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.910984063,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.202351504,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.897391514,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.897391514,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.873477526,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.834511495,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.113024229,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide28,0.939348288,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.187788999,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.135973376
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide5,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.908964664,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.358777035,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.335508436,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.175800015,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.159511938,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.145044879,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide3,0.126818547,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.115117284,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,0.322177042,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,0.138447255
cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.906538428,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.906538428,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.903854708,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.892017545,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.863013968,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.854434408,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide1,0.841131189,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide1,0.829893124,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.742884813,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.694299835
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide5,0.905830114,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,0.337838251,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,0.870819199,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.329234516,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.895528874,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide11,0.441533741,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.156591681,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.147148248,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.132185247,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.281915699
cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide2,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide3,0.900134959,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.083580148,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide4,0.082879783,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide5,0.099677484,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,0.049643903,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.135442861,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide8,0.019935577,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide7,0.019368371,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide0,0.035038917,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide3,0.017476936
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide3,cluster-analysis##02_module-1##01_lesson-1-.txt##slide2,0.896961283,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide5,0.8567284,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide5,0.691234024,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide5,0.688949592,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide4,0.250766785,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.092602356,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.083131861,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.080098095,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.076500324,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.065455799
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.893818037,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.586133505,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.467382913,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.467382913,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.467382913,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.390377274,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.566243735,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,0.526402742,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.503561453,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.942118335
cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide1,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.893796897,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.87210429,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.805699739,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.75679559,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.733661544,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.724750487,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.722523577,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.691624519,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.688695194,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide1,0.634730727
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.893559203,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.640379381,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.640379381,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.640379381,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.630039619,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.839291764,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide23,0.839356924,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.81064832,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide21,0.839356924,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.630039619
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide18,0.893315445,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.780333142,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.398352874,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.363117849,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.349963829,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,0.349886554,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide7,0.301195749,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.300896474,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.276326229,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.258157694
cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.893154593,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.845076067,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.831486241,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.72445537,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.707298769,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.679693294,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.633544659,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.627305458,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide1,0.623274363,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.620816932
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide27,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.893126317,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.17795171,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.876069044,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.876069044,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.86085654,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.802775789,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide15,0.106767285,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.117907109,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide29,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.892425148,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.179044717,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.876803374,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.876803374,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.849975562,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.806647451,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.10892059,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide15,0.10531005,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.181225378,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.126774623
cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.891570418,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.891570418,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.307893061,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.234966593,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.21111843,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.186479483,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.183001218,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.17914054,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.178524614,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.168462673
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide5,0.891370008,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.239146138,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide4,0.138919772,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.13166449,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.109543576,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.109537366,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.101013502,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.081590003,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.081028968,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.074422162
cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide5,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.890801117,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.819763019,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide2,0.415889227,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide4,0.277123874,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide2,0.103386632,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.081918563,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide5,0.066914145,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.056899551,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.056899551,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.183960078
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide30,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.890611531,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.173949192,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.875022494,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.875022494,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.84786346,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.803988543,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.107671442,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide15,0.105118939,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.179710389,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.123912546
cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide2,0.887528189,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide2,0.887528189,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.78031043,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.78031043,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,0.669246077,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.199793223,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,0.168503682,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.61964493,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.61964493,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.054254818
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.885688444,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.427918955,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,0.374856535,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,0.341481881,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.186542816,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.181740607,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.163382999,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide8,0.340781797,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.302455877,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide1,0.137913503
cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide0,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide0,0.885191634,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide0,0.885191634,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide0,0.885191634,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide0,0.885191634,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide0,0.826401886,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.031931925,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide9,0.02486358,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.011800967,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.010076652,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.008790813
cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide0,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide0,0.885191634,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide0,0.885191634,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide0,0.885191634,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide0,0.885191634,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide0,0.826401886,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.031931925,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide9,0.02486358,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.011800967,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.010076652,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.008790813
cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide0,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide0,0.885191634,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide0,0.885191634,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide0,0.885191634,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide0,0.885191634,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide0,0.826401886,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.031931925,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide9,0.02486358,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.011800967,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.010076652,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.008790813
cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide0,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide0,0.885191634,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide0,0.885191634,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide0,0.885191634,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide0,0.885191634,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide0,0.826401886,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.031931925,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide9,0.02486358,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.011800967,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.010076652,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.008790813
cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide0,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide0,0.885191634,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide0,0.885191634,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide0,0.885191634,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide0,0.885191634,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide0,0.826401886,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.031931925,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide9,0.02486358,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.011800967,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.010076652,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.008790813
language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide1,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide2,0.884975543,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.054002897,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide7,0.28348552,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide1,0.081758813,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.035202513,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.023861875,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide17,0.023218915,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.020348615,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide12,0.036238452,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide7,0.017337082
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide17,0.883629236,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.428138965,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.871515266,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.380049502,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide10,0.713897612,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.138598621,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.635179821,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.136322303,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.118111739,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide12,0.475483048
cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.883274542,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.799059834,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.633163421,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.297724004,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.245493047,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.238167138,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.214985269,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.154951086,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.152233295,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.252641357
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide0,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.883039942,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.475466466,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide10,0.167087061,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.325927039,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide2,0.222270615,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,0.647239949,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.162791977,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide7,0.222270615,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.221773547,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide14,0.222270615
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide12,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.883039942,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.475466466,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide10,0.167087061,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.325927039,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide2,0.222270615,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,0.647239949,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.162791977,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide7,0.222270615,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.221773547,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide14,0.222270615
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide0,0.883032507,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide10,0.340736096,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide12,0.883032507,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide1,0.723332984,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide28,0.212262609,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,0.774438358,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide14,0.774094493,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide11,0.767486953,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.249101373,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide6,0.765617062
cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.878736039,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.349263271,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.321794864,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.270369649,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide6,0.254692481,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.238283028,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.225483575,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.213174855,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.193321028,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.176088877
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide16,0.877196065,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide3,0.227336258,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide20,0.870478148,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide23,0.869447299,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide25,0.868458683,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.13765992,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide24,0.868458683,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide19,0.868057476,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide21,0.868057476,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide17,0.866882479
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide17,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.876497901,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide10,0.162708267,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.853131096,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.149970655,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.227767776,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.149970655,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.149970655,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.144482589,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.139051991,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.132593624
cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide9,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.876184637,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.413846286,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.24685988,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.189674209,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.189674209,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.189674209,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.166044617,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.132236498,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.105024866,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.082235088
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.876056775,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide0,0.241274769,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide11,0.181786199,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.129072071,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.110098275,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.10688871,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.06024467,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide0,0.042287554,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.037083033,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide6,0.028737439
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide10,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.876012571,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.137085378,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.121531756,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide1,0.116527642,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.077222468,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide2,0.049003226,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.044608488,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide17,0.087148857,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.050272301,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.042971545
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.87590662,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.297243341,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.815862321,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.297243341,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.297243341,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.226117564,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.149944928,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.144802914,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.139301331,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.120373472
cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.875833348,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.875833348,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.875833348,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.829954479,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.829954479,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide4,0.361332581,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,0.285675915,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.774347905,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.774347905,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.192884306
cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide2,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.873948481,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.873948481,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.244405814,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.178379001,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.178379001,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.178379001,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide8,0.131487004,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.111545047,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.09517994,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.09517994
cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.87169642,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.864187265,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide4,0.710631066,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.649004134,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.550812112,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide6,0.52588199,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.474393976,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.452842829,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.445235173,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.437994534
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.871607794,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.574835937,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.323796839,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide2,0.691085485,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide5,0.732682797,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide6,0.167531666,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide0,0.571934065,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide0,0.157562823,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.140791734,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide7,0.421202698
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide7,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide6,0.871389316,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide6,0.373875421,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.119449847,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide14,0.038601109,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.023889361,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.018285288,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.018285288,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide0,0.014590007,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,0.029190044,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide12,0.014150895
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide6,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide7,0.868794077,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide6,0.443789641,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.082953798,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.030787367,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.028760002,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.028760002,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide13,0.027436086,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide9,0.017213726,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide9,0.017213726,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide2,0.016261551
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide1,0.868677667,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.669714171,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.431563625,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide2,0.714770728,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide7,0.25118695,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.666508089,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide4,0.398736629,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide18,0.36581674,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide7,0.239161599,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.323745445
cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.867504546,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide2,0.47684584,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide2,0.47684584,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide5,0.26336935,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.681123258,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide3,0.109613906,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide28,0.055221316,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,0.239831643,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide3,0.04432639,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide3,0.04432639
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide16,0.86721421,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide3,0.208766925,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide17,0.854490662,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide23,0.844660472,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide12,0.191349772,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide21,0.844457355,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide19,0.844457355,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide25,0.843486229,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide24,0.843486229,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide18,0.842907907
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide9,0.867078729,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.84295213,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.738298489,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.625808073,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide7,0.852914239,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide6,0.803253619,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.480786754,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide5,0.676565831,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide10,0.58980895,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.565304008
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide9,0.867078729,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.84295213,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.738298489,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.625808073,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide7,0.852914239,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide6,0.803253619,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.480786754,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide5,0.676565831,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide10,0.58980895,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.565304008
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide9,0.867078729,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.84295213,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.738298489,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.625808073,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide7,0.852914239,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide6,0.803253619,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.480786754,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide5,0.676565831,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide10,0.58980895,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.565304008
cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide8,0.866262155,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.834316274,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide11,0.708060337,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.707138492,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.576531763,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide12,0.291143827,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide12,0.291143827,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide6,0.195320888,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide12,0.191284547,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide12,0.191284547
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.864475757,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.45599218,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.329571204,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.138018951,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.114684658,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide0,0.114411586,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.084578264,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.084578264,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide4,0.104955613,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide0,0.079514517
cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.86156935,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide2,0.485337504,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide2,0.485337504,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,0.235837497,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide3,0.092879612,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide3,0.040963399,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide3,0.040963399,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide2,0.112196893,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide28,0.039897766,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.119814473
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide17,0.859427364,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.441543523,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide10,0.787576574,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide3,0.159120698,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.811223914,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.473524869,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.691044303,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.507042002,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.150818787,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide11,0.466442709
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide0,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.858600654,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.649326448,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide10,0.630898252,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.469684043,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.326158482,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide8,0.349948048,,,,,,,,
cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide10,0.858516532,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide10,0.858516532,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,0.496168351,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.111117229,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide11,0.57966915,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.095798493,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide11,0.57966915,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.072426468,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.359073772,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.057814568
cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide6,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.858167348,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.407100347,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.363321047,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.351018636,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.296701668,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.288971607,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.278703723,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide10,0.255602261,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,0.254418059,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide6,0.250407374
cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.856864778,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide0,0.53102577,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.457214688,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.449231637,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.425880782,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.402912932,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,0.378986635,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.361556331,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.337121925,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide0,0.331641578
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.850709614,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.598763325,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.494958543,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.432746325,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.423097537,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.389133353,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.384163932,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.331439788,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,0.326478387,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.322483572
cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide4,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.850373229,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.651781702,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.628520894,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.592687455,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.554279384,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.554279384,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.551192163,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.524039084,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.51111417,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.497322404
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide0,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide4,0.849822907,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.174627297,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.056378577,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide1,0.035416488,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide5,0.029443004,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide13,0.026064444,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.024405828,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.015588473,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.013920176,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide1,0.012995878
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.848897611,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.320456643,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,0.232737646,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide3,0.143782652,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.105985444,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.149487835,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.095061746,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.293635218,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,0.094224438,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,0.131033578
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide5,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.848690713,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.216052155,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.134214152,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.087923982,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.084019375,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.083637325,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.078524262,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.073282583,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.068778987,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide8,0.067814063
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide0,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide0,0.846127986,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.207419715,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.17229945,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide4,0.109786739,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide3,0.073168022,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.055565387,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide2,0.055054515,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide2,0.053483698,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide8,0.049998555,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.046414736
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide0,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide0,0.845265624,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.335909928,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.160522742,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide4,0.141558794,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,0.079522424,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.044736802,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.043363974,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.034896163,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide0,0.029768528,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide0,0.025359351
cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide7,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide1,0.84474674,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide6,0.327117136,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.26628001,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.209842789,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.209203523,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.199033116,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.172745473,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.170638273,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.262172769,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.162236567
cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide2,0.842471194,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide2,0.842471194,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.686081746,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.332360425,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.332360425,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide1,0.212936598,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.095546915,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.091435073,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide3,0.111192034,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.07776659
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide20,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.839723125,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.719050814,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.605571498,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.401830549,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.668269813,,,,,,,,,,
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide21,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.839723125,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.719050814,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.605571498,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.401830549,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.668269813,,,,,,,,,,
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide22,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.839723125,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.719050814,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.605571498,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.401830549,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.668269813,,,,,,,,,,
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide23,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.839723125,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.719050814,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.605571498,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.401830549,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.668269813,,,,,,,,,,
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.839369651,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.751764062,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide0,0.733641211,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide0,0.733641211,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.750244051,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.743476727,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide1,0.902132977,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.732814574,,,,
cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.839211368,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.520449716,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.500825164,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.437887883,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,0.434315659,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.399567371,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.374323854,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.356803655,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.332557138,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide0,0.317764496
cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.838742554,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.687165799,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.574547296,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.574547296,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.52166396,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.52166396,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.493009735,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.418423266,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide10,0.402039695,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.393218735
cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide3,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide2,0.838472557,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.166984244,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.112212103,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide4,0.057877789,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.053767685,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.053715849,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide7,0.042824032,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.031633134,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.031204468,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.028985124
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide13,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.836627891,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide0,0.708470719,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide0,0.708470719,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.697068452,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.698583428,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide1,0.919112934,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.693612528,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.676503781,,,,
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide9,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.836346754,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.740845852,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.492918742,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,0.836346754,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,0.836346754,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide4,0.079983572,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide11,0.041304754,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.458362713,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide3,0.050793462,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.385778805
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.834531729,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.694049282,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.50056575,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.757154252,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.748776863,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.519898364,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide13,0.621855683,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,0.377480358,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide19,0.424967898,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.374827659
cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.834286413,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.834286413,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.159420506,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.135858193,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.13306384,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.102157569,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.094743578,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.094743578,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.077435397,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide11,0.057421379
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.83268756,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.293917779,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide10,0.816478506,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide9,0.126388585,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide3,0.585525371,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.771814134,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.415499506,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.951202277,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide6,0.259224207,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide1,0.177757377
cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.830345146,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.691264934,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.637981894,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,0.575424424,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.527315996,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.500704439,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide6,0.47896425,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.475731403,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.475731403,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.456938479
cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide6,0.829237301,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide10,0.424743261,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.320827487,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.280414915,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.264788903,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.264682799,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.262069496,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.197443798,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.190262573,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.19011532
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.829210629,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide5,0.636500437,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide6,0.265116769,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.251921711,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.363860077,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.15602796,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.120699676,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.363860077,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.117228645,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.158842031
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.829044612,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide14,0.353728316,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.321527701,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.343092072,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.664594457,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide21,0.279087052,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.453614608,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide17,0.275876784,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.325544461,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.951119498
cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.8289801,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.753510625,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.66825101,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.556799312,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.441036953,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide6,0.406220762,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.332841188,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide9,0.256863448,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.222433648,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide12,0.210395849
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.82743816,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.711538183,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.493395826,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.491182756,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,0.459816863,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.443883064,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.41513066,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.377210874,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide12,0.353937141,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide5,0.333266972
cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.826040456,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.353149073,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.350987713,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.14539256,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide3,0.134034268,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.114953162,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.114953162,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.114896497,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.113610185,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.111513794
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide7,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.820405118,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.674038167,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.421413304,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,0.820405118,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,0.820405118,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide4,0.072564333,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide11,0.043617666,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.419246546,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.023208736,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.019474955
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide10,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.818981917,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.437663557,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.411565688,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.478735377,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,0.238294264,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.579161702,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide13,0.396365757,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide10,0.951116241,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.271519234,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide19,0.254635812
cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.816228036,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.816228036,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.752903053,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.707054836,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.647665529,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.647665529,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.586514175,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.564263495,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.543000737,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.537565927
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.814178665,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.622218432,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.358643727,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.335798335,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.309647452,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide1,0.227577983,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.197530574,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide2,0.425735528,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,0.273901048,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide3,0.13709342
cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.811918163,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.794823001,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.543923663,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.419277603,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.419277603,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide5,0.402911027,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.402748519,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.384371375,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.384371375,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.383530275
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.811163854,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,0.77381702,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.741240481,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.721283811,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.77381702,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide18,0.807779871,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.748478099,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide14,0.751577621,,,,
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.811040152,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide5,0.546271129,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.312818913,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.202316319,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide4,0.54762924,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.564856183,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.338999589,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.08799514,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,0.538203329,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide6,0.184582209
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide7,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.809374542,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide34,0.421465302,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide20,0.237141202,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide4,0.183300645,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.415814932,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide1,0.335771287,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.293345114,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide21,0.236718783,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.183300645,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide27,0.317138311
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.808144136,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.570305642,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.233191964,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide7,0.214741874,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide2,0.529910918,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.185540537,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.175629433,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.132640828,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.123238502,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.105718403
cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.802492803,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.77287099,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.760009088,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.739951389,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.739951389,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.684733661,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.684535222,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.663054313,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.662611018,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide1,0.657795707
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide26,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.801489041,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.199237546,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.17252852,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.64948515,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.143655949,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide1,0.621583433,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.139910332,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.136142553,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide0,0.110787534,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide3,0.48051314
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.801133623,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,0.633657372,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.464816892,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.461694462,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.434423956,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.413912892,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.407726871,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide7,0.367479104,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide18,0.351359913,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.336990366
cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.798429584,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.741035651,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.219958586,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.154758521,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,0.706310966,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.148694976,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.148281193,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide2,0.139611277,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.107424173,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide4,0.086873901
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.796244002,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide14,0.501303231,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.177851328,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide5,0.159876929,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.644025745,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide7,0.417510877,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.327709106,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide20,0.288357633,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide4,0.159876929,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.459831022
cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide4,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.795529159,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.24752343,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide7,0.666615744,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.168190239,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide1,0.165939533,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.148083017,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.146879344,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.134428871,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide5,0.117140916,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.106920365
cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide6,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.793677782,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.230697903,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.179248762,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide12,0.120761179,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide12,0.120761179,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.118082604,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.093998502,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide3,0.081121926,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.060441207,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide9,0.059813123
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide0,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide6,0.793426703,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide4,0.57636898,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.220506945,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide3,0.57636898,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide1,0.01631011,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide2,0.57636898,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide1,0.57636898,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.01398235,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.190145088,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.013467581
language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide7,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.793210573,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.418271277,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.397048819,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.317424452,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.28571107,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,0.26926258,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.224630442,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.223777416,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.216984953,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.202661774
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.793166245,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.459676394,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.378373302,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.317256865,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.529545239,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.288809764,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.270127907,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide2,0.409115302,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.209907488,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.181972996
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,0.793158334,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.283168235,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.629578094,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.237099633,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide3,0.656562947,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,0.629578094,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.237099633,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide2,0.642690661,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide7,0.629578094,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide3,0.131127052
cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide5,0.792739448,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide5,0.792739448,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.404731064,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.351831942,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.221262963,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.209689208,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.20265755,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.180793409,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.146514584,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.131300688
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide1,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.790949849,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.536160662,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.119493272,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide13,0.101735032,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.142123705,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.187149096,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide2,0.108815032,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide7,0.10150535,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide0,0.289250775,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.154705623
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide21,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.790304544,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.02355839,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide11,0.021483272,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide3,0.021327891,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide10,0.020279138,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide11,0.018180714,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide13,0.017870123,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide15,0.017298753,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide16,0.016704788,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide10,0.080798782
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide26,0.788832557,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.172166919,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide3,0.665854368,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide25,0.683472931,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.160467817,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide24,0.665744064,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide1,0.341306663,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.148663834,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.131720432,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.128212551
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide8,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.788772552,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.457856179,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide22,0.252749124,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide34,0.426642005,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.153185764,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.330113024,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.421346171,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide23,0.241569113,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide27,0.316797936,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide4,0.153185764
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide4,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide0,0.786513603,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide3,0.664635684,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.185394695,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide2,0.720131849,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.087798903,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide2,0.022646555,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide1,0.558231439,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.020699475,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide1,0.020172892,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide15,0.019735161
cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide0,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide1,0.78566528,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.369170691,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide10,0.106567709,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.085770659,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.075129142,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.069024811,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide5,0.067432145,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.056866525,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.048240062,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide9,0.088399444
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide7,0.785093219,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.45849035,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.454810297,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,0.439668847,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.420524661,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.330211145,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.303086087,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.283336427,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.255370373,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.227971758
cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.782860363,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.776209809,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.776132561,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.718596073,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.718596073,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.707166436,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.705612268,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.697835181,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.655936707,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.64898683
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide0,0.782080021,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.697954217,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.598917344,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide13,0.385280978,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.363291674,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide17,0.654111306,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.28634807,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide19,0.654111306,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide18,0.654111306,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.422008319
cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide12,0.780236252,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide12,0.780236252,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.405576101,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.396872343,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.246842492,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.115409758,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.087028209,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.083002888,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.080508749,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.077850751
cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.779404269,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.615509667,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.258240611,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.194415024,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.190453099,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.190453099,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide8,0.167575319,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.161430476,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide3,0.16002852,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide6,0.287619059
cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide8,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,0.77909756,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.416979216,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.283701794,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.233410989,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.204707706,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.172034235,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.169898029,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide6,0.112336513,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.335829045,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide2,0.111774758
cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide6,0.77909162,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.267976424,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.248323402,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.240911365,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.178308528,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.159253625,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.14571783,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.116682389,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.11432548,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.112202247
cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide4,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.778607066,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.778607066,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.777042175,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.777042175,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.777042175,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide4,0.197773685,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,0.151206646,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.702822211,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.116979765,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.702822211
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide15,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.777285976,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide0,0.632386983,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide0,0.632386983,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.618357194,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide3,0.85161337,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.620674723,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.609017739,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.589927092,,,,
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide2,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.777034672,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.166427777,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.108528093,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.103512163,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.093976935,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.088036797,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide3,0.08264975,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.070744236,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.067814527,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.067814527
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.774946022,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.540660312,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.512124035,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.457095762,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.437886598,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.413967338,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.356856301,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.353977191,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide12,0.301761572,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide18,0.268272292
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.77382961,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.215900123,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.707698499,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.155679648,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.116683468,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.098816683,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.097718003,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.111855407,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide10,0.951166041,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.106881792
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.772669959,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide9,0.640529316,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.538055833,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.693171994,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.547029182,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide13,0.496488912,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide16,0.424227491,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.754034929,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide19,0.360372283,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.310237309
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.770454259,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.505742394,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.394375784,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.305311193,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.271836788,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.221925407,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.220844768,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide3,0.161404097,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,0.160388869,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.146186201
cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide6,0.768455424,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.476140154,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.476140154,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide8,0.375822219,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.318531541,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.309038812,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide10,0.22490788,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.200324433,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.196509027,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.375493007
cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide4,0.767920778,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.315270842,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.285456347,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.252811969,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.236628739,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.220513452,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide1,0.217400383,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.154040373,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.129467123,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.115114618
cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide0,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide0,0.765780426,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.587710612,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.50899268,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.352544585,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide0,0.288594928,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide0,0.27949505,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.264813401,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.26050083,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.250771014,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.248506066
cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide9,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,0.763476753,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.763476753,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.368654297,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.334864115,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.328138266,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.326837434,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide5,0.273372505,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.271681795,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.252702104,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.252702104
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide2,0.761723446,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.459178154,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.204033289,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide3,0.693495213,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.171591655,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.20038428,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.13601732,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide3,0.134974573,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.115506856,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.112843395
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide0,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.760242801,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.118301501,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.103004951,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide6,0.080850755,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.074879862,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.073405986,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.07201181,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.070705568,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide15,0.068547246,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.054668488
cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.758754916,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.752623023,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.460555535,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.413249801,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.350780592,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.254798608,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.593412875,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.24988364,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.216306078,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.215982818
cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.757737208,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.531754414,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.22379388,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.186027873,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.714404943,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.180638273,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.129448953,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.123613352,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,0.115818064,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide2,0.33249974
cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.757291676,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.595747135,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.32306511,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.32306511,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.268417248,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide10,0.263785111,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.551328588,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.244218067,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,0.178054199,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide10,0.156646012
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide18,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.757249269,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.379082474,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.262934183,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.219660462,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide7,0.128361845,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.121929282,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.103159223,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.101252147,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide0,0.092219214,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.074163557
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide21,0.756915755,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide24,0.627489628,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.304065871,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.252685119,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.226237787,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.100517444,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.070069963,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide22,0.496802561,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.066597631,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.051698207
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide0,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.756654811,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.33698122,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide3,0.253327397,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide7,0.123512703,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.116589084,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide0,0.070585738,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.052302487,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide0,0.028901629,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.025614151,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide1,0.024564069
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide5,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.756251163,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.593671276,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide21,0.490362289,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide0,0.156925285,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide27,0.511215911,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide23,0.468408561,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.103003984,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide22,0.488918345,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide6,0.96108988,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide3,0.099131362
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide17,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.754007309,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.658596107,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.569363863,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.513425151,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.638763357,,,,,,,,,,
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide18,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.754007309,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.658596107,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.569363863,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.513425151,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.638763357,,,,,,,,,,
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide19,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.754007309,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.658596107,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.569363863,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.513425151,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.638763357,,,,,,,,,,
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide3,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.753500063,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.680243153,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.62432817,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.441521446,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.691279781,,,,,,,,,,
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.75254812,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.448815793,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide7,0.281566103,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.134376548,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide4,0.348489329,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.151339705,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.077058109,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,0.072701809,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide7,0.070666295,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide10,0.065241298
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide3,0.752281785,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.445104159,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.366784967,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.973197395,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide0,0.449257484,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.564360483,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide17,0.656176129,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide18,0.656176129,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide19,0.656176129,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide21,0.717665289
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.751436394,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.600975137,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide0,0.587701608,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.51403963,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide0,0.587701608,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide18,0.970322733,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide6,0.48770617,,,,,,
cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.749868629,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.728634357,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.655960664,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.471095506,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.427932944,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.418894153,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.416053309,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.416053309,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide1,0.391006433,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.37536923
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide6,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,0.749197763,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.589389336,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.351443974,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide4,0.064041468,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.749197763,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,0.749197763,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide11,0.032897412,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,0.031359136,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.020300639,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.312427778
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.748606535,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.564262706,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide4,0.294957893,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide20,0.241184976,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide34,0.493666151,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide15,0.990496749,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.294957893,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide21,0.235292255,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.405697808,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.252821198
cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide10,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide9,0.746035883,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.532986185,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.22629757,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.658222705,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.205067061,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.192613121,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide4,0.62965988,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.160532802,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.160532802,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide3,0.479071126
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.743367989,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.508896226,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide20,0.286236375,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide7,0.730625988,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide24,0.31292127,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide1,0.590992237,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide3,0.623804884,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide10,0.474441163,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide1,0.246468342,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.32777168
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide6,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.742646117,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.71590692,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.358269234,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide22,0.33321452,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.316030389,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.29490262,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.283413871,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.22141078,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide9,0.376717276,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide17,0.125118811
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide7,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.742646117,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.71590692,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.358269234,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide22,0.33321452,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.316030389,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.29490262,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.283413871,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.22141078,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide9,0.376717276,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide17,0.125118811
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.741979926,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.729309434,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.726379771,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.688758614,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide16,0.677765635,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.6753187,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,0.656591366,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.630605509,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.629761421,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.741979926
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.741979926,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.729309434,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.726379771,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.688758614,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide16,0.677765635,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.6753187,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,0.656591366,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.630605509,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.629761421,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.741979926
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.740297179,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.38751875,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide8,0.534483018,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide14,0.706538993,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide15,0.638998896,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.702146882,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide11,0.380000781,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.659694683,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide4,0.38751875,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide7,0.491744955
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide14,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.739690294,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide0,0.521508897,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide0,0.521508897,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide3,0.868534854,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.496798219,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide7,0.494349612,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide5,0.484841121,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide6,0.462764061,,,,
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide11,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.739093603,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide9,0.265716374,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide26,0.222037311,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide10,0.084418618,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.071578444,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.063149992,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide2,0.061858839,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide5,0.051164139,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.049080563,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.04866267
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide15,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.738128768,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.557913189,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.249391848,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.347576693,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide23,0.206911545,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.501371239,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide34,0.537976289,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide4,0.249391848,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide21,0.207210451,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.990094611
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide3,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide2,0.736811798,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.137086917,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide4,0.094753265,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.306039067,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.087719408,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.073530616,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.073096617,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide7,0.122373418,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.072587335,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.064196041
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.736748334,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.401294501,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide9,0.369346827,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.633905865,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.178443073,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide7,0.632196923,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.52297108,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.729490939,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide3,0.522824101,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide20,0.2315436
cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.7359262,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.638449668,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.572615928,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.563830653,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.514314995,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide6,0.454095276,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide1,0.452206975,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.442134276,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide1,0.413029243,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide1,0.406862837
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide6,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide0,0.7354358,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide10,0.04233301,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide9,0.11845042,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide1,0.014562295,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.013806776,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.011578597,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide6,0.945583137,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide4,0.226843928,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide5,0.01689979,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide3,0.143131862
cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.734519019,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.722912878,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.713815571,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.626080801,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide1,0.506629431,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.4821525,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.428181877,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.403091037,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.388180833,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.388180833
language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide6,0.734035002,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.169506618,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.723968379,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide7,0.118739027,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.078664373,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide0,0.075147064,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide8,0.155740026,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide15,0.055329117,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide7,0.123053076,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide4,0.042632777
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.730883978,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.440496625,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.395393242,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.287222327,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide7,0.419467367,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.379297616,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide10,0.208263629,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide10,0.208263629,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.173985831,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide4,0.128116381
cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.730040053,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.677743048,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.618148449,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.382391105,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.322246387,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide1,0.271241948,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.24817941,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.232665469,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.227756863,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.202002951
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide5,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.729776687,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.695496546,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide6,0.157115038,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.60437352,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.116464487,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide4,0.498451214,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.109549709,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.086438075,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,0.511121133,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.082910632
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.728605758,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.677592325,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.668853254,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.640581358,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide6,0.534958527,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide2,0.491328216,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.449403437,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.437134933,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.427493221,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.350933581
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.728555212,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.506037116,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide0,0.492889569,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.483670115,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide13,0.372573072,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.332897876,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.339450696,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.308666429,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.31653471,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.238751194
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,0.727817349,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.679229005,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide9,0.599683793,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide3,0.524991322,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,0.727817349,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.630455284,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.727817349,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.051251932,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.579669182,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide8,0.417537484
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide1,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.727751991,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide0,0.083720105,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide1,0.075518679,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide1,0.066622641,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide1,0.039604326,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.038619046,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.035877005,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.035726161,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.034191478,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.033574045
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.727024948,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide5,0.697977989,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.676945906,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.348168063,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.176385695,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.539064477,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide6,0.154262229,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide6,0.154262229,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide7,0.541400913,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide7,0.248804803
language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide6,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.726425173,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide12,0.171830829,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide4,0.474641221,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.065874073,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide0,0.06249139,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide0,0.056448463,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,0.053807968,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.05356233,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.102190288,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide6,0.034882692
cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide0,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide0,0.724589972,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide0,0.724589972,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide0,0.724589972,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide0,0.724589972,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide0,0.724589972,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide0,0.203786575,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide0,0.184603566,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide0,0.090205603,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.080173911,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.076452568
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.724227709,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.558230954,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.433175775,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide5,0.408403889,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.265810239,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.25301756,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide8,0.118948179,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide8,0.118948179,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide1,0.10309357,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide7,0.31669752
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.724032046,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.449068343,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.393829636,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.207823839,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide4,0.434440873,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.574775279,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide22,0.493640437,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide10,0.471093748,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide3,0.430157508,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide13,0.517070932
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide21,0.723755387,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.233346412,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.040177266,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.038891055,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide11,0.1228011,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.037873246,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.037630016,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide17,0.036032414,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.031581864,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide7,0.028556493
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide11,0.723632196,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide9,0.305617643,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide26,0.178973974,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.084394626,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.075985676,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.102026096,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.069269936,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide10,0.065701763,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.057882033,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.05757953
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide7,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.723043455,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.303880462,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.428388889,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.549007611,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.445689893,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide7,0.950960047,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide19,0.258923016,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.613664542,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,0.156106699,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide20,0.285909986
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide2,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide3,0.722543563,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,0.094697144,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide7,0.077910214,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.38110993,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.075193609,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide4,0.064464662,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide5,0.062318955,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.060969604,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,0.048046172,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.048001951
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.7219027,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.629849875,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.472599252,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.435697717,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide10,0.431187651,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide8,0.422152337,,,,,,,,
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide0,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.721754544,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide0,0.206961142,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.194852316,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide2,0.143770602,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.127407961,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.037371916,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide1,0.143770602,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.027391489,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide1,0.023322959,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide3,0.066587182
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide7,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.721035649,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide14,0.60339799,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide15,0.60339799,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide0,0.081600159,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.026980687,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide11,0.026372222,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.024529859,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide4,0.02295574,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide8,0.021414106,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.019408477
cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.72099445,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide10,0.587724988,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.448909288,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide8,0.332349018,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.229310901,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,0.540431861,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.39767986,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.202332949,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.18913186,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.162334594
cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide2,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.719134778,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.156268228,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.131078975,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.094797256,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.092291752,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.092291752,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.073618551,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.06794575,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide5,0.065754487,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.064960067
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide6,0.715872096,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.667217252,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.404288588,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide22,0.357290459,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.33595455,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.32844418,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide7,0.715872096,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.276149879,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.270271625,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide17,0.158672845
cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide0,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide0,0.714610875,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.603422867,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.388555304,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.386910994,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.232295079,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide0,0.216734009,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.211479473,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.209075209,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.206759873,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.206622815
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide0,0.713930497,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.188916191,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide5,0.685864437,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.187068087,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide4,0.647649424,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide2,0.482886174,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide1,0.58085999,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide15,0.112947354,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.100924802,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.084510233
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.713318789,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide5,0.671168554,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.610348431,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide0,0.409371478,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide0,0.369471263,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.239411001,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.647536697,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.521525418,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.228103378,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.197441853
cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide2,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide2,0.712246757,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,0.544566615,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.52976775,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,0.527837609,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.412842531,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.398868523,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.394440393,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.368658803,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.365450858,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,0.359132514
cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide2,0.711597287,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.294932466,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.192124275,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.18527825,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.128611738,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide14,0.126684791,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.124525655,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.403055855,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.121977816,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.121713635
cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.710021918,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide10,0.502586497,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,0.499067931,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,0.499067931,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.473913777,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,0.404256944,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.372294256,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.369842522,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.342654357,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.304760915
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide25,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.708390066,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.570418363,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.547834361,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide2,0.405251436,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide1,0.282651972,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide23,0.272600426,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.45529583,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.171247016,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide2,0.230631306,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.35052933
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide3,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.707326984,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.163743691,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.099985439,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.099985439,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.081383532,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.075826489,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.064909914,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide2,0.062171979,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.061329132,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide3,0.057007745
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.705797989,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.246915911,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.471579285,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.111502803,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.530036566,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,0.951197691,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide14,0.121136535,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.403629941,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.4440419,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide5,0.355599533
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide5,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide5,0.705607042,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide3,0.616606586,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide5,0.532778782,cluster-analysis##02_module-1##01_lesson-1-.txt##slide2,0.530854122,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide4,0.084867849,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide6,0.052162569,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.051147252,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.050861583,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.04766313,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.043346016
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.705485732,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.633006802,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.346631679,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide14,0.678994519,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,0.50670186,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.24116644,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide13,0.689019606,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide3,0.240975927,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,0.482788455,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide5,0.292889767
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide1,0.70155175,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide1,0.319604576,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide1,0.111420008,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide0,0.28869352,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide5,0.154423365,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.07146102,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide4,0.2291528,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide3,0.054507624,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.052252054,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide1,0.044718403
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide4,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.701187388,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.675733758,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.54186856,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.397853462,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.670401843,,,,,,,,,,
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide5,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.701187388,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.675733758,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.54186856,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.397853462,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.670401843,,,,,,,,,,
cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide1,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.700625313,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide0,0.658446611,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.251511296,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide4,0.15667932,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide9,0.117690927,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.114904205,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.11266187,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide8,0.091973976,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide5,0.084850854,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.083783507
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide1,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.699646416,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.528947824,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.461280203,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.276164441,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide0,0.224618672,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide1,0.204683773,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide0,0.197553009,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide5,0.427018455,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.144147042,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.144147042
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.699478767,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.572330388,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.521989604,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.447174556,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,0.390193693,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,0.385973445,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.362635112,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.359785827,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide12,0.352909723,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide5,0.321241582
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide0,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.698325164,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.382859937,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.382859937,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide1,0.315493912,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.219624711,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide26,0.139234662,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide0,0.137467275,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.092183124,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide0,0.315493912,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide1,0.237171737
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide2,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide4,0.697903599,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.123563924,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide1,0.086915875,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.051677555,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide1,0.037682841,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.027854205,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.023495875,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide9,0.022604042,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.0190075,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,0.020233478
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide5,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide5,0.697777168,cluster-analysis##02_module-1##01_lesson-1-.txt##slide2,0.672985643,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide3,0.618619214,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide5,0.541914051,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide1,0.146681205,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide4,0.13115182,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.128353761,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide1,0.067835746,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.060076501,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.045575756
cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide9,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide10,0.697138503,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.549979028,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.268421719,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.109933659,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.073050862,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.060563468,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.049673053,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,0.047054288,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.044596287,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide5,0.043738619
cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.696908051,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.655051528,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.552210149,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.535452062,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.435853499,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.400625848,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide1,0.334058101,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.307165363,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.273439858,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.264788246
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide16,0.694944246,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide28,0.506843747,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide1,0.353306029,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide1,0.273799469,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.576430558,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide24,0.137093835,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide17,0.527137804,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.128317699,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide24,0.503171808,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.558837614
cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,0.694017277,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.661865472,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.627078913,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.58479903,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide6,0.583332958,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.563005838,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.362256916,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.345965879,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide7,0.177593475,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.130401807
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide4,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.693514216,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide23,0.284102152,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.112419544,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide0,0.099532622,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide14,0.045904985,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.088922607,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide22,0.240972252,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide0,0.290916674,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide21,0.238826896,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.062476162
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide16,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.693406561,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide27,0.360841233,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.195092983,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.194676362,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.191403358,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,0.446137561,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.181562061,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.180779463,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.173848741,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.173848741
cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.691881751,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.589226296,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.469517491,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.42236497,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.414910486,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.390600568,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.386771654,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.368156866,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.347924458,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.300204923
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide4,0.688747222,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide2,0.045466368,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide5,0.503844958,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.043649847,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide7,0.042888285,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.040378516,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,0.039408199,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.035946568,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.035624089,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide8,0.035575112
cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.688733764,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.688733764,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.668540803,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide6,0.608131606,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide10,0.518654035,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.511437453,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.492775089,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.479870792,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.475796776,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.440019471
cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide4,0.688310411,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide1,0.354514405,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.35137836,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide5,0.654137314,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide10,0.279788737,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide7,0.192183683,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide9,0.614473825,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide4,0.245381246,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.098676595,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide8,0.444790922
cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide2,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.686561743,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.525220837,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.411303804,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.311354694,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.243540624,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,0.233849624,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.114705604,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.07351898,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide10,0.059053798,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.132269014
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide5,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.685365378,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.508782877,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.258995952,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.123911495,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide0,0.101383089,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.086689895,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.07975894,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.07975894,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide0,0.066811827,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide8,0.080366555
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.68500841,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.620040589,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.579579631,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide5,0.430438139,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.269265384,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide12,0.340541687,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.099918921,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide1,0.088117673,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide8,0.080961521,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide8,0.080961521
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide25,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.684038207,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.103925928,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.102144136,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.520564969,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide2,0.096697878,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide1,0.496034315,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.081207832,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.076530473,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.073601533,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide3,0.420055634
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide19,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide5,0.683140638,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide3,0.02716092,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide8,0.025207385,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide22,0.021464539,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide12,0.016735133,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide9,0.015647196,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide2,0.015078369,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide3,0.014796641,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide23,0.013839057,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide6,0.013083636
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.681756632,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.5918535,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.339266578,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide14,0.656595799,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.236958108,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,0.455875373,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide13,0.666141542,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide3,0.223302257,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,0.412226228,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide2,0.223302257
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide4,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide1,0.681133976,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide6,0.095516916,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.082855663,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.072994504,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.068566988,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,0.064860666,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide10,0.061686073,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.056804766,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide1,0.055553459,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide10,0.051736903
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide22,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.67951214,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.280251678,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.243245745,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.217807554,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.130463516,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide7,0.101693722,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.101193686,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.088851215,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide8,0.085964077,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.083771417
cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide5,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.678978654,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.617673113,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide4,0.511217314,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.255715623,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.226095278,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide4,0.216907137,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.209724664,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.196905735,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.375370277,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.174808909
cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide5,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.678930785,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.678930785,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.666263891,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.666263891,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.666263891,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.194985433,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.130529883,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide4,0.451164226,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.130529883,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide3,0.130424613
cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide0,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide0,0.678208147,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide0,0.346738037,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.287429381,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.277271993,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide0,0.238473865,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide1,0.220147004,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide0,0.217261679,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.210139441,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.17198634,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.162910561
cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide0,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide0,0.678062806,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.386692009,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.298993672,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide0,0.270624999,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide0,0.219326613,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.208916534,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.186775622,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide0,0.172405149,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.171369,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.1641718
language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.677332409,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.60446374,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.569663187,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.471105839,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide4,0.393596583,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.359366461,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.351783939,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.348693207,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide6,0.346728033,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.321382961
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide21,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.675546097,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide24,0.431915457,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.342513841,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.306834369,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.216730395,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.124403087,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.075897321,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide6,0.073869447,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.062632896,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.056255885
cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide1,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.674543602,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.638435092,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.378483766,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.378483766,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.373487359,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.369197858,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.360597011,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide1,0.35441159,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.341435174,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.302703627
cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.673966494,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide10,0.603635917,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.500989067,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.473725739,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.268926731,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide7,0.581916442,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.240477291,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.234458774,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.234251759,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide17,0.190939939
cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide7,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide4,0.672094668,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.441054726,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.178715157,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.177757373,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.13607929,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.116020648,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide1,0.096869719,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.096051172,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide6,0.084427344,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.076936489
cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide11,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.671247918,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide8,0.618839898,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.509733724,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.477204825,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.421386827,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide6,0.219387914,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide12,0.172489752,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide12,0.172489752,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide6,0.123566538,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide12,0.102990421
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide5,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.671126636,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.421530258,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.396451625,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.197134287,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.071623291,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.066102798,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.061049847,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide8,0.051479395,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide8,0.051479395,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.046488326
language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.671051642,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.133307935,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide4,0.449301373,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide3,0.125486483,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide9,0.116355086,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide0,0.102429892,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.072851159,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.042840266,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.064177669,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.063061327
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide6,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.670992128,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.625257646,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.488486639,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.405284765,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.530865993,,,,,,,,,,
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide4,0.670128694,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.38705765,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.970127606,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide5,0.670128694,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide18,0.60725535,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide18,0.866439964,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide3,0.660763615,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide19,0.60725535,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.466716607,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide17,0.60725535
cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide6,0.669606913,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide6,0.669606913,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide10,0.337275829,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide3,0.152422883,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide14,0.082318861,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide10,0.337275829,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide7,0.440848555,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide14,0.082318861,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide7,0.440848555,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide1,0.076990869
cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide4,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.665962354,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide10,0.614593945,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.181824807,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.05779769,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.056031242,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.056031242,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.050273711,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide5,0.041127877,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.038816234,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide8,0.03611546
cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide5,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide19,0.664379485,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide0,0.042204985,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide0,0.03446659,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide18,0.341260383,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.023749438,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide10,0.019520494,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.018477046,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide9,0.016140168,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.015431257,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide5,0.015057967
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.663695337,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.628004527,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.58184424,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.560911769,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.512596946,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide4,0.504953612,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.500364132,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.499492121,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide10,0.589079266,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide3,0.504953612
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide3,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.661359042,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.533215605,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.475566986,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.436052022,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.411569697,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.402037219,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.307080235,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.269487311,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.237565877,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide2,0.237340496
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.659833881,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,0.559290862,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide9,0.379513586,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.185024026,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.100845638,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.559290862,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,0.559290862,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.080133424,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.067980839,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,0.06250759
cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide2,0.659181849,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.612311969,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.561640223,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide10,0.351106329,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.303868984,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.245578052,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide6,0.611084526,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.154582298,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.1368923,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.603854147
cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide1,0.658922557,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide0,0.271048967,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide13,0.149223758,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,0.149223758,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.094455459,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.078090734,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide4,0.071557548,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide9,0.056495479,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide6,0.055906641,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide5,0.054962503
cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide5,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide11,0.658399154,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide11,0.658399154,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.061793463,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,0.061342767,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.053759886,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.047572064,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.045901748,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.038373054,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide2,0.03541186,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.033714294
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide0,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide0,0.658271781,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide0,0.460939541,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide0,0.424686382,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide4,0.043892776,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide2,0.028297236,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide13,0.013775305,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide15,0.013265208,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide0,0.012512941,cluster-analysis##02_module-1##01_lesson-1-.txt##slide2,0.011919418,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide3,0.011152146
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.655782551,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.62955817,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.302488386,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,0.547291601,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide14,0.601535685,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide24,0.21205824,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide13,0.611666545,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,0.523291353,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.208380116,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide2,0.206750753
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide26,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.655318861,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.542042252,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.529201568,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide2,0.359739209,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide1,0.255342312,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.483673791,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide20,0.241091447,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.171326024,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.258959248,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide17,0.459875532
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide2,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.655163965,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.218644251,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.20816656,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide4,0.0882775,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide0,0.084619691,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.076230826,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.042619956,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.959840969,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide4,0.053319813,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.065717585
cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.654898003,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.603104779,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide0,0.591919883,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide0,0.587273735,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.558186175,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide0,0.46448801,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide0,0.454258799,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide0,0.448276752,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.405165096,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.392500322
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide0,0.654293118,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.556344937,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.556344937,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide0,0.316943737,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.222674578,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.2175378,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide26,0.13436545,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.129112442,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.119851608,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide1,0.316943737
cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide2,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide2,0.654129592,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,0.572790037,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.433304702,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.427724462,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,0.411510409,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.409098025,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.382355236,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.347403288,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.316303552,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide1,0.305173083
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide16,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.653565354,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.642843839,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,0.625632686,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.623683304,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.611020064,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.555746946,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.53386934,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide14,0.974087718,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide13,0.580808426,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.623683304
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide13,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.652702379,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.396925928,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.334155109,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.991062431,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.182245711,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide4,0.395540228,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.530699353,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide10,0.427881373,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide16,0.640702973,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide3,0.384184963
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide10,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.64656168,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.0646064,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.06095452,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide11,0.485882344,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.05201029,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.043027664,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.043027664,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide13,0.36963021,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide4,0.030817575,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.028288311
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide3,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.645681606,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.24716988,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide8,0.20744325,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide7,0.194742888,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.181680906,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.173646668,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.159969395,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.147240385,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide2,0.140573008,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide8,0.122854068
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide2,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.644478441,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.372025391,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide2,0.320859245,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.28903802,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.287350446,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.278667933,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.389107846,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.183567988,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide2,0.18167062,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.15490039
language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide10,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.642728401,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.600848231,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide6,0.589826201,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.465232717,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.343386973,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.340351299,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.303884054,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.224569733,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.202476123,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.191404582
cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.642580402,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide1,0.608341977,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.600212313,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.444518151,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.444518151,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.433722441,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.41402639,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.366757533,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.290452468,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.290452468
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide34,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.642387681,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide8,0.532475125,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide11,0.499005193,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide14,0.610091113,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide15,0.607817758,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.597058669,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.579743319,,,,,,
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,0.64123322,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.634673515,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,0.599436416,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.5862032,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.564320347,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.550603237,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.548323713,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.573251421,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide10,0.987335005,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide3,0.593236096
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide11,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.639813347,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.244052374,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.108629049,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,0.474118341,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.107714225,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide16,0.093749446,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.10839925,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide13,0.617518782,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.17357671,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide1,0.106886863
cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide2,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.639279981,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide1,0.533042767,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.378738322,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.361405637,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.361323073,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide1,0.329043858,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,0.266102911,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.230473401,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.207446072,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide1,0.205463723
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.638872933,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide4,0.384828691,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.088731097,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide34,0.545412045,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.384828691,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.98925156,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.404117133,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide4,0.075697402,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide14,0.279526209,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide9,0.791912207
cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide8,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.638782216,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide3,0.041742522,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##02_4-6-cure-clustering-using-well-scattered-representatives_4.6._CURE_Clustering_Using_Well-Scattered_Representatives.txt##slide1,0.017703314,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.016794446,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.016588628,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide2,0.014775813,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide3,0.014047805,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,0.013993949,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.013821143,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide4,0.013659748
cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide5,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.63875127,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide10,0.236079374,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.058652511,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.051958191,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.049213956,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide4,0.032540152,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide5,0.031069033,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide4,0.027551111,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.022237105,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,0.021454463
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.638529729,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.346788276,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.298313705,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide13,0.56121843,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide2,0.104210762,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide16,0.526168419,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.527769411,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.967165478,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide14,0.27230656,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.339484356
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide5,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.638509481,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.553029321,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.317686829,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.217752823,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide14,0.613433474,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,0.410281605,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide13,0.623020879,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide3,0.199050799,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,0.362736816,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide2,0.199050799
cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.638367982,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide2,0.540563728,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide2,0.540563728,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,0.133938584,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide26,0.116448277,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.073864676,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.05449484,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,0.05449484,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide3,0.054867775,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide2,0.067429424
language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide0,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide2,0.637961255,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide2,0.637961255,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide8,0.106597214,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide0,0.064527794,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.044977776,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.034352917,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide4,0.03290772,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.031803605,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.030037756,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.029680247
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide1,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.637886787,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.167068637,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.120608267,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide1,0.074257984,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.051828505,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.045678093,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.042145224,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide15,0.041859878,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.040583724,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide7,0.04014403
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.637806481,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.123436599,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.572694232,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.122993625,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.094871981,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.082867284,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.079501411,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.113848954,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.148359368,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.120374512
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide4,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.637597363,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.039182131,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.03246868,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide6,0.032165566,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.031327527,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.030361057,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.026762321,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.021505082,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.019572874,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide15,0.019432535
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide14,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide7,0.637093664,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide0,0.453472223,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.095390004,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide0,0.083856664,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide4,0.063413168,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide3,0.027388171,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide1,0.016527746,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide2,0.015399826,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide0,0.01491088,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.010897769
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide15,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide7,0.637093664,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide0,0.453472223,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.095390004,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide0,0.083856664,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide4,0.063413168,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide3,0.027388171,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide1,0.016527746,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide2,0.015399826,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide0,0.01491088,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.010897769
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide25,0.637003857,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.47205941,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide23,0.465675223,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.288598842,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.250708428,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.186909615,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide26,0.593441028,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.186909615,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide27,0.55663043,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide20,0.465675223
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide24,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.636843177,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.634383429,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide2,0.51353224,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.440491997,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide1,0.364420302,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide21,0.316268566,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.436837477,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide2,0.303894377,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.37765004,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide23,0.316268566
cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.636089259,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.59821193,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,0.482942728,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.480096663,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.400485345,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide6,0.303169854,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.28389683,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.26316117,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide9,0.122628484,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide3,0.118813703
cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide6,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.635301661,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.543079096,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.390963841,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.33866313,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.29973258,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide6,0.211080392,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.102260505,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.087036082,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide10,0.066191348,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide3,0.052165148
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide26,0.635263595,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.098416101,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide3,0.536258358,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.089965813,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide24,0.616730786,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide8,0.086947222,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.085714204,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide25,0.517525169,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide1,0.260031759,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.064373413
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide26,0.632781561,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.222125244,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.134945457,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide24,0.586255292,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide3,0.456816315,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide1,0.118909693,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.102825018,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide25,0.50949938,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide8,0.10163974,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.094722279
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.632198798,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.232969529,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide25,0.158575499,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.604530525,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide5,0.556315838,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.552460226,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide20,0.267064458,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide6,0.531861027,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.158744223,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.44149001
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide6,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.631613318,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.264081676,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.581112642,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide16,0.105340883,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.100355616,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide13,0.733909644,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide14,0.125723036,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.161399033,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.530326865,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide1,0.125723036
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide14,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.631035927,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.388741269,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.990742063,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide4,0.064469337,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide34,0.544206141,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide4,0.388741269,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.451488625,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide9,0.867768922,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide1,0.259648353,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide4,0.064469337
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.629601826,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.180675207,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.155203189,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.14275536,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide6,0.262023899,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide8,0.067531648,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide7,0.149002284,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide2,0.118930338,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide3,0.064649449,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.060905753
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide5,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.627827723,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.075702056,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.050165694,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.037798474,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.037614914,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.037378185,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.035245199,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.034876162,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide15,0.034093373,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.033222274
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.627337028,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.386544807,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.075174348,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide34,0.530531409,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide4,0.386544807,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide14,0.98874828,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.42720131,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide4,0.102731563,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide2,0.271063089,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide9,0.847074398
cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide2,0.627095125,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,0.597436705,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.552548864,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.548972813,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.517835519,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide2,0.503177149,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.48264089,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.391349472,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.380199934,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,0.36376295
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.627036129,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.482134417,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.478195229,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.37956196,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide0,0.212384551,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide0,0.201167823,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide6,0.471972802,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.183393557,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.459096598,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide13,0.332467687
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide0,0.625694496,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide15,0.867442659,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide12,0.625694496,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,0.491715334,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide6,0.475648787,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide5,0.417272452,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide11,0.45777176,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide14,0.419746434,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide5,0.417272452,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide9,0.326466511
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide27,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide17,0.625589834,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.623287407,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.527813807,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide2,0.384668884,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.621970197,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide23,0.229745302,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide1,0.190922713,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.545493742,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.151267518,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.147136464
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.625243785,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide12,0.132346589,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.09107481,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide1,0.475061503,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.088603126,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide2,0.08792613,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.421394794,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.069084258,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.057450677,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.056222446
cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.623220036,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide0,0.479101503,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.381536342,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.378989319,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.364765327,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.354540884,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.315039525,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.313447784,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.301915734,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide0,0.270921904
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide9,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.623090491,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.377947314,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.122015002,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.092211649,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide8,0.727636435,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.330833688,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.727594104,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.274036594,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.092211649,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.121360238
cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.621978878,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide10,0.578225576,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.528143103,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.390143923,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.365305027,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.337316232,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.292265908,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.241813093,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.214259478,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.192622257
cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide18,0.621967973,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.263286078,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.244648954,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide13,0.527821663,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.182529061,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide22,0.528895741,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.129350579,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.11967225,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide23,0.505814725,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.119471811
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.621589216,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.114756885,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.601087578,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.075823309,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.073073047,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.342708485,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.509720865,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.951158658,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.562676523,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.104583348
cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide8,0.62149177,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide3,0.302094557,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.298652684,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.29511877,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.259674456,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.248965641,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide6,0.241706423,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide2,0.238173751,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.237568021,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.21081069
cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.620662456,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.513217559,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,0.490695014,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.414550534,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.364776438,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.305322206,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide2,0.244200272,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.204644212,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.189230641,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide7,0.188525487
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.620648498,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.242975658,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.240098422,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.240098422,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide5,0.237224435,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.534905984,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.223583355,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,0.534905984,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide11,0.532771597,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.190584859
cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide0,0.619908896,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.614306609,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.611216374,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide0,0.463113566,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide0,0.393027952,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.392625042,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.36696337,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.366643766,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.359460221,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.336429219
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide3,0.619362562,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.315383284,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.268558016,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide8,0.205426473,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide5,0.187366251,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.17856286,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.129216763,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.580701707,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.109310121,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide6,0.074738608
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.619252879,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide28,0.280335327,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide5,0.262457919,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide1,0.254866057,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide2,0.603740553,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.206045939,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.174256293,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.149591364,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide30,0.275336637,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide2,0.243551686
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide14,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide2,0.618549114,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide0,0.230190242,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,0.075260325,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.074060228,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.070867962,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.070151429,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.070151429,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.06661189,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.060916106,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.059753867
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide24,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.617273986,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.110190843,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.09550352,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide1,0.551327881,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.081117422,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.54210573,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.080537215,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.076104213,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,0.450347869,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.069130248
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide6,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.616485661,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.529140215,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.30727806,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.206895726,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide14,0.591906272,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide2,0.18774672,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,0.384516763,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide13,0.601400061,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,0.334372585,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide3,0.18774672
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide8,0.616430073,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.189330383,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.173380316,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.15995945,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.128101249,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.106968867,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.103775627,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.092112009,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.091186382,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.079463099
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide7,0.616246295,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide26,0.093402324,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.082379072,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.07950596,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.059812616,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.041520513,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide0,0.038280694,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide4,0.037827861,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.037366836,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide4,0.034945573
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide0,0.615837882,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.602396513,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide3,0.265855576,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.17728737,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.139462528,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.110781858,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide0,0.104199683,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide7,0.081635803,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide0,0.08083814,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.079609223
cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.615819619,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide3,0.509841352,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.342362098,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.27126581,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.169680459,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.469032441,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.134152936,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.134152936,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide5,0.134105658,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.133864957
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide5,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,0.615776846,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.434391577,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.222552723,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide4,0.060869389,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,0.615776846,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.615776846,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.058271634,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide5,0.037797143,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide11,0.025049897,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.015815785
cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.614506457,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.48752385,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.465928123,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.412174401,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide0,0.404066647,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.396204278,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,0.354454301,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide0,0.343397898,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.335843783,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.318915995
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide0,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide0,0.612780347,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide0,0.612780347,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.432976375,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.386401271,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.36681835,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.345212202,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.248615373,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,0.23566,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide0,0.921176823,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide10,0.321603947
cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide2,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.61237468,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide1,0.386598735,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide1,0.368474381,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide1,0.306227369,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.275993071,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.265083985,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.247488731,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.232367298,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.230326497,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.224077852
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide28,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.611859551,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.59767618,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.497685031,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide2,0.350919267,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide1,0.296095294,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide17,0.534620444,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide21,0.208622771,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.517148197,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.26234593,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide2,0.222703638
cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide4,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.611322087,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.580479943,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.492824056,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.451523429,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide5,0.333952403,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.16855623,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.158964476,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.061795432,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide4,0.040297688,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide9,0.227521362
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.611317967,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.371887237,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.294967704,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.092561779,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide7,0.181328297,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.067024071,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide9,0.109017914,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide5,0.043916624,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide13,0.041190789,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide1,0.102704221
cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide4,0.609898179,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.462975526,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.36613982,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.34051833,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.335267326,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.304199236,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.23656629,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide10,0.235541608,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,0.272211125,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.167788924
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.608744386,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide19,0.251805961,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.142255539,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.115494024,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.101750086,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide11,0.101124106,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.093496302,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide9,0.079950158,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide18,0.250839836,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide16,0.993690443
cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide6,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.608342031,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.34992578,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide3,0.149341349,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide3,0.149341349,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.141220036,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.112361903,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide2,0.111856625,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide2,0.111856625,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide2,0.111856625,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.111558731
cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide2,0.608099198,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.343488605,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.335325453,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.3319696,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.316667802,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.315267914,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.314068212,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.313701928,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.289927209,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.229106809
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide5,0.608011264,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.526984021,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide11,0.370992321,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide6,0.335356798,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide29,0.327380619,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide8,0.359879826,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide15,0.381242153,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide20,0.362144216,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide7,0.323889739,,
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide5,0.607571894,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.48477729,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.138251029,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.412374015,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.138251029,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide0,0.123817959,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide0,0.123805488,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide0,0.116853852,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide1,0.161211637,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide9,0.116022081
cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide3,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.607459274,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,0.266536542,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.210581348,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide3,0.584227448,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.177152984,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.252110695,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.172936733,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide1,0.159110149,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,0.159110149,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.15769504
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide15,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.605157579,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide19,0.250297053,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.144474676,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.114463619,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.10133851,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide10,0.099372802,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.091669331,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.082159832,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide10,0.079078746,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide18,0.249866887
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide16,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.605157579,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide19,0.250297053,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.144474676,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.114463619,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.10133851,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide10,0.099372802,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.091669331,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.082159832,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide10,0.079078746,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide18,0.249866887
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide2,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.604955514,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.239022006,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.239022006,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.23691379,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide5,0.23675787,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide11,0.520605298,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.232107506,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.518309494,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,0.518309494,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.194734359
cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.604579989,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide0,0.530273407,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.471409668,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.441306963,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.439916759,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.425918154,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide0,0.386745174,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.380191826,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,0.362603305,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide0,0.332607731
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide14,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.603691304,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.220433565,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.091673778,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.075565419,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.07163853,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.089424692,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide2,0.109967301,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.386816879,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.138953345,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide15,0.109967301
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide13,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.602775309,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.476688637,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide6,0.419160196,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.591737073,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide7,0.401668264,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.433355082,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide21,0.335406218,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.392767868,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide5,0.373773326,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.428830855
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide19,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.602264691,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.258428498,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.217401493,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.207951331,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide7,0.106228715,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.094437564,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.094319123,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.088481036,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide11,0.264344229,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide15,0.205387891
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide9,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.600838169,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.050975128,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.050975128,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.043885699,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide11,0.290649389,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide1,0.037693763,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.035485812,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide27,0.023160098,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide4,0.022910389,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.022631496
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide3,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide4,0.599710701,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide5,0.222876802,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.136214476,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide9,0.059037998,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.058317277,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide1,0.044406486,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide4,0.222876802,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.031549299,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.025041691,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.023389899
cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.599658098,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.577028072,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.520338935,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.427327819,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.375412506,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,0.30961899,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide6,0.2872526,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.212349563,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide12,0.119871188,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide12,0.119871188
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide17,0.598795483,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide18,0.099526913,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide13,0.055345017,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.032502481,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.019319213,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.018655577,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide1,0.016509331,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.014932021,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide3,0.013993566,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide22,0.013946886
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide7,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.596499957,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.108305192,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.086707461,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide11,0.536498423,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.061708372,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.053314569,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide13,0.444080011,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.053314569,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.034220536,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.032078185
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide13,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.596127481,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.207777322,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide25,0.147439508,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.569305738,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide5,0.521337933,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide20,0.291991984,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.515204292,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide6,0.497481886,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.149709925,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.400018105
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.596056006,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.566823709,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.563902203,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.555116162,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide6,0.554846792,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.553330965,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.52458016,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.497846803,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide4,0.554846792,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide3,0.554846792
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide5,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.596056006,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.566823709,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.563902203,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.555116162,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide6,0.554846792,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.553330965,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide4,0.52458016,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.497846803,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide4,0.554846792,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide3,0.554846792
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide1,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.592973573,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.280489299,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.167389531,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.099197262,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.123137793,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide13,0.568895591,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.143767038,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.114557945,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.167672045,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.099197262
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide27,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide5,0.592717461,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide31,0.403821185,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide15,0.408772743,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.591295105,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.2076874,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.147574519,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.358453664,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide10,0.362671988,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.202869086,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide18,0.30490324,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.14303586,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide4,0.173873427,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide19,0.229248941
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide0,0.590738188,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide5,0.011711689,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide9,0.08101088,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.01067336,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide6,0.010194243,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide1,0.942285623,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.024253911,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.099838385,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide8,0.433417238,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide4,0.010825809
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide0,0.590738188,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide5,0.011711689,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide9,0.08101088,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.01067336,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide6,0.010194243,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide1,0.942285623,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.024253911,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.099838385,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide8,0.433417238,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide4,0.010825809
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide0,0.590738188,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide5,0.011711689,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide9,0.08101088,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.01067336,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide6,0.010194243,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide1,0.942285623,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.024253911,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.099838385,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide8,0.433417238,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide4,0.010825809
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide0,0.590738188,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide5,0.011711689,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide9,0.08101088,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.01067336,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide6,0.010194243,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide1,0.942285623,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.024253911,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.099838385,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide8,0.433417238,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide4,0.010825809
cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide2,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.590043987,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.548680425,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide5,0.450443925,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.178011236,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,0.121595594,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide7,0.074746389,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.058469776,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide5,0.042132009,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.035953182,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.03453111
cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,0.589344131,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.374008139,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.374008139,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide4,0.272472961,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide6,0.2372957,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide6,0.129417799,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.103908443,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide8,0.204765704,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide5,0.224376465,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.240110305
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide8,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.58933302,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.054221858,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.054221858,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.049067294,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.042596292,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide11,0.289887841,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide1,0.041801561,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.028833035,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide29,0.026518667,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide4,0.024924363
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide14,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.588834414,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.390680543,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide7,0.075652626,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.062726921,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.050991454,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide22,0.740702067,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.307668369,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide12,0.054776238,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.806603551,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide8,0.07222476
cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.588697145,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.523552249,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide0,0.502010795,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide0,0.417112912,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide0,0.399608958,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.374978209,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.361591961,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.357931517,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.323449377,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide0,0.320750974
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide10,0.588644857,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.330434899,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.152552991,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide7,0.489097316,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.95106961,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.109619947,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide6,0.449025957,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.223403415,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide5,0.438230347,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide21,0.120138348
cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,0.588201711,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.480551999,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide6,0.134146427,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.104154644,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.098936716,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide3,0.377096582,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.096557968,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.066653286,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide7,0.093393757,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide2,0.054962773
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide3,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide4,0.586659843,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide1,0.166312797,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide3,0.272055305,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.058357676,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.053308599,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide6,0.042842185,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide1,0.03951853,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide2,0.037308701,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.036383721,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.078411589
cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide8,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.58653364,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.243908558,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.20302283,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.197082121,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.147514236,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.145215485,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.143312918,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.135192508,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.131223706,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.130223031
cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide4,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.585214749,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide10,0.159037479,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide9,0.482702076,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.108665797,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.08084054,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.071213616,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.070277748,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.066082587,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.065329973,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide6,0.278505585
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide3,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.583979754,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.373877248,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.246155773,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.55001095,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.401390183,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.500056509,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide3,0.95100964,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,0.17117482,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide19,0.207516197,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide11,0.217574586
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide18,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.58350571,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.311636089,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.233800756,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.935146471,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide4,0.448734262,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.127332942,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide16,0.375717929,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.222035805,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.103420903,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide10,0.317994717
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide13,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide14,0.581664838,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide5,0.059014172,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.061918193,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide9,0.388015108,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide7,0.035089075,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.384870629,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide7,0.328336533,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide10,0.431726126,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide12,0.338573245,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,0.046575472
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide3,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.581528047,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,0.180431506,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide9,0.113279638,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.070008907,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide2,0.055196129,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.0534373,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide2,0.045035983,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide14,0.122661756,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.049209775,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide3,0.950958523
cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.580500698,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.484598714,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.416804128,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.261888879,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.215334209,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.394793921,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide10,0.198943642,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.194392447,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.190405321,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide8,0.167881625
cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide5,0.5803858,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.347219815,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.310606654,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.301804529,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide6,0.293320926,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.255982289,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide4,0.249571692,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.280945221,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.219535086,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.187098288
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.578314656,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide10,0.464576721,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide10,0.464576721,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.559764349,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.220904787,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.194755553,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.182156286,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.091737572,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.07433231,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.065233262
cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.577098362,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,0.396157302,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide6,0.215447051,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.165511428,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide5,0.556958537,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide4,0.35697465,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.129223933,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.127570096,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.127570096,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide2,0.125775496
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide0,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide1,0.576546388,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.410124438,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.144280247,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide4,0.527927088,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.144280247,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide5,0.12042249,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide2,0.423623262,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.091475918,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.063893393,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide0,0.039039037
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide21,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.576251218,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.253019114,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.217160118,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.203058179,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.121881986,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide7,0.118670034,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.102659148,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.101997379,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.100441176,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide9,0.098041845
cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide0,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide0,0.576250022,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.538184421,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.476970044,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.338635906,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.283445659,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.264384655,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.263296126,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.261159046,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.253088742,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.242251943
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.576164776,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.482748008,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.411999293,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.372945567,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.163197592,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,0.412846005,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide4,0.363305422,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,0.16275012,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.477784971,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.268680038
cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide0,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide0,0.573613704,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide0,0.476769067,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.45306708,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.431095414,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.323227022,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.296855294,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide0,0.277034274,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.274072478,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.265955937,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.264576077
cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide6,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.57320461,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,0.362944944,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.331673233,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.324066264,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.310298696,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.304064063,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.237898635,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.215670856,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide9,0.14051803,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide10,0.10849805
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide25,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide24,0.573201322,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.309651965,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.292688614,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.280273319,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.176551277,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.114537643,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.074974442,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide11,0.067376491,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide5,0.059626438,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.058962145
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide24,0.571140259,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.51287407,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.340956755,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide21,0.328335668,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.18339835,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.15421821,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.127545569,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.129759412,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide25,0.515976253,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide12,0.104624754
cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.570508813,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.457258365,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.217384659,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.124305788,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide3,0.10459641,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.10262887,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.095371879,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.095371879,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.094508997,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.076385311
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.570373147,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.179463331,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.139279675,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.477460404,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide18,0.128280804,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide3,0.12161317,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.118931439,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.115403336,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.115403336,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.115403336
language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide7,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.569229406,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.393939005,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.360692459,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.32160336,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.321259712,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.206233652,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.166877613,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.144926906,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide18,0.121627037,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.092559086
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide6,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide4,0.568890042,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide10,0.526082911,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.489808771,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.348408478,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.334904911,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.293626809,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.279099778,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.279099778,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.482371016,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide0,0.230071968
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide15,0.568265514,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide11,0.419063464,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide34,0.960317357,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide8,0.560687065,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide11,0.45635847,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.546447872,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide14,0.499831628,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.489490678,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide5,0.479540208,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.441329566
cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide4,0.568216713,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.406455087,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.319902512,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.262736018,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide5,0.196703303,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.194304851,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide3,0.525702333,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.096460364,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide6,0.527944294,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide10,0.044878155
cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide5,0.567303279,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.397976771,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide4,0.315271419,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.308920241,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.290668002,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.355347761,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.253552084,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.244259085,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.216316811,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.21060249
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide13,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.566435815,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.126419451,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.108561965,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.091365424,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.082970627,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.068777968,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide4,0.049617055,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide4,0.049617055,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.048482329,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.038427048
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.566367763,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.540510468,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.387328359,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.359551465,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide11,0.356191852,,,,,,,,,,
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide11,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.566367763,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.540510468,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.387328359,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.359551465,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide11,0.356191852,,,,,,,,,,
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.565319553,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.522290858,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide3,0.52203791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.493824097,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.493025151,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.466624801,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.447237806,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.418987993,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.398102032,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide6,0.52203791
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide8,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.564989832,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.543066812,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.387957261,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.365527635,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide15,0.953814854,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.354008234,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide6,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.564966808,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.131890006,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.125175332,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide11,0.490442916,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.081268401,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide13,0.467064374,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.062308442,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.062308442,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.042734785,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide4,0.040792234
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.563638634,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.5423154,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.387624021,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.365536202,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide15,0.953656322,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.354632895,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.563267998,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide27,0.46671901,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.24532631,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.236146679,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.22003356,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.218976301,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.209330219,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.209330219,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.20930727,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.206735072
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.56155929,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide27,0.367636352,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide17,0.366402708,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide10,0.305570317,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.285961902,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide20,0.244608654,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.2096305,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.187830929,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.18228854,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.168603989
cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.561142176,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.344379954,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.290318384,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide7,0.2290366,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.18822804,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.156691529,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.13776909,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.235158836,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide6,0.387111558,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide3,0.217768512
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,0.560660162,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.131621646,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.087262318,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide1,0.12806893,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.06576713,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide6,0.056702035,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.046983609,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide4,0.044244796,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.043667191,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide8,0.093004625
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide14,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,0.560417325,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.53361181,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.437151078,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.435177308,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.43206675,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.394973932,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.385598697,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide16,0.97566359,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide15,0.434637343,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide17,0.820118688
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.560138946,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.369114532,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.331024229,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.328647466,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.317899165,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide6,0.315771185,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.305084879,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.310965518,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.259888469,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.289620227
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide4,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide3,0.56011167,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.139975246,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide2,0.085966779,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.116613592,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.079880138,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide8,0.073072449,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide2,0.125465305,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.067594721,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.060252698,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.056529385
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide4,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.557189633,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide5,0.412100632,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.107476211,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.090491596,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.086282684,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.079051294,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.2474568,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide7,0.055144346,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,0.048657147,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.041421117
cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide0,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.556003172,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.39285168,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.377517695,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide0,0.255637548,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.231102842,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide0,0.217401489,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.199760565,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide0,0.188625111,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide0,0.1720679,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide0,0.171866922
cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide10,0.555313486,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.167400716,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.10869644,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.085312715,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.076704513,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.068255759,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,0.059421686,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide4,0.05561681,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.054938123,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.053981553
cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.554972534,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.427988134,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.334143152,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.297932615,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide10,0.284918403,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.273432194,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.481188353,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.198869169,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide10,0.188670009,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.180540809
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.554971121,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,0.207037289,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide3,0.2013731,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.148542762,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide11,0.137547068,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.131231778,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide28,0.057613623,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide4,0.049800462,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide5,0.137547068,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide5,0.917106366
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.554846329,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.424570283,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.374308918,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.327133411,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.259683957,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.212913166,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.208092807,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.36804018,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,0.164772646,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.364241225
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide14,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.554511313,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.179837837,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.141550179,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.52811205,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide20,0.321174076,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide5,0.480297489,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.473637584,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide31,0.697226395,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide6,0.456859237,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide21,0.148233518
cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide4,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.554336594,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.194177737,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.139398413,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,0.134935218,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.11713412,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.088184252,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.082585566,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.078760862,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.068728184,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.067837617
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.55421604,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,0.319780268,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide9,0.204823119,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.120060424,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.115993767,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide3,0.096895019,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.095019407,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.089580259,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.086201561,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.083329655
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide24,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide25,0.554169117,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.525801045,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.35942284,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.211661852,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.095343347,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide24,0.477986795,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.075458446,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide26,0.481733332,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.056963545,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.035866206
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.553077519,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide5,0.259560806,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.134136835,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,0.553077519,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide5,0.087840563,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.084143485,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide11,0.550315422,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.101257349,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.063517931,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide0,0.059974944
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,0.552852355,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.485322657,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide5,0.343526972,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide5,0.200832904,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide2,0.468923932,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.136312541,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.128100679,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide3,0.333830055,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide2,0.249852686,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide4,0.200832904
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,0.552852355,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.485322657,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide5,0.343526972,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide5,0.200832904,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide2,0.468923932,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.136312541,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.128100679,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide3,0.333830055,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide2,0.249852686,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide4,0.200832904
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide17,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.552310147,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.145577908,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.124206647,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.121033669,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.078466859,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.077292997,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.077028994,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide4,0.061671364,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide4,0.061671364,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.058468615
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide4,0.551599577,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,0.536012642,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.495635817,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.386096841,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.371949808,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide1,0.944012284,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide3,0.520876881,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.408340465,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide5,0.551599577,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide10,0.408288443
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide4,0.551599577,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,0.536012642,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.495635817,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.386096841,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.371949808,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide1,0.944012284,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide3,0.520876881,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.408340465,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide5,0.551599577,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide10,0.408288443
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide6,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide4,0.551599577,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,0.536012642,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.495635817,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.386096841,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.371949808,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide1,0.944012284,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide3,0.520876881,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.408340465,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide5,0.551599577,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide10,0.408288443
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide1,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide4,0.551403669,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide1,0.227221469,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.100707407,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide1,0.054431573,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.043515498,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.041477436,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.040876532,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide9,0.035640953,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.028052482,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.027816044
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide5,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide0,0.550285004,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide1,0.095608978,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.042203759,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide1,0.506625159,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide8,0.041421451,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide2,0.468572575,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide4,0.048646018,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide3,0.468572575,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.037108463,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide6,0.035530682
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide11,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,0.549675495,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.482845411,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide5,0.336463919,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide1,0.199410081,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide2,0.470880552,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.137494423,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.128981927,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide3,0.323745344,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide2,0.242850158,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide4,0.308444754
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.548212323,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide27,0.396071924,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.258997429,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.236790374,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.216746116,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.211233525,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.210526006,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.203865235,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.201922755,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide10,0.200206411
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.547961054,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.121129393,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.07776929,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide5,0.077279555,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.076394758,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide6,0.059436437,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide12,0.057838003,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.048055599,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide17,0.14446363,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.044322295
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide5,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.547304243,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.137801536,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.125194426,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide11,0.475551176,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.067482539,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.051658129,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide13,0.417053228,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.051658129,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.040109538,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.032912014
cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide5,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.5471692,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.405831986,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide10,0.404780474,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.221178628,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.18950175,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.173295684,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.527169887,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide10,0.166974111,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.151669859,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.147793541
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide23,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.547025084,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.196939578,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.190369856,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.135948616,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.100282674,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.074361851,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.073775755,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.066001413,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide4,0.052042697,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide9,0.378535298
cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide5,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,0.54551025,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide6,0.18850002,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide4,0.167876711,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.150443389,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.150443389,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide6,0.123784876,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.112665119,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.093095235,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide8,0.189700791,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide2,0.071735685
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide7,0.545252614,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.524370044,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.515046178,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.433409031,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide22,0.259055729,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide24,0.258479399,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.216654956,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide6,0.545252614,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.174740719,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.158076126
cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.544569865,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.47818304,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.41810666,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.409381041,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.180724728,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.177444922,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.175979618,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide5,0.246235136,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.151395537,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide2,0.136742757
cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide2,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide5,0.544235308,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide2,0.320432937,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide4,0.136735896,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide3,0.446794164,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide4,0.027959862,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide6,0.027277955,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.023243161,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide7,0.128408474,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide9,0.023023925,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide4,0.213163544
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.543895457,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide4,0.101622323,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide13,0.347322104,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.988930355,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.091115131,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide3,0.366962988,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide15,0.284626698,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide6,0.387073307,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide5,0.101622323,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide14,0.25679854
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.543757069,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide5,0.47196996,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide4,0.206459055,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.132912688,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.129593498,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide7,0.094700159,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.209503348,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide12,0.079030856,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide1,0.090538515,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,0.050647076
cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.543542924,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.499764671,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.458416354,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.426122385,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.409344486,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide0,0.294460198,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.257295435,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.217447513,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide0,0.199661098,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.188758834
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.543542924,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.499764671,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.458416354,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.426122385,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.409344486,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide0,0.294460198,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.257295435,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.217447513,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide0,0.199661098,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.188758834
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,0.543107541,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.46183764,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.408915644,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.386396445,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.335327133,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.213350791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.445780571,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.184137959,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,0.163840098,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.375205627
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide17,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.542244627,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide1,0.104681643,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,0.098271136,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide12,0.067778667,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.049140527,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.040109367,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide5,0.038345437,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.030947528,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide18,0.030587733,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.029432945
cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide5,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide2,0.541973875,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide7,0.171312977,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide1,0.324057962,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide4,0.071668058,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide0,0.155545791,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide1,0.152763382,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.067317225,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.062351106,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.059300364,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.059123704
cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.541963383,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.296351072,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide4,0.237649076,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.20320103,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide4,0.456237673,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide4,0.202112885,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.176978991,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide6,0.154335346,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.147169195,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide8,0.128046822
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.541914143,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.51117633,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.444768456,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.402855329,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.366606878,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.36626339,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.329101662,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.322409796,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,0.321892945,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide12,0.278522432
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide1,0.541810254,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.532681457,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.310015977,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.267731283,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.469012934,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.218425258,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.332719491,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide4,0.354508082,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide7,0.313841352,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.430339013
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.540143464,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.49992096,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.423015467,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.379169032,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide4,0.353245858,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.348232359,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.374223697,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.395146104,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide10,0.497265111,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,0.980913947
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide12,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.539756042,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.400820209,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.316312786,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide4,0.282826634,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.250306266,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.473867265,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,0.222333394,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.221052188,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.255759527,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.975052961
cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide3,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide3,0.538575643,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,0.445384228,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.376660952,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.107239653,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.099904548,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.08276454,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide1,0.068252221,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide4,0.289047225,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide4,0.057432851,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.054761533
cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.538575172,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.487909133,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.460548327,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.439916952,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.389202301,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.349609453,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.34648428,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.319173295,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.304785542,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.291568808
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide9,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.537670543,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide7,0.298417609,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.29757127,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.218401821,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.195131014,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.372302027,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.145736678,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide25,0.11694141,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.092981235,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide17,0.090885641
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.537111572,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide5,0.408548791,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.18962141,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.173222057,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.17021839,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.359530863,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.085934294,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.359530863,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.166777608,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.071718228
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide3,0.536200039,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.357018035,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.270662739,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.235880349,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.222465183,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide2,0.21182836,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.205776246,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.182309492,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,0.460597867,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.167391474
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.535325479,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide7,0.0584138,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.312616942,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide7,0.975925475,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide12,0.498191631,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide12,0.047880151,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.482987111,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide9,0.514771947,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.029474442,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide14,0.429013413
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide2,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide14,0.535178426,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide11,0.056634757,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide10,0.053787508,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.044604492,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.042627438,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.041572443,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide7,0.041101162,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.04039861,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.038146173,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.036223312
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.534978203,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.46463509,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.293302905,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.286281782,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.209926277,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.201860674,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.427440649,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.286809484,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,0.223827029,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.490747391
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.534385034,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.496169073,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.286558044,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.234635519,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.224488427,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.174598992,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide1,0.131056332,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.080944634,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.377121298,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.123371587
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.534122368,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.463839061,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.460123736,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.412145989,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.242902641,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide1,0.172532376,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide26,0.159933576,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide18,0.307052671,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.228577442,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.236599636
cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide3,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.533226784,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.368868713,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.355757307,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.280617798,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.194481041,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide6,0.185333971,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.113231487,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.069578198,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide5,0.066008958,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide5,0.05816532
cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.532909061,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.199327085,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.17613288,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.140128738,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.136571054,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.43479265,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide5,0.114631954,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.112060649,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.092241961,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.084538009
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.532470356,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.406891753,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.402040804,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.382467925,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.364333743,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide18,0.225874134,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.278489357,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.375469094,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.239194966,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.510301824
cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide6,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.531940192,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.45312912,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,0.414476964,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.401402053,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.371138179,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.31970301,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.289620292,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.28312094,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.260909358,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.260909358
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.531673963,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.309657339,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.309657339,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.271093058,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide10,0.224901265,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.223104482,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.214275234,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.205263648,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.205263648,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.192472237
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.530742952,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide8,0.430344822,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide8,0.430344822,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide8,0.430344822,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.314872039,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.257210208,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.228180601,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.228180601,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.222026741,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide10,0.188824752
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide3,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.529808382,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide24,0.330603504,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide23,0.217365731,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.16878199,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide0,0.119362478,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide26,0.305808921,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.120597927,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide3,0.951159086,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.26213431,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide22,0.153522632
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide14,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide13,0.529268246,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.100257666,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide3,0.460699943,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.063654188,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide14,0.405284492,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.061777652,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.352297042,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,0.046120909,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide1,0.052328987,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,0.060659804
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide0,0.528459812,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.402351244,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.153925224,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.127518378,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.121806822,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.111077165,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.087074667,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.086368175,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.085623668,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.085350342
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide4,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.527816579,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.396058672,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide3,0.294499551,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.243609378,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.155075932,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,0.131526528,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.106763889,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.286032691,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.106763889,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.090806721
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide5,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.527816579,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.396058672,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide3,0.294499551,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.243609378,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.155075932,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,0.131526528,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.106763889,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.286032691,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.106763889,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.090806721
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide4,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.526007119,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.505362074,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.266104914,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.204569193,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide2,0.163707654,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide14,0.478862527,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,0.382947052,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide13,0.489852857,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide24,0.130201903,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,0.331003959
cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.525451781,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.250701534,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.208456478,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide1,0.342923095,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.189802819,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide2,0.187506468,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.176671403,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide4,0.284584466,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.126295923,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.109401125
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide2,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.524800406,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.410693772,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.26938198,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.163571972,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,0.133545706,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.109110836,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.109110836,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide3,0.102360094,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.08505969,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide0,0.07554028
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide21,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide5,0.524316929,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide15,0.368650942,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.329287446,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.226363484,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.216756629,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide14,0.152609403,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide6,0.480578306,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide14,0.368650942,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.063209154,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide11,0.031500892
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide0,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.523890406,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.249666106,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.24055654,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide0,0.192469175,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.174973393,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide0,0.146326345,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.081390616,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.070085896,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide4,0.068605009,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.058238452
cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.523266169,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.415255151,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.349177625,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.332435191,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide0,0.332253697,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.318210217,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.275064848,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide0,0.273167108,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.272806989,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.26836629
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide2,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.522971898,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.197839698,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.054555096,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide11,0.042136601,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide11,0.035153862,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide0,0.177167362,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide0,0.050889766,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide7,0.037320957,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.061326851,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide26,0.044662456
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide22,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide5,0.522662311,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide14,0.364354056,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.331391037,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.222736802,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.213061622,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide14,0.151264228,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide6,0.475521433,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide15,0.364354056,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.063012225,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide11,0.031718327
cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,0.522285878,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.450613845,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.409920681,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.355101146,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.350147648,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide0,0.327107614,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide0,0.321838202,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide0,0.317576234,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.314694384,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.305047638
cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide5,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.521773382,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.359947505,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide1,0.358722726,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,0.358722726,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.213120916,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide1,0.146905516,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide4,0.122630416,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.105851795,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.173008418,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.090601206
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide11,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.521588909,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.262360316,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.208293262,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide17,0.240679356,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.114556492,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide4,0.325900734,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide6,0.090683732,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide22,0.218946592,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide1,0.12521357,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.132875991
cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.520849552,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.443487603,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.321377981,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.19882875,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.195775171,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.17792451,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.169231312,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.150449797,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.147129465,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.214296881
cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide3,0.520070861,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.513322526,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,0.493033663,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.103149801,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.090383047,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide1,0.084868239,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,0.08364013,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.082575914,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide5,0.071424245,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.057974236
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide9,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.520014446,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.488767591,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.393787016,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide3,0.948317821,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.309014359,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.322855201,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide17,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.519907372,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide27,0.501604176,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.255797036,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.182511924,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.17301088,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide5,0.172618286,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.159136178,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.148548292,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.147395771,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide8,0.249274925
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide5,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.518477702,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.489649008,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.475127706,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.442574373,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.441828015,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,0.440404745,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.394791831,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.38522294,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.369006231,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.36666412
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide0,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,0.51822926,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.408130279,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.222583331,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.173815624,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide0,0.164333349,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide0,0.152373065,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.122628316,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide0,0.125101313,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.096176219,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.088554715
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide6,0.518187146,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide3,0.512010342,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.410348123,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide10,0.307455837,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.287600234,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.281689338,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide0,0.440383753,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide4,0.410348123,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.257619177,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.25690813
cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide6,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide7,0.516853683,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide6,0.489204242,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.032260556,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide2,0.027319585,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide6,0.026599116,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide5,0.024110913,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.024097069,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.023491876,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide0,0.0215812,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.021580803
cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide6,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide7,0.516853683,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide6,0.489204242,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.032260556,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide2,0.027319585,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide6,0.026599116,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide5,0.024110913,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.024097069,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.023491876,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide0,0.0215812,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.021580803
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide11,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.516521236,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.142789639,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide4,0.125442773,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.062944991,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.390714013,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.062877325,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide11,0.951175111,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.064808284,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.106942542,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide3,0.125442773
cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide0,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.516042465,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.516042465,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide0,0.439724565,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.303633531,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.270522955,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.252449595,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.242563841,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.220105152,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.215719259,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide0,0.212388872
cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide0,0.516042465,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.516042465,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide0,0.439724565,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.303633531,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.270522955,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.252449595,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.242563841,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.220105152,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.215719259,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide0,0.212388872
cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide0,0.516042465,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.516042465,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide0,0.439724565,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.303633531,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.270522955,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.252449595,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.242563841,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.220105152,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.215719259,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide0,0.212388872
cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide0,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide0,0.516042458,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide0,0.516042458,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide0,0.162847148,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide0,0.124146579,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide0,0.080302371,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide0,0.078551187,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide0,0.075943969,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide0,0.065883291,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.049046321,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide0,0.043488739
cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide0,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide0,0.516042458,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide0,0.516042458,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide0,0.162847148,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide0,0.124146579,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide0,0.080302371,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide0,0.078551187,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide0,0.075943969,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide0,0.065883291,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.049046321,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide0,0.043488739
cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide0,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide0,0.516042458,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide0,0.516042458,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide0,0.162847148,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide0,0.124146579,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide0,0.080302371,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide0,0.078551187,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide0,0.075943969,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide0,0.065883291,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.049046321,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide0,0.043488739
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide18,0.515384033,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.451943163,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.222338803,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide14,0.315465003,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide5,0.099156194,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide5,0.317800558,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.050958119,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide8,0.138966099,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.048612014,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide7,0.201993682
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide26,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide24,0.515238904,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.249036374,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.242812873,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.216838968,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.137579997,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.100320762,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide11,0.098867121,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.09049321,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide4,0.088370734,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide2,0.084292736
cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide4,0.51468751,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.30277916,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.221006204,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,0.209371264,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide5,0.148729999,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.144459378,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.141434146,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.11936301,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.089457749,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.313743535
cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide0,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide0,0.513373163,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide0,0.247236319,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide1,0.154024667,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.065137875,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.064105335,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,0.051561594,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide0,0.047658037,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide0,0.044174625,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide0,0.042715859,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.042704779
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide0,0.513013961,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.330614148,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.314294933,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.280990019,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.237210131,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.169540763,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide10,0.951206864,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.330975668,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide18,0.412190699,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide17,0.412190699
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.512968848,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.475783103,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.454872237,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.424646176,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.411078432,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.415747995,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide3,0.477437283,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide3,0.433243805,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,0.502131127,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.599841973
cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide0,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide0,0.511045644,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide0,0.462705351,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.274672985,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,0.208278623,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.186345632,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide0,0.177693167,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.173602907,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.165400473,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.164975802,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.154359888
cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide5,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.509566627,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.445778113,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.417450586,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.388417147,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide10,0.23100967,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.167785902,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.139174496,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.035060909,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide3,0.032071556,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.025494497
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide24,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide24,0.50951969,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.237490491,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.227338715,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.20282826,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.135739134,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide11,0.098581956,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.095630287,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.089630227,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide5,0.08649826,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide3,0.08302576
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide9,0.509344231,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.291762596,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide7,0.26309179,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.179592002,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.172630394,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide22,0.143604794,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.142395579,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.111744166,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide17,0.109531295,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.395549105
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.508244347,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.41916925,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.382424456,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.358348591,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.316506649,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.316506649,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide0,0.285354932,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.273966397,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.263937532,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.204351352
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide0,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide0,0.507921056,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide0,0.394774214,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide0,0.370492022,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.182648794,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide4,0.092796771,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.087848672,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.087799648,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.071646918,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide0,0.071628121,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide0,0.070350001
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide22,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide24,0.50715688,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.400771169,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.397441725,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.358693836,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.250278917,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.150279501,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.091962273,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide6,0.081488987,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.070738193,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.066040326
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide11,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide0,0.506581376,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.127537418,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.127537418,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.140737232,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide16,0.126777255,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide9,0.313198993,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.122603105,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.12142104,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide10,0.287447145,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide12,0.952090004
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide8,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.506436091,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,0.429497166,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.372847037,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide16,0.075403198,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide8,0.917862231,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide14,0.14086808,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.26474463,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide11,0.075403198,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.429497166,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,0.429497166
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide0,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide11,0.505366701,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.124501052,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide0,0.074586629,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide12,0.330638154,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide18,0.047189614,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.060021814,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.193304973,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide2,0.06565649,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,0.394480436,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide15,0.107372416
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide11,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide10,0.504375649,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.376816782,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide21,0.234457509,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide7,0.503707397,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.20920032,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.304404541,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide6,0.455789063,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.260518387,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide5,0.442032422,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.316848972
cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide4,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide5,0.50380348,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.16036414,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,0.123466906,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.104110953,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.103837131,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.084892416,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.081674566,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.081112799,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide8,0.063342481,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide8,0.063342481
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.503222564,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide18,0.306008188,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.135456006,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide7,0.10187791,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide15,0.498945996,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.071007055,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide16,0.498945996,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.068974137,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.05723285,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.055563867
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide1,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.503077823,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.16941524,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.149279903,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.147171081,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide0,0.137380542,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.107370458,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.106620567,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide0,0.275780571,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.102054148,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.09322668
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.502798205,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.368765196,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.292381881,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide3,0.266553184,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.201697882,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.173142302,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.169260121,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.125412663,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.124837879,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.119389752
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide23,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide5,0.502731336,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.323365753,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide15,0.229721362,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.146789181,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.132539318,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide14,0.087781722,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide6,0.479613066,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide16,0.046322112,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide11,0.031459625,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide2,0.156437654
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.502295585,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.388684501,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide9,0.301686712,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.249438048,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.154778896,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.138542128,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.120052652,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.071936438,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,0.388684501,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,0.388684501
cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide10,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.501076219,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.43290922,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.421394581,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.304969845,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.302331691,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.294696694,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide6,0.293054329,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.247238998,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.238170291,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.238170291
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.500400039,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.340754582,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.280897013,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.228927548,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.224739732,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,0.217445402,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide23,0.211771283,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide18,0.204917877,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide0,0.286788003,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide14,0.954328921
cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide6,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.500239471,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.497297497,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.493137268,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.407209464,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.341881796,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.318572657,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide7,0.316276667,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,0.300805581,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide3,0.284686646,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.259978068
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.500105988,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.419024004,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.405106072,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.386335462,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.382875032,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.343664474,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.317307636,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.309041704,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.262422861,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.261236141
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.499782695,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.473708814,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.408885364,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.406875369,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.320153616,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.222010863,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide10,0.173205002,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.148427371,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.148051811,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.107553631
cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide6,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.497315549,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.315252262,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.265609705,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide2,0.217815938,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.172777467,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.141659494,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.136268722,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.114800401,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide14,0.102007123,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide10,0.092681255
cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide2,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide3,0.496942214,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide3,0.415293199,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.375858946,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.247711417,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.210507941,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.206958891,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.206286055,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.189617587,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide2,0.158574176,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.142008013
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.494752861,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide6,0.482688317,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.285739769,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.225290293,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.4558832,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.337002322,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide18,0.959946283,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide14,0.224858876,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide10,0.327228379,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide11,0.291203871
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.494328863,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.396245196,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.342650133,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.336747587,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.336747587,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide0,0.305510855,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide0,0.264957093,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.264315095,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide0,0.208060867,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide0,0.206348153
cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide0,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide0,0.493351574,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.340793292,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide0,0.268248653,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,0.250255139,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide0,0.226361683,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.225090491,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.213081352,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.204135924,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.203042345,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.194127644
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.49134391,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.190431015,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.149037218,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.148079686,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.090439641,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide0,0.057831289,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide4,0.265499991,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.047498812,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.047498812,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide7,0.045384577
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.491084108,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.485196505,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide14,0.424401102,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.318741084,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide20,0.291549003,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide13,0.461173016,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.217927062,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,0.358307933,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide15,0.424401102,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide14,0.448620775
cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide7,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide6,0.488493552,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide6,0.488493552,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide3,0.090535854,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide2,0.071815041,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.0662614,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.065189419,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide3,0.060472941,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.056365536,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.055663326,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide2,0.054688887
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide18,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.488454679,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide7,0.332757963,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.151469414,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.018163232,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.017780273,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.017780273,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide6,0.251886303,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.017780273,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide10,0.016911655,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide17,0.993488252
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide19,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.488454679,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide7,0.332757963,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.151469414,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.018163232,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.017780273,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.017780273,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide6,0.251886303,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.017780273,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide10,0.016911655,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide17,0.993488252
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide20,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide5,0.487955492,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide15,0.458412313,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.293090046,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.279892262,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.215483203,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide14,0.171789757,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide14,0.458412313,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide6,0.469238288,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.084840719,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide11,0.035278121
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide1,0.487304114,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.296609745,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.149510847,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,0.134964971,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide2,0.131133918,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.129649724,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.129649724,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide10,0.103808121,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide0,0.103544951,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.096510281
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.487197414,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.302364121,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.294772458,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.210553567,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide7,0.463926403,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.35789863,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide3,0.41751522,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide6,0.177504395,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide1,0.152141248,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.951138225
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,0.485191571,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide0,0.453597524,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide0,0.441866896,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide0,0.425577129,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.280041029,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.22526175,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.208300476,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide1,0.425577129,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide1,0.401869119,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide1,0.394121476
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide19,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide2,0.485109632,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.181314891,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide11,0.116213528,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide12,0.100222622,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide0,0.372451117,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.06814417,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide9,0.059734471,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.052983864,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide1,0.338625447,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.043580664
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide0,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.484208788,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.429605557,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide0,0.291349502,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.18967936,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.18967936,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,0.124155203,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide0,0.117701189,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.111940895,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide0,0.111464388,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.082783182
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide1,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.484208788,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.429605557,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide0,0.291349502,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.18967936,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.18967936,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,0.124155203,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide0,0.117701189,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.111940895,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide0,0.111464388,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.082783182
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide10,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,0.483984431,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.375361962,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.268493464,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide3,0.076913701,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide14,0.21024369,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.018146328,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.483984431,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,0.483984431,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.017825361,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.017810872
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide4,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide3,0.483872617,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide2,0.204088082,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide4,0.118148698,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.112181571,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.110963849,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.09416408,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide3,0.051129182,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.050201461,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.041747464,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.041247229
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.482994701,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide6,0.087236419,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide15,0.268226502,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.987353338,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide3,0.325802498,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.075655115,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide13,0.315794807,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide7,0.355507361,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide3,0.087107695,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide14,0.229644361
cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide0,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide0,0.481920901,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.309734433,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.309719912,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.25422042,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide0,0.200973728,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.185473561,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.177815001,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.171792896,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.161368055,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.160928025
language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide4,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.480260418,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide0,0.154285862,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide0,0.141953068,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide6,0.468599541,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.068035235,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide8,0.040091191,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.037519199,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.037063529,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.03331959,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide1,0.029590299
cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.479576847,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide7,0.296940398,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide5,0.243803632,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.243228508,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.243228508,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide9,0.167938231,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.072999056,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.069327547,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.05003628,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.05003628
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.478575048,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.455088345,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.44407705,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.432878769,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide0,0.325074489,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.391054182,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.190529339,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.18313789,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.417077061,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.145162779
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.478575048,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.455088345,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.44407705,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.432878769,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide0,0.325074489,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.391054182,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.190529339,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.18313789,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.417077061,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.145162779
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide3,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.478575048,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.455088345,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.44407705,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.432878769,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide0,0.325074489,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.391054182,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.190529339,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.18313789,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.417077061,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.145162779
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide4,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.478575048,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.455088345,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.44407705,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.432878769,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide0,0.325074489,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.391054182,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.190529339,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.18313789,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.417077061,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.145162779
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.478575048,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.455088345,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.44407705,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.432878769,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide0,0.325074489,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.391054182,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.190529339,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.18313789,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.417077061,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.145162779
cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,0.478421402,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.393839068,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.323371014,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.27754172,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.225137425,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.210706261,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.199970677,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.192323335,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.182640157,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.153636779
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.47790834,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.39516865,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide12,0.382166107,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.268325797,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide4,0.228645173,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.214979125,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.276123299,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.24651496,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.255321772,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.219084649
cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide0,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide0,0.477426063,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide0,0.373831942,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.258124057,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.239548421,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide0,0.211913193,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.210392873,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide0,0.195599934,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide0,0.182283523,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.166745916,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.165527292
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide6,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.477141552,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.392846518,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.294931714,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.271831203,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.222275368,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.165323287,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.153257953,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.143082592,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.251727795,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.165643049
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.476948779,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.297634909,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.238595213,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.231913082,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.178548501,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.136907486,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.120624843,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.109734542,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide1,0.07425296,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.074043956
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide20,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.476141185,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.173277018,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.135171885,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide9,0.122343556,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.118857004,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide21,0.073527738,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.069192606,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide11,0.262793631,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide9,0.142032837,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide15,0.117177361
cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.475864155,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide2,0.158387962,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.083226993,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.074477931,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.074477931,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.072796016,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.071673418,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide11,0.067388273,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.063745967,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.060123796
cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide0,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.473977883,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.472136674,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide0,0.448560458,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.411257556,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide0,0.295811136,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.292232128,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.280508318,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.271688716,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.269233823,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.239243322
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide4,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide6,0.472737294,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.104030566,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.101240593,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.090227633,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.083777152,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.073345303,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide7,0.236563451,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.071940575,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.071873547,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.067279394
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide17,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.471582288,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide7,0.343333957,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.160315341,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.018363196,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide10,0.017368555,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.017284206,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide2,0.252596355,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.017284206,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.017284206,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide6,0.260491201
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide7,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.471212214,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.302472076,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.247349483,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.166755037,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.071229775,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.07081668,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide18,0.059553978,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.055955782,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide9,0.049890836,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.048055644
cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.471084616,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide4,0.373321785,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.359651424,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.350290974,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide6,0.306282922,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.291885904,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,0.281496171,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.227322371,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.196704997,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.196704997
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide5,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,0.471033943,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.076769837,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide9,0.063195111,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.048518001,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide10,0.048384797,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.03864359,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide8,0.038397325,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.036344016,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.031241562,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide25,0.030261815
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide16,0.470817569,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.353608039,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.143586603,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.127634752,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.120855455,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.11607982,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.328614105,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide28,0.114517365,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.105818485,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.24208935
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide23,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.470424428,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.088471192,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.088422954,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide1,0.424167318,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.071987735,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,0.071755425,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.430664039,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.067845057,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide0,0.067533837,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.062031923
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide15,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.47040032,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide14,0.061780497,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.030644568,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.027954107,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.026713627,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,0.026602515,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.026367296,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.025446401,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.025397189,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.024401819
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide1,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.470170476,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.371932458,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.163936496,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide1,0.102554442,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide2,0.095406645,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.09023497,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,0.082991942,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.079957857,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.077188079,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.071256422
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide6,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.469042525,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.158261636,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.157057328,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.155432016,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.1283532,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.106187879,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,0.300091618,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide6,0.951096786,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide17,0.228473021,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.138328119
cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.468701658,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.349015278,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.248019313,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide3,0.156901097,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.097338793,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.080229333,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.079661362,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide17,0.077491496,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.072406041,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide8,0.059660306
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.468457267,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.405991344,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.218090625,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,0.193871008,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide4,0.193330395,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.352317535,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.233972684,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.195305543,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.962885587,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide9,0.202479568
cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.468083773,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.367837754,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.309689199,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.308211717,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.228343167,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.196432208,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.194076016,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.185624548,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.17836487,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.164441177
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide14,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide14,0.467160179,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide5,0.072596521,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.038087415,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide12,0.418656882,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide7,0.021926896,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.033369304,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.3458163,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide13,0.95767219,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,0.049743693,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide9,0.3215856
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide24,0.465203129,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.358831755,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.127649815,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.092893402,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide26,0.380393619,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.091642911,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,0.089448322,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.087823977,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide24,0.069127929,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide3,0.233950769
language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide8,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.465170474,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.364374717,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.274865804,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.230170177,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.227868487,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.198590637,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.138995815,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.126621429,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide18,0.108806472,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,0.069510263
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.464917278,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.301180306,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.190880074,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide2,0.127088117,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.114839845,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.088540073,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.087476989,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.082736747,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.073469186,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide9,0.073227731
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide20,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.464749431,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.282912586,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide24,0.242375207,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.235824175,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide8,0.37056023,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.218721993,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.218721993,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.200420829,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide8,0.140487207,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide8,0.140487207
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide21,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.464749431,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.282912586,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide24,0.242375207,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.235824175,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide8,0.37056023,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.218721993,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.218721993,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.200420829,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide8,0.140487207,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide8,0.140487207
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide22,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.464749431,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.282912586,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide24,0.242375207,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.235824175,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide8,0.37056023,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.218721993,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.218721993,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.200420829,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide8,0.140487207,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide8,0.140487207
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide23,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.464749431,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.282912586,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide24,0.242375207,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.235824175,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide8,0.37056023,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.218721993,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.218721993,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.200420829,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide8,0.140487207,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide8,0.140487207
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide4,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.464003221,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.146882499,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,0.146608205,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.111317174,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide9,0.095457586,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.084997671,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.078636148,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.074058162,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.073920548,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.073187322
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide3,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide14,0.462977888,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide7,0.033829466,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide7,0.966528733,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide9,0.398516197,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,0.045575351,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide12,0.384056535,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide12,0.056580187,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide17,0.027140182,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.327636895,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.597387506
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide13,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.462906739,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.194727733,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.189524424,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.565505987,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.331346614,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide3,0.10318492,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide18,0.277720612,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.186232706,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.194727733,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide19,0.198182318
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.462072562,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.271783065,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.226783569,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.226751051,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide4,0.125561562,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.110079543,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.337492588,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide4,0.076347319,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide5,0.101833722,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.068048654
cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide6,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,0.461964392,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide10,0.243429823,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide1,0.063164859,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.046050299,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide9,0.033265459,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.031540087,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.031503356,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.029650621,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.028846034,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.205456619
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.461914086,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.372949939,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.372949939,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.224701261,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.204888115,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide0,0.20297014,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.193690569,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide0,0.189441877,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide0,0.172068302,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide0,0.143800152
cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide8,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide10,0.461060307,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.424377191,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.316313429,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.291767643,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.232620524,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.141929545,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.141929545,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.121928535,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.113286997,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.102333209
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide3,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide3,0.460555568,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide9,0.356994224,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.069240121,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.069143564,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.063927398,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide2,0.26773306,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.054849564,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.053108634,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide8,0.052252926,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide6,0.045812027
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide18,0.460527796,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide7,0.183215934,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide7,0.157295638,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide11,0.059773028,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide19,0.460527796,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide15,0.129446599,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.044397238,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide17,0.442307869,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.038531376,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.036492749
cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.460108321,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.459396968,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.295496715,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.220025574,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.300175582,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.174465144,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.169405841,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.137601677,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.134452842,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide6,0.339824725
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide11,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.45837851,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.306661552,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.087590389,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.057702658,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide9,0.036511615,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.030877074,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide3,0.042996752,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide10,0.022598487,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.022516858,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.01732805
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide10,0.458011287,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.401206749,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.315092669,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide10,0.311299539,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.310077952,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.308681711,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.279606364,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.278593649,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.277034569,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.263575854
language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.457054007,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,0.395430734,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.389846161,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide7,0.3691383,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.365778824,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.314191833,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.271643031,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide18,0.258879518,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide6,0.22582356,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.219350392
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.456593929,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide22,0.209523052,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.202244684,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide8,0.181128399,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide8,0.181128399,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide8,0.181128399,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide27,0.178734398,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.169285581,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.169285581,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.142245338
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide16,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.454806559,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.087160309,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.085841486,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.063389053,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.060090922,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.041186134,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.038922888,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.031730501,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.030380111,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide12,0.449291219
cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide1,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide3,0.454359225,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.125605696,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide4,0.444974924,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.103075964,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.093389069,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide10,0.080652968,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.065227343,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide5,0.325885732,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.064447111,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.062209463
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.453732524,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.308610255,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide12,0.273054153,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide12,0.280301964,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.247727067,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide13,0.254521782,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide4,0.130182978,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide18,0.213756691,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide14,0.240301005,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide9,0.202542214
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide2,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide24,0.453646723,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.371939899,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide19,0.279141038,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.217343712,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide1,0.196881044,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide1,0.10829719,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide27,0.370664117,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide2,0.951191411,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide18,0.131390183,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide25,0.369915532
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide2,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.453474606,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.033933266,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide9,0.040024031,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide6,0.026914618,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide2,0.02513168,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide3,0.024078312,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.023866512,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide12,0.022950922,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide3,0.026189155,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.025323561
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide5,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide1,0.452947182,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.141862455,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,0.080462069,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide6,0.076745634,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide1,0.071059378,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.064440095,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.062990997,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,0.06151583,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.055353314,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide1,0.050849883
language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide3,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide1,0.452697502,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.415265832,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.239535795,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.208206493,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide15,0.20651312,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.153519931,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.135180174,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.116471887,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide4,0.106040459,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.097882112
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide4,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.452370379,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.369149721,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.289336782,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.449611688,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.214888828,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide18,0.363473674,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide6,0.296607631,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.156668535,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide11,0.280986809,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.372461661
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide9,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.451514782,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.125297167,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide14,0.266638946,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide6,0.050564081,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide3,0.380554026,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.047655227,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,0.045137813,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide13,0.322422482,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide3,0.037500809,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide2,0.038428046
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.451399754,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,0.116684736,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide0,0.111001495,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide1,0.108045544,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.070238556,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.054531117,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide8,0.048756847,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.041195566,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide1,0.981650224,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide18,0.128472398
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide2,0.450911189,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.198255791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide4,0.188397585,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.142498039,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.142498039,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide3,0.450911189,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide4,0.127931692,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide26,0.103078777,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.096903074,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide17,0.095649819
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide2,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.450827764,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.176113382,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.141160186,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.135948169,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.08822007,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide0,0.056823297,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide4,0.247861467,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.045697791,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.045697791,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide7,0.042106351
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide24,0.450699779,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.133187851,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide3,0.409357881,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.117481307,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide1,0.11472103,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.078858973,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide26,0.441177549,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide1,0.072596909,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide8,0.071207597,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide25,0.388837019
cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide3,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide1,0.450690455,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide2,0.44571645,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide0,0.290575234,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide1,0.127531738,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide7,0.166517728,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide6,0.162555572,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.103237958,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.072539986,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,0.111630417,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide9,0.095896348
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide11,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,0.450533777,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.369251857,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.362475905,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.349370344,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.335167889,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.364000903,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide17,0.430050343,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide4,0.534691636,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.435783701,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide3,0.341984249
cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide4,0.449708076,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.223269153,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.184379752,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.150164812,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.148470008,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.148404477,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.12559128,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.115040917,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.113506626,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.261371409
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide16,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.449023973,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.294412966,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide12,0.259787802,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.126875283,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide11,0.293614745,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.263134416,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide13,0.243189255,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.126361907,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide18,0.212791302,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide15,0.232313536
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide1,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.44894374,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.208369516,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide3,0.160963181,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide8,0.10809226,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.101626773,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.07961822,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide0,0.077713326,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.055426901,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide2,0.050173889,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.049800234
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide9,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.448620751,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.164718832,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.449593414,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.08170238,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide16,0.073827372,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide13,0.566582763,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide2,0.084185654,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide6,0.061106762,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.483340688,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide1,0.084185654
cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide4,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide1,0.448296475,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide1,0.092218481,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide2,0.213655817,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide0,0.188227485,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.05793362,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.057129215,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.056622999,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.056468219,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide7,0.08725109,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.055039152
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide9,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.448173082,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.17520367,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide12,0.128845916,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide0,0.063918185,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.15571115,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide4,0.053782014,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.053153929,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.046910842,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide6,0.032750457,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide13,0.024638188
cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide4,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.447808743,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.199688933,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide1,0.415114482,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide4,0.416322445,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.189253442,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.175060257,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.169703312,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.424441013,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.154464889,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.139829441
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.44772025,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.403919072,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.222446302,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.197310128,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide11,0.113708553,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.078715075,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.068242737,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.067863307,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.06046736,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.059992864
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,0.447451437,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.328504206,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide9,0.307967917,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.278312516,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.190620409,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide4,0.18579871,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.173682654,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide0,0.135089292,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.447451437,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,0.447451437
cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.446945924,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,0.36325491,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.285106821,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide4,0.216277752,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.142022576,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.142022576,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,0.135195145,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide3,0.097761646,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide6,0.095598633,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.077671801
cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.446322515,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide6,0.243226982,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.213626372,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.188357802,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.171930452,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.171703176,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.167872738,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.136536503,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.120938407,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.380430695
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide21,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.446120573,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide7,0.446120573,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.212790852,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.212790852,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.122905725,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.099117565,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.446120573,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.055315574,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,0.446120573,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide3,0.040657308
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide16,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.444609822,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide7,0.36770681,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.165626607,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.01838859,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide10,0.017667605,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide2,0.280587858,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.016393266,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.016393266,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.016393266,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide6,0.280586797
cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.444590742,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.407749002,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide1,0.368489159,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.326998601,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.315682528,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.286688573,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide2,0.280644014,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.269688546,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.254265681,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.230606303
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.444434458,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.438342558,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.307051999,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide1,0.094318709,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.187486443,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.350924159,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,0.240909839,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.951179287,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,0.312502822,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide11,0.223517543
cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.443139864,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.306754602,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide7,0.236219474,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.153866076,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.153866076,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide6,0.118469764,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.115597865,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.095707291,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide5,0.093239709,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.091892014
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.442936885,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.375969096,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.293366799,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.152497462,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.144441143,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide17,0.24002001,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.240781851,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.951039153,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.219481052,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide10,0.205754903
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.442771266,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,0.123804787,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide1,0.117437567,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide0,0.105341738,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.071996319,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.058762997,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide8,0.054375133,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.045884601,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide2,0.986742184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide11,0.11883053
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide1,0.44259906,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.172829744,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide13,0.145454999,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide7,0.037521186,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.035807139,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.409323103,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.025962969,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide7,0.010939172,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide15,0.020638533,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide10,0.03805027
cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide9,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide4,0.442438157,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,0.401944402,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide11,0.124350748,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.081875711,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.036299713,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide7,0.033786676,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.033678474,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.033291709,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.033224222,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.033064787
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide16,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.440950792,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.279097588,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide8,0.125646161,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.115721835,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.085728924,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide10,0.677973567,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide12,0.071856308,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.259525856,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide7,0.107885102,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide8,0.386917366
language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide9,0.440070823,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide8,0.40149162,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.131318928,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.099009859,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide9,0.372100457,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide11,0.087032708,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.362612402,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.15677654,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide8,0.224010923,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.248721364
cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide0,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.439785045,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.439785045,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide0,0.439785045,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.293410045,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.282242415,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.268342772,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.267128124,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.245820454,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.238747794,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide0,0.221071626
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide5,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.439552666,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.147513606,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.122877058,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide4,0.188905916,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide5,0.951172369,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide1,0.094147357,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.295154853,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.095687533,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.438053599,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.238149704
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.43941254,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.378420819,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.319397229,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.272245274,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.238056189,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.22622262,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.185664884,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide4,0.345995326,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide15,0.271123471,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.371089537
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide14,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide20,0.438756072,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.408028527,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.276357145,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide14,0.241743027,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.232563532,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide6,0.088095875,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide2,0.050796694,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide21,0.387981119,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide1,0.037589981,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide16,0.227923281
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide15,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide20,0.438756072,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.408028527,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.276357145,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide14,0.241743027,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.232563532,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide6,0.088095875,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide2,0.050796694,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide21,0.387981119,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide1,0.037589981,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide16,0.227923281
cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide4,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide4,0.438613778,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.171115769,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide4,0.090001754,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.085861341,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.084976557,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.078732998,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.29188635,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.077784456,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.077030829,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide3,0.063192272
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide23,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide24,0.438439381,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.347828814,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.323558295,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.312832992,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.208306287,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.137102181,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.127941347,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide13,0.122572104,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.122379632,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.062450244
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide1,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide0,0.437101959,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.152596354,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.152596354,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide9,0.117174188,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.116027277,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.106820901,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.101810906,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.101810906,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.071903594,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide4,0.066298476
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.437087291,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.392215546,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.378968596,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.322578714,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.315468115,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide4,0.30428778,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.299675827,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.291819832,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,0.285071422,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.284853016
cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide0,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide0,0.434584343,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.383977249,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide0,0.373191888,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.347009344,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.321105469,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.318926536,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.31754509,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.311355952,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.266875507,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.252322661
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.433935882,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.093282614,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.043034211,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.042963613,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide1,0.041566474,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide1,0.038955191,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.038582027,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide1,0.038027473,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.03661619,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide3,0.036274595
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide2,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.433511873,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.274603649,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide0,0.259636478,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.113358181,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.113358181,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide28,0.091834948,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.087629145,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.067296393,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.067296393,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.065060505
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide26,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.433504107,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.433504107,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.246634455,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.246634455,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.144097834,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.09443842,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,0.433504107,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.063502774,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide7,0.433504107,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide3,0.032616025
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide22,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide7,0.43269357,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.43269357,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.211722223,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.211722223,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.125699688,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.09239075,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.43269357,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.055497956,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,0.43269357,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide3,0.043761324
cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide7,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide10,0.431851911,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.375832534,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.128985964,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide4,0.080221035,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide4,0.05429569,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.052776498,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.045782614,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.045358843,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.045090675,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.041092976
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide4,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.43031307,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.372236399,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.322181081,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.095952225,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide13,0.379245781,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide2,0.302079574,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.074785321,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.071412821,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide11,0.340570386,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.059920095
cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide3,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide4,0.429952851,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.070568564,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide8,0.065930532,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide5,0.369772398,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.061241102,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.052078457,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.04894628,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.048537017,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.048357091,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.045125924
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide1,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.426928103,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide1,0.088933545,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.025940265,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide8,0.019331272,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.015643638,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.015643638,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide2,0.01338684,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide0,0.012603496,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide1,0.012026155,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide1,0.011966998
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.426764483,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.242180379,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.108026655,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.107278333,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.090692381,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.071628684,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide1,0.134191326,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide7,0.059595147,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.054703462,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,0.054411602
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide25,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide7,0.426664798,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.426664798,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.233806731,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.233806731,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.134228498,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.085662827,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.426664798,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.060158915,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,0.426664798,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide3,0.031185689
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide16,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.426306925,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.251530543,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.235584847,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.231772165,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.231772165,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.185967892,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide3,0.182613132,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.176474414,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.174565602,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide8,0.349236489
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide17,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.426306925,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.251530543,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.235584847,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.231772165,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.231772165,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.185967892,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide3,0.182613132,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.176474414,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.174565602,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide8,0.349236489
cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.425731568,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide5,0.424713727,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.248321551,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.18064234,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.177970087,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.146872049,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.146872049,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.136571223,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.127423947,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.12296398
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide3,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.425443351,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.276175764,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.173350817,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.144428751,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,0.111559657,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.104046901,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.10046616,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide3,0.098719212,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.079891285,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.098200677
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.425363156,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.364627917,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.334410856,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.312362805,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.277752064,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.151837787,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.139170051,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.352184342,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.127500579,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.142202252
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide7,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.424684995,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.235239473,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.209161013,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide12,0.228359502,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.079969046,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide4,0.079788478,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.072996923,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.140757807,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide5,0.049673876,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.156601266
cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide6,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide8,0.423569638,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide6,0.205321087,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.056916188,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.02568456,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide7,0.396568707,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide11,0.022252132,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide10,0.020474052,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide10,0.019744102,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide9,0.342690971,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.018005913
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide11,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.423342783,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.260311293,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide6,0.124868947,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.095115921,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,0.084669952,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.079206649,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide22,0.070152138,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide26,0.066767374,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,0.076269383,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.197105112
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide24,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide7,0.422612809,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.422612809,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.227054139,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.227054139,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.134571843,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.111380137,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,0.422612809,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.059814803,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.422612809,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide3,0.036953337
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide19,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.421840828,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.31693351,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.30023185,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.174838899,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide24,0.155707025,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide8,0.343646951,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.146346508,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide10,0.144050111,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.135808161,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.135808161
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide16,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.421074233,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.215731621,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.166930916,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.148520892,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide6,0.107445733,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.331588272,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.100756878,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.089711136,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.086954874,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.226385393
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.420617538,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.247956156,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.24061836,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.192563283,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide10,0.137059833,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.12403191,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.112873493,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.240888559,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.109557438,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.223193728
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide7,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.41933946,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide7,0.060447572,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.215842053,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide14,0.233685524,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.602003765,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.387499691,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.157557128,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide11,0.11729113,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide6,0.059781779,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide12,0.292693642
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.418853821,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.244567347,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide7,0.215001321,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.169070664,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.116004805,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.098662699,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.095460901,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide22,0.074361362,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide24,0.055446021,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide17,0.054994741
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide6,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.418442749,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.216626265,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.150384465,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.144864876,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide9,0.141463167,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,0.107693207,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.066207674,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.04794078,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.046473478,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide1,0.050807532
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide0,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide0,0.417255134,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide0,0.41040638,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide0,0.32549555,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.100309685,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.08968777,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide4,0.05960341,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.056191898,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.051318532,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide4,0.040964721,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide0,0.028651678
cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide2,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.417114004,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.386294421,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide17,0.328248236,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.154673036,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.111083934,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide3,0.109926822,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.215837465,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide15,0.310799828,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,0.085054151,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide15,0.037493454
cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide2,0.417078805,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide8,0.302348797,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide1,0.140989319,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.132812768,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.111270416,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide3,0.295616296,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.073501093,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.064864196,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.064405622,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.064405622
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.417047195,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.093454771,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.076536253,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.396225333,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,0.951167518,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.263064277,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide15,0.065645616,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.352389781,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide13,0.377018072,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.304238824
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide0,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide0,0.416874628,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide1,0.096194797,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide0,0.062945908,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.052909043,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.052909043,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.042820131,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.041677079,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide3,0.037099916,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.036275877,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.02995776
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide6,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.416592773,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide7,0.059235202,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.202500652,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide14,0.205586578,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.629631448,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.379227411,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide11,0.099665719,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.139699604,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide6,0.058642513,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide12,0.281477477
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.416339223,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.296224542,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.159250029,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide1,0.153891949,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.120624639,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.113829953,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.103088616,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide17,0.241061461,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide11,0.159375127,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.951039146
cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide6,0.415714876,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.365430037,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.351888665,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.316706456,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,0.311047226,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.301783511,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.258341095,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.258341095,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.204362134,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.179924853
cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide9,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.415222919,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,0.271273627,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide6,0.210082037,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.180173994,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.172253915,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.169632075,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.168251849,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.156301338,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide12,0.148544182,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide12,0.148544182
cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.414976456,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.403101614,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide6,0.380636708,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.32582629,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.32582629,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.266565346,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.232366372,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.232366372,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.227836138,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.227836138
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide0,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.414926549,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.414926549,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.384224066,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide0,0.330709427,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.301758849,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.297518293,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.251636399,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.194598433,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.180466899,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.154579398
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.414034804,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.402961353,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.155702814,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.315246434,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide4,0.414034804,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,0.414034804,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.355820254,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.414034804,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide8,0.153737383,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide3,0.414034804
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide0,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide0,0.413517134,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide0,0.281952457,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide0,0.230339498,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide0,0.177836333,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.062585968,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide0,0.054770755,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.042558466,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide4,0.042320013,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.036888431,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.034795423
language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide3,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.413354846,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide8,0.249078921,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide3,0.150538072,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.147028881,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.11284681,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide12,0.087198617,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.061488975,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide11,0.058597902,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide20,0.049028396,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide9,0.083629619
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide7,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.411977138,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.173378867,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,0.108900946,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.086177919,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide9,0.08428721,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.078174911,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.074138699,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.069922073,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.063595206,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.058357144
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide12,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.409532898,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide14,0.351655624,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.135753938,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.06529017,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,0.060592102,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide3,0.357090053,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide4,0.051274006,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.045281886,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide13,0.280036726,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.050851747
cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.409485187,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.409485187,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.383933089,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide6,0.344836738,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,0.340563762,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.334007058,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.329208026,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide12,0.283963324,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.283022014,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.278159729
cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide0,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide0,0.409370069,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide19,0.061039421,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide0,0.046991898,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.035265668,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide0,0.029223726,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.028705329,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.02859986,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.028336822,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide0,0.027879098,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.027802761
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide15,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.408768668,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.107853001,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.053395266,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.038405434,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.04707825,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide9,0.02322064,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide11,0.023811651,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.04015009,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.055007391,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide6,0.034178723
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide16,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.408768668,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.107853001,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.053395266,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.038405434,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.04707825,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide9,0.02322064,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide11,0.023811651,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.04015009,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.055007391,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide6,0.034178723
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.408500664,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.302165552,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.166577045,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide1,0.934957696,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.282408403,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide14,0.371943867,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.286379024,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.166577045,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.361934517,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide7,0.161159839
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide4,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.408500664,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.302165552,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.166577045,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide1,0.934957696,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.282408403,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide14,0.371943867,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.286379024,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.166577045,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.361934517,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide7,0.161159839
language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide9,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.408316036,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.340510048,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.229167325,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.200842009,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.187657921,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.178169556,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.136908769,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.123517663,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide18,0.107322214,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,0.062132919
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.40829965,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide24,0.256930665,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.251412168,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,0.231598211,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.21457951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.104093876,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.094093318,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.34914736,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.95097493,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,0.208420392
cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide3,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide5,0.407511556,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide1,0.197789383,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide17,0.139354962,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide2,0.095213298,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide6,0.385663986,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.067326617,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide15,0.031249421,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.028376762,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide2,0.080323973,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.017655498
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide12,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,0.406450766,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.39571191,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.234061902,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide4,0.07038643,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.024503927,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.021287506,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,0.406450766,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.406450766,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.021287506,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.018929585
language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.405394861,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.339566703,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.214704407,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.204062568,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.204062568,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.204062568,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide17,0.105015275,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.101776157,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.084621755,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide16,0.079016891
cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide3,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide10,0.404944129,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.196866708,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.145826179,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.045032252,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide2,0.037941476,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.036525654,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.034200487,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.032852194,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide7,0.030148619,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.02903182
cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide7,0.403170733,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.393154396,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.393154396,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide7,0.297992658,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.24412111,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.24412111,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.24412111,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.098445199,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.098129926,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.076355015
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide15,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.402865065,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.271152367,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.232967684,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.232967684,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.232888668,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.221147955,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.206526358,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide8,0.384678148,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide3,0.178530439,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.170685443
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide2,0.40203921,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide6,0.145664258,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.058193215,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.053689499,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide7,0.053221545,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.096896256,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide7,0.052977134,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide3,0.099842135,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide2,0.052404867,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.052234992
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide14,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.401140285,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.328523649,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.319044787,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.286460034,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.221714722,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.212598528,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.212598528,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide3,0.185101969,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.16754911,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.331826228
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide1,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.4011103,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide5,0.129969275,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide2,0.109050736,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.08695463,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,0.081401738,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide2,0.993008327,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide11,0.227805906,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide18,0.284822968,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.226247602,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide9,0.067455596
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide1,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.401086334,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.098518699,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide1,0.054367772,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.053622354,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide1,0.047509904,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.033685779,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.031372676,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.028350853,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide5,0.023756655,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.03899338
cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide3,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.400361368,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.31031101,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide0,0.252055046,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.177741116,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.175503932,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.175503932,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.144381289,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.142401104,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.134999505,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.128724353
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide8,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide14,0.400081332,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide7,0.035970521,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.046437542,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide9,0.388026068,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide12,0.307112398,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide7,0.317596914,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.223775634,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.297360958,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.296627595,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.029245094
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide2,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.39893397,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide1,0.110976019,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,0.09588363,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide1,0.092795423,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.068614229,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide0,0.987070917,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.172318731,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide5,0.058669043,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide2,0.085061936,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide29,0.881032474
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide9,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.398507929,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.203074363,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide4,0.164349109,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide8,0.156146029,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide6,0.074523612,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.073800356,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide6,0.057630726,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.247649717,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide4,0.054054525,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide4,0.063860554
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.398337832,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide9,0.333413275,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.112910985,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide8,0.095254364,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.951177735,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,0.13965,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide2,0.081268067,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.138605489,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.192621841,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,0.150136015
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide22,0.397181908,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.288019167,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.288019167,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.267637942,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide17,0.223212356,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide8,0.196155676,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide8,0.196155676,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide8,0.196155676,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide10,0.142529669,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide21,0.397181908
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.397044687,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide12,0.35799992,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.233862712,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide13,0.269483988,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide5,0.227210055,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.148752963,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.067948815,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.067948815,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide8,0.120395286,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.233242012
cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide9,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.396355243,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.31030928,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.167141267,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide4,0.142376139,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.126883134,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.11676079,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.105204132,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.079973703,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide12,0.0657309,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide12,0.0657309
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.39552296,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.332065116,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.077469975,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide3,0.072386615,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide1,0.072227572,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.071233347,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide13,0.069423478,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide9,0.067676131,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.065742004,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.064702605
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide1,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.395389448,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,0.113983004,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide0,0.110966062,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide0,0.108280136,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.063701011,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide8,0.056947914,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.052295677,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.048906304,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide2,0.98586419,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide11,0.119469224
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide1,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.39506238,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.35523105,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide19,0.195617865,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.167290327,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide24,0.134427993,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.109459698,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide1,0.951113372,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide1,0.078286052,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide18,0.154358909,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide25,0.128890227
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide27,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide24,0.393038203,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.254487889,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.192432439,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.155224994,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.112805203,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide11,0.083274239,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.077172726,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.073874202,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide3,0.068414059,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide5,0.066741278
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide9,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide6,0.392911201,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide6,0.244570508,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.030113691,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide10,0.026393861,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide10,0.02222832,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.016089778,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide11,0.014918613,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.012897912,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide11,0.011823018,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide13,0.010808703
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide2,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.392696941,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide5,0.119504061,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide2,0.107621013,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.086816804,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,0.075869519,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.072132517,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide1,0.993003465,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.227368485,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide18,0.290939596,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,0.227368485
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide3,0.391985524,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.360366312,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.157852027,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.157852027,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.157852027,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.137753222,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.134146741,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.131938381,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.103377172,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.242474001
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide2,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.390436625,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.209882672,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.196756371,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.19097941,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.178446484,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.153549683,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.127628966,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.110972736,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.101828465,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,0.38116334
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide20,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.389751861,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.274833905,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide21,0.986963401,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.169665586,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide6,0.1781742,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.142438012,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide16,0.246215632,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.229176289,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide3,0.250325691,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.24516222
cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide7,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.389552762,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide6,0.307780813,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.286293028,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.278787474,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.244761585,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.237188157,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.222995294,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.213732613,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.197131668,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.18364321
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide22,0.388429671,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide24,0.353657333,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.29200102,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.21471107,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.178526193,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.108977562,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.085878968,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.078027087,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide27,0.324542523,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide3,0.074578959
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide8,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.387851834,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.367124225,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide1,0.315388631,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.297767813,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.296367082,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.292321246,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.27851993,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.299423246,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.967665722,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide3,0.334732692
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide0,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.38678087,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide0,0.358799202,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.174312635,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.158677613,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##09_6-8-relative-measures_6.8._Relative_Measures.txt##slide0,0.157197024,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide8,0.084923138,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.058641898,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,0.053095493,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.052808596,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.051508657
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide19,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.385425873,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.18301312,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide20,0.979563364,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.139712796,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide6,0.134636381,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide16,0.337656113,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.215960768,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide4,0.139153407,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.120906638,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.161540256
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide18,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide5,0.38523725,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.07351837,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide17,0.030431759,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide5,0.027550951,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide5,0.027550951,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide3,0.022924834,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide3,0.020592313,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide9,0.019949475,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide22,0.019614407,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide8,0.018478976
cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide0,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide0,0.384024591,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,0.367951107,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.340074115,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide0,0.226128796,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide0,0.198444061,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide0,0.197326466,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide0,0.161477622,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.135990899,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.134047553,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.130826737
cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide9,0.383339012,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide11,0.111068193,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.065129902,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide7,0.042548356,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide2,0.031028573,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.226722359,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide1,0.021989855,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide4,0.021293672,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.021278084,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.021178584
cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide4,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.38318016,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.368468398,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.335732309,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide5,0.213786807,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.147650985,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.143483934,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.129633867,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide2,0.129055767,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide2,0.100519015,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.081732274
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide15,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.38277682,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide2,0.258003713,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.019578971,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.117788025,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.016191647,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide20,0.016021999,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide16,0.989494704,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide6,0.019564036,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide6,0.200454607,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.017010945
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide17,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide2,0.382445501,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.205609949,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide3,0.20223784,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.191257596,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.123634221,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide15,0.087503314,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.037495179,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide4,0.025376749,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide1,0.055595325,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide3,0.024695973
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide7,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.381942213,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide2,0.106063734,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.104307289,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.094131723,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.094084718,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide19,0.089592453,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide13,0.119776774,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.067619742,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.06686903,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide21,0.070309538
cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide7,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.38170259,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.263590313,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.22335858,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide5,0.219574054,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.204970067,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.203659974,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.197276811,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.187344119,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.167326791,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.161553203
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide6,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide6,0.380706873,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide7,0.306322075,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.07408972,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide20,0.028378504,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.027139153,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide9,0.018022819,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide9,0.018022819,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.01796515,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.01796515,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.017630471
cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide2,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide8,0.380646135,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide1,0.214925233,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide3,0.285216202,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide5,0.238132752,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,0.108548585,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,0.102215404,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide2,0.253507549,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide9,0.203089076,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide1,0.122847638,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide6,0.107087391
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide2,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.380301306,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.100049844,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide1,0.050529911,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.042115637,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide1,0.032203523,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide1,0.031709891,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide1,0.02478726,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide5,0.021511256,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.019921908,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide5,0.028288506
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide1,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.379768466,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.163169346,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.051928958,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide2,0.045947731,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide3,0.233789457,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.042848511,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.042276893,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide9,0.149185241,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide1,0.041673886,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.040392952
cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.379603314,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.369437989,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.341421482,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.329523231,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.292444317,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide6,0.272990743,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.258947134,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,0.223871108,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide9,0.15970308,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide10,0.096602424
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide8,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide6,0.378917038,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide6,0.23360665,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.031778271,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide10,0.024501342,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide10,0.019785322,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.013973265,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide11,0.013874341,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.011598554,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide13,0.010965475,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide3,0.019190798
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide12,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.377805415,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.186547972,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.172090885,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.13306548,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide10,0.077087455,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.07361761,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.06798979,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.065162519,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.06395087,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide21,0.062001688
cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide0,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide0,0.37744487,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide0,0.254059234,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide0,0.094519928,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.060023365,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.05727994,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.055118722,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide0,0.05480944,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.053229831,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide19,0.05191865,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide0,0.051484743
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.376821347,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.305608994,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.218723066,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide13,0.116603209,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide2,0.152854775,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.376821347,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide3,0.376821347,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.98907004,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,0.376821347,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide7,0.167321873
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.376213389,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.202841975,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide5,0.189656953,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide3,0.115190066,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide28,0.101448829,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,0.091571363,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide6,0.048472976,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.046780697,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide3,0.241528977,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide13,0.063349328
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide11,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide16,0.376194428,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.280733825,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.190187549,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide3,0.069192492,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.015085085,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide14,0.173436788,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide4,0.014441442,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.376194428,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide4,0.014306554,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide15,0.376194428
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide28,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,0.374881425,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.374881425,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.15269769,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.114612742,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.114612742,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.101793419,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.047656045,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.374881425,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide3,0.036145116,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide7,0.374881425
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide13,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.374373336,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.355388725,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.270829952,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.270239905,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide0,0.173859581,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide4,0.239141337,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide13,0.95110461,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide15,0.186209609,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.33058335,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.265131563
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.374010689,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.286100756,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide5,0.197791232,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide13,0.101822021,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide5,0.093755372,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide1,0.092941101,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide3,0.074287658,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.147378406,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.069226987,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide5,0.327305278
cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide3,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.374005305,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.324901985,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.294871403,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.28212583,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.262775381,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.25096528,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide1,0.20108118,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.200344067,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.189042718,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.179495588
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide16,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide2,0.373254871,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.19635752,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide3,0.18505203,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.18032103,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.11825083,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide15,0.084717096,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.033900419,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide3,0.023909619,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide4,0.023208713,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide1,0.049535133
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide12,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.372844745,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide20,0.320744539,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.259308492,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.171733062,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide14,0.165523953,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide5,0.055857563,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide2,0.051380408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide22,0.046808053,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide16,0.24984947,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide1,0.041495605
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide10,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide6,0.371791691,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,0.272140536,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.070806002,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.057429257,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide1,0.04340565,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide6,0.084435169,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide3,0.20208909,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.038692834,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide8,0.258370613,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide7,0.116857648
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.371089814,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide7,0.293724854,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.224651975,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.279406794,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide9,0.136145345,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.120606731,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.119166596,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide7,0.109075675,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide15,0.099518835,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.090994875
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide8,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.370782512,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.208128703,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide3,0.198377691,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.082399671,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.082265561,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.067205472,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide1,0.065756421,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide19,0.064840905,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.308467719,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.061085196
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide4,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide0,0.369753381,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.170609527,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.138003633,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.138003633,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.103180775,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.092646297,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.079939708,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.075296735,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide5,0.070834193,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.069701237
cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.369686713,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide6,0.33244922,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.293663245,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide6,0.281489371,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.213576094,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.178333117,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.173309975,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.16659881,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.161933052,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.160871772
cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide4,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide3,0.369299454,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.254770172,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,0.175400795,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.17081417,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.24280742,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.143481512,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.135826526,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.109291083,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide2,0.275901073,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.108754394
cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide0,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide0,0.369274693,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,0.175052137,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.170649662,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.161527109,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide0,0.156403755,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide1,0.155828587,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide0,0.134888666,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.132297963,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.121515998,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.115329223
cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide3,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide2,0.368138937,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide7,0.037778681,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide18,0.0285972,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide9,0.025637496,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide2,0.019258275,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide1,0.019106697,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide8,0.019103098,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.01757275,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide2,0.017443544,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide9,0.017066539
cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.367996826,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.365904184,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.334925796,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.300953526,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.259181252,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.233912677,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.230552909,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.225264892,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.180719091,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.14438904
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide3,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.36776667,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.179121191,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide4,0.078470806,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.06839207,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide8,0.05747901,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide10,0.054381649,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.053140313,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.052678534,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.05219587,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.051749086
language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide5,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.36753555,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide4,0.196423054,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.131203987,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide11,0.089671249,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide9,0.218076172,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.051982489,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide8,0.100811489,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.044751238,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.028480436,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide5,0.150704957
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide0,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.367129266,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide0,0.33756156,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.298849568,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.254204559,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,0.21461026,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.197569087,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.193020183,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide14,0.257156269,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide3,0.155944643,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.153594111
language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.366931163,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide1,0.305048097,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide17,0.209874544,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.132401582,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide4,0.056724099,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide2,0.044901689,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.050603903,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.037128436,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide6,0.031913878,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide3,0.076946969
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide5,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.366293663,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.351788583,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide0,0.251427763,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.149862774,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.149862774,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.134597493,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide1,0.129205517,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.10833219,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.262847258,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.090812332
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide1,0.366161519,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.30056669,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.308439968,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide2,0.220591017,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.217247835,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.211505188,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.299020111,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.166743688,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide15,0.250818619,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.156790069
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide21,0.365626141,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.261417266,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide4,0.181628563,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide9,0.126025004,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,0.119859548,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide22,0.352166543,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.188561026,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide4,0.117413672,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.256774874,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.206977979
cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.365499695,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.332296001,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.309480322,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.263994993,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide8,0.260919143,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.210689474,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.116192799,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.105104694,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide3,0.101867897,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.088936923
cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide4,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,0.363795553,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.280128947,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.261570827,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide6,0.176588222,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.137860807,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.238737385,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.120571667,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.120571667,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide8,0.250507295,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.105461556
cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide5,0.363669623,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide5,0.363669623,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.134233992,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.085651164,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.073721523,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.055787788,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.055559992,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.055559992,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.055559992,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,0.050418952
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide8,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.363505479,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.363505479,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.331252225,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.331252225,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.179366308,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide2,0.100696643,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide13,0.054291395,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide2,0.053579286,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide13,0.053331458,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.048352677
cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide6,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide3,0.363322436,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.119855083,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide2,0.114604573,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide7,0.099051134,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,0.088658785,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.076469259,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide4,0.060495143,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide6,0.058181557,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.051825504,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.050371046
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide3,0.362668556,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.25788158,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide1,0.162945755,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.11441499,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide4,0.105306212,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.099426131,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.099128979,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.098899971,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide6,0.097751055,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.097187348
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide27,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.362590425,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide7,0.362590425,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.178665826,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.178665826,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.112944556,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.102006286,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide3,0.056122187,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.362590425,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.045905468,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,0.362590425
language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide8,0.362315965,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.224535626,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.181236396,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.175341782,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide3,0.14920334,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.14377979,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.125084851,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide9,0.153380857,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide12,0.124414,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.081653617
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.361931912,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.340704217,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.247594929,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.244486867,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide17,0.135234921,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.951112507,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.360251561,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide7,0.345574901,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide17,0.176346214,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,0.170844959
cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide0,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide0,0.361784412,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide0,0.25326802,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide0,0.150315284,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide0,0.082299433,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.074630411,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.074578775,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.070769459,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide0,0.067730034,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.067008873,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,0.064804281
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide3,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide3,0.361170865,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide9,0.294760679,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.12256655,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.061591538,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide4,0.056664529,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.049873871,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.046134461,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.043629446,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide2,0.043424841,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.038042181
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.36088108,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.297774963,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.253587537,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.197355349,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.171463544,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide2,0.141193074,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.14047542,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.134773419,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.297774963,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide3,0.297774963
cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide2,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.360672929,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.235587409,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.149212252,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide5,0.095872726,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.08582669,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide5,0.058355427,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide0,0.040941139,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.040058442,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.039199504,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide8,0.038121826
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide2,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.359586825,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.347751847,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.120354314,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.11460434,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide1,0.078845472,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.062105256,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.051112496,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide4,0.066921194,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,0.118274425,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.250841572
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide1,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.359490683,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.150831423,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.0695699,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide6,0.090733202,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide7,0.146180928,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.034557767,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.029404384,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide2,0.088672224,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.019261466,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide8,0.079450316
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide1,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.358642719,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.342587306,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide12,0.246357193,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide13,0.879313383,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide0,0.246357193,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.300489329,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.332493872,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide7,0.176175835,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide6,0.860354049,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide14,0.313529146
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide2,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.358642719,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.342587306,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide12,0.246357193,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide13,0.879313383,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide0,0.246357193,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.300489329,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.332493872,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide7,0.176175835,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide6,0.860354049,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide14,0.313529146
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide7,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.358642719,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.342587306,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide12,0.246357193,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide13,0.879313383,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide0,0.246357193,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.300489329,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.332493872,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide7,0.176175835,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide6,0.860354049,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide14,0.313529146
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide14,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.358642719,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.342587306,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide12,0.246357193,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide13,0.879313383,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide0,0.246357193,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.300489329,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.332493872,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide7,0.176175835,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide6,0.860354049,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide14,0.313529146
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide15,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.358642719,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.342587306,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide12,0.246357193,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide13,0.879313383,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide0,0.246357193,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.300489329,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.332493872,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide7,0.176175835,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide6,0.860354049,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide14,0.313529146
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide7,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.358105266,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.180251517,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.143253387,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide4,0.142379687,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.134861949,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.115516758,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.347410712,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.107527511,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.09727102,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.079177313
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide18,0.357484568,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.323717178,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.238152989,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.229705466,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.95117884,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide13,0.278126636,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide19,0.357484568,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,0.249861246,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.635405782,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide17,0.357484568
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide8,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide3,0.356640675,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.243913084,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.233045349,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.226169618,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide8,0.951218411,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.232717752,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.290295682,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.418195437,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide8,0.22818385,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.731166577
cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide8,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide2,0.356342186,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide3,0.045077645,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.044065863,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide4,0.043920505,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide1,0.037957409,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide3,0.120439684,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide3,0.031362828,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.026142848,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide0,0.026112923,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.026049179
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.356169084,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,0.225165137,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.221860385,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.168969578,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.129737106,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.116774982,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.108270891,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.098829994,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.079330892,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide4,0.072239559
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,0.355760415,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide0,0.331433655,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.300968923,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.272500062,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.272500062,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.244894997,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide2,0.222064579,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide1,0.205912492,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.188598437,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.164620077
cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide0,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide0,0.355162727,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide0,0.091584822,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide0,0.087088528,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide0,0.079131852,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.03578872,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.035717426,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.035283688,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.03435348,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.032839512,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide0,0.032663426
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide9,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide10,0.354672983,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.230057676,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.20457471,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.184983361,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.184983361,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide1,0.158353054,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.139512165,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.132143847,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.116970592,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide11,0.354672983
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide10,0.354672983,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.230057676,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.20457471,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.184983361,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.184983361,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide1,0.158353054,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.139512165,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.132143847,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.116970592,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide11,0.354672983
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide11,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide10,0.354672983,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.230057676,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.20457471,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.184983361,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.184983361,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide1,0.158353054,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.139512165,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.132143847,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.116970592,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide11,0.354672983
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide0,0.354354724,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.314952786,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.256613637,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.183213106,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.172375591,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.15678535,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide19,0.205385268,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.125679827,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.104113066,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide17,0.26987841
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide0,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.354017979,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide1,0.11520101,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide8,0.034070196,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.033660083,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide1,0.01967479,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide0,0.019398126,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.058337015,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.014861676,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.014861676,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide0,0.01485712
cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide6,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide12,0.353689592,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.331278535,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.304384657,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.231358978,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,0.231358978,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide10,0.2232448,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.213568281,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.213568281,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.179770189,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.140675208
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.353503997,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide12,0.178866704,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.096999683,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.08756437,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.332394552,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,0.085174207,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.083590117,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide17,0.332381118,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.951036968,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,0.283633099
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide9,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide20,0.351884292,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.235669318,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.226644529,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.226644529,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide8,0.16874129,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide8,0.16874129,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide8,0.16874129,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.156568672,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide10,0.129725921,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide22,0.351884292
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide14,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide6,0.351707569,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.258208804,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.140882129,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.233198074,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.2280783,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.157719756,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.244708184,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.046858447,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide6,0.042538372,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.041254213
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide7,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide16,0.351487017,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.186508382,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide13,0.020809909,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide23,0.018237427,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide6,0.020708773,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide17,0.329038584,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,0.019044437,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.038564202,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide18,0.319950549,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide4,0.971315158
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide13,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.351131733,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide20,0.297477835,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.241472301,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.154520173,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide14,0.154458011,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide2,0.057022164,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide6,0.05396793,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide22,0.046160646,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide16,0.233722338,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.039391589
cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide4,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide3,0.350950009,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide7,0.151492337,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide11,0.225933654,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,0.091187115,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide1,0.224991791,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.07795938,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.058354243,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide7,0.051162645,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.05023785,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.049475375
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide4,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.350927829,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.211182272,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.177749232,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.143357807,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide4,0.101482711,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.099960234,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.092234076,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide14,0.07132788,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.296361546,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.056669304
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide28,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide17,0.350698403,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide27,0.294461421,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.279271033,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide10,0.26211391,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide22,0.176153415,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.16965702,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.16965702,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide10,0.136121482,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.131823231,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.102433426
cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.34912941,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.278262566,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.19706171,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide3,0.185955533,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.160796324,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.156095153,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide3,0.155692285,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide3,0.155692285,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.150069379,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.146559583
cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide7,0.347653263,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.244427134,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.207703746,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.195723591,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.153231322,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.150528431,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.138975989,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide6,0.311205537,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.116438497,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.105739414
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide3,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.347315606,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide1,0.2002934,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.069417471,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide9,0.197073128,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.049672496,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.049128969,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide1,0.041203122,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.040591639,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.039935818,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.035120387
cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide5,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide3,0.346881577,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide2,0.123911515,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide7,0.080087864,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.052989623,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.051305042,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide4,0.042882534,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide9,0.039317121,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide10,0.0344322,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide1,0.082660006,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.030632654
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide12,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.346423475,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.339248858,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.338088095,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.33465985,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.274517605,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.265816939,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,0.264485789,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.261764785,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.254312344,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.251096622
cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide3,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,0.345589257,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide10,0.127943324,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.110555508,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.090814943,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide11,0.075588234,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.075543221,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide3,0.07179378,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.069176481,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.048113286,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.048113286
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide17,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.345579842,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.238300416,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.235780698,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.215783369,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.207578622,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.2845777,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.206127851,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide10,0.22281851,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.198806546,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.95425786
cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide12,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide6,0.344673373,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,0.287414802,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.287414802,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.271632605,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.261873242,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.190412453,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.190412453,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.155708229,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.149585027,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.1431615
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide3,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.344561389,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.125139298,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.055087327,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.367984592,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.058192371,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide16,0.057207168,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.093898271,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.055036523,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide3,0.951165632,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide14,0.065310119
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide9,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.343918415,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.308758128,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide2,0.114667654,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.349412948,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.230138751,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.332097009,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.308758128,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.230138751,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide1,0.11436778,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.323115717
cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide0,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.343245073,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.229393532,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.184221841,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.171319352,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.103719933,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide5,0.103424966,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide6,0.088614527,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.084506909,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,0.07466639,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,0.064554563
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide3,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.342636791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,0.174320431,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide3,0.151505097,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide4,0.096333196,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.095255008,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,0.086108747,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide28,0.085928865,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide9,0.056356491,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide3,0.937132829,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.052038521
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide13,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.341881332,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide23,0.12214902,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide6,0.099781068,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide10,0.055436121,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide9,0.080775459,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide13,0.077592195,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide6,0.082110507,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide26,0.089765495,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.068319457,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.16649772
cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide7,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.341465955,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.341465955,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide11,0.305666342,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide11,0.305666342,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.280652017,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.193785169,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.100827485,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.151313733,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.075716021,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.070330257
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide0,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide1,0.341302668,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.256343414,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.201206109,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.201206109,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.195020199,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.14279523,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide0,0.115131402,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.105852879,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.097964083,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide0,0.080436049
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide5,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.341072359,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,0.259211603,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.119658122,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide5,0.113293104,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.034273122,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.034273122,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.034273122,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,0.341072359,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide3,0.993758439,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.112568766
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide9,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide3,0.34096625,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide3,0.322458173,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.049852416,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.041497674,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide11,0.039466968,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide1,0.038223358,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide1,0.038223358,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide7,0.038026192,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide4,0.14850858,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.035312899
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide6,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide14,0.340505116,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.093358392,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.077663743,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.071959439,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,0.054887239,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide3,0.050914454,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.046740105,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide8,0.044218717,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,0.041811409,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.039689701
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.339363006,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.33154947,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.119064492,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.113629708,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide12,0.082348761,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.073646688,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.057802266,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,0.324585386,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.049727648,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide1,0.057703902
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.339363006,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.33154947,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.119064492,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.113629708,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide12,0.082348761,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.073646688,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.057802266,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,0.324585386,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.049727648,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide1,0.057703902
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide13,0.337424331,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.182541006,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,0.079673526,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide3,0.277841842,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide6,0.073990484,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.066219952,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide14,0.305143691,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.055371856,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,0.087968651,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.241307101
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.336636877,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.329860915,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.317935586,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide20,0.22258556,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.215655294,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide13,0.192924541,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,0.180179576,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.283818244,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.215690423,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.17536699
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.335400458,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.259369138,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.241916905,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.218303297,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,0.204866372,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.200604272,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide9,0.157253569,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.124211583,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.327461335,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide4,0.951024905
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide11,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.335102746,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.335102746,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.317441574,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.317441574,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide2,0.166472416,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.157749944,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.066189305,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide13,0.060120316,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.056669508,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.055234878
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide12,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide11,0.334718859,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.204100954,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.177792412,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.162784314,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.162784314,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide1,0.159274661,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.120442335,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.119411192,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide6,0.114720303,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide0,0.112864337
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide6,0.334613971,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,0.304236721,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.118767292,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.089293,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.082390502,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide6,0.104589757,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide3,0.208878026,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide1,0.051504785,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide8,0.288026237,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide18,0.031821865
language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide2,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide1,0.334270179,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.333223982,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.225789102,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.220243789,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide14,0.17727142,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.112048087,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.101399945,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide4,0.096020063,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.085346812,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.082641088
cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.333394228,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide6,0.320302157,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide9,0.319589299,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.305158169,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.305158169,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.300323048,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.295917454,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide6,0.294922788,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide12,0.279948371,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.279202376
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide3,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.332163673,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,0.258267665,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide5,0.116328117,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.112181041,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.032872942,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.032872942,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.032872942,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,0.332163673,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.031697706,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide5,0.993751684
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide20,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,0.331807713,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.331807713,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.139406577,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.139406577,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.085182078,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.08058141,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.042304517,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide3,0.034896925,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.331807713,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.048446068
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide21,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide13,0.331720651,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.209652885,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide20,0.986988823,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide6,0.156485451,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.139633554,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.320464431,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide16,0.201900009,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.144216361,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.104291719,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide11,0.23180891
cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide4,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.330649683,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.219868051,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide4,0.215024569,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.184359838,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.162987799,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.151591946,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.14786855,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.145602117,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.139217192,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.138256351
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide3,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.330429746,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.191267103,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.13559092,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.132793843,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.13247802,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.130872514,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.09640348,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.093146103,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.087596281,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.085436431
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide9,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.329735049,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.329735049,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.306978954,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.306978954,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.173870261,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide2,0.146098519,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide13,0.064004265,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.063046565,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.051971863,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide2,0.044961401
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide13,0.328261878,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.279854008,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.279854008,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide8,0.155358531,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide23,0.116533969,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide4,0.077966156,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.067554957,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide12,0.242964082,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.056164639,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.044829786
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.326703644,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.187654014,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.161512687,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.16018438,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.155884961,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.148306624,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.143753524,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.123401386,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.114454522,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.095484361
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide8,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.326241274,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide9,0.102154154,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.140914688,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.094594727,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.08552641,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.055642361,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.051821642,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide11,0.046829232,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide8,0.04611082,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide6,0.045497136
cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.325398269,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.254026491,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.24774445,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.20355912,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.190400516,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide5,0.190316938,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.174362648,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.162798573,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.153340846,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.152703457
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide7,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide3,0.324731871,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.198080207,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide1,0.307622203,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide17,0.077858724,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.041849196,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.040809273,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide23,0.02926202,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.051109546,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide7,0.942187063,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide7,0.110902999
cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.324654019,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.213215539,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.180890641,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide2,0.172931088,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.151674148,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.131184024,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.109820807,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.086749655,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.084846774,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.083664802
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide4,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.324447642,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide5,0.068360112,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide5,0.068360112,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide1,0.05204273,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide10,0.048156116,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.044551758,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide12,0.032325193,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide22,0.032131689,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.02601596,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide3,0.022325113
cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide0,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.324203467,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,0.270489673,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.219464009,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.214092721,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.184112086,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.171336555,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.16187899,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide0,0.13610146,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide0,0.132351968,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.131906264
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide18,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.323655287,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.261203964,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide8,0.08269987,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.072203488,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.065819411,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.627477537,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.299077306,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide12,0.049459865,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide8,0.341800515,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide7,0.065265081
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide8,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.32353259,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.169658121,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide12,0.147474454,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide2,0.093596373,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.052075451,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide0,0.041869331,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.032592009,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.032012391,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.031370289,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.145659981
cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.323274734,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.307870826,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.182647251,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.136143652,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide4,0.177594916,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide10,0.110455941,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.109986416,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.109686964,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.301560274,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,0.099677692
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide5,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.322853151,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.242119816,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide5,0.166491981,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide3,0.136380739,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.117433986,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide5,0.077393112,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide9,0.048499011,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide13,0.092317606,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide3,0.051170971,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.915835372
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide4,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide13,0.321858573,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.070995659,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.042394687,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.041668972,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.040167012,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.115771639,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide3,0.037471777,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide7,0.198784897,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.037331375,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.033800145
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide5,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide13,0.321858573,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.070995659,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.042394687,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.041668972,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.040167012,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.115771639,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide3,0.037471777,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide7,0.198784897,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.037331375,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.033800145
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide6,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide13,0.321858573,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.070995659,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.042394687,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.041668972,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.040167012,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.115771639,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide3,0.037471777,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide7,0.198784897,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.037331375,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.033800145
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide7,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide13,0.321858573,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.070995659,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.042394687,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.041668972,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.040167012,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.115771639,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide3,0.037471777,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide7,0.198784897,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.037331375,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.033800145
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide8,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide13,0.321858573,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.070995659,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.042394687,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.041668972,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.040167012,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.115771639,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide3,0.037471777,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide7,0.198784897,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.037331375,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.033800145
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide9,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide13,0.321858573,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.070995659,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.042394687,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.041668972,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.040167012,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.115771639,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide3,0.037471777,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide7,0.198784897,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.037331375,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.033800145
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide10,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide13,0.321858573,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.070995659,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.042394687,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.041668972,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.040167012,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.115771639,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide3,0.037471777,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide7,0.198784897,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.037331375,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.033800145
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide11,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide13,0.321858573,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.070995659,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.042394687,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.041668972,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.040167012,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.115771639,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide3,0.037471777,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide7,0.198784897,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.037331375,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.033800145
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide13,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide8,0.32137552,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.040905803,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.038825038,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide2,0.022532286,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide10,0.32137552,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide10,0.018132226,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide2,0.017305748,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide7,0.32137552,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide6,0.017168833,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.015418403
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide4,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.3211853,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide5,0.119209126,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.085801218,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.08030764,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.08030764,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.061676296,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide5,0.057546931,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.055015313,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.262456527,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide1,0.052915348
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.321159121,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.320107916,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.178046466,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.230235634,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide2,0.158327934,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide3,0.321159121,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide9,0.263042506,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide4,0.321159121,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide7,0.182327548,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.321159121
language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.320266054,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,0.287816104,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.127888767,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide6,0.100668689,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.093800804,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide6,0.070187203,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.064999053,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.063117868,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.058054809,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.051257215
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide13,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.320014484,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.120088244,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,0.343212061,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide17,0.082835954,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.065468202,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide13,0.951182378,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide16,0.071048699,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide4,0.051844067,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.583954504,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide3,0.599745515
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,0.31971659,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.253439644,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.192200409,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide1,0.182039144,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide4,0.247329224,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.116667168,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.103656482,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.080960981,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide2,0.074039257,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.070879898
cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide7,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.31966932,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.297430094,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.16378517,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.16378517,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.155418833,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.152141149,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.136349024,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.12713654,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.119376999,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,0.104058013
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide15,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide2,0.319484593,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.1645661,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.139310484,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.076065286,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide3,0.060389978,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.045161779,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,0.020510272,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide15,0.020159926,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.016111202,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide6,0.015728944
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.318982446,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.29036959,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.283845406,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.269952524,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.257815478,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.200959875,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.200959875,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.163882468,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide16,0.161846603,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.16135306
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide5,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide3,0.318637598,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.108932254,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.087950637,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.087218722,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide4,0.081607401,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide2,0.05432552,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.039847321,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.038848209,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide2,0.038533631,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide13,0.038295613
cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide2,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide2,0.318399781,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide2,0.17908427,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.171289496,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.156632715,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.150598843,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.122221837,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.105532687,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.105481198,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.099080897,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide10,0.096960928
cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide2,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide2,0.317886279,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.203236716,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.193589281,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.190074338,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.162422998,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.145243233,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.13493934,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.127446966,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.109458244,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide4,0.098252727
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide4,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.317859916,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,0.25605361,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide4,0.110307944,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.105335823,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.031454401,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.030126338,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.030126338,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,0.317859916,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.030126338,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide3,0.993734091
cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.31762754,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.31762754,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.234613734,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.224083682,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.177650595,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.175325771,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.158718113,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide0,0.120480715,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.098611245,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.095021386
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.317498824,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.278252842,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.222094025,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.196167816,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.184556252,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.153982807,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.128415926,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.122872576,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide2,0.106053258,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.105582272
cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide4,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.317438108,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.303092447,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.301971956,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.172063192,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.161543034,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.159101364,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.156081428,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.143302859,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.131916546,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.129575369
cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide6,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide4,0.317315225,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.204533742,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.197405241,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.173926847,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.152434443,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.144155704,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,0.181640602,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide7,0.13705465,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.134454226,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.126851143
cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide5,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.316804646,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide3,0.088305872,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide1,0.066856087,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide3,0.059228025,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide7,0.053421777,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide6,0.034249512,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.032950457,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide4,0.03205574,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.02976584,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,0.027068708
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide0,0.315582713,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.182878799,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.146438352,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.107197936,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.076588999,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide13,0.810139206,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide4,0.089260119,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.07808662,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.740642481,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.09816407
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide0,0.315582713,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.182878799,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.146438352,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.107197936,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.076588999,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide13,0.810139206,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide4,0.089260119,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.07808662,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.740642481,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.09816407
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide0,0.315582713,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.182878799,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.146438352,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.107197936,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.076588999,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide13,0.810139206,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide4,0.089260119,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.07808662,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.740642481,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.09816407
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide8,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide0,0.315582713,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.182878799,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.146438352,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.107197936,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.076588999,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide13,0.810139206,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide4,0.089260119,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.07808662,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.740642481,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.09816407
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide9,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide0,0.315582713,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.182878799,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.146438352,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.107197936,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.076588999,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide13,0.810139206,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide4,0.089260119,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.07808662,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.740642481,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.09816407
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide0,0.315582713,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.182878799,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.146438352,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.107197936,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.076588999,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide13,0.810139206,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide4,0.089260119,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.07808662,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.740642481,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.09816407
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide11,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide0,0.315582713,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.182878799,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.146438352,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.107197936,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.076588999,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide13,0.810139206,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide4,0.089260119,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.07808662,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.740642481,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.09816407
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide7,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide6,0.315572038,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide6,0.188524736,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.053119605,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide9,0.03203181,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.031792441,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide6,0.029960473,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.022540967,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide13,0.021528092,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide10,0.018525349,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.017952885
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.315222908,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.17179968,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide6,0.14660651,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.140564172,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.139683355,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,0.121855506,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide23,0.113998329,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide4,0.144352766,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.096061101,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide1,0.223814746
cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide1,0.314894056,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide1,0.292968714,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.139942347,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.120493972,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide2,0.1029714,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.171241661,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.068133762,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.061651792,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide3,0.124755647,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.060579083
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide3,0.314760229,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.28347078,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.209821065,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.188685826,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.174514896,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.141646578,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.111865959,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.080912936,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.079314863,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.230436923
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide13,0.314649552,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.157128513,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.11269309,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.083839454,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.083513116,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.079331534,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.077513106,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.294175717,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.127933562,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.951103383
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,0.314463208,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.247632418,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide1,0.247274569,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.177033338,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide2,0.146752646,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.146438502,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.136372614,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.104848152,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide7,0.117956388,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,0.089411853
cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide7,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide5,0.314241174,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.109690136,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.083781664,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.083781664,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide2,0.065091011,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.038032022,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.034033415,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.034033415,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.095154997,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.034033415
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide18,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.314189467,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide2,0.27818152,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.253368834,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.216943996,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.171392262,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide10,0.209868737,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.962497212,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.195948796,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.215014541,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide7,0.202759527
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide9,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.313954163,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide9,0.114119022,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.114035615,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.112219323,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.099498082,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.09119421,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.055982381,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,0.05464759,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide5,0.196557958,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide4,0.056827265
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.313854949,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.268309281,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,0.250578784,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide2,0.173189635,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.158667615,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.118588675,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide2,0.107890847,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.096465111,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.087732237,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,0.252424803
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.313365161,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.304946557,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.203202622,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide0,0.130125109,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.130311012,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.30174396,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide23,0.273668901,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide15,0.91673098,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide9,0.307249149,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.300186556
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.313365161,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.304946557,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.203202622,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide0,0.130125109,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.130311012,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.30174396,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide23,0.273668901,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide15,0.91673098,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide9,0.307249149,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.300186556
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide22,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.31309841,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.148949631,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide17,0.185729146,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide6,0.116969543,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.094391694,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide1,0.078216712,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.148590986,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.09764102,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide11,0.177608569,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.148955952
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide19,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.31124957,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.26976757,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide7,0.100800623,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.070459104,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.067300199,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.049005615,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.040174511,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.737771209,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide8,0.446214889,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.202801871
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide0,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide0,0.310660722,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide24,0.019836461,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.018885389,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide24,0.0169458,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide8,0.016713093,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.014740102,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide7,0.01325835,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.011981878,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.012162886,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide18,0.014474739
cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide5,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide7,0.310624379,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.16235841,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide3,0.065999826,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.060254937,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide11,0.049317127,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide1,0.044549235,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide6,0.032535104,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide5,0.031178198,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.030942435,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.030942435
cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.310235234,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide4,0.217407688,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.202492422,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide5,0.180345765,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.17896769,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide5,0.178880008,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.109416885,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide5,0.106626708,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.079163506,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide9,0.176095351
language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide8,0.30992778,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.288050948,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.266216748,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.244536683,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.192104504,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide3,0.165272354,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.162927781,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide9,0.161996323,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.161982213,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.143851274
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide10,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.308998627,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.303337749,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide2,0.104191354,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide6,0.095765634,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,0.213338974,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.3013523,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.379621694,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.308998627,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.213338974,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.213338974
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide11,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide13,0.30818943,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide4,0.166026308,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.095351407,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.089722757,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.072697028,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide6,0.044073529,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide10,0.042761319,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide12,0.035172641,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide10,0.092631666,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,0.992522127
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.308007399,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.302466319,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.284068358,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.249831104,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide4,0.144868881,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,0.125889061,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.105315549,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide12,0.141793752,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.103995788,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide3,0.094057509
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide13,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.307468041,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.268902499,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.250720865,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.246655639,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.206402756,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.206402756,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.15659678,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide10,0.137338873,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.2584965,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide3,0.135314523
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide8,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.305961818,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.159903671,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide1,0.070728073,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.06231878,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.053769959,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.05367849,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide5,0.053033335,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide3,0.052654301,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide13,0.048321883,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide2,0.047754063
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide9,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide11,0.305746708,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.194961464,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide19,0.105263736,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.122924901,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,0.824862171,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide12,0.19175034,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.120794597,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.194961464,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide15,0.109381749,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide15,0.845145381
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide13,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide11,0.305588989,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide4,0.104846898,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.088971796,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.084383806,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.078847044,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide6,0.043803702,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.03137076,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,0.272452579,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide24,0.030038652,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide10,0.083487563
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide13,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.305453896,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide9,0.263866775,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.232549197,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.167907975,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.154598407,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.154598407,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,0.128324502,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.123369653,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,0.118211553,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.11221215
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide10,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.304772636,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.078641346,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.071820605,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,0.466650672,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.109322559,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide12,0.066414086,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide1,0.065304141,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.662632049,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide4,0.093686483,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide15,0.065304141
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.303993797,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.247966061,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.189446664,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.178933741,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.174921713,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.158843558,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.151496907,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,0.151832163,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.289900222,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.950863982
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.303593288,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.294495865,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.231595847,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.192467889,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide9,0.170422778,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide3,0.134002572,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.244851752,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.104086707,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.094476527,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.153793356
cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide0,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide3,0.303164853,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.146569628,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide0,0.086498784,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide0,0.079316093,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.074386143,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide4,0.19800622,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide1,0.068403739,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide0,0.097941332,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.066700384,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide0,0.064353774
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide9,0.302929991,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.168638812,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.133666712,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.12437256,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide17,0.118489307,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide6,0.108079061,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.08559507,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.077138143,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide25,0.074959378,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.065138212
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide13,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.302902308,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.295922398,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.284008422,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.248721876,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.248721876,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.248721876,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.243781921,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.243752925,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.232552917,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide0,0.219269715
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide23,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.302353032,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide7,0.302353032,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.137087497,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.137087497,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.115935299,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide5,0.090652563,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.04254358,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide3,0.042090357,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.034522213,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,0.302353032
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.302023802,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.173421818,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.171724287,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.115475232,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide6,0.111783901,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.106647248,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide2,0.089585703,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide15,0.085472176,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.081451563,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.076146021
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide7,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,0.301554307,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.097327455,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.093603673,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.086748122,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.082754164,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.080323996,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide2,0.073321629,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.069640079,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.066108545,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.064007231
cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide5,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.301365614,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.280738131,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.244130503,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.198259963,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.128001269,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.128001269,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.125127423,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.125127423,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.104840215,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.098751731
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.30129488,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide1,0.177475315,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.14895782,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.142083068,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.120901151,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.095097515,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.089037813,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide4,0.075897296,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.073788766,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide0,0.104236603
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide10,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.301207931,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.301207931,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.282944314,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.282944314,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.159456507,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide2,0.159176545,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.06629547,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide13,0.064247051,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.051102446,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,0.042323728
cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.30066201,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.255475296,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide2,0.164685151,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide2,0.164685151,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.154704722,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.138062038,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.135218748,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.132385406,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.120614456,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.118390928
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide8,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide9,0.300467606,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide7,0.088069215,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide6,0.079247217,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide7,0.068169204,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide8,0.300467606,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide4,0.052166849,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide10,0.300467606,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.043351005,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide26,0.042866425,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide11,0.978711759
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.300153584,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.29583033,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.168370482,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.100690877,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide2,0.078439744,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide4,0.072498479,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.16733849,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide12,0.070041773,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,0.136342475,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.179162347
cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.298365908,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.265884343,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide5,0.254436949,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.227599685,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.21858763,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.187150108,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.16067813,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.154715768,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.15433786,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide4,0.153284303
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide6,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide8,0.297918467,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide6,0.204531642,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide2,0.054585994,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.025923328,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide10,0.02406789,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide10,0.020433607,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide11,0.020176997,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide7,0.244657642,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.017358122,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.015054412
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide17,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.297286674,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.099817904,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.083161812,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.072758123,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.070448202,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide6,0.058291511,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide8,0.050350941,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide5,0.047332069,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide22,0.039783064,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.038859707
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide0,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide0,0.297146865,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide0,0.141308496,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide0,0.098221731,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide1,0.054025823,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.050168333,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide0,0.0464624,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide0,0.0464624,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide0,0.0464624,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide0,0.041698296,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide3,0.038039013
cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide5,0.296628652,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.180052623,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.10055699,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide1,0.096650281,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.089274757,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.074545177,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide1,0.062383652,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.113070376,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.062247808,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.049141118
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide1,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide12,0.296450673,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.107576698,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide3,0.064666818,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide28,0.060996688,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.054520632,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.042510205,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide4,0.036380068,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.033784594,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.027556076,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.025955886
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide0,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide0,0.296009542,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.224296623,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.189454683,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide0,0.15193279,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.137138859,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.12492575,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.096455666,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.081042093,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide0,0.077849703,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.073102874
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide0,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.294493601,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.192216653,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.192216653,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide0,0.178125479,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.13781448,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.131035068,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide0,0.105342662,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.096002071,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide0,0.066531133,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.044687799
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide8,0.294212653,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide7,0.196279716,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide12,0.151761577,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.093025361,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide4,0.083743066,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide20,0.07245923,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide10,0.234651245,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.09189821,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide8,0.545802505,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide11,0.213865266
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide9,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide8,0.294212653,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide7,0.196279716,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide12,0.151761577,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.093025361,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide4,0.083743066,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide20,0.07245923,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide10,0.234651245,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.09189821,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide8,0.545802505,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide11,0.213865266
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide10,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide8,0.294212653,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide7,0.196279716,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide12,0.151761577,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.093025361,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide4,0.083743066,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide20,0.07245923,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide10,0.234651245,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.09189821,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide8,0.545802505,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide11,0.213865266
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide8,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.293439436,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.200451804,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.166034775,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.149879346,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.138107093,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.094844553,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.085049991,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide1,0.075882798,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.072527449,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.069010578
cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide7,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide5,0.29297017,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.229543776,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.052487532,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.052487532,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.049137718,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide9,0.046875213,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide8,0.036650485,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide7,0.028754348,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide1,0.025267067,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide7,0.024572761
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide4,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.292609175,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide7,0.039580194,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide14,0.148538548,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.705478598,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.178959968,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide18,0.027821099,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.266014808,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide11,0.032975594,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide14,0.278720453,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.705478598
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide9,0.291770292,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.02115158,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.021148117,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide11,0.019458951,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide23,0.019239449,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide13,0.019106225,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.018812301,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide12,0.018462859,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide19,0.017918007,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.017798707
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide0,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide4,0.291751549,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.05382619,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.041595455,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide7,0.025425144,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.021323629,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,0.020272238,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide15,0.015311527,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.013023071,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.012122804,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.011199821
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide16,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.290427345,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.271872197,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide0,0.203637419,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide13,0.843527741,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide12,0.203637419,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.243458929,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide11,0.1552945,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide7,0.146421694,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide5,0.824117471,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.270821956
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide7,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.290026198,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide4,0.284923766,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.227022004,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide1,0.141480489,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,0.092319084,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide0,0.055017501,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.041627623,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide1,0.198442402,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide2,0.041218831,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.036573979
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide12,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,0.289926091,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.125768484,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.107868859,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide8,0.102197822,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide8,0.102197822,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.091189965,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.081540733,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.081540733,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.079727707,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.054207968
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide4,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,0.287981056,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.146336759,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.116023935,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.113355204,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.106279839,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.090050601,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.086685025,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.086016579,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.080599719,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide2,0.071441225
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide3,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.287748892,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.149228812,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.126022507,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.123979213,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide0,0.122106787,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.109578612,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.106397869,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide8,0.103298932,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,0.07332117,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.072275852
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.287370695,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.133954582,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.079241318,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.075786346,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.070100548,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.068340936,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.064932858,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide4,0.064850245,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.063241849,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,0.062323248
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide10,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.287218898,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.162286356,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide0,0.135251473,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.083854761,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide2,0.07432628,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide20,0.332227311,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide1,0.07619199,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.068293041,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide1,0.084460655,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.190074447
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide20,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.286881873,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.281103388,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.101672638,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.096384504,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide8,0.071797116,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.690711241,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide8,0.580880572,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.226427383,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.149219547,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.071557837
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide13,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide1,0.286599545,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide0,0.018047842,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.011492065,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide3,0.010727019,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide2,0.010604997,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide10,0.010080179,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide7,0.010061946,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.009742978,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide7,0.009680562,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide11,0.009290204
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.285612536,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.268800481,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.223256351,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide7,0.210089909,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.206009966,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide15,0.190929199,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.176463993,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide1,0.151734606,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.125920554,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide7,0.119830938
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide6,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.28401083,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.084899535,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.082315214,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.055052699,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.052247518,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.046717268,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.044750803,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.042178405,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.040145487,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.040018612
cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide6,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.283647374,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.249748711,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide5,0.221420008,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.214371467,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.19293509,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.184201155,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.178296548,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.138940044,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.134659689,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.131158857
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide1,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.283591504,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.167910171,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide17,0.145498282,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide0,0.222656503,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide3,0.072556961,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.029312451,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide1,0.943798684,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide12,0.012318959,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide4,0.086851254,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.034212048
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide10,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide11,0.2833727,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.165216347,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.138839009,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide21,0.115958898,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.133981197,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide12,0.192494533,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,0.851356402,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.165216347,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.133981197,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide15,0.860405206
cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide5,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide7,0.282774125,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.201069017,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide3,0.1283206,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide3,0.1283206,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide5,0.109867705,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide10,0.100729758,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.063055635,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide14,0.057244812,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide14,0.057244812,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.042728865
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide0,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.282548157,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide0,0.227519611,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide0,0.040852923,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide0,0.040622196,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide0,0.040039623,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.038970984,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.036923616,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.036508743,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide0,0.036119835,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide0,0.036119835
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide10,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,0.281539312,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.052875976,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.049009186,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide4,0.047457007,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide2,0.043080949,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.040022638,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide14,0.039392699,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.039193061,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.036956839,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.031521647
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide7,0.281248187,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.231222451,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide10,0.186755778,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.144602803,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.135043532,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.273492147,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.090015696,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.082461629,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide4,0.139083212,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.089267608
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide14,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide14,0.281105,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide20,0.213027035,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.18685502,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.096359836,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide6,0.061386613,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.050469685,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.032215935,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.032215935,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide10,0.027808435,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide15,0.281105
cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide0,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide0,0.280380715,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.138612407,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide0,0.088301363,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.087696774,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.081950203,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide1,0.081248217,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide1,0.081248217,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.080301927,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide0,0.079699861,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide1,0.078411952
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide15,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide14,0.279517225,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide7,0.040955503,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.049831751,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.21723536,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide10,0.490090368,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.261155735,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.188050305,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.032051389,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.089723511,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide12,0.68196815
cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide10,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.279514556,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.279514556,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide6,0.246244423,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.231601563,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.218012034,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,0.218012034,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.174077483,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.168063439,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide12,0.14786478,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.132918612
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide10,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.27948726,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.144972116,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide4,0.072231748,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide0,0.070779202,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.113269419,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide10,0.951074428,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.044191516,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.072231748,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide0,0.070779202,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide34,0.167299629
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,0.279458841,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.258234252,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.140233426,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide4,0.132363742,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide9,0.184681259,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide10,0.094403296,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,0.115374677,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide2,0.12485233,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide11,0.169712196,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.118470673
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide4,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.279252249,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide7,0.26957716,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide2,0.160734023,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,0.112467516,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.263104642,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.078127461,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.07664624,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.073638517,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide0,0.062073317,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.058201876
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide28,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.279247359,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide13,0.226303577,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.207510351,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.174885597,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.128607812,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.128607812,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.08116862,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.060032884,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide2,0.058216132,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.058006424
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide1,0.278708347,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.261557379,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.233716341,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.190347134,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.16210692,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.134129474,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.127510867,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide6,0.194181343,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.112572203,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.111647493
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide11,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.278579139,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide4,0.143897051,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.085918791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.034973875,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.976359721,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.27465792,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide3,0.044938729,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide17,0.045985501,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.034973875,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide9,0.947795116
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide17,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide6,0.278217444,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide4,0.06614514,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide18,0.046876062,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.046654168,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.043605464,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.035909101,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.034668704,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide2,0.031300293,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide3,0.029807208,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide17,0.951146786
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.278095214,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.278095214,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.187534478,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.170284627,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.170284627,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide2,0.114918292,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.104099014,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,0.940752017,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.154715651,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.236609523
cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide3,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide10,0.277907097,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide10,0.277907097,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide6,0.142169478,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide5,0.124011638,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide5,0.124011638,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.106045797,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide28,0.029124316,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.081933791,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide1,0.093954662,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide14,0.072250417
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide17,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.276906511,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.240963022,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.068869296,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,0.060944514,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide24,0.057905104,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide3,0.039688265,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide22,0.512758576,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.806402982,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.149018529,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide16,0.058934099
cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide6,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide17,0.276853903,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide3,0.053968241,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.027268056,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.027268056,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide9,0.022070321,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.021036497,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide3,0.021016245,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide5,0.020540507,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide11,0.019493101,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.019295557
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide1,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.275799576,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.149350335,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.107456835,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.08025497,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.076805404,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.076240208,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide2,0.046674889,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.040964201,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.040703283,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.039541461
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide31,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.275571506,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide14,0.22260671,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.185412934,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.162369194,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.127741646,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.127741646,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.074536331,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.058718404,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.056231142,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.054799645
cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.275384086,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide7,0.213995019,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide4,0.198695967,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.196718857,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.171017542,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.145516049,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.143366627,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.139705168,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.139218759,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.13450946
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide30,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.27518439,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide14,0.225458807,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.180709523,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.161576725,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.117236643,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.117236643,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.074133308,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.057599777,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.053686447,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.053374378
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide13,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.275022252,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.139851537,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide1,0.040327183,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide8,0.036385056,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.034793613,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.034700187,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide10,0.03369954,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide22,0.514200585,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide11,0.044994563,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.844398026
cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide3,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.273956229,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,0.258021806,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.20008147,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.148356398,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.101075423,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.094693248,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.088863474,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.078992509,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide5,0.073502681,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.072762611
cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide2,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide1,0.273050896,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide2,0.225110181,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.218590203,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide1,0.179569634,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide1,0.153699397,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.132210745,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.132210745,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.104790584,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.086464916,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.086464916
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.272647569,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide1,0.211321549,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide22,0.574844643,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide3,0.303395352,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide5,0.149429015,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide10,0.881740266,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide3,0.303395352,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide14,0.125918079,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide22,0.574844643,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.911213301
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide27,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.272494908,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide14,0.224496408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.176715599,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.160767335,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.112282072,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.112282072,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.072370143,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.055660337,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.05279099,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.050443227
cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.271983687,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.206959533,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.135747072,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.124812532,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.113959736,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide4,0.104131251,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.103147459,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.100378653,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.09211518,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.084526498
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.271891548,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.232892884,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.2576436,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.162706858,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.993707203,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide7,0.164393706,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,0.216762601,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide16,0.164404696,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.216762601,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide3,0.143266725
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide15,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide19,0.270688759,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.230058413,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.212255933,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.208250428,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide15,0.951156905,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.210537216,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.226451101,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide8,0.212008461,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.212255933,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.211375228
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.270146159,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.093050172,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.083437836,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.078420448,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.076226155,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.068177807,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.067088465,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.062254277,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.051694706,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.045183487
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide12,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide11,0.269558805,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.159739675,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide21,0.11527286,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.124063961,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,0.846980675,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide12,0.172990839,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.13479264,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.159739675,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.124063961,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.19097121
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide10,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.269126601,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.066139033,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide9,0.051145029,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.043405542,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.035591418,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.035511987,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide4,0.035255251,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.034638592,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.03361356,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.032097065
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide2,0.268825298,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.200322546,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide7,0.132769229,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide0,0.083058172,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,0.080739806,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide9,0.080227702,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.076259249,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.07196859,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.069018702,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,0.055814958
cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide6,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide7,0.268808263,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide6,0.075931916,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.071194364,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.054751439,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.051614726,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.047852622,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.046529139,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.045344316,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.044976511,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide4,0.203108681
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.268679346,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.226101156,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.195417572,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.147056319,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide1,0.112147408,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.095563554,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.086414367,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.078203644,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.073984118,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.073473627
cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide3,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.268656061,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide5,0.115797622,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.059465929,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide2,0.048589313,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide12,0.046560261,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.043862692,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.03706836,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide8,0.032615158,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.022957161,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide6,0.021977494
cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide2,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide3,0.268005873,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,0.219343082,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.179247202,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide1,0.103239798,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.087286782,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.066877758,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.064946066,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide10,0.063747988,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.056151355,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide2,0.055577334
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide3,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.267846758,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.18599767,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.131804868,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.109934075,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide27,0.064622265,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide12,0.031864684,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.025799226,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,0.023061906,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide10,0.032215523,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.18599767
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide0,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide0,0.267719468,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.170770412,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.170770412,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.078865712,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide7,0.066498476,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.056442205,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide10,0.121137101,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide2,0.032987876,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.030186558,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.029123012
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide15,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.267576199,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.136337634,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.118341659,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,0.095806405,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide14,0.992375389,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.122654844,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide4,0.116163737,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.123310606,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide22,0.273571857,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,0.095806405
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.267479693,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.200591392,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.193856281,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.175618534,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.174109325,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.146169253,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.126857382,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.123888888,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##09_6-8-relative-measures_6.8._Relative_Measures.txt##slide1,0.114281123,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide2,0.110263608
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.267356536,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide12,0.086222809,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.070495869,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide6,0.056848915,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide6,0.049052504,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide1,0.04730465,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.045774363,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.126788595,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.044695508,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.184941906
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide26,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide11,0.267104922,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.256074656,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide9,0.2070807,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide10,0.096663181,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide4,0.063681458,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.053311192,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.042990398,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.028715547,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.028366629,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.027672668
cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide3,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide4,0.26683092,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.127512347,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide1,0.055556138,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.055254266,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.050562868,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide2,0.049492315,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.039266761,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.037411405,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide2,0.035697731,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide2,0.035441657
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide6,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.266323047,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.197483756,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide5,0.082545716,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.052968189,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.048567979,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.108452074,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.042651306,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.039509651,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.038653324,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide7,0.038548976
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide11,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide11,0.2660023,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.151094962,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.14129068,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide19,0.117922239,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.132876746,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,0.857587559,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide12,0.179787519,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.151094962,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.132876746,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.190445634
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide4,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,0.265718013,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide2,0.16706402,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##09_6-8-relative-measures_6.8._Relative_Measures.txt##slide1,0.111045361,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide1,0.085210959,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.065277769,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.054249395,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.051048794,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.047738039,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.044480882,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.042983178
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide0,0.264887886,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide14,0.033675087,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide0,0.022020966,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide0,0.021451471,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide0,0.020458853,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide0,0.020102886,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide0,0.020031355,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.018421279,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.01813317,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide0,0.016059174
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.264579823,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.139098866,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.132591145,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.116379973,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide5,0.082881531,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.076784561,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide0,0.074399907,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide3,0.072168936,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide3,0.072168936,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.071564036
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide1,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.26449055,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide7,0.152273274,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide17,0.11431603,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,0.095222565,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.263529113,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide2,0.087548805,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.055177123,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.049782008,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide2,0.045299353,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide3,0.038119948
cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide6,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.264490176,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,0.219883508,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide4,0.166288981,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.070477042,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.070477042,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.061603583,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide2,0.058129048,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.054906248,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide2,0.054362562,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide1,0.05075954
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide2,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide3,0.2642746,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.01923243,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.018823446,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide22,0.017109469,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide8,0.01589212,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.014980315,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.013437993,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide12,0.013405496,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide2,0.065187686,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.012916894
cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide4,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide11,0.26400616,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide11,0.26400616,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.084103393,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.081559763,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide4,0.070797321,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.065040645,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.062930294,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide9,0.059181512,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.055566005,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide2,0.0549592
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide29,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.263561658,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide14,0.225620267,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.175157542,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.155594231,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.107297346,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.107297346,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.07007988,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.057621467,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.049729784,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.049226482
cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.263400249,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.215621498,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide3,0.198351674,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.174952306,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.174132757,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide6,0.142600433,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,0.109210524,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide10,0.098652162,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.094942186,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.079250922
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.26319712,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.258944021,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.194806371,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide0,0.116800755,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.146588742,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide19,0.196158018,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide19,0.107152526,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.103927596,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,0.096607545,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,0.138734664
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide14,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.262783563,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.127363753,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.119989003,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,0.104092113,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide15,0.992372968,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide11,0.087493516,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.111323391,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide4,0.114882939,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.113889215,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide22,0.264911797
cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.26276942,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.26276942,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.16906945,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.130678504,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide13,0.12308592,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,0.12308592,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.094137213,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.088219165,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.088219165,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.088219165
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide7,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide8,0.262474989,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide6,0.113656623,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.075808116,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.069058285,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.067700542,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.055562151,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.211652147,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.053739183,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide13,0.20529441,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.076980814
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide14,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.261850372,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide21,0.112421317,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.088525537,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.087350714,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.065200275,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.063502729,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.058951695,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.058233569,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.047498321,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide10,0.43159653
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide0,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide2,0.261636626,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.083163444,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide7,0.064139006,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.041603548,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.041603548,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.041603548,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide1,0.261636626,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.040935757,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide0,0.828912331,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide7,0.261636626
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide8,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.261071296,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.120988968,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.110771992,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide6,0.10446313,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,0.102483982,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide4,0.074281829,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.066307356,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.061676685,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.059725753,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.057773964
language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide0,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide1,0.260774832,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide17,0.157202647,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.150477347,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.044714589,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,0.044114476,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide2,0.053192507,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide5,0.037608376,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.03486206,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide0,0.210991368,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.025800674
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide24,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.259278903,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.157759995,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,0.110639556,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.097848368,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide1,0.091568739,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.072839061,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.070142516,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.064948701,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.06423836,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.059093242
cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide0,0.258506917,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.12932103,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.117972597,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.090669994,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide8,0.088385041,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.084637151,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.082471257,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.080378478,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.079566569,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide4,0.070061781
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide4,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.258494564,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.221783679,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide0,0.165900464,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide7,0.164643267,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide2,0.118240035,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide6,0.09441227,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.093016005,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.092425043,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide7,0.057905477,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide3,0.066282057
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.258196607,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.229741903,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.252791813,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.162235215,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.993706041,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.141960984,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.210504221,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide16,0.164739063,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,0.210504221,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.229741903
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide8,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.258111337,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.072490567,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide1,0.072394936,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.060415232,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.051060551,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide3,0.040244635,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.039696677,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.032931396,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide6,0.029562354,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.027677998
cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide1,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide2,0.257386995,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.173088383,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide5,0.137891951,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.133117719,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.100544793,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide5,0.142010338,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide2,0.094024837,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,0.084804921,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.084283626,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.076614034
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.256972207,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.256972207,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.196701573,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.136503218,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide6,0.136299144,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide10,0.127458248,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.125952848,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide21,0.103644356,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.091174545,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide1,0.090446775
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.256682692,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,0.164603129,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.1587213,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide2,0.113128296,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.095349809,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.084644247,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide5,0.075305602,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide3,0.062335699,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.051030142,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide6,0.045583296
cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide0,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide1,0.256675919,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide0,0.20944495,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide0,0.150496824,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.112130473,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide0,0.108908103,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.090045832,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide0,0.07481007,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide0,0.074693855,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.073442104,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.061735129
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide18,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide13,0.256306964,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.194199258,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.121984057,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.0865558,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.075543843,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.152637186,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.227462635,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.955777216,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide7,0.079087683,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide16,0.189408426
cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide3,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.25628278,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.189026942,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.170423991,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.170423991,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.170423991,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.158477003,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide2,0.119779549,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide5,0.087605008,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide4,0.071053496,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.066721371
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide1,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide1,0.256145992,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide2,0.1372969,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide1,0.080302837,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.0383486,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.038042969,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,0.034058934,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide1,0.106839307,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.032545542,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide3,0.031091413,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,0.030502949
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide4,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.255568362,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide1,0.051202292,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide6,0.044666619,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.043528495,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide1,0.041411657,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.035244768,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.030559222,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.029713704,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.029088292,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide0,0.028187784
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide2,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.25545364,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.249328993,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.249328993,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.19763524,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide2,0.075381338,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.057996785,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.055086702,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.050844197,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide2,0.04935648,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.043095744
cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide9,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide13,0.255398485,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.251397587,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.188547746,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.130313032,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide1,0.117352199,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.100284375,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.099712816,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.089030932,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide3,0.166769909,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.077657678
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide11,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.255017694,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.2524198,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide3,0.086204163,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.215853463,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.441529876,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide4,0.215853463,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.255017694,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.245947296,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.215853463,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.096499137
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide12,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.255017694,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.2524198,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide3,0.086204163,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.215853463,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.441529876,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide4,0.215853463,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.255017694,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.245947296,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.215853463,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.096499137
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.254889892,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide10,0.245843058,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.095229737,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.176964824,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.089018374,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide6,0.088559273,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide6,0.140852055,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide2,0.077864064,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.071569164,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.061700815
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.254558764,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.254558764,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.231221995,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.1808107,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.139227187,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.139227187,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,0.099861881,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.042234578,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.041678826,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.033460423
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide1,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide13,0.254522685,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.116615954,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.108745909,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.102800173,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.097776026,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.095879483,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.089459429,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide0,0.084864551,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.083135005,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.066741639
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.254482736,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.142260222,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.104338226,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.080246901,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide2,0.063233522,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.057553128,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.056667247,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide5,0.055179737,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide23,0.05000291,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.04642111
cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide3,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide2,0.254474132,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide2,0.135879875,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.068375599,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.068375599,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.042673211,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.042673211,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide7,0.039692953,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.039692953,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.026200079,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.025717674
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide2,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide16,0.254466196,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.121910295,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.016083714,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.015759172,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide8,0.207526389,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide10,0.013534693,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide6,0.987516459,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide23,0.018542404,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.075292333,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide15,0.246978803
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide8,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.252978014,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.137047847,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.123431598,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.117268665,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.111474775,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide0,0.110465112,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide2,0.105458935,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.095615003,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.089899918,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.089899918
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,0.252918923,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.15795721,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide9,0.125974353,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide23,0.12308674,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.119910459,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide4,0.081546137,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.078108654,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.071602758,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.066454495,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.05604379
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,0.252613455,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide10,0.198789389,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.119620394,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.119620394,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide17,0.142467378,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide4,0.179409719,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,0.788709662,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide11,0.192952299,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.118364718,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide11,0.193443648
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide1,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,0.251721719,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.10831714,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.087014167,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.079122424,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.059702765,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide3,0.059073335,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.156337589,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide3,0.055467194,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide3,0.055467194,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide0,0.050538601
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.251580212,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.212662456,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.204868027,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.18721458,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.162836306,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.154299809,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.150203359,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.137061442,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.118545001,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide8,0.115025709
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide5,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,0.251111245,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.086033924,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.071969381,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.062732344,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.06179666,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.060821619,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide2,0.059984838,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.056581006,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide14,0.056343765,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.052231428
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide9,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.25095415,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide11,0.222997977,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide26,0.175700936,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide10,0.08174094,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.069229008,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide0,0.0659391,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide9,0.0489743,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide4,0.047337743,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.0359831,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.03514809
cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide5,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide2,0.250702691,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.030610931,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide6,0.030588001,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.028387858,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide1,0.026340134,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide6,0.025587706,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,0.025493495,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide4,0.024839248,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.022373372,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.02216754
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide9,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.25047412,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide15,0.179832901,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide4,0.096861878,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.029299457,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.021984529,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.021744783,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide8,0.021580654,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.0199034,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.166678153,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide8,0.987441045
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.250105289,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.179052167,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide1,0.157134803,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.155990025,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.151502806,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide24,0.130417576,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.120392102,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.119357287,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.100562873,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.095040965
cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.249601976,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.219823635,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.217584694,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.217417361,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.206986188,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.195189174,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.181601812,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide2,0.180879683,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.176423246,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.171531564
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,0.24935131,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.114601363,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.089751123,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,0.058363117,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.057259767,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.053695522,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide16,0.053190136,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.049393775,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide2,0.0450087,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide3,0.044425241
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide1,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.248976934,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide0,0.171916912,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.082782457,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide12,0.043785296,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.037462026,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.036269622,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide9,0.030416149,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide5,0.02483418,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide0,0.024624561,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide2,0.023654033
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide2,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.248976934,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide0,0.171916912,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.082782457,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide12,0.043785296,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.037462026,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.036269622,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide9,0.030416149,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide5,0.02483418,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide0,0.024624561,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide2,0.023654033
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide0,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide0,0.248741436,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.239151975,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.217946365,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.174187083,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.159371455,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide0,0.134530965,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.122875549,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.097361293,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.097361293,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.081498418
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,0.24802031,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide4,0.156659209,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.068085748,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide5,0.062331759,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide8,0.05318449,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.038064542,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide16,0.036765156,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.027492767,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.027492767,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.156659209
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide2,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.247186763,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.246027951,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide0,0.195109622,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.186394592,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide5,0.119746764,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide5,0.101568101,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.100308927,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.100308927,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.075309281,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.216958774
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide3,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.247149743,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,0.242564772,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.190883538,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide0,0.184880449,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide4,0.116587082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide3,0.098608007,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.09309778,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.09309778,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.074628283,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.205099906
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide3,0.24659008,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.184394545,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide0,0.175183521,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide7,0.148087915,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide2,0.123341246,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.121361922,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide2,0.096064982,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide2,0.096064982,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.08585316,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide6,0.072849484
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide7,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide4,0.246357582,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.162596928,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.137087048,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.12452542,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.102618042,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide0,0.085025308,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.078053506,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.078053506,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.071658594,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.069818837
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide0,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide17,0.246339677,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide0,0.231113986,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.184355673,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide1,0.213002679,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide10,0.127411431,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.026019283,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide7,0.0151585,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide0,0.924183877,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide3,0.029159261,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.028455086
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide18,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide2,0.245656695,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide11,0.085462342,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.042773471,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.018104825,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide0,0.015993415,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide24,0.015188168,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide1,0.243529058,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide1,0.01511066,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,0.015107769,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide10,0.015021861
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide12,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide2,0.245464508,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide1,0.234477096,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide3,0.206886177,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.055985217,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.034198941,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.033386708,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide8,0.030332267,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide3,0.024216629,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.021621328,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide4,0.021498283
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide0,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.245148563,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.243642183,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.207060641,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.169402521,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.148963412,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.135874572,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.13307971,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.114286883,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.093160071,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide10,0.088774702
cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide5,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide6,0.243917709,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide12,0.165166593,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.159917173,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.153011865,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.127857161,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.126465765,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.120634572,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.114542721,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.094020152,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.094012798
cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide5,0.243785962,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide2,0.218213035,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.113053977,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide4,0.111004302,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.239356211,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.092684155,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.086085975,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide3,0.078097547,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.077742276,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide3,0.076395799
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide2,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.242605693,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide6,0.147908362,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.117924788,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide3,0.06751691,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.053382824,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide2,0.047418913,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.035868801,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide6,0.035542198,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.034547844,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.032951346
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.242057413,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.23061838,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide3,0.155061149,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,0.071848928,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.071136388,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.068077192,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.221676568,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.951147503,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide4,0.155061149,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.156375569
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide23,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.241632789,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.237849641,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.113849147,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.109694931,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.738159199,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.120776237,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.241632789,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.236049074,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide3,0.120776237,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.216126618
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide13,0.24150209,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.124003439,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.110450064,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide4,0.102268927,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.086837864,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide9,0.061950143,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide26,0.042241429,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide10,0.106142785,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide11,0.992511613,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide6,0.083507
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide10,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide13,0.24150209,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.124003439,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.110450064,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide4,0.102268927,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.086837864,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide9,0.061950143,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide26,0.042241429,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide10,0.106142785,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide11,0.992511613,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide6,0.083507
cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide10,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.241384097,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide4,0.192684505,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.078146179,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide4,0.133282224,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide8,0.073785341,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide8,0.073785341,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.069672835,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.068828426,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide7,0.066080831,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.0633728
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide3,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide2,0.241251261,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.036977529,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide3,0.036093604,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,0.031168121,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide1,0.024152964,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.020154895,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.0198341,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide8,0.019171069,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide1,0.018440126,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide3,0.0182768
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide0,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.241104272,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.237373964,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide19,0.201178322,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide25,0.100110937,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.090968765,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.065433729,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide0,0.951165761,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide18,0.10772209,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide29,0.071141947,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide1,0.067306686
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide9,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.240836621,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.223317727,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.138959347,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.134014467,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.194586821,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide9,0.950982487,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide18,0.142299728,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.218201517,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,0.213630943,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,0.198362618
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.240019454,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.147277053,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.125398926,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.094069384,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide3,0.164153078,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide1,0.093928907,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.084041914,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.076936899,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.068626023,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.068453422
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide2,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.239976683,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.114493901,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.075966543,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.067492871,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.053741292,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.048705672,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.046165586,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,0.045881651,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide6,0.044980558,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.042950096
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide13,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.239956898,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.183747248,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.182295126,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.174038104,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.155437386,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.153287399,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide13,0.951173319,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide17,0.727539229,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide8,0.177476436,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide17,0.727539229
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide6,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.239297172,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.084612156,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.082460003,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.070525532,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide6,0.067746143,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.051551913,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.046065447,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide4,0.045648464,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.044878043,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.044767156
cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide1,0.239011215,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide7,0.234406795,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide5,0.203793393,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide2,0.160458742,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.122973175,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.122922376,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.101887146,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.086112749,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide4,0.227440193,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide1,0.160220972
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide10,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.238988049,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.17073747,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide5,0.201844171,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.033652062,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.025910439,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.025910439,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.025910439,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide15,0.218249531,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide8,0.031199472,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,0.024805255
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide3,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide2,0.238981356,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.089945403,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide2,0.076748797,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.0761306,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.073247589,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.069620399,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.058392424,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.056603897,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.048615514,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide14,0.046833778
cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide6,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.238892317,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.155443551,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.143201504,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.134384553,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.12760522,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.12607176,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.116439427,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.093703192,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.092930544,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.090820674
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide0,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide15,0.238761758,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.154286872,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.121641196,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.120941433,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.112156476,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide14,0.238761758,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.105317384,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.101171401,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide1,0.097982568,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.097102824
cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide9,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide2,0.238754274,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.187142327,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.070812969,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.053396942,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.052750705,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.046818956,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.043409152,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.042395512,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.040551687,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide3,0.091489731
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.238746691,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.177122652,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.171666239,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.158415648,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.122152848,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide1,0.100759156,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.091427865,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide3,0.087722108,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide2,0.084202253,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide1,0.077808165
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide6,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide16,0.238409094,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.13738167,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.016873298,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide24,0.016830121,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide6,0.019074048,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide10,0.137158089,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,0.022287527,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.048417878,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide17,0.221546067,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide4,0.976778549
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide1,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.238037587,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.187143157,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.116971682,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide1,0.093754112,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.091450422,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.07739146,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.049727507,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.112715026,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.036680668,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.031861702
cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide2,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide2,0.237986785,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.232621254,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.177912222,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.155156079,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.151399897,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.1452663,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.122496967,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide4,0.227927451,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide6,0.110886714,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.095622824
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide10,0.236539721,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.186108646,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.185926796,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.131985779,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.107229478,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.090512285,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide6,0.075514731,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.150470982,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide10,0.066549321,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide6,0.170568545
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide12,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide0,0.236330901,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.156016501,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.156016501,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.122609413,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.112225887,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.133669055,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.176898701,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.094837157,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide9,0.132435905,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.126502694
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide15,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.23576336,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide10,0.161203138,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.148466136,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.132982068,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.131796914,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.098901682,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide15,0.951044617,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide15,0.129380457,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide11,0.16080994,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide11,0.156751832
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide2,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide1,0.235651345,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.217132052,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.175245859,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.116223175,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.092887977,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.083008832,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.068324005,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.061795617,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide1,0.989317198,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide10,0.289909492
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide3,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide1,0.235651345,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.217132052,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.175245859,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.116223175,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.092887977,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.083008832,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.068324005,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.061795617,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide1,0.989317198,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide10,0.289909492
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide24,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide2,0.235282879,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.077982327,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide23,0.046417031,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.041961314,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide27,0.039032127,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.035272146,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide12,0.018988946,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide0,0.015408577,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.014109264,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide0,0.013858968
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide3,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.234441915,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.180975852,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide2,0.159143295,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.141281087,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide1,0.132473542,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.123078924,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.121739292,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.118942595,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.118863363,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.117629861
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide9,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.233940531,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,0.21950178,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.159625596,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.15170967,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.143743656,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.131015834,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.121205722,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.10776422,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.10776422,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.098031701
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide22,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.233776791,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.229577743,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.121614836,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.112255794,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.738235379,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide4,0.123678384,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.229422433,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.233776791,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.123678384,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,0.123678384
cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.233265589,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.227752759,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide2,0.091167532,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide2,0.091167532,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.062036467,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.037240449,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.037240449,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide3,0.035298747,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide6,0.037263658,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide7,0.070412938
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide21,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.232074818,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.166009361,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide14,0.119991443,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.108346952,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.100728482,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.053574664,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide8,0.10272371,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.56438591,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide12,0.058808703,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.207772223
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide5,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.231586852,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.112527339,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.086991545,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.082132109,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.081939986,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide6,0.076345434,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.071533894,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.0661864,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.048252489,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.032114696
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide11,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.231110933,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.190005464,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide5,0.191763118,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.033476489,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.027890455,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.027890455,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.027890455,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide15,0.212623142,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide6,0.025763785,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide8,0.029107863
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.229448031,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide8,0.222768125,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.116240291,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.09495136,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.087217944,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide5,0.07196821,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,0.064709714,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.048398843,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide6,0.107565707,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.047123031
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide20,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.228908829,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.222654225,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.119246057,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.119073073,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.834347588,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.120117812,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.228908829,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.221565664,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,0.120117812,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide4,0.120117812
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide6,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.227989778,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.109947237,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide4,0.085191488,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide7,0.082423922,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.081917968,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.081917968,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.081434016,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.078732428,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.075084545,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.07389775
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide5,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.227489677,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.158817914,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.09094275,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.08735426,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide0,0.059446369,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.040860058,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.038420611,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide1,0.151244295
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide6,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.227489677,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.158817914,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.09094275,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.08735426,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide0,0.059446369,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.040860058,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.038420611,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide1,0.151244295
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide7,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.227489677,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.158817914,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.09094275,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.08735426,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide0,0.059446369,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.040860058,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.038420611,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide1,0.151244295
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide8,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.227489677,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.158817914,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.09094275,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.08735426,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide0,0.059446369,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.040860058,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.038420611,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide1,0.151244295
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide9,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.227489677,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.158817914,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.09094275,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.08735426,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide0,0.059446369,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.040860058,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.038420611,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide1,0.151244295
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide10,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.227489677,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.158817914,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.09094275,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.08735426,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide0,0.059446369,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.040860058,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.038420611,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide1,0.151244295
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide11,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.227489677,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.158817914,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.09094275,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.08735426,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide0,0.059446369,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.040860058,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.038420611,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide1,0.151244295
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide12,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.227489677,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.158817914,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.09094275,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.08735426,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide0,0.059446369,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.040860058,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.038420611,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.163480791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide1,0.151244295
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide21,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.227254805,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.217032785,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.105178868,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.500000433,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.117369946,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.77974593,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.227254805,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.21576815,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,0.112790265,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.105178868
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide13,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide28,0.227029201,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.12987228,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.123374997,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.119937395,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.070853384,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.070853384,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.052221424,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.051625677,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide14,0.992943067,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide27,0.096809637
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide0,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.226140893,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide0,0.062311882,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide0,0.062311882,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide0,0.062311882,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.047405284,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.043339353,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide0,0.036397402,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.034203373,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.034203373,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide0,0.032540846
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide12,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.226055577,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.208302806,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.033998255,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide5,0.184921919,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.028772114,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.028772114,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.028772114,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide15,0.204745937,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide6,0.026146386,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide8,0.02849318
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide13,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.226055577,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.208302806,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.033998255,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide5,0.184921919,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.028772114,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.028772114,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.028772114,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide15,0.204745937,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide6,0.026146386,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide8,0.02849318
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide14,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide29,0.225479273,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.124822511,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.120451208,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.108918552,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.073762759,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.073762759,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.05210714,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.047350447,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide15,0.993163065,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide27,0.224583802
language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide3,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide7,0.225209139,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.096180144,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide14,0.135654266,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.040786811,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide7,0.037813747,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide1,0.069074308,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.031805731,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide23,0.028739404,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.025323415,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.054795385
cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide7,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.224930215,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide10,0.15911604,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,0.093343646,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,0.093343646,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,0.092525132,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide2,0.079978288,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide12,0.07767143,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide12,0.07767143,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.071268758,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.069255742
cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide2,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.224821957,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.1673707,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.131706849,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.126224603,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.112138079,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,0.112009276,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.070268534,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.063631236,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.057319413,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.051530633
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide3,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.224745677,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide5,0.121936556,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.108197152,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.084744251,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.081261286,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide4,0.047477158,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.046885797,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide1,0.029570643,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.052618925,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide3,0.938714541
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide19,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide13,0.224637907,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.198146222,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.126969503,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.072973913,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.066810186,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.062665634,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide20,0.115019379,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.210104357,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.118032434,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide16,0.163056399
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide1,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide1,0.224587354,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.212607185,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.168619549,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.164720072,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.111948962,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.093513941,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide1,0.073820898,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.063584756,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide2,0.990196481,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide2,0.104581046
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide1,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide1,0.224490397,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide5,0.160867512,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.099915842,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide1,0.068617116,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide3,0.066248354,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.043504451,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.034118729,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.031455853,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.02787689,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.023777123
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide14,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide10,0.224427317,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.2035629,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.185330826,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide2,0.016015821,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide5,0.01001485,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide17,0.00964574,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide5,0.009616629,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide14,0.009251076,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide3,0.159276573,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide11,0.181474612
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.224123019,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide17,0.191584531,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide7,0.189508836,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide23,0.17267357,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.171974817,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.160236412,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.159220439,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.089757067,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.077145864,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide11,0.7689333
cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide4,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.223951249,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.199438051,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.129446108,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.128367336,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.124878027,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.124511632,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide7,0.080373596,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.067775747,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.065130849,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.061356739
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.223202106,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.20394149,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.203258576,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide0,0.197432572,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.108994938,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide10,0.087189028,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,0.085136056,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.083186525,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.076426163,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.067878379
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide5,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,0.222962407,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.123994782,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.114293009,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.093294739,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.091914283,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.082263745,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.102864757,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.084064498,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.181957931,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide5,0.95098721
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.222897894,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide7,0.203491181,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.190104451,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.185674028,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.175577134,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.113869376,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide17,0.10019731,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.072825481,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide22,0.069939976,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide3,0.043219945
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide10,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide8,0.222686585,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide7,0.090016001,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide7,0.074400685,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide6,0.066908214,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide4,0.059678672,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.055669648,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide9,0.222686585,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide13,0.048054844,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide10,0.222686585,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide8,0.967983711
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide17,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.22224467,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.074275619,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.073710039,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.063067555,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide0,0.061181026,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.059573049,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.056366503,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.055002646,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.210256893,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide3,0.045942888
cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.222160042,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.180728013,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.180728013,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide6,0.126743165,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide5,0.125815732,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide1,0.096734976,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide3,0.091961232,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide3,0.091961232,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide2,0.087894733,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide2,0.087894733
cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.22195904,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.180464323,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.151014701,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide3,0.121940726,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.112791453,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.105176804,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.07035903,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,0.066679807,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.066258311,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.062114085
cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.221829579,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide4,0.178771261,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.178268227,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.170427511,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide2,0.159808622,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.132164633,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.116511396,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.10655899,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.105954831,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.093449528
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,0.221827143,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.157673133,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.136902248,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.136902248,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide8,0.110396442,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide8,0.110396442,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.088740717,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide16,0.074010478,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.056304382,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide8,0.055664812
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide1,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,0.221651088,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.158381578,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,0.141329054,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.128980803,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.119045056,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.116546409,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.109339153,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.084971751,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide8,0.096511757,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.084830237
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide7,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.221378641,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.221378641,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.094001386,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.05943559,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.05943559,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.043344308,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.039914627,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide19,0.609731091,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.296080906,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.701304003
cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide3,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.221324228,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.186205511,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.148971507,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.126720357,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.104821174,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.084525148,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.070767355,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.065836887,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.061109712,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.058146831
cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide9,0.221104561,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide0,0.143674132,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.130951749,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.121457434,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.108595129,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.096944345,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.096612749,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.091990566,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.091990566,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.091293906
cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide3,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide4,0.220987518,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.034804092,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.032063574,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.023863988,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.022711636,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide13,0.059138005,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.021244317,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide9,0.02060756,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide10,0.019620667,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide6,0.050813261
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide2,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide12,0.220983601,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.087196125,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide3,0.08622015,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.068830041,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide10,0.053597431,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,0.03803057,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide13,0.040252412,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide7,0.037212695,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide8,0.033822401,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide2,0.950950042
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide4,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide5,0.21945865,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide11,0.172209334,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.127673532,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.057254729,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide14,0.03640219,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide1,0.103789213,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide1,0.030880788,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.030184502,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.026683279,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide16,0.044660163
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide17,0.219182075,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.169433074,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.14889102,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.118916869,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.064410926,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide17,0.085729869,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.110965848,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.07228426,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.950772768,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.088254627
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.219016817,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.121783823,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.119945939,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.091338088,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.079424731,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.074818056,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.066950899,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide6,0.065586677,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.061157117,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.101406274
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.218995177,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.188832398,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide1,0.179430291,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.153022365,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide4,0.325559012,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide22,0.267567229,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.107983357,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide10,0.173306258,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.218995177,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.153022365
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide14,0.2184899,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide12,0.18129684,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide4,0.15354488,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide12,0.116372734,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.193380354,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.94282913,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.193380354,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide12,0.131348375,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide8,0.127225415,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide3,0.145289201
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide12,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide7,0.21837314,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide6,0.192019294,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.179281255,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.110080478,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide6,0.109933447,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.061941287,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide14,0.192014042,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.048588053,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide18,0.150257525,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide1,0.072757776
cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide1,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.218234442,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.213199001,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide0,0.195036642,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.130844012,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.124835575,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide1,0.11629941,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide1,0.116137223,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.107253331,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.101906311,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.099690387
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide17,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.218207892,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.202852011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide3,0.108360378,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.886515501,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.121121261,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.110062932,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.209656616,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.202852011,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,0.110062932,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.203614272
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide2,0.218162007,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide2,0.194137839,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide0,0.185505471,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide0,0.111451406,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.087298516,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.044434154,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide1,0.041544023,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.040577464,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide8,0.036213508,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.034632159
cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide8,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.217980156,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.217980156,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.171649263,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide7,0.157238281,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.157238281,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.151540067,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide8,0.151501797,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.147830247,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.131320736,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.121215954
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide0,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.217391221,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.210808151,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide2,0.077500069,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.077425177,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.071055826,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.057065549,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.047085026,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.038243233,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.035376165,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.034567952
cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide3,0.21704655,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.158881544,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.141651853,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.121344182,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide3,0.114240946,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.108924163,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.105437773,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.103857774,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide2,0.09019024,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.158329555
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide8,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.21686318,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide1,0.067103079,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.058793579,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide4,0.053217078,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide4,0.040675683,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.031860215,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.029217346,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.027550968,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,0.027383824,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.025276869
cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide4,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide3,0.216455022,cluster-analysis##02_module-1##01_lesson-1-.txt##slide2,0.211318669,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide5,0.143202865,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide5,0.127458541,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide5,0.094104119,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.04244221,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.037252919,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.036255598,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.032731018,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.032459944
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide7,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide9,0.21640876,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.028003818,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.018052316,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide10,0.016648893,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide2,0.015977916,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide5,0.21640876,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,0.011536245,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,0.011536245,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.017699716,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.010598599
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide9,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide9,0.21640876,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.028003818,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.018052316,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide10,0.016648893,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide2,0.015977916,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide5,0.21640876,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,0.011536245,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,0.011536245,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.017699716,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.010598599
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide11,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide9,0.21640876,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.028003818,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.018052316,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide10,0.016648893,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide2,0.015977916,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide5,0.21640876,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,0.011536245,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,0.011536245,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.017699716,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.010598599
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide8,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide22,0.216013901,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.071230717,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide1,0.068640291,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.025988347,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.15004657,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide10,0.019978904,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide3,0.013710853,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide18,0.035456165,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide23,0.019336315,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide7,0.01602172
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide7,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.215808556,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.173622754,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide13,0.039795754,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide9,0.029823082,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.026383608,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide8,0.019566382,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.018872674,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide4,0.018481429,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide13,0.044711183,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide7,0.025392301
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide15,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide31,0.215691955,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.130173294,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.125482527,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.122758498,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.086782747,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.086782747,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.052115775,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.051193026,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide14,0.993159851,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide27,0.093337591
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.215615148,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.163125792,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.134769217,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.114441423,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.114441423,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide3,0.106840317,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.102198571,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.101644743,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.099817304,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide2,0.081642505
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide2,0.215390525,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.198045524,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide2,0.161040588,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.118233458,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.115987849,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide2,0.099017081,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.098018484,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.064748832,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.064362959,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.057691495
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide2,0.214694948,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.174473636,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.149393056,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.105953645,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.105953645,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide2,0.076544503,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide28,0.076304573,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide28,0.063300744,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide7,0.045693242,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.047559775
cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide8,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.214414683,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide8,0.180555661,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.160074255,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide3,0.119320464,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.095507172,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide10,0.04442728,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide3,0.043604007,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide4,0.114596768,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.042713487,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.042486947
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide11,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.213645393,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.192354832,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.042741557,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide14,0.042663269,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.039902644,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide10,0.038722599,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.03544239,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide8,0.038865109,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide22,0.563267053,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.099718795
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.213644826,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.126925522,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide3,0.120416565,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.113923113,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.108331897,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.095530682,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.091996445,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide3,0.086339721,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.079681089,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.073336059
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide7,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.212448644,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.068897829,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide4,0.06428284,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.052609628,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.04099179,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide4,0.040534465,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.037553015,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,0.032620194,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.027304769,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.027288364
cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide7,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide1,0.211345804,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide4,0.191198702,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide12,0.181608586,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.093753806,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.089418021,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.085541789,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide10,0.074045224,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide5,0.068561189,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.067395204,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide6,0.115196634
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide5,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.210426371,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.14173606,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.092073458,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.077096547,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.063466686,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.06111581,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide6,0.05571956,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide1,0.055271464,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide1,0.054096126,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.048243991
cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide4,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide4,0.209673992,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.163941016,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide2,0.13907087,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.116518928,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.104690899,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.099055595,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.091893848,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.085151991,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.068895575,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.067303413
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide9,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.209243397,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.206924804,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.141576204,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.098095207,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.09533984,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide1,0.094169312,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide10,0.085913373,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide10,0.085913373,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.08144473,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide0,0.081364239
cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide9,0.208733573,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.123417391,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide5,0.092309101,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide5,0.092309101,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide7,0.084663888,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.080468023,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.065648693,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.121039696,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide2,0.058341573,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.128196197
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide8,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide2,0.208550209,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.183469541,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide15,0.205965767,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.026322839,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.025703652,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide8,0.025482501,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.024203608,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide5,0.201444215,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide0,0.023916071,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.023540235
cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.208221522,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide1,0.085579794,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.057489692,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.056958255,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.056728355,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.056620326,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.051900911,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide4,0.050745226,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide2,0.049459924,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.048812698
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide3,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide3,0.207744135,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide12,0.034722358,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide9,0.025463916,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.025272703,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,0.024440606,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide7,0.023936532,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.022984048,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide15,0.022687124,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide2,0.018852941,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.018199874
cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide4,0.207688876,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.178249073,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.133632361,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.107867584,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.099497887,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.094617344,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,0.18463527,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.093412664,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.09188226,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.090531214
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.207153953,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.097816946,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide5,0.058848053,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide1,0.056906489,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.051278581,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.042488976,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide22,0.040826524,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##09_6-8-relative-measures_6.8._Relative_Measures.txt##slide1,0.033588619,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide7,0.030152775,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide1,0.028065586
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide8,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.206474429,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.192573864,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.181035152,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.164016329,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.15997759,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.15997759,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide0,0.151937689,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.13520403,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.13520403,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.12416458
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide2,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide1,0.206442608,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.092460799,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,0.035850176,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.029486176,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide4,0.185367088,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.027488308,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide11,0.023645535,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide6,0.023302363,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide21,0.022327657,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide9,0.021714571
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide18,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.205918646,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.168455177,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.128693479,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.550719021,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.134610139,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide3,0.17004423,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.197221151,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.168455177,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.852438679,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.104111667
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.205496527,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide0,0.15422405,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.145749521,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide1,0.095541615,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.093620117,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide3,0.090911449,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.08508943,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.079983433,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.079983433,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.073876104
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.205122753,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.099231811,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide5,0.090889137,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide1,0.086544824,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide24,0.085522235,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.070998968,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide2,0.065233662,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide10,0.985030845,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.063176058,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide11,0.158476816
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide0,0.204585101,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.138852658,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.138852658,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.127284182,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.112473595,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide7,0.096665854,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide3,0.078714757,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.074235667,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide2,0.054055172,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide2,0.051026955
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide6,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,0.204331336,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.085822112,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.079284438,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.077376668,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.072935766,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide2,0.061799132,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide14,0.056092604,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.051318232,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.050243856,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.050188787
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide1,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide2,0.20382396,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide25,0.070932673,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.059798266,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.058837314,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.055724638,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide1,0.939103889,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide9,0.120023825,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide8,0.066701424,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide22,0.052635204,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.055167731
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide0,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.203726171,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.190572761,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide2,0.16922574,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide2,0.16922574,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.143940683,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.141676629,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.140235997,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.139210827,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.130656778,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide6,0.119099984
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide13,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide15,0.203372343,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide22,0.152203758,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.135867562,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.069111545,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.052732538,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.052732538,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.047075875,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.039460032,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.024565674,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide8,0.022938232
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide19,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.202494896,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.202494896,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.179969489,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.121552071,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.121552071,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.060611063,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,0.027880256,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide12,0.625191717,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.121707028,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.121707028
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide20,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.202494896,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.202494896,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.179969489,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.121552071,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.121552071,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.060611063,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,0.027880256,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide12,0.625191717,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.121707028,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.121707028
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide21,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.202494896,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.202494896,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.179969489,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.121552071,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.121552071,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.060611063,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,0.027880256,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide12,0.625191717,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.121707028,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.121707028
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide22,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.202494896,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.202494896,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.179969489,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.121552071,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.121552071,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.060611063,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,0.027880256,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide12,0.625191717,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.121707028,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.121707028
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide23,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.202494896,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.202494896,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.179969489,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.121552071,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.121552071,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.060611063,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,0.027880256,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide12,0.625191717,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.121707028,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.121707028
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide14,0.202236214,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.062841756,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.050990763,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.048776455,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.045412159,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide8,0.044158366,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.039025811,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.0310582,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide2,0.030546359,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide4,0.029589668
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide5,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.202223108,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide5,0.088715749,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.052739767,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.0508806,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.04904986,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.04904986,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide5,0.048710075,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide1,0.046923938,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide2,0.03503372,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide4,0.990893104
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide4,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.202207729,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.192270631,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.146927611,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.125966192,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.121123801,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide1,0.096610403,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.084230274,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.080413674,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.085222786,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide17,0.170386615
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide22,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide9,0.202109493,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.062056913,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide22,0.951186645,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide23,0.139192041,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide28,0.054212579,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide14,0.069151966,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide23,0.139192041,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide34,0.102201337,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.062056913,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide10,0.178992657
cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide3,0.201855076,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.146903043,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.136493956,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.103128984,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.085531108,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.084829884,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.081695954,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.075534918,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.071118909,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.068465818
language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide0,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.201234697,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.150950793,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.149202392,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.146874148,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.144657032,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.142261096,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.125410999,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.106401936,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.106310593,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.102007247
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide4,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,0.200831402,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide2,0.121953795,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide3,0.11657306,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.115611686,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide6,0.031918879,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.031506877,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.031506877,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.031506877,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide3,0.993927731,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.200831402
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide5,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,0.200831402,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide2,0.121953795,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide3,0.11657306,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.115611686,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide6,0.031918879,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.031506877,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.031506877,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.031506877,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide3,0.993927731,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.200831402
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.200472484,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.122164216,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.118564586,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.114218022,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide11,0.114211255,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.112444816,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.093447502,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.086304879,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.109082395,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.200472484
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide3,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.200106801,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.11685105,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.092730354,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.090207366,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.050746241,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.0453893,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.045076087,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.026522322,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.026508217,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide1,0.025418041
cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide4,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide2,0.200048532,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.161982936,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide4,0.099078783,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide2,0.098541045,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.090838153,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide7,0.070292508,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.067462195,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.067436567,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.062694689,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.053288233
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.19856131,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.12522263,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.124949024,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.124539015,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide7,0.101287721,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide6,0.088967513,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide6,0.074549849,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide12,0.05575666,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.051580395,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.051580395
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide12,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide4,0.198135155,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.03243748,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide10,0.020675677,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.017037939,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide2,0.014712106,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,0.011199324,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide5,0.198135155,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,0.011199324,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.018523214,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide5,0.009847888
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.198114468,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide9,0.153895918,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,0.123923341,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.121138474,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.102339942,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.057895367,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.057684684,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide23,0.05511444,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.052660053,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.057414768
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.197747371,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.127109333,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.09301958,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.08908284,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.085433029,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.081587028,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.071950412,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.071867007,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide6,0.070515148,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide4,0.069521075
cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide6,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.197510487,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide10,0.065081122,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.035595553,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,0.029424837,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,0.028914224,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.02578733,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.025561384,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide5,0.025530083,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.025457051,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide6,0.024089432
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide2,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.197435822,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.091385203,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.071881476,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.065062922,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide10,0.062357969,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.159714746,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide16,0.06755495,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.192841529,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide3,0.227575041,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.066478702
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.19742763,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide9,0.153649766,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.106683017,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.077707202,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.076438849,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.071105363,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.067645165,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.059118216,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.057628183,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide1,0.048804692
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide2,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.197197397,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide7,0.053240272,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.149210095,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.049313587,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.048047858,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide9,0.03987279,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide9,0.03987279,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.038988502,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.035363994,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide3,0.033949387
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide11,0.196896816,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.141006804,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.131535447,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.111170841,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide6,0.110487044,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide14,0.104675689,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.101798137,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,0.088973002,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.070614244,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.0689862
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide13,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.196889286,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.196889286,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.079826724,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.058069357,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.058069357,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.029792249,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.028057333,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide22,0.541596381,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.31351925,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.784308982
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.196709081,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide1,0.106188558,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.069462778,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.066527931,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide12,0.048881125,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.039011972,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide3,0.037429849,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide4,0.034827554,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide5,0.033372323,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide23,0.035398965
cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide3,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.196507772,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.172969663,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.140609032,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.129162791,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.108037531,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.092255654,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.05921314,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide7,0.059018109,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.058807798,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.05487477
cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.196413026,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide3,0.1777815,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.164909613,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.131798235,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.10472463,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.102273833,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.094078027,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide5,0.087619126,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.084993421,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.082855945
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide6,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,0.196396045,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide3,0.122799473,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.038972103,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide2,0.032568809,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.031199549,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.027918161,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide14,0.024360182,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.024226583,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide5,0.024145402,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide1,0.023367379
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.195788457,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.117752814,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide2,0.089788088,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide6,0.077915852,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.045284537,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.044034645,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.043988927,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.043538303,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.079519546,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide4,0.046475203
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide3,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.194852979,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide2,0.121313778,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.116256008,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide3,0.115350271,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide6,0.032565599,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.030659745,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.030659745,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.030659745,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide11,0.024024089,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,0.194852979
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide9,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.194824398,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide6,0.093518143,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.070520406,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.069453919,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide5,0.06227136,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.060364772,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,0.056877148,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide6,0.034126106,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.032379004,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.030552398
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide11,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide10,0.194338428,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,0.11368778,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide4,0.108634475,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.100395483,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.083782904,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide13,0.042113556,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide9,0.194338428,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.979059799,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.092646776,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide8,0.194338428
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide19,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.19382872,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.164068157,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.146678736,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.098675125,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.493878286,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.155705484,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.192740792,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide3,0.161080452,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.164068157,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.81378627
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.193760558,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide19,0.139812479,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.131984884,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.130431819,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.124873059,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.126241031,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.074887607,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,0.053594458,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.049206477,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.047147257
cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide10,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.193424279,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide7,0.165114544,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.149967633,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.133708074,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide8,0.110159289,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide6,0.09980014,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide8,0.085088273,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide11,0.082793125,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide11,0.064955109,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide9,0.048004197
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide6,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.193126708,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide20,0.133400048,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.0607253,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.182695192,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide6,0.951094926,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.155810321,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide15,0.063071325,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.149999671,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.229071876,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.0607253
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide0,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide0,0.193029288,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide14,0.036301744,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide20,0.028693537,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.027594807,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide6,0.026751516,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.025863843,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide1,0.024839572,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide5,0.023392199,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide7,0.023111247,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.02265572
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide22,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide8,0.191904634,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.065629253,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide10,0.039299467,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,0.027213765,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide2,0.024293536,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide19,0.023370685,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.022856407,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.022593801,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.021628375,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide3,0.022205657
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide16,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.191437897,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.191437897,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.191437897,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.11976188,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.097590418,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.096325839,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.094839804,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide8,0.083654789,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide3,0.082924779,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.08182333
cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide3,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.191357299,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide2,0.165835964,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.136789291,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.135465659,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.134488906,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.1205083,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.116026192,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide6,0.106295499,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.092068318,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.072183523
cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide9,0.191138773,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide9,0.191138773,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide2,0.164818207,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.148443521,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,0.115394707,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.111572963,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.079068824,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.078946124,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide6,0.077886647,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.074511924
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide13,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.191030007,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide8,0.133910085,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.104437795,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide22,0.589570902,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide3,0.27233194,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.105432921,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.665505869,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide10,0.665560746,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.109522443,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide8,0.133910085
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide6,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.189971958,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide14,0.082641577,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.025805475,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide11,0.01787569,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.117174499,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide3,0.013942427,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.012985235,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,0.01285474,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide18,0.011785901,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide13,0.015579351
cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide0,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.189948914,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.188666094,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.183782268,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.182672441,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide0,0.173725159,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.171271765,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide0,0.165366434,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide0,0.145185568,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.141875257,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide0,0.138652022
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide9,0.189926097,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.096620019,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.058804201,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.057509449,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.053172667,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.051418649,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.048436967,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.047965603,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide5,0.047800676,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.04531777
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.189708293,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.139916559,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.110854562,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.088751342,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide6,0.079601768,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide1,0.076286982,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.071014273,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide1,0.068630351,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.067575497,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide1,0.065385816
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide12,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.189658993,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.085844439,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.069137439,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.057678554,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.056380469,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide4,0.051359477,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.048270149,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide19,0.179324472,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.064981282,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,0.047605477
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide7,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.189611995,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.104965216,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.089814132,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.076621788,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.062311303,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.062197356,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.057321986,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide6,0.041322483,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide8,0.040843808,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide8,0.040843808
cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide11,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide4,0.189495229,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,0.111951045,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide6,0.048101084,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide7,0.033408811,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.033276253,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide4,0.105410589,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.032198375,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.030846183,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.029620536,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.028023928
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide2,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide4,0.189209056,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide4,0.155456054,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide2,0.118675409,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.087247775,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.086227899,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.061964636,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide12,0.179189833,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.054941641,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide2,0.046790437,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide5,0.04903179
cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide4,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.187269124,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.161119842,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.111456103,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.104509114,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.096544509,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,0.086808741,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.084224433,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.069399219,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide10,0.06416168,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide1,0.062522835
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide5,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,0.187210535,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide0,0.0562327,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.05599011,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.042738691,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.041660592,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.040816155,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.038395324,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.038157061,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.038057515,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.037730886
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide1,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.18659905,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide6,0.181742527,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide0,0.140173256,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide16,0.081597089,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,0.078250296,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.073087128,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide1,0.94716612,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide3,0.075444542,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,0.078250296,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide1,0.94716612
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide1,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,0.186338663,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.06801635,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide5,0.052322011,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide24,0.051455718,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.050599584,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.048067916,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.040974729,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.03757173,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,0.037002467,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide1,0.036808224
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide12,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.185375056,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide4,0.077030032,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.069421006,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide23,0.054578198,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.052736646,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide12,0.951199618,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.04041729,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,0.042511105,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide6,0.070512032,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide26,0.048115948
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide5,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.18503456,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.063857111,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide4,0.061802342,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.050460179,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.03867686,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide4,0.036175752,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.036088804,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,0.030352813,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.025457261,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.025164544
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide6,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.18503456,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.063857111,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide4,0.061802342,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.050460179,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.03867686,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide4,0.036175752,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.036088804,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,0.030352813,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.025457261,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.025164544
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide0,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide12,0.184572935,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide4,0.083821578,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.03496135,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.024812654,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.023108319,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide2,0.020699169,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide2,0.016506621,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.012827352,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,0.012387053,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide7,0.049742948
cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide3,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.183867577,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.156674627,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.154207223,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.138969378,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.056130677,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.054975328,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.054676731,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,0.054064613,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.043870058,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide6,0.043788803
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.182726416,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.151830769,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.134692147,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.132783429,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.101305687,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.095264098,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.092401673,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.088678872,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.081777575,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.076621201
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide15,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide8,0.182541972,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.083650085,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.083650085,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,0.04643261,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide5,0.02718372,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.024590965,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide4,0.020445995,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide5,0.020145933,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide4,0.019932437,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide13,0.018759541
cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide4,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.182496508,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.112549508,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.111500113,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.10523802,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.103787372,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.090818581,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide2,0.086892171,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide8,0.080190366,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide8,0.079716005,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.079352787
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide4,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide2,0.182428714,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide3,0.050560155,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide4,0.050367323,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide20,0.045576648,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide9,0.035935424,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,0.033966182,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.029405826,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide4,0.941786769,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide18,0.028578061,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide9,0.151955694
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide10,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.18162027,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide11,0.069336982,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,0.059523489,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.055805974,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.055763623,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.052608448,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide14,0.834512503,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide5,0.050153679,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.052608448,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide10,0.065954921
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide11,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.18162027,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide11,0.069336982,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,0.059523489,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.055805974,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.055763623,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.052608448,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide14,0.834512503,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide5,0.050153679,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.052608448,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide10,0.065954921
cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide5,0.181505211,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide4,0.11294451,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide3,0.109706181,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide7,0.085297963,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.066926534,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.055548418,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide2,0.072497997,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.04289989,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide2,0.070712785,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide0,0.066608582
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide6,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.179726796,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.179726796,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.092310328,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.092310328,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.076728988,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide8,0.057943181,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide23,0.044563546,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide8,0.856927363,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide13,0.076728988,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide17,0.044563546
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide7,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.179726796,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.179726796,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.092310328,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.092310328,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.076728988,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide8,0.057943181,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide23,0.044563546,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide8,0.856927363,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide13,0.076728988,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide17,0.044563546
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide13,0.179645968,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.161331221,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.150452758,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide1,0.115805087,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide23,0.067076456,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.064372489,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.951222147,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide10,0.117881685,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide0,0.07090523,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.075773389
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide1,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide2,0.179534192,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.061134787,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide4,0.047165904,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.034529572,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.032004536,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide12,0.031330613,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.02669089,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.024911598,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide3,0.020918113,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide3,0.179534192
cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide2,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.179495273,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.145315152,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.135309566,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.135309566,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.130070755,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,0.105656509,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.09330718,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.08828528,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.085733014,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.083698173
cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide2,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide1,0.179055827,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.157877375,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide1,0.132600181,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.113811113,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide2,0.056426204,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.052676436,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.045939834,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide4,0.040277,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,0.040049262,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.034107009
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide8,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide4,0.178425548,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.028963951,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.03862612,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide10,0.018604674,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.017632852,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide2,0.015526639,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide8,0.011659555,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide5,0.178425548,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide8,0.011659555,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide5,0.010287953
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide7,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.178137199,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.161104608,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.113062741,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide0,0.106646164,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.100599382,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide12,0.085714739,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide24,0.087978392,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.064357308,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.105592232,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide12,0.262287051
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide6,0.176982052,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide4,0.15171534,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide7,0.101405991,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide7,0.089086793,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.081429429,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.081247527,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide3,0.074342611,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide2,0.071624067,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.063178428,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,0.062880402
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide1,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.17669152,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.097954328,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.095652823,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.094566062,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide6,0.080413221,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.073463426,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide2,0.06871089,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide2,0.067786282,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.063693703,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,0.062367599
cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide7,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.176507483,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.133548954,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.092145266,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.091069779,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide15,0.076997857,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide7,0.066446784,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.065516818,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.065339882,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.064115552,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.059057146
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide8,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide3,0.175874541,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide5,0.115130721,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.177913083,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.05426869,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide7,0.064120619,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide1,0.0838272,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide22,0.723667065,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.08373855,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,0.081159313,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide21,0.20953858
cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide4,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,0.175586613,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide1,0.086948305,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide1,0.081802126,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide2,0.076393184,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.061136061,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.044970637,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.044429755,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.04195835,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide5,0.038103973,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.037002331
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide2,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.174974828,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.14090201,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide6,0.098810093,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide4,0.093054672,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.086208454,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.084222462,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.083121596,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.076350189,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.073594253,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.06839584
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide10,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.174751176,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide14,0.031156217,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.024110683,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.020712617,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.019824097,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,0.018887015,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide19,0.018504441,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide4,0.01846407,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide4,0.01846407,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide7,0.01726058
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide7,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide9,0.174636017,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide13,0.161251785,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide4,0.076554339,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.07649105,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide9,0.07457465,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.04529183,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide8,0.174636017,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide7,0.924570723,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide10,0.057588502,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide10,0.174636017
cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide3,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.174278132,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide10,0.087785891,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.084946944,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.076374429,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.068187156,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.046114429,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide6,0.045592621,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.041431329,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide6,0.040091937,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.038793881
cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide9,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.173913843,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.136265768,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.108411439,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.108411439,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide1,0.087056824,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide1,0.079984603,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.070863491,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.067293305,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide4,0.058614945,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.053782489
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##09_6-8-relative-measures_6.8._Relative_Measures.txt##slide0,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide0,0.173261925,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.099511181,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.040488746,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.03943015,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide0,0.037257938,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide4,0.024282727,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.021702573,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.018522584,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide11,0.017151412,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide6,0.016730435
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide1,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide24,0.172740932,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide4,0.103411075,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide8,0.080827977,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide3,0.047925906,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.047854062,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,0.042753864,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.042490404,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide7,0.039873111,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,0.038332928,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide6,0.035075449
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide1,0.172412866,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.092898714,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.088225573,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.063137603,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.054448207,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.052530205,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.051467204,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide2,0.049769799,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.040068047,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.039375966
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide1,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide4,0.171953798,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.14115398,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.116421063,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.112556469,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide1,0.108029163,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.099873384,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.14153874,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.051047244,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.047526782,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,0.042110386
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide17,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.171765959,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.113833322,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.102537378,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.086875026,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide22,0.81915231,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide3,0.394047354,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.06718438,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.075466099,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide18,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.171765959,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.113833322,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.102537378,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.086875026,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide22,0.81915231,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide3,0.394047354,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.06718438,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.075466099,,,,
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.171406787,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,0.14945172,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide4,0.114593319,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.093271875,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide5,0.078604042,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.075784984,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.070290113,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.064020101,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.0622249,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide3,0.054174144
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide3,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide16,0.170570011,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.046725979,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.022568015,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.020588913,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,0.015852161,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide4,0.993967182,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide8,0.135476266,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.037168055,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide27,0.01605602,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide16,0.068796916
cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide7,0.170054448,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.164713512,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide4,0.13780588,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.131606685,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.128709955,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.119698339,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.119180249,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.118571333,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide2,0.117939531,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.109107089
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide1,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide3,0.169872638,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide5,0.059612105,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.049078728,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide5,0.037101892,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.029579208,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##09_6-8-relative-measures_6.8._Relative_Measures.txt##slide1,0.026566984,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.025241404,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide15,0.024419006,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide4,0.0214939,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.019652097
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide9,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.169739383,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.082803457,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.080727371,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.07915875,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.078739208,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.076844368,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide8,0.993444636,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.09426679,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide1,0.381010249,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.09426679
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide1,0.169739383,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.082803457,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.080727371,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.07915875,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.078739208,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.076844368,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide8,0.993444636,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.09426679,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide1,0.381010249,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.09426679
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide13,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,0.169425292,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.159079696,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.132632023,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide14,0.090565614,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide6,0.084874363,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.07407831,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.057476645,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.057476645,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.053295575,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.050206227
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide13,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide11,0.169346144,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide16,0.100770648,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.107528198,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,0.943666125,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide12,0.117828021,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.128115192,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide15,0.745310687,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.107528198,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide21,0.140964606,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,0.943666125
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.168974781,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.127095896,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.108871977,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.103587828,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide12,0.10146806,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide0,0.070050959,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.067930254,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.058530086,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.056810806,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide3,0.05678134
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide6,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.168328818,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.093816826,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.088917778,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.087158336,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide1,0.086369289,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.083215779,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.071044466,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.068041628,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.06557298,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.063249178
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide11,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.168322377,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.148421631,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.099333561,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.091359239,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.072853756,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.067675068,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.06059446,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide17,0.052938428,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide0,0.051944055,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide0,0.049658089
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide9,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.168233541,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.046115145,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.036207225,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide5,0.026777423,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide8,0.02353133,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide4,0.021281908,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide4,0.021281908,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.019085656,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide9,0.019009088,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.01888588
cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.167043142,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide7,0.111037455,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.105626951,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,0.095092705,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.086477167,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.078483979,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide8,0.078004223,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.077699856,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.077699856,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.06957499
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.166949256,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.15179962,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.149007832,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.117058955,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.116902623,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.114526266,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.109311747,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide2,0.108370902,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.103662502,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide1,0.103631436
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide12,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.165495653,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.114469845,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.086159767,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide7,0.068289899,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide5,0.057367339,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.053581808,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.042952884,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.042952884,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide9,0.024723967,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide23,0.45274164
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide12,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.165388562,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide11,0.065735854,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,0.054558377,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.051790137,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.050765884,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.048311222,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide2,0.822867393,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide5,0.04949642,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.048311222,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide10,0.063845526
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide13,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.165388562,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide11,0.065735854,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,0.054558377,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.051790137,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.050765884,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.048311222,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide2,0.822867393,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide5,0.04949642,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.048311222,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide10,0.063845526
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,0.165242014,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.076750401,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.047274778,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.047071275,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide10,0.044614029,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.975288058,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide20,0.100012297,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide5,0.051362735,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.288639255,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.070216273
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide8,0.164390303,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide28,0.111071303,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.085608743,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.075669862,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide5,0.075669862,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide7,0.957409842,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide5,0.082356894,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide3,0.142341987,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide14,0.100708597,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide23,0.076661374
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide25,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide2,0.1640168,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.095293019,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,0.057634683,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide5,0.047238287,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide23,0.028274582,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide27,0.02681762,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.025035429,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.023896972,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.02207184,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.02207184
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide17,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide1,0.163928953,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.148193606,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide4,0.108223373,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide8,0.105396292,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.099523481,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide13,0.097600414,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.088453077,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide17,0.9509459,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide9,0.2336077,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.133272061
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide3,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide2,0.163898307,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide9,0.050401077,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.049929045,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide5,0.045232002,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide7,0.044168461,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.036789035,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide2,0.966973052,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide9,0.12610276,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide13,0.045981519,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,0.044690539
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide7,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.163585223,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide3,0.120491928,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.096576345,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide6,0.972292558,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.080793288,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide2,0.163870697,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide17,0.090397965,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.15187724,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide3,0.316367741,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.120491928
cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide6,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide3,0.162873345,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide3,0.053761661,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide0,0.033773453,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide6,0.031290262,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide2,0.132517794,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide4,0.026813923,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.024158792,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide9,0.021465196,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide4,0.020626015,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide2,0.020192285
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide10,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.162848783,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide7,0.13598342,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.133961142,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.110330982,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.104849865,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.093822914,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.068120922,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.067626365,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.06472673,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide9,0.06251873
cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide4,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide5,0.162795779,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide2,0.118287636,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.074521909,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.042688405,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.0384562,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide6,0.030090995,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide10,0.029265675,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.027553287,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.027452241,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide8,0.027139855
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide5,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide10,0.162194564,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.069808459,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.029375597,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.028088339,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide2,0.025191513,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide2,0.025191513,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide9,0.021226499,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide16,0.158772606,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide4,0.980235515,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.053691803
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide11,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.162188217,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.162188217,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.062522346,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.050377948,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.050377948,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.022197866,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.018295497,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide21,0.514660521,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.148371105,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.941633243
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide3,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide17,0.161967067,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.076589565,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.057761332,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.052790468,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide3,0.951074951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.072674259,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.111540592,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide8,0.254588574,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide18,0.1447326,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.057473579
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide0,0.1614387,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.160295464,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide4,0.132619034,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.12712099,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.119700977,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.112235837,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide4,0.107191864,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.104259888,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide9,0.094122232,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.084489183
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide11,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.161136934,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.155770685,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.143150097,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,0.129797703,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.110182946,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.105614044,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide1,0.095178722,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.081585301,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.07494641,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.072249517
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide15,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.161095773,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.128889527,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.099944369,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.063629625,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide22,0.722918854,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide3,0.33145665,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide0,0.078947828,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide14,0.874819104,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide15,0.099944369,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.063629625
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide12,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.160744753,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide8,0.118884915,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.105581179,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide7,0.093941055,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide20,0.571356263,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide5,0.267601078,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.097170044,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.674767467,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,0.092172649,,
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide1,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.160558362,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.160558362,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.116991114,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.109038418,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.095583309,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.06066976,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide5,0.055059232,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.054922837,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.039951807,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,0.03969892
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide1,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.160538477,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.146381707,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide1,0.13278804,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide2,0.126694476,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.08256353,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.08256353,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide0,0.081736785,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.071029777,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.068459944,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide4,0.123264153
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide19,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.160319534,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.156123604,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.078854504,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.078854504,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.075718858,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,0.072935335,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide17,0.055450815,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.052962049,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.051657083,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide9,0.060583794
cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide2,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.159735775,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide11,0.117479967,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide3,0.079136435,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.071332998,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.070172473,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.06815123,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.06815123,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide5,0.060541967,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide1,0.0569914,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.050786061
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide24,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.159401751,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.139954945,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.135180773,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide21,0.724772259,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide4,0.286387256,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.099860957,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide8,0.083106836,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide10,0.755574237,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.666658455,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.139954945
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide9,0.159346596,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,0.113667006,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.111815409,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.102341942,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.085715715,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide18,0.042821359,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide10,0.159346596,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide11,0.979031266,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.099552021,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide8,0.159346596
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide6,0.159336278,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.10410122,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.095101297,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.091664235,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.085914181,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.084172899,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide3,0.080300627,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide6,0.080230031,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.079157484,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.074461889
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide8,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide2,0.159257169,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.078157944,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide2,0.074924672,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide5,0.072928702,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide4,0.072703239,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide4,0.071681984,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide9,0.992564086,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.103654714,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide1,0.343227256,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.103654714
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide5,0.159128063,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.158887931,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide13,0.091805924,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide1,0.068422695,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.063701767,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide0,0.062033604,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide2,0.058028262,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.056246976,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide3,0.052161323,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.951130416
cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.159052147,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,0.157663521,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide13,0.157663521,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide4,0.13211227,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide4,0.13211227,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide8,0.130679107,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.101266645,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.101266645,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.101266645,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide6,0.099727
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide4,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.157198894,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.075261468,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.053631403,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.044417053,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide4,0.042082636,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide4,0.039781586,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.038933564,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,0.03626382,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.030033975,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.027408826
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide9,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.157014369,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.107121793,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.081893034,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.08065292,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.059133608,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide9,0.052116236,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.041081967,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.040661836,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.031904621,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.03111679
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide2,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide24,0.156693899,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.08161726,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.071563345,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.070146546,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.061587055,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.056346978,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.056268209,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide5,0.055987856,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide1,0.05368628,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.051292729
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.156670818,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide14,0.12541647,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.107861907,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.065391901,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide17,0.05642804,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide13,0.052795631,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide8,0.04355054,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide1,0.037751531,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide3,0.050771996,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.065039427
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,0.155650566,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.090642541,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.068674009,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.203317275,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.981587401,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide5,0.05050451,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide22,0.109642754,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.081233914,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide7,0.056339661,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.248856888
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,0.155536604,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.089685179,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.057781128,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide24,0.211451899,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.977767,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide5,0.049176455,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide22,0.094147299,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.079941332,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.256428999,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide7,0.048701251
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,0.155536604,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.089685179,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.057781128,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide24,0.211451899,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.977767,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide5,0.049176455,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide22,0.094147299,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.079941332,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.256428999,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide7,0.048701251
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide5,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,0.155536604,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.089685179,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.057781128,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide24,0.211451899,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.977767,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide5,0.049176455,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide22,0.094147299,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.079941332,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.256428999,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide7,0.048701251
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide7,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide3,0.15553383,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide2,0.073345627,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide0,0.06338475,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.061547512,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide9,0.057733269,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide8,0.047689403,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide15,0.046687391,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide5,0.046475271,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide6,0.045477501,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide7,0.043436317
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide3,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.155321137,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.101111456,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide0,0.093406505,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide7,0.080906451,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.072299645,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide6,0.060254751,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide2,0.059565688,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide7,0.054317704,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.048865604,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.044058843
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide0,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide0,0.155193178,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide8,0.07524958,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.067940397,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.062656613,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.054755197,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.049033657,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.04813506,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.043441318,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.040199217,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide0,0.037476433
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.154170832,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,0.049217442,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide11,0.044707707,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide0,0.030303115,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.025092737,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide4,0.024352792,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.023818584,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.023244305,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.020623355,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.019238623
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide15,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.15343673,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.113837797,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.11197712,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.111886335,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.103509501,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.097271767,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.094764915,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide16,0.092640281,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.086284868,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.08453302
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide10,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide8,0.153280719,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide14,0.061442911,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.022775361,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide10,0.013849362,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.013445392,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide2,0.012435378,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide10,0.011320651,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide5,0.010221856,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide4,0.153280719,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide2,0.009330327
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide6,0.153005933,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.126042143,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.110273851,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.102825974,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,0.097975148,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide15,0.093600159,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.951158987,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.277663339,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide8,0.120090504,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide18,0.093600159
cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide0,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide0,0.152994647,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide0,0.144021388,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.141386433,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.136937464,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide0,0.128072005,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide0,0.127368234,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide0,0.12219925,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide0,0.11870197,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide0,0.109769946,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide0,0.107107276
cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide2,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.152965751,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide2,0.103736088,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.100090089,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.100090089,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.096572816,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.095185277,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide2,0.093213156,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.092501494,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide4,0.090096178,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.082313469
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide16,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.15286707,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.15286707,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.064817246,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.045659863,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.045659863,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.024296605,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.023010043,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide19,0.517672723,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.330620578,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.795052447
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide8,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.152726315,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide16,0.11508393,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.06389685,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.055051989,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.052555403,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide5,0.038036922,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide8,0.951077494,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide3,0.038088216,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide17,0.042241962,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide12,0.047635436
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.152323275,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide24,0.112652894,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide18,0.094796063,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.077219449,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.060006579,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide7,0.056292991,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.053440864,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.045569543,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide10,0.981232812,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide25,0.073271837
cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide6,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide6,0.15216804,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide11,0.151247873,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.115910018,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.104052433,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.09708852,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.094791476,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.094310871,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.084608588,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide11,0.078408875,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide10,0.077666913
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide12,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide10,0.152039719,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide8,0.039888715,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide0,0.02959244,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide6,0.014563757,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide7,0.014366111,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide4,0.010142671,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide0,0.009138109,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide14,0.014988887,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide9,0.152039719,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide12,0.884920033
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide11,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.151951932,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.125372793,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.075074067,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.058964105,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.051278985,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.047749455,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide1,0.044150533,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,0.043133077,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.037803859,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide5,0.035552898
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide1,0.151936323,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide8,0.064892478,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide1,0.027160558,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.026588305,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide23,0.025441865,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide16,0.023887695,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.022839343,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.020622451,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide5,0.150184678,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide17,0.019848831
cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide1,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.150912477,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.094100754,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide6,0.061941709,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.025163694,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide18,0.020621958,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.019480088,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.018061635,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.017108629,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide5,0.077670555,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide7,0.025524511
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide6,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,0.150310378,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.090781529,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.067723578,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.199657899,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.981658353,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide21,0.107779825,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide4,0.048691887,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,0.150310378,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.081182631,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.244873639
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.150299134,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.084004876,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.071869004,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.071869004,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide8,0.06046441,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide8,0.06046441,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.060344246,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,0.059657663,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide16,0.051403913,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.047821353
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide16,0.149679208,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,0.113930903,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.106380495,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.091066084,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.951151959,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.260528095,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide25,0.149679208,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.144359213,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide8,0.104546664,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.424830053
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide5,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.149648357,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.115550223,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.070830248,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.058047793,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.057356738,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.052879306,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,0.045034749,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.044537117,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.043981565,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,0.057883497
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide17,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.149091024,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.149091024,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.056878143,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.041243542,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.041243542,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.02617765,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.021917555,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide20,0.473041076,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.324017475,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.775857179
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.148550033,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide1,0.114815451,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.111678646,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.07373207,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.054547602,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.05177315,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.051088835,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide5,0.05085989,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide2,0.0452424,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide2,0.0452424
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide0,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide5,0.148277234,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide24,0.141655997,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.077174878,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide28,0.053380201,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide10,0.066392517,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide0,0.515435422,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.189952637,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide4,0.09792722,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide29,0.06923951,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide9,0.044105194
cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide15,0.14815597,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.078770871,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.066690136,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.060399953,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.05734878,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.055429723,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.054612918,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.054428765,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.053043426,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.05079782
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide17,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.147917142,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide9,0.087767178,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.082971069,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.049283028,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.046022311,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.039403293,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide1,0.03258256,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.030785113,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.028447277,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,0.028047215
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide7,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.14778038,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide0,0.127573883,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.111716916,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide3,0.077646328,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide3,0.077646328,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.072409706,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.066151361,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide1,0.065121436,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.059317215,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide0,0.04879715
cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide7,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide3,0.147370801,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.048139274,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide4,0.041974185,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide4,0.037117832,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.036606572,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,0.027790447,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide3,0.027134087,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide2,0.097233025,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide0,0.031444713,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide5,0.044684326
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide2,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide2,0.146903532,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide1,0.0741429,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.063492972,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide3,0.034577745,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide3,0.032392528,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.024277629,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.023845061,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide10,0.019346162,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide1,0.016801767,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.01565515
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide2,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide3,0.146868094,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide9,0.098456736,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.071579423,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.066794026,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,0.040991335,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.038701171,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.037478033,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.036515167,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.035797493,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide9,0.034009942
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide5,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.146837894,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide11,0.064215519,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,0.049244683,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.048131096,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.047564979,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.045552439,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide14,0.797103438,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide5,0.048519983,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.045552439,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide10,0.063233749
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide6,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.146837894,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide11,0.064215519,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,0.049244683,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.048131096,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.047564979,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.045552439,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide14,0.797103438,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide5,0.048519983,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.045552439,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide10,0.063233749
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide4,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide16,0.146460999,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.033853991,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.020800825,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.018946101,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide2,0.014587346,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide2,0.014587346,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide8,0.134933617,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,0.014513263,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide3,0.994407353,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.03060063
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide2,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.146109988,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide4,0.115958871,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.06486492,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide4,0.059117366,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide3,0.058592392,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.058031283,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,0.057570618,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.049438161,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.049328489,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.047364352
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide6,0.145896654,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.134106987,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide20,0.06616458,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide4,0.062466106,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide12,0.061363795,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.057437554,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.056919036,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide1,0.052256447,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.051586827,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,0.049158704
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide10,0.145467093,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide8,0.132725425,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.109361048,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.106013298,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide26,0.064234401,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.951179236,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide10,0.150439084,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,0.145467093,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide6,0.130653842,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide19,0.064234401
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide23,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.144726128,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.129399929,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.072533647,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide8,0.049884648,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.046063246,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.044505444,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.042709577,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide12,0.328004835,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide8,0.677333089,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.047776537
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.144316992,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.09194951,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide2,0.055207586,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide23,0.055060486,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide9,0.050731893,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.047034305,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.043466262,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.042198287,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.040047931,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide1,0.038722668
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide6,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.14421247,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide6,0.11284491,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide7,0.10128485,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide2,0.098016572,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide8,0.080533502,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide7,0.074980656,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.074327925,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.068994023,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide6,0.066065286,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,0.063564282
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide6,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.143925403,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.13074471,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide13,0.121342962,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.078467328,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.074524012,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.067747721,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide3,0.117651973,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.077109771,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide6,0.125209224,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide6,0.951072422
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.143872424,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.134940811,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.098735946,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide8,0.088982928,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.086897958,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.086310437,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.085938261,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.082247373,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.069660885,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide7,0.06704103
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide5,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide5,0.142889085,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide11,0.131322244,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.104841801,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.027344126,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.022089131,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide1,0.095482962,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide3,0.018625963,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide4,0.017166487,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide3,0.014759877,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide11,0.01993821
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide16,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide31,0.142691125,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.09612126,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.082281438,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.067459346,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.062658932,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.062658932,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.047078166,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.039993251,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide17,0.993244387,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide27,0.119321433
cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide5,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.142346947,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide2,0.141663346,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.10205833,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.082173787,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.079107058,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.073079133,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.072201924,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.066003482,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide2,0.062332887,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.058644697
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide8,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.142320649,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.121741712,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.093363621,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide1,0.092882695,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide5,0.08675749,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide4,0.073119481,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.06558931,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.060720317,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.060720317,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide11,0.050721886
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide12,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide8,0.142209612,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.138398478,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide0,0.090301051,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide1,0.071018309,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.03822144,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide4,0.031346177,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.028937002,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide0,0.028643832,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.028218568,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.027279563
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.141869456,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.064407168,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.06174327,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide12,0.058109028,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,0.05265609,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide15,0.847579721,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide10,0.05457675,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.056224808,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,0.057017732,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,0.05265609
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide9,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.141869456,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.064407168,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.06174327,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide12,0.058109028,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,0.05265609,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide15,0.847579721,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide10,0.05457675,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.056224808,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,0.057017732,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide16,0.05265609
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide23,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.141731122,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.069693277,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.058189124,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide14,0.752663674,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.161047045,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.078501654,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide20,0.811459828,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.058189124,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.069693277,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,0.05931136
cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide4,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.141109923,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide9,0.057960301,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,0.04881096,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide9,0.037805673,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide9,0.037805673,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.036828712,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide9,0.026274606,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide2,0.025859659,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide7,0.022157773,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide10,0.021995449
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide15,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.141056502,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide22,0.080543268,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.087073106,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide12,0.089776855,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,0.71068115,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide0,0.44198642,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.087073106,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.125021633,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide11,0.127558208,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide15,0.935802897
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide4,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.140432032,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide1,0.130530898,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.078671989,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.056847257,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide2,0.044200777,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.043273458,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.039732785,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide0,0.026641477,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide7,0.02446308,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide2,0.031395564
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide11,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.139852674,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.029729344,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.028821175,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.028206425,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.027911143,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide0,0.026397224,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.026307504,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide4,0.024195133,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.021810956,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,0.02115529
cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide6,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide2,0.138872779,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide2,0.053366596,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.039127325,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.033309479,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.027359379,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.024756976,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide12,0.022613123,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide12,0.022613123,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide12,0.020865161,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.020234406
cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide7,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide5,0.138805312,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide2,0.100050273,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.041432613,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide2,0.037532679,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide10,0.030955591,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide6,0.028688978,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.02524664,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide7,0.024853543,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide8,0.024049788,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.022755367
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide1,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide14,0.13855217,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.065780061,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.03961701,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide3,0.022555199,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide9,0.017185052,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,0.015387426,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.014271377,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.065684239,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.012704041,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide5,0.968161004
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide2,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide14,0.13855217,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.065780061,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.03961701,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide3,0.022555199,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide9,0.017185052,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,0.015387426,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.014271377,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.065684239,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.012704041,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide5,0.968161004
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide3,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide14,0.13855217,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.065780061,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.03961701,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide3,0.022555199,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide9,0.017185052,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,0.015387426,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.014271377,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.065684239,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.012704041,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide5,0.968161004
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide4,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide14,0.13855217,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.065780061,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.03961701,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide3,0.022555199,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide9,0.017185052,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,0.015387426,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.014271377,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.065684239,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.012704041,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide5,0.968161004
cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.137590722,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.109531696,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.109531696,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide9,0.103921291,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.094280168,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.086441335,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.085544694,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.079892101,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,0.0615056,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.059935688
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide7,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.137471255,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.068542352,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.048578823,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide7,0.039770522,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.037391613,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide19,0.032204022,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.031180892,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide4,0.037769089,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide7,0.942710079,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide10,0.033197131
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide10,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide21,0.137368139,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.12699033,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide19,0.048983258,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide20,0.064868759,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.043941326,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide10,0.040451895,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide3,0.040451895,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.035828657,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.034995821,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide2,0.047640216
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide3,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.137357384,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide2,0.079440952,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.072387156,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.057051768,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.051054206,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide4,0.045965425,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide5,0.045218381,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,0.086652057,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide6,0.042823547,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide1,0.042172962
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide20,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.136826292,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide10,0.07014269,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide20,0.951141164,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.062939572,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide24,0.115391873,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide15,0.09153702,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide24,0.115582987,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide23,0.502340212,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide18,0.25255862,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide31,0.065655936
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide18,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.136522671,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide9,0.10385914,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.067867809,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.059958218,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.052603424,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.052182067,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide7,0.051314225,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.041083314,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide19,0.032707969,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide2,0.036936871
cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide2,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide3,0.136191792,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.118814494,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.036611318,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide2,0.024038002,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide2,0.024038002,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.021622617,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.021572847,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.021572847,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.021572847,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide2,0.021020801
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.135967126,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.102408172,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.078762689,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide1,0.062811555,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide7,0.060973849,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.072642496,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide0,0.282497118,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.099997656,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide14,0.060973849,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide4,0.062943101
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide20,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.135682476,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.109183293,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.075397423,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.075397423,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.066263885,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.066263885,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.066259305,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.064812869,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.064812869,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide1,0.061308519
cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide8,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.135262658,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.113813534,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide5,0.110162779,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.095178615,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.082924443,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.0757043,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.067933246,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.066574865,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.054889038,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.052845271
cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide3,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.134039251,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide2,0.106843114,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.077831901,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.050861731,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.050265372,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.050265372,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.050265372,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.049008477,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide4,0.048621037,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.044045839
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide20,0.133990797,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide16,0.075395096,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.08028876,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide14,0.935276925,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide11,0.096393627,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.08028876,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide15,0.710465388,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide22,0.117529025,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide0,0.382245615,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide6,0.08028876
cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.133639441,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.058457289,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide6,0.04874894,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide14,0.048619294,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide1,0.031549489,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.03100994,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.029438002,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.028149118,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide5,0.027934624,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide1,0.027485128
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide3,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide9,0.133601936,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.108468263,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.094950448,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide7,0.069644203,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.066940113,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.064451939,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.062127208,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.061177478,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.056986548,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.056805863
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide8,0.133252983,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.118015389,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.103421237,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide10,0.0891276,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide15,0.078014616,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.951199023,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide6,0.131587029,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide23,0.078014616,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide11,0.09829878,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide11,0.086458831
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide1,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.133067317,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.104755188,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide3,0.095279732,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide3,0.095279732,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide0,0.094325947,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide5,0.074810759,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.061282939,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.051603955,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.050350305,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide4,0.044981335
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide21,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide25,0.132937234,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide9,0.086869622,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide21,0.951102609,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.074351702,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide16,0.077878523,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide15,0.091389859,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide24,0.164808872,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.091385379,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide23,0.347419394,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide13,0.162030986
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide11,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.132620283,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.116724832,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.113031489,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.113031489,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.093766538,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.08649302,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.080724765,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.076001807,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.076001807,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.075880434
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide2,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide2,0.132375198,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.082942466,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide25,0.060943391,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide7,0.060558034,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.048411988,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.04587822,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.04587822,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.045140729,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide3,0.97115201,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide7,0.057289192
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.132014273,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.109392558,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.101319852,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide10,0.09854687,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide18,0.051714997,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide10,0.11568236,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide7,0.992909309,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.12860212,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide11,0.105185739,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide11,0.09262313
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide4,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.131548221,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.12920715,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.107906429,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.107849306,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide12,0.101792394,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.060853616,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide12,0.048387145,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.040192181,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide0,0.07753037,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide2,0.059925476
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide1,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.13138068,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.082482999,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide12,0.076398189,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.053377045,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,0.049886203,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.048393613,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide8,0.04652767,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.046464016,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide9,0.041236678,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.039794344
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide17,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide13,0.131143594,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,0.087974137,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.060293252,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide16,0.989729127,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide11,0.065000389,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide22,0.197999312,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.096028684,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide14,0.067309096,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide11,0.194756534,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.096590347
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.130569957,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.091170101,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.087216843,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,0.078415019,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide24,0.042098024,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide7,0.993901708,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.046396434,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.097286975,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.126450792,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide10,0.108273118
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide1,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide11,0.130263168,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.124703851,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.095158149,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide4,0.079480887,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.070529849,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.064082996,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.060416602,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide14,0.059802886,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.046543175,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide8,0.04586961
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide14,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide2,0.130069154,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide4,0.103009014,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.101620438,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.096053241,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.072024036,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide3,0.044549091,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.042800756,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide10,0.040890292,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide10,0.084943851,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide15,0.029216746
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide7,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide2,0.129520621,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide2,0.023559228,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide8,0.015043676,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,0.013136163,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide8,0.013005884,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide2,0.012214898,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide19,0.011844081,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide5,0.010844895,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.010412553,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide5,0.010121073
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide7,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide3,0.129496124,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.052242309,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.037060009,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.036745544,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide11,0.036525848,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide0,0.034408162,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide3,0.034367265,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.03375108,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide6,0.033722126,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide3,0.032684149
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide7,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.129338601,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.091890702,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.08853912,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide10,0.079559029,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide19,0.04158686,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide8,0.993901034,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.045052677,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.095414835,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.125259804,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide10,0.108646761
cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide0,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide0,0.129288195,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide0,0.121005694,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide0,0.121005694,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide0,0.121005694,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide0,0.086278477,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide0,0.070749521,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.044743583,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide0,0.043098763,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide0,0.041302338,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide0,0.038920366
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide1,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide1,0.128292612,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.085008639,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.07645833,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.07645833,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide2,0.068487633,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide1,0.055639022,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.053387659,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.05157639,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.04951149,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.047393945
cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide4,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide11,0.128096957,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide4,0.104884556,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide4,0.104884556,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.086283503,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide13,0.080695662,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.0758818,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.0758818,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.0758818,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide6,0.059855337,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.054361991
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide9,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.127975642,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide24,0.067427401,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide5,0.054794789,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide2,0.053161134,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.061329859,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.045668999,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide2,0.042020672,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.041208263,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.982634636,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.04394069
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide10,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.127975642,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide24,0.067427401,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide5,0.054794789,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide2,0.053161134,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.061329859,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.045668999,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide2,0.042020672,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.041208263,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.982634636,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.04394069
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide11,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.127975642,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide24,0.067427401,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide5,0.054794789,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide2,0.053161134,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.061329859,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.045668999,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide2,0.042020672,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.041208263,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.982634636,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.04394069
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide12,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.127975642,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide24,0.067427401,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide5,0.054794789,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide2,0.053161134,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.061329859,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.045668999,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide2,0.042020672,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.041208263,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.982634636,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.04394069
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide13,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.127975642,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide24,0.067427401,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide5,0.054794789,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide2,0.053161134,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide26,0.061329859,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide3,0.045668999,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide2,0.042020672,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.041208263,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.982634636,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.04394069
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide28,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide2,0.127961661,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide2,0.085220696,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.082644037,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.071456119,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.070665111,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.069913633,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.044761597,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.030560464,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.028452348,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.028452348
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide3,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.127954556,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.109703146,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.108704984,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide4,0.098924216,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.084927915,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide12,0.073888152,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.160861992,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide2,0.211259595,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.112052916,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide3,0.942063
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide6,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.127918133,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.091027661,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.086608923,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,0.077604566,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide12,0.040839447,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide7,0.993706562,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.045053176,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.093797073,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.123651299,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide10,0.106054521
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,0.127617267,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.120835339,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.107824112,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,0.101081079,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.096933514,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide1,0.091299209,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.09052156,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,0.077803246,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.076267151,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide2,0.074897708
cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide6,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.127149777,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide5,0.122630724,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.057967056,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.057767134,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.05090696,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide5,0.050583685,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide4,0.050126248,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.048892284,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.048789685,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.047129466
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide6,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.12714494,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide8,0.049150802,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide5,0.04381801,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.041794474,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide17,0.03098288,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide6,0.026427492,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.024429006,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.024401766,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.024384846,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.022962206
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide11,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide4,0.126759732,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide1,0.104289127,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide5,0.078530778,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide2,0.059871205,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.044026216,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.037084148,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide4,0.036876996,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,0.035928594,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,0.033784752,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.033476943
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide3,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.126215996,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.088522539,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.069437219,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.065836264,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.061215743,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.060084939,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.058976599,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide6,0.047401914,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.045528813,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide1,0.044193446
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide6,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.126178294,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.086401823,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.082759767,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide19,0.080875008,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.067723334,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,0.97972372,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.069031751,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide2,0.179872108,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide17,0.076181507,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.120203132
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide10,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.125924881,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.103529944,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.083524489,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide7,0.042825468,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.040731949,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.040731949,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide5,0.039815376,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.036522091,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.019289947,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide11,0.018914974
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide0,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide0,0.125908874,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide3,0.089177087,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide3,0.089177087,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.066251624,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.037529497,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide7,0.114064426,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide1,0.03109764,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide0,0.02863351,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.027370841,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide0,0.024515455
cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide6,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.124153469,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.115843081,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.09507986,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.09507986,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.084742564,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,0.083557669,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.083250618,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.078209099,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.075602446,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.075602446
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide25,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.124031827,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.10820129,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.088590503,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide20,0.63798823,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide5,0.231402549,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide10,0.571676654,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide4,0.081889148,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.49794578,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide14,0.088590503,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.10820129
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide0,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide0,0.123985528,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide3,0.076912015,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide3,0.076912015,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.064208923,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.037144837,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.029786621,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.018397418,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.0182434,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.061318114,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide9,0.015073
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,0.123526333,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide9,0.081900814,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.078339967,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,0.072026749,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.07132005,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.066811173,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.054680683,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide2,0.040404381,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide5,0.054179331,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide2,0.055996261
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide16,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.122712509,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.052643108,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.047349225,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide14,0.734784848,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.146993308,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.049270623,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.053765593,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide20,0.96338906,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.047349225,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.146993308
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide27,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide2,0.122686472,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.09452552,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.036177934,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.033220797,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide4,0.022355476,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide23,0.019867772,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide17,0.019380229,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.016434572,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide11,0.016407081,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide5,0.01796117
cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide4,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide5,0.122655552,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.090063121,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.070515547,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.061758527,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.058332242,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.044531848,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.042101923,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.039945953,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide1,0.036819358,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide1,0.036819358
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide16,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.122594558,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide17,0.097039911,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.053254968,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide17,0.990700616,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide12,0.057320898,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide22,0.135049929,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.071608609,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide13,0.064997776,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide15,0.090105321,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide11,0.072236932
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.122582969,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide4,0.104696836,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.084887535,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.079665635,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide1,0.060477142,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide11,0.050214393,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.048082551,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.048078751,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide5,0.044249686,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,0.042132736
cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide5,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.122549881,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide15,0.075142135,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide1,0.069012287,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide1,0.033841387,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide6,0.02423483,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide18,0.020735111,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.020728053,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.020641872,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide17,0.020474285,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide4,0.019507918
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.122340905,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide8,0.120258153,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide2,0.085365699,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide31,0.041703604,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide15,0.037079365,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.045741858,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.031082993,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.02464097,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide9,0.020946237,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.020417426
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide11,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.121799198,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide6,0.066065981,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide7,0.048456146,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide6,0.04353161,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide13,0.037142796,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide10,0.035803855,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.031968599,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide6,0.030950954,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide3,0.049225167,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide4,0.02578777
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide1,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.120750051,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide16,0.081520589,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.080776216,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.059037551,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide3,0.058325374,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide7,0.053819276,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.051089599,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.047245658,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.046726895,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.046561754
cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.120498861,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide6,0.073049477,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide3,0.067782517,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.05648419,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,0.052934929,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide13,0.052934929,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,0.048965973,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide2,0.048379993,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide4,0.043037499,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.041169467
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide1,0.119985202,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.072369749,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide3,0.054245368,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide1,0.053652569,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.033970468,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.031775267,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide0,0.03122836,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide2,0.119985202,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide0,0.029813,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide0,0.029720247
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide4,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.119888101,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.117353173,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.02772336,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide14,0.021655514,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide6,0.054580647,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide22,0.021556418,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.044586621,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.018477196,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.017704006,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide3,0.037245031
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide2,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.1192469,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.091914938,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.087743523,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.085499322,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.08079908,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.079596083,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.0724537,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.052168454,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,0.050739731,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.049818421
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide2,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.119238634,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.103413976,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.101153752,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,0.0836161,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.074020511,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,0.070723837,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.068736133,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.0641977,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.061281717,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.058612769
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide3,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide2,0.119102213,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,0.073763753,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide16,0.037995654,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.030873104,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.024613421,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##09_6-8-relative-measures_6.8._Relative_Measures.txt##slide1,0.024546092,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide1,0.023998638,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.022653971,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.022445013,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide4,0.021813018
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide13,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.119080258,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.077185566,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide6,0.048201326,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide4,0.043550543,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.042759091,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.036733415,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide2,0.035017564,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide4,0.034766582,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.032487265,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide3,0.032170183
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide5,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide20,0.118997876,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide4,0.090176801,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.089852531,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide10,0.076457592,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide6,0.986255613,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide2,0.26676116,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.077436038,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide3,0.170417711,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.115635356,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.090176801
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide14,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.118777332,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.118777332,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.048664742,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.038201148,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.038201148,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.01684421,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide10,0.016020806,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide19,0.480273962,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.156267676,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.945626226
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide2,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,0.11862431,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide1,0.070986412,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.064248463,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.059722205,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide2,0.057391741,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,0.053110148,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.050352145,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.047976967,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.047646189,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.044859398
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide2,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.118509897,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.085369368,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.056622984,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.023418631,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.017678481,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.017678481,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide1,0.017512653,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.016797146,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide13,0.0429266,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide7,0.016372921
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide14,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide6,0.117956544,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.052865632,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,0.050084027,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide2,0.041034214,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.038207619,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.037681768,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.034745705,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide9,0.034412171,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide9,0.034274571,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.033222471
language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide11,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.117723166,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.091901652,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide5,0.028503013,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.026609174,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide8,0.026009885,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide5,0.023878396,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide7,0.022649488,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide16,0.021541997,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide9,0.021381234,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide4,0.021019565
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide17,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide31,0.11758772,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.085729586,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.070058812,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.055844742,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.055844742,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,0.051477973,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.045211737,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.037772252,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide27,0.123868931,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide0,0.157152561
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide10,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.117574203,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.117574203,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.055818252,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.037479705,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.037479705,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.022204217,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.020488577,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide22,0.550476654,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.298187968,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.776597396
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,0.117523281,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.060745808,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide13,0.04437794,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.038269039,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.144025704,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.944426173,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide20,0.143717888,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide13,0.04437794,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,0.117523281,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.056147795
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide9,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide4,0.117467637,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.09557154,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.094955274,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.088830093,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.084987977,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.0631301,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.061625165,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.057452029,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.055337962,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.052822529
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide1,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.117400997,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.105513822,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.075269226,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide1,0.056572154,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide1,0.035858432,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide12,0.030440084,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide5,0.025239713,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide5,0.025239713,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.022639168,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide13,0.048562639
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide0,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide0,0.117252253,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide8,0.112904068,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.100813787,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide26,0.096643875,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.086189364,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.070363246,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide0,0.069843945,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide8,0.069430003,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.066495611,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.054733092
language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide6,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.117047651,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.101867496,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.101728723,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.09109041,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.086359881,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide3,0.08251093,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.082242619,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.080486165,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.071620405,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide2,0.069452595
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide0,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide12,0.116788172,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide9,0.083226832,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide4,0.040286796,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.027125856,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide8,0.019715167,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.019057993,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide3,0.011274622,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide3,0.011274622,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.010891978,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.009827458
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide12,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.116770288,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.068024763,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.066110851,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.041737988,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.022450974,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide7,0.018932127,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide10,0.017497257,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide3,0.027363468,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide7,0.015215957,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide6,0.013575692
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide17,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide12,0.116739514,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide13,0.11242261,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.108977442,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide12,0.679681749,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide13,0.081889227,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide16,0.098696172,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide5,0.076210189,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide5,0.655399757,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide8,0.091920503,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide12,0.105946024
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.116252315,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide7,0.097703665,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.082578033,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,0.07639734,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide4,0.052541914,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide4,0.052541914,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.046158532,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide9,0.991123032,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.083159284,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.105587831
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide9,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.11620291,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide0,0.041088324,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.037747792,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.032077046,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide23,0.029040534,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.028544154,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide1,0.036796625,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide5,0.041895292,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.026212714,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide9,0.951155083
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide4,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide9,0.116066652,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.073178348,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.060495604,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.058825222,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.053417667,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.048476944,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.04696399,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.046794501,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.043871966,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.043078327
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide7,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide14,0.115556644,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.058192237,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.034282163,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide3,0.016523555,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.01530437,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide9,0.014836511,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,0.013665326,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.012979506,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.012954182,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.053550289
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide14,0.115556644,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.058192237,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.034282163,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide3,0.016523555,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.01530437,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide9,0.014836511,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,0.013665326,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.012979506,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.012954182,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.053550289
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide9,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide14,0.115556644,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.058192237,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.034282163,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide3,0.016523555,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.01530437,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide9,0.014836511,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,0.013665326,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.012979506,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.012954182,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.053550289
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide10,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide14,0.115556644,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.058192237,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.034282163,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide3,0.016523555,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.01530437,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide9,0.014836511,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,0.013665326,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.012979506,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.012954182,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.053550289
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide4,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide9,0.115506327,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide3,0.080612408,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.054834506,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide13,0.048917142,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.040626338,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide3,0.037132968,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide3,0.037132968,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide9,0.048983742,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.034044804,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide4,0.033155213
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide12,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.115153179,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.04705925,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.046819653,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.045018514,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.042732841,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide9,0.04174731,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.041676462,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.040457655,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.039463914,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide2,0.034219031
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.115150697,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.080473046,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.061628192,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.059125163,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide2,0.055099156,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide1,0.054736426,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.050264975,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.049686434,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.047177807,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.047177807
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide1,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.115147216,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.086139584,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide3,0.063262611,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.049968848,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,0.046996928,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide12,0.068413456,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.043949297,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide18,0.049055496,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.037647259,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.034708079
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide6,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide4,0.115074161,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide13,0.110963336,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide22,0.064908827,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide12,0.026447912,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide6,0.020975245,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide4,0.020705838,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.065848528,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide4,0.020201943,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide13,0.01951209,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide33,0.018671968
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide1,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.114995711,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide5,0.08445885,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide15,0.083865645,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide9,0.075477005,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.03343803,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide16,0.031046904,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide1,0.02691434,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide2,0.954560414,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide4,0.029492234,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide0,0.217620273
cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide7,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,0.114858918,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide1,0.102792621,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.029883198,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.023465434,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,0.021201809,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide3,0.02109258,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##02_4-6-cure-clustering-using-well-scattered-representatives_4.6._CURE_Clustering_Using_Well-Scattered_Representatives.txt##slide1,0.020880418,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.018832089,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.018682809,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.016886612
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide11,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide12,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide13,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide14,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide15,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide16,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide17,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide18,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide19,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide20,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide22,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide23,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide24,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide25,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide26,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.114665028,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,0.086222408,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.059473953,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.938486624,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.066776951,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,0.086222408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.081507894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.768663082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.097349468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.086222408
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide6,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.114592013,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.071664055,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide8,0.052479458,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.044168095,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide1,0.042179181,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide18,0.040293122,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.036924261,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide5,0.036121474,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.035808709,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.035479272
cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide7,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide10,0.11447549,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,0.089429235,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide1,0.063370785,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.063035846,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.059726349,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide2,0.059596193,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide9,0.046578994,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide9,0.046578994,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.044270112,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide3,0.028540118
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide8,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.114284368,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.114284368,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.053731531,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.038586515,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.038586515,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.018528252,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.016314761,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide21,0.589030923,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.167974508,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.932892574
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide2,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide8,0.113967234,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,0.097562854,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.096619159,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.076290272,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.06365291,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide5,0.057785596,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide7,0.054986958,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide3,0.051353196,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide1,0.048463856,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.04809194
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide15,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.113756119,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide5,0.109378962,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide1,0.100365456,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.097345231,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide22,0.049466838,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide1,0.044201344,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##09_6-8-relative-measures_6.8._Relative_Measures.txt##slide1,0.043076384,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide16,0.029451507,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.028970774,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide10,0.027890043
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide3,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.113661287,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.068486488,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide5,0.067393333,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.06065946,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.037735811,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide2,0.037504625,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.03674457,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.03674457,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.0338506,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide10,0.032789148
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide2,0.113606524,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide3,0.103960362,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide1,0.088315389,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.070587253,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.060056987,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.057799663,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide10,0.092922473,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.044625106,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide9,0.041786857,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.041287849
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.113243085,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.102508871,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide1,0.101418807,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.093574447,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide3,0.074331616,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide6,0.065460119,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.060725988,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide1,0.05847388,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide11,0.056386883,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.055487987
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide0,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide3,0.113240421,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.037414922,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide0,0.054479479,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide0,0.033947014,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide9,0.03227125,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide0,0.029845805,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide5,0.029062096,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide2,0.0263184,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.025469991,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide2,0.025281897
language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide5,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.113002823,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide10,0.109803204,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.094749055,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.091990041,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.091140239,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.087045592,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.081561754,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.081325821,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.079777993,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.072975351
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.112637466,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide8,0.087072899,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide6,0.067139985,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.06586703,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.064832174,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.062443402,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.057537892,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide4,0.057008147,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.055951815,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.050059069
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide1,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.112622823,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.10317153,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide6,0.09911064,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.091924325,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.086822654,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.083914379,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,0.082782969,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.080131751,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.075829042,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.072339957
cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide4,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.112309396,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.099619158,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.080050058,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.074751793,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.073557267,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.059185181,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide2,0.054014116,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide12,0.048138558,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.047223308,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.045030407
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide16,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.112252031,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.108485433,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide1,0.063496586,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide11,0.060145721,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide11,0.060145721,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.059887423,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide1,0.056801271,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide1,0.056801271,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.056647287,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide1,0.056647287
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide6,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.112217105,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.103586051,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.086812515,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.066242301,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.061511771,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.056697896,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide3,0.05287805,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide16,0.04023299,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide1,0.038359949,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide6,0.933116541
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide4,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.111927189,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.109513422,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide10,0.09546359,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.093184632,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide8,0.053957036,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide18,0.051895637,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.111649844,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.992601728,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide10,0.099292871,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide10,0.053957036
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide22,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.111361646,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.098442712,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide8,0.054040001,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.051336159,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide13,0.046338595,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide3,0.045104718,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide10,0.266086042,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide8,0.748854032,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.070624016,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide13,0.046338595
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide5,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide14,0.110787804,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.063286766,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide0,0.033364026,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide1,0.02613057,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.024868695,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide4,0.023754563,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.016241918,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide9,0.01282605,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.060290865,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide4,0.958987857
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide4,0.110626152,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide6,0.091660139,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.08444407,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.078267091,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.07696633,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.065581505,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,0.065424892,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.064755504,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.063303359,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.062879858
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide0,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.110585213,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.091113957,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.084273893,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.067116957,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide0,0.066278144,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.062787407,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.060843827,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide0,0.052646533,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,0.051848164,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide1,0.051544933
cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide6,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,0.109944089,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide13,0.109944089,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.104606228,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide6,0.103232129,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide4,0.101795571,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide4,0.087539467,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.083544465,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.083544465,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide7,0.06692846,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.05957481
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide2,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.109756483,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide9,0.088726271,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide7,0.085165669,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide3,0.070385479,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.066221357,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.061306255,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide8,0.060449346,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.060003574,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide1,0.056884206,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.054291827
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide16,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide5,0.10944101,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide8,0.047304663,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.041997766,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.039081937,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.038564224,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.038389475,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,0.035121306,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,0.030500171,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.030417641,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide17,0.029722597
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide0,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.109396347,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide1,0.094081001,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.093200687,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide0,0.075666796,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide1,0.069066687,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.068365046,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.067310616,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.067207705,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.065935202,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide1,0.062868704
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide6,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide12,0.10928909,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.107961212,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,0.077810219,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.070282954,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.059130889,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.090762039,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide9,0.048447822,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.046180282,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.043297154,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.04289954
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide2,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.109160059,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide8,0.095933591,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.071535673,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.054849635,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide6,0.048981285,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.043377166,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.036675455,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.034958662,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.033983732,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.032304346
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide6,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide9,0.109052069,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide32,0.048963154,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.025295357,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide28,0.024038686,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.021597284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide10,0.020915101,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide5,0.019968872,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide2,0.018088089,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide28,0.017996231,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide6,0.937637952
language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide5,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide1,0.108918276,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide2,0.035258114,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.027899511,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide0,0.026723723,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.024184732,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide13,0.020662835,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.01998106,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide3,0.018154431,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide3,0.018154431,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide4,0.017412974
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide8,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.108773106,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide2,0.081129164,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide6,0.079620892,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide3,0.07121358,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide6,0.051826824,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.043189194,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.043021838,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.042303429,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide3,0.038135708,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.057284915
cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide3,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide6,0.108508212,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide2,0.09689408,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide7,0.093487235,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.061643609,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide2,0.042252605,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide2,0.042252605,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide6,0.046155988,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.029892613,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.029892613,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide3,0.027246124
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide6,0.108490399,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide5,0.105508463,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.08869016,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.080066672,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,0.078561192,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.075024248,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide6,0.074717307,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.066108297,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.055409648,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.048353859
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide5,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide5,0.108436996,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.061321702,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.061321702,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide2,0.06090073,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.053492384,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide7,0.036944454,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.03560004,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.088319563,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide11,0.035466515,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.033704369
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide19,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide6,0.108241409,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide9,0.105826978,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.076689538,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide6,0.051581286,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.051250409,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide7,0.050162782,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.04930034,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide7,0.043246286,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.038144481,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide2,0.032495657
cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide5,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide3,0.108189724,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.060697479,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.053860781,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide5,0.028749152,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide7,0.028392973,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide7,0.028392973,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.027481768,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.021800844,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide7,0.019507057,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.018502295
cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.108134736,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide8,0.051203643,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.04517076,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide0,0.044299029,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide9,0.042431578,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.041432759,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide5,0.038313305,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##09_6-8-relative-measures_6.8._Relative_Measures.txt##slide1,0.032883498,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.03134177,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.03134177
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide1,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.107625903,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.102846689,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide2,0.093878067,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.05807624,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.057079753,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.056819866,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.046430365,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.03981137,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.038178413,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide3,0.037931342
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide4,0.107515952,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##01_general-em-for-gmm_w2c1_alex.txt##slide1,0.095635464,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide8,0.09328591,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide17,0.089886672,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.078396872,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.055250767,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide7,0.054050438,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.045819789,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.04426595,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide3,0.04136485
cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.107433559,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide5,0.099074264,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.079101295,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide4,0.078211183,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide4,0.072242928,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.062107817,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.050769965,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide2,0.049935539,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.044488446,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.044306371
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide2,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.107210335,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.097250716,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide2,0.083076974,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide7,0.06725851,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.061353896,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,0.056942634,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.051272459,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide2,0.050668123,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,0.036741027,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide13,0.035120741
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide2,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.106838851,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide2,0.10343348,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.087857768,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide1,0.074685632,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.066681453,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.052199963,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.051260676,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,0.050530906,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.04933406,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide1,0.049029916
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide2,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.10678422,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide7,0.043808351,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.042395094,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.032420237,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.029745307,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.028891551,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide5,0.027317574,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.026024982,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.025883328,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide11,0.025838341
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide8,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.106607621,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.095481118,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.086409227,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.08276485,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.079248508,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.074609204,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide1,0.064144742,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide4,0.061101238,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.060306017,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.059455332
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide12,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide19,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide11,0.78019729,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide22,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide11,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide12,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide13,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide19,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide11,0.78019729,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide22,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide11,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide12,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide14,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide19,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide11,0.78019729,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide22,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide11,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide12,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide15,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide19,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide11,0.78019729,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide22,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide11,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide12,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide16,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide19,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide11,0.78019729,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide22,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide11,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide12,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide19,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide11,0.78019729,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide22,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide11,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide12,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide18,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide19,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide11,0.78019729,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide22,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide11,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide12,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide19,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide11,0.78019729,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide22,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide11,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide12,0.106583088,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide19,0.917825894,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide5,0.192159924
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide18,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide31,0.106441021,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide11,0.078566941,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.062935525,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.052595903,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.052595903,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.043998308,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide12,0.043955803,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.036312377,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide0,0.123372287,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide26,0.166749864
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide11,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.106011368,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,0.102923138,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide8,0.098326827,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide8,0.098326827,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide16,0.066239533,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.065631234,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.065631234,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.057414312,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide10,0.045188164,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.042121759
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide16,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide3,0.105942761,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide3,0.105942761,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.104391289,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.097929254,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide1,0.096061459,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide8,0.073045954,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide8,0.073045954,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide11,0.076752448,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide16,0.95114277,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide16,0.061392098
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide4,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide8,0.10580436,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,0.079881616,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.077914747,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.054961428,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide5,0.05128418,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.047865674,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.047784093,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide1,0.046454093,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.045334909,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.044678391
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide7,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide19,0.105295901,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.097196067,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.052437223,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.051065018,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.045444179,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide7,0.951155179,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide20,0.056283613,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide12,0.038616424,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.030942283,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide5,0.691263885
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide8,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.10461317,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.078635503,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide1,0.058021082,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide1,0.054276697,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide3,0.050640752,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide4,0.048947556,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.047144959,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.046252515,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide6,0.044915187,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide6,0.042919117
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##02_4-6-cure-clustering-using-well-scattered-representatives_4.6._CURE_Clustering_Using_Well-Scattered_Representatives.txt##slide1,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.104075217,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.064926523,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.060150524,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.059566624,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.054083773,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.041967709,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.037680857,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.034017758,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,0.033815752,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.033627784
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide13,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide8,0.103998132,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.077467149,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide0,0.0672349,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide2,0.061851694,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.034329982,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide4,0.027296113,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.026843032,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide1,0.022659047,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.021379851,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.021119404
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide0,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.103914406,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.049265698,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide0,0.062738738,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide0,0.036098287,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide0,0.035340774,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.03463708,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide0,0.031958202,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.031503816,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide0,0.026717618,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.026256562
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide14,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide0,0.103491702,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.093834975,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide5,0.071654918,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.070252714,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide21,0.473339092,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide3,0.195395777,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.058454298,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide1,0.070133704,,,,
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide5,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide5,0.103396879,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide11,0.036779158,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide5,0.032477637,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.031278488,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.030046752,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide17,0.026494731,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide6,0.037995985,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.056795109,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide2,0.114242739,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide1,0.173887712
cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide5,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide5,0.10282217,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide2,0.063695667,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide2,0.063695667,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.036473553,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide2,0.032517644,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.015884557,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.015194656,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.015194656,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.014753268,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide6,0.014699111
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide4,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.102673369,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.091585065,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.086983558,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide0,0.067353302,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.049879703,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,0.048575299,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.046433885,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,0.043084463,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide5,0.038859153,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide5,0.038641495
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide9,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.102294873,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide1,0.0818813,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide10,0.072438979,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.054728756,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide0,0.049071745,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide13,0.038968361,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.038581846,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide2,0.030927918,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.03025616,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,0.029846726
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.102131981,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.092140971,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide10,0.073167425,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.073041742,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide5,0.068720438,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.067606448,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide2,0.064903805,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.061474281,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.060673022,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.06056655
cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide9,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.101034015,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide4,0.08951192,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.053397272,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.052562286,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.052455262,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.050701153,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide8,0.043378101,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.042595453,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.040686108,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.039327736
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.100147049,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide6,0.099611587,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.091063265,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.089144932,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide0,0.083076883,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide1,0.078141301,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.076668064,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.076364182,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.067602478,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.066549267
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide18,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.099933604,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.064345448,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.054381506,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.052677077,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.050869939,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide9,0.049168546,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.041316299,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.038264434,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.036660317,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide4,0.036345469
cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide5,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide4,0.099685842,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide4,0.099685842,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.062032047,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.062032047,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.062032047,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.057811537,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide4,0.043917711,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide6,0.03943973,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide3,0.039343452,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide5,0.940717536
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide5,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide8,0.099476856,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide4,0.074224589,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide17,0.06085998,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide3,0.04719576,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.043612927,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide7,0.03773094,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide4,0.036395422,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide4,0.03499759,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.033231493,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.032571068
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide0,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide5,0.099398605,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.085807745,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.062424761,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.061300095,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.056341817,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.045083729,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide3,0.038657221,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.038136215,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide0,0.033754582,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.033446446
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide2,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide4,0.098771729,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide3,0.076007345,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.0668876,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide5,0.063264686,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.062160202,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide2,0.059271326,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide6,0.057167057,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.055909049,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.054310829,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.051326623
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide7,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide9,0.098765762,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.092015624,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.063350229,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.039371405,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide3,0.03912499,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.036124001,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.03087407,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide4,0.030306229,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide3,0.029972766,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.027802768
cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide4,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.098026424,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide6,0.066450439,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.057984973,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.054993483,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide4,0.054115173,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.048078393,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide7,0.04085382,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.039172652,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.038872382,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide4,0.03693822
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide6,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.097978384,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.036759124,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.029738204,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.028980204,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.02864296,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.027677686,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.025612326,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.025109977,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.024354669,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.02249152
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide3,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.097657668,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide17,0.09247017,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.078316975,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.048755802,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide7,0.079748827,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide5,0.02860846,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.03510768,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide6,0.035153306,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide4,0.080752076,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide2,0.042963521
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide5,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.097613931,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide7,0.087517589,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.079103423,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.070941781,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide3,0.068331751,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide7,0.062257277,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.062242054,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide9,0.05764481,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide2,0.05537579,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.051925881
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide26,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide2,0.097411227,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.069756179,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.037278504,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide19,0.029373377,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide4,0.025363317,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide23,0.023072966,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.018081768,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide17,0.017605327,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide11,0.017310666,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide5,0.021188136
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide9,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide21,0.097064953,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide19,0.075692203,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.07146882,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide21,0.069888791,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide13,0.06141289,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.057495356,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.055499949,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.054240209,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.051743763,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide19,0.04559862
language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide7,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.096946466,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.088240234,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.088127889,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.080678956,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.0799755,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.077834605,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.075000612,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide3,0.071897481,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.071721283,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.070795028
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide15,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide3,0.096939807,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide9,0.087850253,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.086797709,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.082915599,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.082915599,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.082915599,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.050034635,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.049379647,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide26,0.047671918,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.043902816
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,0.096803219,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide8,0.032679007,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide8,0.032679007,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.032557104,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide11,0.02700047,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide12,0.023966608,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide1,0.022387486,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide18,0.101697039,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.946568595,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.02231462
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide7,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.096518927,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.086227081,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.056565567,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide1,0.055133438,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide1,0.046366378,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.044101828,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.042984526,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.04188608,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide1,0.041717644,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide1,0.041717644
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide0,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.096440321,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.086733408,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.085202623,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide0,0.078493421,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide1,0.076814655,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide1,0.071079359,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.066055377,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide4,0.062419311,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide4,0.059870818,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide1,0.058168481
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide6,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.096428843,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.06920043,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.054742964,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide2,0.087174458,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.053424198,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.04319206,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.041902551,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.039996731,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide4,0.038898735,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide5,0.03841443
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide8,0.09584818,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.092809346,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.087377088,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.086105858,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.085130504,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.083229665,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.070509171,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.070497582,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.064487258,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.061875998
cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide6,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide9,0.095390219,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.076430765,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide7,0.04891892,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide7,0.04891892,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.04372897,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.043094096,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide8,0.038006133,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.036465727,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide1,0.03242516,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide7,0.015763389
cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide7,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide6,0.094452523,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide7,0.094403322,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide11,0.070594327,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide11,0.070594327,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,0.065173359,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide5,0.043618059,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,0.031230465,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide2,0.024479471,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.021479409,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide2,0.020086567
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide10,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide7,0.093577144,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,0.080187838,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.079702115,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide10,0.077737946,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide25,0.039836697,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide10,0.951169229,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.066546471,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.124236165,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide8,0.092921701,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide11,0.080002597
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.092906805,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.061025834,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,0.029800283,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.029200756,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.026284141,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide7,0.024125744,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide1,0.01995661,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide1,0.019597405,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide0,0.017459908,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide4,0.017291815
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide2,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide3,0.092474963,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.052609295,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide10,0.04548669,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide12,0.040030985,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.037247296,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide7,0.036966869,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.035221004,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide21,0.035167605,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.033001365,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.030614463
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide3,0.09191775,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.072603342,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide6,0.068513487,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,0.040771953,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide4,0.037769057,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.036739078,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide4,0.034880264,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide24,0.033453019,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide10,0.032986577,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide12,0.031946861
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide12,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide17,0.09165354,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.051244882,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.044321108,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide5,0.036670377,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide6,0.036252304,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.026038442,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.022744731,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide4,0.022341818,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.021835464,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide1,0.018859648
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide2,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide9,0.090839875,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.050714402,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide7,0.049315597,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.048717649,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide5,0.039950859,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide5,0.031393541,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide1,0.954771923,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide15,0.033729051,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide15,0.029530499,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide0,0.22063284
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.090735656,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide4,0.087384161,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.077955839,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.065992367,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide13,0.057460766,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.0460582,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.042873273,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide0,0.042788562,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide15,0.041751758,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.041586092
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.09037942,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide4,0.081893469,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.07850969,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.062065001,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.061103885,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide5,0.055834117,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.054034534,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.052707217,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide12,0.045173959,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide8,0.044504148
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide4,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.090184478,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.090184478,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.07395573,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.053451839,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.053451839,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.033873701,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide2,0.033123609,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.841690902,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.688177262,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.033873701
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.090084791,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.081201357,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide13,0.063709172,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.0594248,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.057560423,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.057248207,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.045760427,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide3,0.979275791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.058517518,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.048139174
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide5,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.090084791,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.081201357,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide13,0.063709172,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.0594248,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.057560423,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.057248207,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.045760427,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide3,0.979275791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.058517518,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.048139174
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide6,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.090084791,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.081201357,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide13,0.063709172,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.0594248,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.057560423,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.057248207,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.045760427,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide3,0.979275791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.058517518,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.048139174
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.090084791,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.081201357,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide13,0.063709172,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.0594248,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.057560423,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.057248207,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.045760427,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide3,0.979275791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.058517518,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.048139174
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide8,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.090084791,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide1,0.081201357,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide13,0.063709172,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.0594248,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.057560423,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide8,0.057248207,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.045760427,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide3,0.979275791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.058517518,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide6,0.048139174
cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide4,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide6,0.090032783,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide3,0.07736788,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.060634069,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide7,0.059600279,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide12,0.040208366,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide1,0.039238456,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide27,0.032851749,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide1,0.028393603,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide6,0.027987621,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide5,0.025477583
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide13,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide7,0.08998905,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.080732554,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.073224335,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.059826729,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.053181687,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.042643614,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,0.040742996,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.039219238,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide3,0.038328602,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.037827394
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide3,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide6,0.089918235,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide7,0.057455826,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide7,0.048307802,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.046733712,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.046285578,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.046182573,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.041663167,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide5,0.040570832,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.039947298,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide19,0.039747002
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide19,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide1,0.089584523,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.059810161,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide6,0.055772391,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.046530308,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.041846167,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.040997427,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide14,0.040079834,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.039369509,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,0.038329464,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide1,0.038193525
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide1,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide5,0.088768187,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.06589223,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide8,0.029911075,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.029342202,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.027882135,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide7,0.022217227,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide2,0.019329028,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide14,0.016864273,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.015792758,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide2,0.015106636
cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide2,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide2,0.088749397,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.06116887,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.054338001,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.054121421,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.052375778,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide17,0.04319588,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide2,0.040931625,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide4,0.039002749,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide12,0.038804298,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide2,0.941275736
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide10,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.088561829,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,0.04712016,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide5,0.045233843,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.042300824,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.041784575,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.040353154,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide7,0.039583051,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.036575332,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.036225612,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.033834013
cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide9,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.087639103,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.06871691,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.06231398,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.061393224,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.0594668,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.05506922,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.051434127,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide9,0.049542224,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide4,0.044889272,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide2,0.043001969
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.087601046,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.08455051,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide11,0.068111821,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,0.066624326,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide4,0.056663755,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide4,0.056663755,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide8,0.053882545,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide20,0.048605707,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide7,0.99388565,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.062219689
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide23,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.087556403,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide10,0.072708246,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide1,0.044418771,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.042473999,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide16,0.03516722,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide2,0.034604943,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide3,0.030251339,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.030189809,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide3,0.028944923,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide19,0.028758571
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide5,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.087440431,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.055114701,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide2,0.035645183,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.030964165,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide1,0.030494123,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.030116387,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.029491733,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide3,0.029372365,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.028417835,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.028342955
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide7,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.086881431,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.083363061,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide9,0.071356377,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide11,0.069908922,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide4,0.055991999,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide4,0.055991999,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide9,0.052083738,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide21,0.049727113,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide8,0.993880596,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.061581306
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide10,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide9,0.086811925,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide11,0.081756862,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide26,0.077064121,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.066604327,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.044009627,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.04221748,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.033310653,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.033173818,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide2,0.028099944,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.027028736
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##09_6-8-relative-measures_6.8._Relative_Measures.txt##slide1,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.086428582,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.084810813,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.079040985,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.071898621,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide4,0.062503932,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.053768573,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.047397104,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.043864403,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.039231749,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.034619772
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.086322087,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.071477507,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.066390683,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.065742409,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide5,0.060548788,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.056371564,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.048164139,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.04381496,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide0,0.042691584,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide6,0.04258911
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.086242678,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.060568424,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide2,0.0593023,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.05034433,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.047491233,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide4,0.046855575,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.043335037,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.033424997,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide1,0.044620896,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide2,0.03234658
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide12,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.086079175,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide7,0.017268632,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide6,0.015515981,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.014989575,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide3,0.014700281,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.014486823,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide5,0.014313966,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide9,0.014280051,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide11,0.014088933,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide0,0.013146541
cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide3,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.086054795,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.064364709,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.057886266,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.044625831,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.041007864,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.03622504,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.035992665,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.035434888,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.034698028,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.033784224
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide7,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.085469533,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.076989023,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide3,0.051756696,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide4,0.050829963,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.047997339,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.044450923,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.040563847,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.039892686,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.039892098,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.039222919
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide3,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.085190154,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide8,0.084095491,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide13,0.059703263,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,0.040621532,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide3,0.035697157,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide2,0.029081375,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.02599326,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.025434728,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide2,0.023166064,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide19,0.023021901
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide0,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.085144436,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.052711141,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.050815916,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.050767562,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide6,0.050004312,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide7,0.048362873,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,0.048296124,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.048279675,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide7,0.041720961,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,0.04121706
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide13,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide7,0.085079594,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.072095173,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.052285583,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.04564203,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide1,0.045290054,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide7,0.043725231,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide7,0.043725231,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide7,0.043725231,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide13,0.943278635,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide7,0.541410946
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide12,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.085011488,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide4,0.050505907,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide4,0.032816517,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide4,0.02938677,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.028490964,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.026473804,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.025738515,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.025688657,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.025126807,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.024897883
language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide4,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.084777533,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide3,0.076346339,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide3,0.066574355,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.065357048,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide17,0.043280901,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide8,0.036780393,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.036565399,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.032323664,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide12,0.031594998,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide3,0.029812868
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide0,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.084246601,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.054167059,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide7,0.052928129,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.04042421,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide5,0.03674478,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide6,0.036306888,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide1,0.035751597,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.029877756,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide9,0.029175544,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.028820107
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide12,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.084239795,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.084239795,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.05722624,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.04620552,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.04620552,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.020781034,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.012838513,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide19,0.50116904,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.153231726,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.976721076
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide1,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.083992489,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide5,0.079027243,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide1,0.056672756,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.039827735,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.036995151,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.036995151,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide1,0.036054646,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.035673004,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.033163007,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.030586098
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide8,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide2,0.083520533,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide3,0.075358547,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.059161547,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide6,0.04390776,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide13,0.043241935,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.04122908,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide2,0.039546724,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide19,0.036799355,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.03464739,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##03_k-means-m-step_w2c2.2_alex.txt##slide0,0.033969488
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide4,0.083491576,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide13,0.07582478,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.072484051,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.05483435,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.054653552,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.043202469,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.041301821,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.038768397,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.037441081,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide4,0.036735738
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide9,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide5,0.08257486,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide9,0.079777631,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide7,0.068452143,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide11,0.06652092,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide4,0.055088536,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide4,0.055088536,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide9,0.053460031,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide24,0.049419243,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide8,0.993786215,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.058390605
cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide10,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,0.082334952,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide4,0.075234395,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide6,0.07027973,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide6,0.039957311,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide4,0.03948894,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide1,0.036874215,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.031015242,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.030952459,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide7,0.028971015,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.026366738
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide2,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.081973211,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.080568371,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.077183042,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.067351187,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide1,0.059540352,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.057563015,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.053424052,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.049225742,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide1,0.04833245,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide6,0.046690222
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide3,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.081922665,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide5,0.077968155,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide7,0.061789401,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.052791645,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide1,0.03395708,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.033477531,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide6,0.032500316,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide6,0.029608738,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.029233561,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.027919221
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide11,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.08153227,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide9,0.021820262,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.020193741,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide7,0.050964146,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide21,0.0189245,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.018466679,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide10,0.0173627,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide13,0.016827732,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide4,0.042060141,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.017982188
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide5,0.08149691,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide2,0.075363491,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.018004144,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide4,0.017543464,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide7,0.013836817,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide11,0.94792239,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide10,0.017374368,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide9,0.025077669,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide18,0.038536907,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide28,0.06058732
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide1,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide7,0.081472992,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide6,0.076210374,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide6,0.068472296,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide12,0.039092025,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.025542003,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.025213366,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide0,0.022532226,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.018241657,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.017396223,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide0,0.017135902
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide7,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.080927069,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide2,0.064561362,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide1,0.054218584,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide3,0.050376006,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.046972882,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.039006711,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.038705176,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,0.038264087,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.036245613,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.036192755
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide28,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide3,0.080549962,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.07995989,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide2,0.055458503,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.034292765,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide3,0.025241211,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide7,0.023146129,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.023080169,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide1,0.045609098,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide16,0.022998694,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide10,0.028937005
language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide6,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.080400308,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.077861224,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.069421492,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.06842314,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.066550741,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.066422712,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.066098835,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.065689739,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide1,0.064892697,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide1,0.063654096
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide10,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide0,0.080382355,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.042426145,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide5,0.0328705,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide4,0.023004662,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide5,0.022738479,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.022514184,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide1,0.021669467,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide2,0.020884609,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.020831523,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide4,0.02069393
cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide5,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide2,0.080321215,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.074092181,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.04579763,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide9,0.035251215,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide9,0.035251215,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.028907644,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.027640694,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.027640694,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide12,0.024783775,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide9,0.023282895
language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide10,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide23,0.080297895,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide2,0.071565068,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide11,0.058115426,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide2,0.052332877,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.047706682,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.046580555,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.044742234,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide11,0.044728917,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.042821479,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.040660162
cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide7,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide17,0.080277498,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide5,0.077270611,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide7,0.075958498,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.069636404,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.069257931,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.068403523,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.066376397,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,0.060733273,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.058632647,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.058591718
cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide1,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.080086946,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide2,0.062339388,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.049540105,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide1,0.041208546,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide9,0.040963513,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide3,0.040942973,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.034085473,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.033702675,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.02929222,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.02929222
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide6,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide0,0.079883284,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide1,0.064994967,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide10,0.059815532,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.054869356,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.036545158,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.036516224,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.028273134,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide8,0.019834682,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide0,0.018810806,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide11,0.036798954
cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide5,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.079801771,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.072647544,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide3,0.066567414,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide3,0.063705177,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,0.063553217,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide3,0.061773447,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.05809346,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.05809346,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.049249748,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.049249748
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide0,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide5,0.079076252,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.07353409,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.07353409,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.054118121,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.053954456,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide3,0.052542379,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.049881853,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide5,0.04190766,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide0,0.035862668,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.035375892
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide12,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide3,0.078911594,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide2,0.055372442,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.050917433,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.049268872,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.034374492,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide2,0.03389345,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide4,0.023183181,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide0,0.031793655,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide5,0.034687679,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.018905143
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide13,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide3,0.078911594,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide2,0.055372442,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.050917433,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.049268872,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.034374492,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide2,0.03389345,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide4,0.023183181,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide0,0.031793655,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide5,0.034687679,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.018905143
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide14,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide3,0.078911594,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide2,0.055372442,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.050917433,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.049268872,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.034374492,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide2,0.03389345,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide4,0.023183181,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide0,0.031793655,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide5,0.034687679,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.018905143
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide2,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.078871682,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.052158856,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide10,0.050714203,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.039416639,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide5,0.039086835,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide1,0.036412115,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide4,0.035003823,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.033141654,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide11,0.031866843,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.031095605
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide1,0.078843124,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.076626463,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.064737774,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.06349491,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide2,0.05640907,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide8,0.054587423,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.051520927,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.050626771,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide6,0.047427183,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.046776694
cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide8,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide3,0.078703655,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.063817746,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide0,0.049999242,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.049492266,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide5,0.049039944,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.042957525,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,0.042067388,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.038345968,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide2,0.037595748,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.036904754
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide14,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide4,0.07864272,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.040914502,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.037280451,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide15,0.02690296,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide1,0.023592656,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide1,0.023419079,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide1,0.022782174,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide7,0.022313802,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide4,0.022286765,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.022049264
cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide2,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.078360458,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide11,0.065956889,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide11,0.065956889,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.055508733,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide10,0.055177797,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide2,0.053575488,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide4,0.048546359,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.047946236,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide2,0.042967336,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide6,0.040514669
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide0,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.077465136,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide0,0.07203346,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide0,0.046436733,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide1,0.042250775,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide1,0.036157353,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide0,0.03564667,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide1,0.03507123,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide1,0.034557516,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide2,0.031788752,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.031484321
language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide8,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide3,0.077447206,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.074734004,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.063816302,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide2,0.062697794,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.062441141,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.056728835,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.052200904,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide7,0.050870222,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.049717706,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.048301497
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide20,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.077352004,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.069319774,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide14,0.483823803,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.173315878,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide7,0.034078188,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide20,0.937388284,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.039883008,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.237464165,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.069319774,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.173315878
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide21,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.077352004,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.069319774,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide14,0.483823803,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.173315878,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide7,0.034078188,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide20,0.937388284,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.039883008,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.237464165,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.069319774,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.173315878
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide22,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.077352004,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.069319774,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide14,0.483823803,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.173315878,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide7,0.034078188,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide20,0.937388284,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.039883008,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.237464165,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.069319774,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.173315878
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide12,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide5,0.07723815,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide11,0.052171631,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.026008597,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.021658775,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.036954234,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide5,0.018146415,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide12,0.951150535,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.018165369,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,0.02468144,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.065163389
cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide3,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide11,0.077205345,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide6,0.043052553,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.042059324,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide4,0.038675551,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.038626799,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide5,0.038323827,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide7,0.037904729,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.036768162,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.036768162,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.036030961
cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide5,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.077030072,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide5,0.048387275,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.046457066,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.036056095,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.035482742,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.033680853,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide9,0.031728669,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.031364779,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.029379125,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.029227485
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide5,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.076926976,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.076926976,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.063525547,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.047802573,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.047802573,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide13,0.040329191,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide17,0.476765166,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide2,0.042828993,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.830183716,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide13,0.040329191
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide8,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,0.07689617,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.058879308,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide5,0.039517942,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.030334762,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.024739464,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.022724762,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.022410659,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.023088481,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide7,0.992193757,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide3,0.022218962
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide11,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.076197501,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.050502671,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.048240529,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide7,0.042622171,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide3,0.040498421,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.038146537,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.035033422,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide26,0.028510307,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,0.027600497,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.025476255
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide4,0.076196045,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide18,0.056077511,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide3,0.053400768,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.04848006,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide18,0.033422619,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide1,0.02991351,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.939558676,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide4,0.181473661,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.034710345,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide5,0.032320791
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide7,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide3,0.076087134,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide1,0.073748504,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.049120729,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide5,0.047983669,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,0.04024916,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.038557909,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.029985573,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide19,0.029819363,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.029524719,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide6,0.028167127
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide4,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide5,0.076059692,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide5,0.032118001,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide11,0.028804647,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.028401517,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide8,0.024846791,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide3,0.023043866,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide6,0.022667634,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide25,0.022617443,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.041094842,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide2,0.142521519
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide5,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,0.07597521,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.073427273,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide2,0.071529683,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,0.067788642,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.062173232,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.056243686,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.045604988,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide2,0.044537647,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.043545424,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.042536542
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide12,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,0.075729595,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide7,0.015819208,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.012906235,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide7,0.011798291,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide10,0.011673131,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide3,0.010994146,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide9,0.010734466,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide5,0.010722574,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide10,0.010558892,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide6,0.010417555
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide1,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide8,0.07566736,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide9,0.051727787,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide2,0.034204719,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.030561724,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide6,0.027581045,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide6,0.02746039,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide21,0.027255528,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide3,0.025616808,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.025083129,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.023987392
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide5,0.075656452,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.041867411,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide8,0.028266546,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide8,0.018605836,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide7,0.018359061,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.017721062,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide23,0.017178395,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide14,0.017118229,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide21,0.016990806,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.015612091
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide7,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.075357435,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.03379204,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,0.027780355,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.026928332,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide6,0.02611936,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide8,0.024599004,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.023144422,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.022582788,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.022569508,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.021614045
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide19,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.07496337,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.069437435,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide14,0.487342371,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.179831245,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide7,0.033284525,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide22,0.993802619,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.039274255,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.241665077,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.069437435,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.179831245
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide3,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.074791245,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide0,0.051884371,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,0.032158051,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide2,0.030379911,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide6,0.025686076,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.023972729,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.02280073,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.022431749,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.020929004,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.020825039
cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide5,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide0,0.074744977,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide3,0.068319339,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.063984913,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.047957582,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.04639768,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide0,0.046041279,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide1,0.045226501,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide5,0.043280951,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide1,0.04015032,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide2,0.037794151
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide11,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.074705332,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.059545134,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,0.049176092,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.043269193,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.043121341,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.043121341,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.043121341,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide5,0.04232663,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.037796241,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.036837856
cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide2,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide5,0.074696495,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.064684926,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,0.063422615,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.057520872,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.05548565,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.053797778,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.050217767,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide10,0.047612283,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide1,0.040984767,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide4,0.039771376
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide0,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide0,0.074553318,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide0,0.063137151,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide0,0.060780372,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide0,0.045818953,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide0,0.044935599,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide0,0.038831915,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.037623467,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##04_6-3-constraint-based-clustering_6.3._Constraint-Based_Clustering.txt##slide0,0.035601204,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.034844358,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide3,0.0342333
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide11,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.074346583,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.066712266,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.063487423,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.06116571,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide3,0.060856762,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.057875387,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.054623395,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.054615957,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,0.049113391,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.047818854
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide13,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.07416008,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.0513124,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.04732995,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.032662731,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide7,0.030317501,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.029192729,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide10,0.023649373,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide6,0.022784449,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide7,0.019827458,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.017491752
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide14,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.07416008,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.0513124,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.04732995,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide0,0.032662731,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide7,0.030317501,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.029192729,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide10,0.023649373,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide6,0.022784449,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide7,0.019827458,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide16,0.017491752
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide4,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide9,0.073862837,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide3,0.067643369,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.056347686,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.034821129,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.034321902,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.034321902,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.029718393,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.028534586,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.022859578,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.022485669
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide0,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide6,0.073556863,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.073158987,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide4,0.071719149,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.070304503,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.069602789,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide3,0.065361351,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.063604638,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide4,0.05956033,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,0.057863947,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.054129692
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide0,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide0,0.073421212,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide0,0.042975468,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide0,0.027320006,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.02106003,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide0,0.018221563,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.017538196,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide0,0.017421271,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide0,0.017265615,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide1,0.017038073,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide5,0.016475633
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide1,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.072974985,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide5,0.059748175,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide8,0.058610408,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.042222604,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.039475621,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.0392673,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.036577013,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide2,0.033437296,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide11,0.033309757,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.03251769
cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide3,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide3,0.07291915,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide3,0.057971154,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.056292319,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.046909073,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide8,0.041574116,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.037203747,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.035898569,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide3,0.030568343,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.0285537,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.028160607
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide2,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide4,0.072888971,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.06571144,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,0.058796757,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.048143177,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.047316644,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.044483396,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.041489202,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide2,0.039059679,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide9,0.038516299,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide9,0.038516299
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide2,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,0.072612895,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide1,0.031682648,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.031126447,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##09_6-8-relative-measures_6.8._Relative_Measures.txt##slide1,0.030547529,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.030014198,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,0.029250105,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.028778426,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,0.027392305,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide2,0.026413981,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide2,0.025907501
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide11,0.072408216,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide2,0.037547389,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide6,0.022048114,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.019432975,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.030341307,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide6,0.018879818,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.018879818,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide14,0.04042168,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.947764012,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide10,0.066452839
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide10,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.072327958,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.019798651,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide21,0.018830965,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide10,0.057705009,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide10,0.018137576,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.017241746,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide16,0.017176051,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.033850063,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.019772882,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide9,0.945109812
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide14,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,0.071570606,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide5,0.051016112,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide3,0.040026517,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.03849366,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.034518116,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,0.033280412,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.032515852,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide8,0.032201902,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide5,0.032008074,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.031664954
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide0,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide3,0.071250051,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide2,0.054904208,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.051124093,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.049493671,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide2,0.036603934,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.034438604,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide9,0.028888306,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide4,0.02268573,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.019751235,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide0,0.026586157
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide20,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide8,0.071056128,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide10,0.048900606,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.039197479,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.036987685,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.035950909,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.035474516,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide22,0.031265879,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.030058755,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide2,0.029373184,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide3,0.028528989
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide15,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide17,0.071031561,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide2,0.044866511,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide7,0.043326514,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide6,0.040794156,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.031386182,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide3,0.031104033,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.021886526,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide3,0.019532287,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.017835742,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide6,0.016689654
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide14,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.070736683,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide7,0.06823356,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.064040932,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.064040932,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.064040932,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.061986573,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.053389447,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.05178723,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.039830543,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.039217211
cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide2,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide6,0.070638618,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.064210193,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide5,0.060705077,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide7,0.050555366,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.042746828,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide2,0.040124784,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide7,0.039451747,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide10,0.035976888,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.033990797,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide7,0.033132378
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide7,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide12,0.07011673,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide3,0.033842592,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.032464975,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.030814856,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.030430333,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.029737871,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.029621403,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.029594824,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.029375452,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.028857645
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide4,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.069848355,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.061399399,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide11,0.059318248,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.057331096,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide6,0.056511566,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.056166708,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide6,0.055952319,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.053246476,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.053246476,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.050092873
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide0,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide0,0.069542157,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.051819588,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.051336686,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide0,0.048089288,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide4,0.045693067,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide7,0.044922223,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide0,0.039186308,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide0,0.038528491,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide0,0.033975225,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide8,0.031065345
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide9,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.069476739,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide5,0.02761302,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide7,0.060334014,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide9,0.01976487,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.018763023,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide2,0.018226433,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide10,0.017634073,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.017360552,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide11,0.979126763,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,0.017771927
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide13,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide5,0.069475484,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.063845128,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.063613065,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.063613065,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.063613065,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.053904449,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.043555524,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.04172813,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide1,0.038903228,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.038793778
language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide5,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.069432731,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.049190526,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.049164074,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide11,0.044658042,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide3,0.035782713,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.031132422,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide1,0.030379483,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.029411427,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide6,0.04156833,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide2,0.04801645
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide1,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.068942618,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.060042691,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide7,0.057839817,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.052724201,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide3,0.051067731,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide6,0.049828432,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide1,0.045495005,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.044938857,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.043728153,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.041941254
cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide2,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.068908874,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.05284145,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide1,0.050625246,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.044891854,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.037897186,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.037031616,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.026843063,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide4,0.024768158,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide1,0.024573657,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide3,0.023943727
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide9,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.068871939,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.061491001,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.061364139,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide6,0.052046395,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,0.04375872,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.036936988,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide5,0.036661813,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,0.03658804,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,0.033593546,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,0.031389386
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide12,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide4,0.068606203,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.063219286,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide7,0.061044935,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.055064407,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide5,0.053003268,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.05238847,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide3,0.039663345,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide7,0.038632238,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide2,0.040893471,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide1,0.048579412
cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide4,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.068507278,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.065502346,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide8,0.060318058,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.056767715,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide5,0.055467358,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.053954706,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide9,0.047399019,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.044623291,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.044623291,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.042547242
cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide4,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide1,0.068376451,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide4,0.061144204,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide8,0.060317257,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide4,0.043356257,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide8,0.042820615,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide2,0.042558109,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide4,0.036878936,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide4,0.036878936,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.034044073,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide0,0.03231306
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide5,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,0.068250525,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.053945036,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.051870093,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,0.046359758,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.040695069,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide5,0.040288096,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide1,0.03930055,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.035871627,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.035574612,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.031037399
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide9,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,0.067213082,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.04643037,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide5,0.035313895,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.027311972,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.02238641,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.022233638,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide3,0.020202832,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.02246535,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide8,0.991325825,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide17,0.023034124
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide2,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,0.067203609,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.051647842,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide11,0.050718119,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.049838823,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.048217324,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##08_6-7-internal-measures-for-clustering-validation_6.7._Internal_Measures_for_Clustering_Validation.txt##slide0,0.043582034,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.042722061,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide2,0.042400938,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.038187259,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide5,0.036864064
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide3,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.067168087,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.061038658,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.060188817,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.058851691,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.053851323,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.052717385,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.052350982,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide9,0.050338345,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.047051026,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide7,0.046945717
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide18,0.067034877,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide1,0.063137564,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide3,0.048164711,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.035596763,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide2,0.032990209,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.951134838,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide11,0.046393323,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide21,0.056188803,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide8,0.039447148,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide1,0.04234978
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide5,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide0,0.066832877,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide18,0.022463993,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide5,0.015979246,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.012454617,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide4,0.012176683,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide7,0.962620311,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.110512404,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide6,0.012614432,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide20,0.013948468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide6,0.024836823
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide12,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.066595041,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide5,0.056806609,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.051916656,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.049901711,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.049901711,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.049901711,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.049619435,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,0.0438892,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.039721434,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide10,0.0365924
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide5,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide7,0.066452849,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide2,0.053564844,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,0.049609314,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide4,0.045416882,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide6,0.045336123,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,0.044484049,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.044092644,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.042972951,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.042972951,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide3,0.039346198
cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide5,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.065765484,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide0,0.046195703,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide1,0.043789378,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide6,0.041326515,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.038009652,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide4,0.037754772,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.037649893,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide9,0.033772097,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.028382302,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.028229795
cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide3,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.06563444,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.063114591,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.043843285,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide1,0.042613649,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide5,0.04256347,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide1,0.04083184,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide5,0.035685859,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide1,0.035660988,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.033646642,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide1,0.033258392
cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide4,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide0,0.064950349,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide0,0.046999828,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide9,0.036105089,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide0,0.028941646,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide0,0.027808979,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.026022842,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide2,0.025934008,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.023415559,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.022526082,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.021427702
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide0,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide4,0.064937574,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide5,0.05896875,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.041051895,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide23,0.033187764,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.031581961,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.027379021,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.027291846,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.025578314,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide0,0.025312079,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide16,0.02315691
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide4,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.064702503,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.058808389,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.058242515,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide3,0.054854642,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.050399239,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.049861753,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide1,0.03709729,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.034730192,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide9,0.033278397,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.032437834
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide1,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide5,0.064439079,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide5,0.016043242,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide8,0.013857493,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide0,0.012782792,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.010465493,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide13,0.010155342,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide9,0.009655142,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide6,0.009438618,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide3,0.009065904,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide11,0.008002108
language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide0,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide0,0.06434639,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide0,0.043487948,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide0,0.042269849,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide0,0.041014122,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.040600722,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide0,0.040203381,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide0,0.037714711,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide0,0.034941143,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide0,0.034409899,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide0,0.032646588
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide0,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.064206513,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.026731711,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.022266054,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide8,0.021559017,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.02074105,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,0.020576834,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide8,0.020062937,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.019873961,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide2,0.019180322,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide2,0.019180322
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide13,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.063971259,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide3,0.047912485,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.046175235,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.045440194,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.043274946,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide2,0.043033023,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.03872444,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.036577124,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide9,0.035622884,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide2,0.03345175
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide3,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide10,0.063314343,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide1,0.062919501,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide1,0.055683239,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide5,0.042365982,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.03190003,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.026925628,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide12,0.026813641,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.023779268,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.02252075,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide4,0.021121295
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide4,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide6,0.06329334,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide7,0.017961298,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide0,0.015998637,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide8,0.013945949,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.013306759,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide17,0.01306282,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.01283423,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide3,0.012146615,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide6,0.01198341,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide1,0.011667346
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide2,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.063202916,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.031479666,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##11_6-10-clustering-tendency_6.10._Clustering_Tendency.txt##slide2,0.030259092,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide1,0.026032943,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.025921685,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.025493878,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.023533481,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.022038081,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.020488222,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide2,0.020465855
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide6,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,0.06316352,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.056558245,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,0.051993555,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide5,0.05182556,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide0,0.050616836,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide1,0.048300765,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.044642394,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.043366382,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide0,0.04127873,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide3,0.039999572
cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide6,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.062205958,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.030022378,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide0,0.027490419,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.026478957,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide7,0.024033426,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.023078591,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide9,0.02298414,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.022794687,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide5,0.022506918,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide0,0.020584166
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide15,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.062081298,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.062081298,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.04620356,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.036642722,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.036642722,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide10,0.017271107,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.016544866,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide22,0.476110997,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.164648606,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.978431717
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide6,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide8,0.061901587,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.055661625,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide8,0.053934759,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.040739839,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.031498148,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide12,0.024591495,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide19,0.019724145,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide15,0.046729955,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide5,0.024725408,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide3,0.020171059
language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide9,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.061886725,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,0.05795642,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.054636129,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.053760433,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.049670516,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.045030923,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.04269737,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.041966603,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.040719242,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.040352805
cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide4,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide1,0.061738683,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide3,0.061275037,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide5,0.049036187,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide1,0.035110938,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.032386182,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,0.029126608,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide12,0.027104344,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.023819856,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide4,0.023590756,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide3,0.022927759
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide6,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide2,0.061559169,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide0,0.046955366,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.044807056,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.043471738,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide7,0.041914388,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide3,0.041177229,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide5,0.039699936,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.039203023,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.038675898,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.037979372
cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide5,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide2,0.061072808,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide3,0.048582844,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide5,0.047430097,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.03190308,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.0316787,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide1,0.030105349,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.027914315,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.027570995,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide3,0.026150818,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide2,0.025806139
language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide4,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.061052845,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.034516017,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.03404532,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.033748468,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide5,0.032400751,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.030423293,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.029399252,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.027658614,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.027591284,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide1,0.027522191
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide5,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.061032628,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide17,0.05920949,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.05786069,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide3,0.055709652,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.041025288,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide12,0.038081537,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.03734271,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide9,0.034127694,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.031645398,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.031435209
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide8,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.06083538,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.06083538,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide7,0.032943825,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide1,0.031098037,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide11,0.02844492,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide6,0.027047834,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide5,0.026882889,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide22,0.026312304,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.02342638,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide17,0.044213513
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide18,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide4,0.060659823,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide5,0.057891002,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.050674107,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.04894398,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide6,0.04832287,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide2,0.048192468,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.045803873,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide4,0.045459066,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide4,0.045288117,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide3,0.04512144
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide3,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide7,0.06002136,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.048079293,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.0416252,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide1,0.029366383,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide3,0.026490591,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide3,0.026490591,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide5,0.015175264,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide20,0.014137039,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide1,0.041970409,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide3,0.013824598
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide1,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide6,0.059913518,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide5,0.043419295,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.041485624,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide8,0.040860343,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide8,0.03831997,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.032399277,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide6,0.03167706,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide17,0.030191157,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide3,0.029806864,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide3,0.029610401
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide9,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.059521638,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.059521638,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.049873816,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.035980284,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.035980284,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide6,0.017737199,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide10,0.014227556,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide19,0.580390344,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.181147244,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.973534118
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide0,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide12,0.059460583,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.056243265,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide0,0.047921183,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.04135467,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide0,0.0404173,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide0,0.040127632,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide0,0.040089934,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide5,0.030696277,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide8,0.030441926,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide6,0.028989697
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide6,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide2,0.059331243,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide5,0.047603331,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.042434432,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.041053219,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.040310535,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.03752664,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide4,0.034180755,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide2,0.032713699,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide0,0.03006135,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide6,0.029785036
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide5,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide10,0.059158084,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide12,0.048125684,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide8,0.022607055,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.02228773,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.021744728,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide1,0.019840164,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.017192429,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide0,0.014204023,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide6,0.013372876,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.012876754
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide9,0.059010154,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide4,0.044274979,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.043256397,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide8,0.040483516,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.037972473,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.036332971,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide6,0.035582987,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,0.034641079,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide24,0.033696021,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.033234899
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide5,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide1,0.059003352,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.035370608,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.027380358,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide3,0.025213774,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.022587175,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide1,0.02079537,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide12,0.020722295,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide12,0.020722295,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.019958715,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide1,0.019186245
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide3,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide4,0.058981338,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide9,0.033620862,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.032514846,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide5,0.031442304,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide5,0.031316902,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide11,0.030587076,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.03048503,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,0.030438429,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide6,0.028899374,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide0,0.027413198
language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide3,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide2,0.058811601,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide4,0.057209326,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide4,0.051016851,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.044207292,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,0.043550875,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide1,0.039938856,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide3,0.030533891,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.030490841,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.028348861,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.026421212
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide10,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.058796455,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.043241551,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.035529435,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide17,0.027819855,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide11,0.02109748,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.020488412,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.019856469,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.017870678,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide2,0.017165873,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide5,0.028326871
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide1,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide0,0.058659532,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.055443323,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide7,0.05433738,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.038278501,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.036541871,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide0,0.033603148,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide0,0.031018406,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.030689664,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.030150604,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide5,0.030132946
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide13,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide1,0.058601043,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide5,0.029294655,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.023177628,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide1,0.018900382,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide4,0.017625484,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide1,0.017599366,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide11,0.017343313,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.017265871,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide5,0.011481816,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide2,0.011199377
cluster-analysis##02_module-1##01_lesson-1-.txt##slide1,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.058418214,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.052706733,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide3,0.050286926,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide2,0.033461436,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##03_6-2-clustering-evaluation-measuring-clustering-quality_6.2._Clustering_Evaluation_Measuring_Clustering_Quality.txt##slide1,0.031064729,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##02_6-1-methods-for-clustering-validation_6.1._Methods_for_Clustering_Validation.txt##slide1,0.029734739,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide5,0.028555315,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide0,0.026848178,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide10,0.026830713,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide4,0.02579919
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide0,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide8,0.057965437,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide12,0.02930122,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide8,0.026290713,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide6,0.011720462,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide1,0.01084094,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide4,0.008613986,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide7,0.008073718,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide0,0.008037639,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide19,0.008240379,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide0,0.917744957
cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide5,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide2,0.057963775,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide2,0.018585829,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide10,0.013104149,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide7,0.013057472,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide7,0.012295521,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide8,0.011961322,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide6,0.01188413,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide6,0.011129441,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide2,0.010784728,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide7,0.00993619
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide9,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.05759861,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide3,0.045765518,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.036594125,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.034988542,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide5,0.033395696,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,0.033070038,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.031817636,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide0,0.03063735,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide3,0.030617953,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide6,0.03000656
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide6,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.056862942,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide7,0.045337109,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.04451507,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide6,0.036733902,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide4,0.036373046,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.036072319,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.035916905,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide7,0.033135048,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.030650581,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide13,0.03038025
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide13,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.056811102,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.040262975,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide10,0.029761528,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.024578241,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide17,0.022305425,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide5,0.01496412,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide6,0.01467367,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.012395775,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide6,0.011460278,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide17,0.011375868
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide4,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide7,0.056670518,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.055430584,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.037206264,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.031619717,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide17,0.029657301,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.023733557,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.022255016,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide4,0.945520125,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide9,0.019646075,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide3,0.02071638
cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide3,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide8,0.056435265,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide5,0.056401696,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide3,0.054317783,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.053669516,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide7,0.053135305,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide7,0.045458039,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide16,0.044633894,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.043764798,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide8,0.042471669,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide2,0.042248889
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide10,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide5,0.056258932,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.036417814,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.034499495,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.03357465,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.030277674,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.030144153,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.029380024,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.028593557,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide1,0.026731572,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide1,0.026621991
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide6,0.056233213,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.037342613,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide4,0.03167236,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide10,0.024491905,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.021321135,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.020025374,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide18,0.019157507,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.021243294,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide9,0.991295682,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide17,0.022144106
cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide5,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide2,0.056023377,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.031398517,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.02968644,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.028693141,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,0.025925309,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide3,0.024279086,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.023553581,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide4,0.021635861,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.020322679,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide5,0.01998305
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide14,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.055381751,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide0,0.039831714,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide0,0.015983926,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide4,0.01407509,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide11,0.01315714,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.01145762,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide6,0.011446518,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide10,0.011080328,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide0,0.010830303,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide1,0.010819713
cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide3,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide6,0.055247444,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide4,0.050024419,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide2,0.040829043,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.040755475,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide6,0.033849375,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.032105403,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide8,0.028821489,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide9,0.027935538,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.027465594,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide6,0.026019458
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide3,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide4,0.055008825,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.05206536,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide19,0.050705129,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide3,0.041835952,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide5,0.032403632,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide11,0.029767131,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide11,0.029767131,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide13,0.029469822,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.026806363,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide18,0.02484841
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide14,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.054950188,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.051499049,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,0.048781912,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide1,0.046356702,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide5,0.04074879,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide6,0.038967579,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide9,0.037702187,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide4,0.037454574,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.034783258,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide4,0.034074236
language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide4,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.054859539,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide3,0.053066683,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.044338004,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.041105608,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide12,0.040756139,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.040663585,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.038347474,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.037011857,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.036294049,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.035769128
cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide6,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.054618237,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.03363128,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide15,0.027847692,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide1,0.026293106,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide8,0.024112441,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.023544702,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide18,0.023054288,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide3,0.02091795,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide15,0.018324941,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide0,0.022604137
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide5,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.054346046,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.036691583,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.035338372,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide9,0.035212888,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.034198447,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide7,0.034081785,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.029655057,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide1,0.029529755,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.029067337,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide5,0.028797616
bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##02_mean-field-approximation_w3a2.txt##slide7,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##07_metropolis-hastings-choosing-the-critic_w4b3_after_board_alex.txt##slide2,0.054337788,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide1,0.037005802,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.032864176,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide1,0.03018818,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.027233663,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide3,0.019588833,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide3,0.018482643,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide1,0.018223782,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide22,0.018117276,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.018923639
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide9,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.054226986,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,0.053384871,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide3,0.050754294,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.048115541,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide1,0.047652144,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.046697581,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide24,0.046226334,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide8,0.044510574,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.044274232,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.042618775
cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide2,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.054192675,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.043335133,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.041174084,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide4,0.041006148,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,0.038336868,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide4,0.035803181,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide3,0.035510336,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.034925207,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide4,0.033768213,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide4,0.032064545
cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide5,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide3,0.054082905,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide4,0.041187541,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide4,0.032007634,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.025519726,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.025519726,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.025519726,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide7,0.024349518,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide16,0.023319941,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.021919976,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide2,0.020692281
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide0,0.053743012,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide18,0.030832166,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide6,0.02695945,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide9,0.992559039,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.118701697,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide8,0.024379885,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide17,0.030832166,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide6,0.02695945,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.118701697,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide0,0.053743012
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide7,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.053680044,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide13,0.030297052,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.025574534,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide0,0.023843421,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide0,0.02291556,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide28,0.022618056,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide0,0.021460237,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide0,0.020547016,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide0,0.020173821,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide0,0.019758724
cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide4,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.05365303,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,0.039210233,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,0.034297077,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide4,0.03276563,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide3,0.031332782,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide2,0.02730233,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.026650569,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide8,0.024309652,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide5,0.024251582,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide12,0.024131488
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide9,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide2,0.053600525,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide19,0.033075127,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide5,0.027905132,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide7,0.027669315,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide10,0.027374076,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide8,0.021262151,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide18,0.019029791,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide3,0.017931043,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide11,0.01585738,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.014562558
cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide9,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide4,0.05338922,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide4,0.041207879,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.028761082,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide9,0.025726118,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide5,0.023849609,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.022103528,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.022103528,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.022103528,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.020643684,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide8,0.019808553
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide15,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.053059001,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide10,0.048456178,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide0,0.032434804,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.030713614,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide15,0.932278628,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide3,0.028568248,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide0,0.085905178,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.119201232,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide23,0.038640103,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide0,0.085905178
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide8,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.05303635,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide8,0.052844984,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide3,0.048809418,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide12,0.045544497,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide3,0.041911946,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide0,0.039410159,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.038003009,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.034893747,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.034217066,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide6,0.031500806
cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide6,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.052976414,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide6,0.042941837,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide1,0.035707094,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.02984093,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide1,0.027852184,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide1,0.027300049,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.026609102,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.026000015,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide4,0.021438531,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide9,0.021056886
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide9,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide5,0.052763999,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.042434371,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.042399719,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide3,0.041927209,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide4,0.032355706,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.032269451,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide3,0.030302226,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide2,0.029948951,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.028634189,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide7,0.027933314
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide8,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide2,0.052373075,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide5,0.04962798,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.0479737,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide2,0.043695736,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide7,0.040939304,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.039765481,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide0,0.039682435,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide3,0.037651515,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide3,0.036626288,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.034259556
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide2,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide0,0.052224701,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide13,0.047836586,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide0,0.02909182,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.024936129,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide1,0.018349122,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide5,0.017953627,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide7,0.013996565,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide7,0.013996565,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide0,0.011906224,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide11,0.011527573
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide5,0.05191569,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide8,0.043066848,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.037244638,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide13,0.03705034,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.034093755,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.033136008,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide3,0.032004997,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide1,0.03119543,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide3,0.030103343,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide11,0.029619747
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide11,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide7,0.051704856,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide2,0.050630947,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide3,0.049320132,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide7,0.041340872,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide14,0.039594192,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide7,0.03302242,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide5,0.031875928,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide5,0.031032391,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.030473973,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide2,0.028914441
language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide2,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.051024855,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.048098467,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.045010248,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.042293859,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.040281724,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.03844653,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.037612646,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide6,0.035330965,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide5,0.032851484,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.034484832
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide2,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.050728057,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.043468934,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.037684061,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide2,0.037292475,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.035295281,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide3,0.033635068,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.033593601,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.03267633,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,0.032586272,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.032586272
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide0,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide8,0.0503053,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.044898744,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide5,0.035841616,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.035479498,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide5,0.034916668,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide11,0.034834993,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide3,0.03474622,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide3,0.032713177,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.032136215,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide14,0.032121023
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide2,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##05_5-4-grid-based-clustering-methods_5.4._Grid-Based_Clustering_Methods.txt##slide1,0.050304399,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide4,0.0459131,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.030621839,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide3,0.030294762,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide2,0.02316606,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.02233601,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide3,0.017053061,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide3,0.016832747,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide2,0.016714323,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.016630303
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide12,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.050103754,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.047709887,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide5,0.041230868,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.040577192,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.03905141,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.035727309,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,0.034197653,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.033712513,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide1,0.033274496,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.032841494
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide16,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.049849183,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.044250393,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide5,0.042205372,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide8,0.039525519,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide1,0.033892348,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide2,0.033881322,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide4,0.033767442,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide2,0.033646146,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide2,0.033385593,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide3,0.031915018
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide10,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.04976839,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide4,0.021523145,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.021491193,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.021001277,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.019522052,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide1,0.019278514,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.018795856,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide2,0.018534375,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.018528656,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide1,0.018160084
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide10,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide3,0.049209399,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.04469244,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.042181347,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.041489849,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide6,0.039705174,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.038598579,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.035949201,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.035456709,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,0.034272827,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide0,0.033913668
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide6,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide3,0.049206031,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide0,0.02577884,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide4,0.013394417,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide3,0.012816902,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.01257856,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide2,0.012342234,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide3,0.011557187,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.011375215,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.011375215,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide2,0.011065291
cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide3,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide1,0.049132942,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide1,0.044864191,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide1,0.036217482,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide1,0.032144659,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.030379534,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide1,0.029860826,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide1,0.02927426,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide1,0.028761037,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.016850643,cluster-analysis##02_module-1##01_lesson-1-.txt##slide1,0.016848823
cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide1,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide6,0.049033899,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide17,0.037908659,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide2,0.036724584,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.032996309,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,0.024847554,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.022759574,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide8,0.021713253,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide8,0.021713253,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide15,0.017340032,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide4,0.01732717
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide13,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,0.048732559,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide7,0.017803349,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide16,0.014082436,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide7,0.013988869,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.01279426,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide10,0.011585404,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.010259183,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide7,0.009472864,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide0,0.009368549,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide18,0.015428607
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide5,0.048256321,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide5,0.048256321,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide5,0.036639978,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide2,0.030528945,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide2,0.030528945,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide10,0.017972029,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide6,0.01703589,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide21,0.428987944,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.159634518,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide18,0.938441988
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide13,0.047430348,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.04514785,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide25,0.037638051,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide10,0.031997904,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide7,0.028002797,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide11,0.135172969,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide2,0.042448451,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide1,0.082539463,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.938629627,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide10,0.031997904
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide5,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide7,0.047395924,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide9,0.031359163,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##07_extensions-of-lda_w3b4.txt##slide1,0.030649191,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.030151946,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide6,0.028170292,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide7,0.027617109,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,0.027601407,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide8,0.026822137,cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide8,0.026734839,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.025991188
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide4,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,0.046969363,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.03379477,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.031852692,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.030670416,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide7,0.028898079,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide14,0.028023991,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.027722987,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,0.02704227,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,0.026940132,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.025570414
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide3,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.046779749,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide8,0.034539107,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide9,0.033877008,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide11,0.033083241,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide13,0.03024844,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.029097263,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide2,0.024079133,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide6,0.023825418,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide9,0.022527107,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide2,0.021958545
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide2,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide6,0.046462344,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.045703032,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.043569002,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide17,0.031311428,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide17,0.024458516,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide20,0.022038049,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide5,0.018981615,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##06_metropolis-hastings_w4b3_alex.txt##slide3,0.018445054,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide24,0.017983197,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide15,0.017693473
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide15,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##03_how-to-define-a-model_w1a3.txt##slide6,0.046309954,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide3,0.040850689,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.032909806,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide4,0.03024306,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide4,0.03024306,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide9,0.028589991,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.028036452,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.02582583,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide10,0.023612267,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide9,0.023137832
cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide2,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide2,0.046073714,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide17,0.042119565,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide6,0.040900506,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide3,0.039816803,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,0.031727207,cluster-analysis##02_module-1##02_lesson-2-similarity-measures-for-.txt##slide2,0.031187607,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide13,0.028571777,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,0.028571777,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide2,0.023058993,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide4,0.022628376
cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide8,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.045997761,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide4,0.032988135,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide7,0.030162041,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide9,0.030103152,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##07_6-6-external-measure-3-pairwise-measures_6.6._External_Measure_3_Pairwise_Measures.txt##slide0,0.028409031,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide2,0.02522574,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide7,0.020997799,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide5,0.018168993,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide5,0.017269512,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.017087013
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide11,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.045460212,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.036175967,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide17,0.03044937,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide8,0.026867525,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.018366338,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide7,0.016867358,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide6,0.016496936,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide2,0.015610907,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.015389379,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.015333148
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide15,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide4,0.044896928,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.038898918,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide2,0.038575384,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.035743549,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide10,0.03105371,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide3,0.029651111,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.029600325,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide10,0.027023497,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide5,0.026292556,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.026203495
cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide5,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.044762725,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide3,0.04367908,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide10,0.040973162,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.036056137,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide9,0.03597778,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.035770625,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide3,0.034852991,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.03448621,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide6,0.03344818,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.032198285
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide12,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide6,0.044622339,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide9,0.038786511,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide17,0.029469941,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide27,0.027967544,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide9,0.02662243,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide2,0.026386765,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide23,0.025997919,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide10,0.025654867,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide8,0.024547532,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide8,0.023990298
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide17,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide8,0.044373983,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide5,0.036791152,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.034333371,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.034333371,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.019152073,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide7,0.012675071,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide2,0.012398442,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.012252967,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide15,0.011905941,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide12,0.010721298
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide0,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide2,0.043749362,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide6,0.022031577,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide8,0.018908092,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide0,0.013038574,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##10_6-9-cluster-stability_6.9._Cluster_Stability.txt##slide2,0.011893506,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide10,0.010684207,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide4,0.01031498,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide11,0.009516388,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide4,0.009504356,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide8,0.009264186
cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide3,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide8,0.043513252,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide3,0.035346522,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide13,0.034043882,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide5,0.03393755,cs-410##11_week-10##02_week-10-lessons##02_10-2-text-clustering-generative-probabilistic-models-part-1-optional_TM-23-clustering-gen-model-part1.txt##slide4,0.030348442,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide2,0.029649742,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide6,0.028449561,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide4,0.025449872,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide4,0.025449872,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.025082948
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide2,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide5,0.043362319,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.031682461,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide19,0.021467136,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide5,0.021350855,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,0.017703922,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.01767228,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide18,0.016792546,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.016251538,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide12,0.01571955,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.015648754
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide3,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide5,0.043362319,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.031682461,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide19,0.021467136,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide5,0.021350855,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,0.017703922,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.01767228,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide18,0.016792546,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.016251538,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide12,0.01571955,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.015648754
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide4,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide5,0.043362319,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide0,0.031682461,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide19,0.021467136,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide5,0.021350855,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide4,0.017703922,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide5,0.01767228,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide18,0.016792546,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.016251538,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide12,0.01571955,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide11,0.015648754
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide12,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide26,0.043283057,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide0,0.036504827,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide6,0.020570856,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide2,0.016460659,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide8,0.01413665,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.013860225,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide5,0.013178297,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide11,0.011511737,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide5,0.011280054,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide5,0.010994306
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide13,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide26,0.043283057,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide0,0.036504827,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide6,0.020570856,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide2,0.016460659,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide8,0.01413665,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.013860225,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide5,0.013178297,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide11,0.011511737,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide5,0.011280054,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide5,0.010994306
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide14,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide26,0.043283057,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide0,0.036504827,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide6,0.020570856,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide2,0.016460659,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide8,0.01413665,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.013860225,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide5,0.013178297,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide11,0.011511737,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide5,0.011280054,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide5,0.010994306
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide15,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide26,0.043283057,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide0,0.036504827,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide6,0.020570856,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide2,0.016460659,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide8,0.01413665,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.013860225,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide5,0.013178297,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide11,0.011511737,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide5,0.011280054,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide5,0.010994306
language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide13,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.043225705,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide1,0.039350821,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide5,0.035765844,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide9,0.035052479,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,0.032664125,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,0.029861998,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide11,0.029010883,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide1,0.027009387,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide3,0.026158212,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.025946664
language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide3,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide5,0.042797022,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide9,0.041688337,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide8,0.036995202,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide6,0.033516114,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.028384389,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide3,0.028046017,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.028037338,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide0,0.025338116,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide5,0.025203275,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide4,0.024599754
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide13,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.042666879,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide5,0.037378592,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide7,0.036428372,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide7,0.030397243,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide2,0.02952841,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.029256743,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.028465293,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide11,0.02831863,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.026069507,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.025772866
cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide3,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide3,0.042322418,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide2,0.039974001,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide14,0.037901588,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide3,0.032486285,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.032111054,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.030825468,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide9,0.030529819,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide9,0.030529819,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide4,0.028324995,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide3,0.946396982
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide5,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide4,0.042107006,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.029072481,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.026853699,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.026396203,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.024633558,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide11,0.023113667,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.022053621,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.021686701,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide1,0.021650376,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.021568282
cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide4,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##03_4-2-agglomerative-clustering-algorithms_4.2._Agglomerative_Clustering_Algorithms.txt##slide2,0.041856832,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide9,0.038888117,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.035975437,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide5,0.030307194,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide1,0.023955317,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide3,0.023140491,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##02_4-1-hierarchical-clustering-methods_4.1._Hierarchical_Clustering_Methods.txt##slide2,0.020976128,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide2,0.020442541,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,0.019809299,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide4,0.019463037
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide11,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.041472312,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.028768206,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.02853558,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide18,0.025524453,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide3,0.023560642,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide7,0.02242256,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide7,0.02207963,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.021699452,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide8,0.021529505,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide8,0.021114138
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide21,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide18,0.04142533,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.023696443,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.021397887,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide20,0.993432865,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.02581277,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.021397887,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide20,0.993432865,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.023696443,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide9,0.02576055,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.021397887
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide22,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide18,0.04142533,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.023696443,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.021397887,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide20,0.993432865,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.02581277,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.021397887,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide20,0.993432865,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.023696443,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide9,0.02576055,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.021397887
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide23,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide18,0.04142533,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.023696443,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.021397887,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide20,0.993432865,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.02581277,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.021397887,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide20,0.993432865,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.023696443,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide9,0.02576055,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide8,0.021397887
cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide7,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide4,0.040949755,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide1,0.039383316,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide3,0.028501215,cs-410##11_week-10##02_week-10-lessons##03_10-3-text-clustering-generative-probabilistic-models-part-2-optional_TM-24-clustering-gen-model-part2.txt##slide3,0.028195066,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide0,0.027687608,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide0,0.027681587,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide8,0.026356569,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide8,0.026356569,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide4,0.02632114,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide6,0.026036799
cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide6,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide7,0.039194939,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide2,0.031914855,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide5,0.028870563,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide5,0.026303208,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide3,0.020219433,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.016730956,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.014601888,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide3,0.013942004,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.013415668,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide4,0.018213969
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide3,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide7,0.03894649,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.014116536,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.014116536,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide1,0.013419274,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide1,0.013419274,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide10,0.012791698,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide10,0.012791698,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.012630747,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide1,0.012450983,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide4,0.012224105
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide10,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.038889243,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide13,0.024978148,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,0.02336247,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide13,0.022614494,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.021732434,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide3,0.02169661,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.021555182,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide3,0.021106144,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide12,0.020499458,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide4,0.020480789
cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide4,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide3,0.038713145,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide3,0.031894389,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.030775648,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.030418956,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.029240121,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.029140456,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.029140456,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide7,0.027703153,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.026445007,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide1,0.025324351
cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide4,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide4,0.038636585,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide3,0.0356045,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide5,0.029847992,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.026371875,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.025772986,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide2,0.025165842,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide6,0.024713318,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide3,0.024432508,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide3,0.022348993,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide4,0.020646562
bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide18,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide3,0.038501539,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.037882877,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide8,0.034863779,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.0338654,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.033100437,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide12,0.032954509,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide1,0.029742118,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide18,0.951119928,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide9,0.03571276,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide15,0.044345665
cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide5,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide3,0.038142732,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide2,0.02612621,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide4,0.025293206,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##06_5-5-sting-a-statistical-information-grid-approach_5.5._STING_A_Statistical_Information_Grid_Approach.txt##slide2,0.022565906,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide8,0.018951283,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,0.016135585,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide2,0.014643212,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.012740332,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide2,0.01179217,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.011662797
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide11,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide10,0.038057077,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide8,0.035443583,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.031884983,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.022680622,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide7,0.021833941,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.020156381,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide1,0.01856498,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide0,0.018315656,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide1,0.018076356,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide0,0.018021248
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide0,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide6,0.037987479,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide3,0.03052413,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide10,0.030042275,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide4,0.028376625,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.02753392,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide3,0.02580292,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide2,0.021319174,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide2,0.020515882,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##01_nonparametric-methods_w6a1.txt##slide3,0.019722136,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide17,0.019459492
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide20,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide19,0.037858843,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.024130863,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.0194414,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide19,0.993400666,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.023736757,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.0194414,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide19,0.993400666,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.024130863,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide9,0.022970565,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.0194414
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide10,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide5,0.037855541,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide2,0.028369454,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide8,0.018953614,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide6,0.014007295,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide6,0.013082614,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide11,0.012993396,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide6,0.012409576,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.009330938,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide1,0.008682148,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide10,0.008030765
cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide4,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide3,0.037509168,cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide5,0.033064544,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide13,0.029924655,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide13,0.029924655,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide5,0.020693767,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide15,0.018340102,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.016435583,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide5,0.015533335,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide5,0.015533335,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide11,0.015466383
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide16,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide5,0.037497443,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.035834411,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,0.035328429,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.034574212,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.033883859,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.033651541,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide1,0.032324071,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,0.032099744,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.031937452,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide5,0.031935108
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide18,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide17,0.03746583,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.02371728,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide25,0.993693253,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.023259762,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide25,0.993693253,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.02371728,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide9,0.022164096,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide25,0.993693253,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.02371728,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide23,0.028844237
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide9,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide9,0.037218602,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.034884151,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide13,0.032578057,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide3,0.025027712,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide3,0.024913491,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide8,0.023492342,cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide6,0.023418859,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,0.022803372,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide12,0.021800482,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide5,0.021706506
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide26,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide17,0.036414384,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.022508033,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.018580137,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide24,0.993465084,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.023086256,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.018580137,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide19,0.993465084,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.022508033,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide9,0.022160878,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.018580137
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide25,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide19,0.036321422,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.022407285,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.018232157,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide19,0.993658849,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.023146399,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.018232157,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide24,0.993658849,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.022407285,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide9,0.021976495,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.018232157
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide3,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide18,0.036271323,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.031565827,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide9,0.023270692,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide5,0.022969692,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide21,0.020893674,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide7,0.017062257,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide5,0.017055961,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide11,0.01689076,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide18,0.016827134,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide2,0.016754635
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide18,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide8,0.035989606,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.029531038,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.029531038,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide5,0.02792577,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide12,0.019209111,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide16,0.016939087,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide15,0.013918536,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide7,0.013067381,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide11,0.012746173,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide9,0.01203122
cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide3,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide4,0.035415629,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.034020672,cs-410##04_week-3##02_week-3-lessons##02_lesson-3-2-evaluation-of-tr-systems-basic-measures_3.2_Evaluation_of_TR_Systems_-_Basic_Measures.txt##slide3,0.033481047,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.033376761,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide4,0.032809634,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide5,0.032526067,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide6,0.032410545,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide2,0.031979591,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.031833594,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide4,0.031160881
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide3,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide11,0.035288846,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide6,0.026062203,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide1,0.018332189,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.017479046,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##02_3-1-partitioning-based-clustering-methods_3.1._Partitioning-Based_Clustering_Methods.txt##slide1,0.016840493,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide1,0.015639389,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide2,0.015148551,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide0,0.015011243,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide9,0.014013088,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##06_4-5-birch-a-micro-clustering-based-approach_4.5._BIRCH_A_Micro-Clustering-Based_Approach.txt##slide3,0.01375739
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide10,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide4,0.035202782,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide5,0.031220165,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide2,0.030896472,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide11,0.027869963,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide6,0.023356232,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide3,0.023323082,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide1,0.02312498,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide8,0.022120074,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##01_why-approximate-inference_w3a1.txt##slide4,0.021350973,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide20,0.021263971
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide1,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide8,0.034951127,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide6,0.031604577,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide10,0.02889118,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide12,0.023099321,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide0,0.017909183,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide2,0.017679385,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide18,0.018674358,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide3,0.193498112,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide2,0.083779245,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide1,0.936915542
cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide4,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide2,0.034916074,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide2,0.034916074,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide5,0.034355878,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide2,0.032897181,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide4,0.023320924,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide4,0.023320924,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide4,0.023320924,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.021472387,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide5,0.020896056,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide3,0.01911109
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide8,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide8,0.034846557,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.033109284,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.033109284,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide0,0.018622944,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide3,0.01860581,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.018059906,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide3,0.017079036,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide3,0.017079036,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide3,0.017079036,cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide5,0.015821662
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##01_analytical-inference_w1b1.txt##slide12,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##02_dirichlet-distribution_w3b2.txt##slide4,0.034308648,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide11,0.030960751,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide31,0.027148105,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide3,0.025547724,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide8,0.025414959,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide9,0.02004148,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide4,0.019910945,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide4,0.019910945,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide13,0.019771182,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide5,0.01924723
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide5,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide7,0.034297475,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.020499633,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide4,0.015174664,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide4,0.015174664,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.014550551,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide2,0.013632988,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide0,0.010258439,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide4,0.0085114,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide5,0.944666419,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide3,0.009640952
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,0.034293142,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide0,0.014253899,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide0,0.014253899,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide0,0.014253899,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide0,0.014253899,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide0,0.014253899,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide16,0.010042651,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide7,0.00999496,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide13,0.009420291,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide7,0.008763435
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide5,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide3,0.034249281,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.029047016,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,0.026878685,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide5,0.026659334,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.025979706,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.02495817,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.024126504,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.023866922,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide4,0.023605691,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,0.023415358
cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide5,cs-410##12_week-11##02_week-11-lessons##07_11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optional_TM-37-sentiment-ordinal.txt##slide5,0.033998004,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide6,0.03037754,cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide5,0.02647124,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide5,0.02647124,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.022756201,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide2,0.021880785,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.021812378,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide14,0.021486691,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide9,0.021445745,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide8,0.021211636
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide3,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide0,0.033728852,cs-410##09_week-8##02_week-8-lessons##05_8-5-topic-mining-and-analysis-motivation-and-task-definition_TM-12-topic-mining-task.txt##slide4,0.033537035,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.027410802,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide12,0.024882428,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide0,0.023595969,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide16,0.023539242,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide5,0.021129784,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide9,0.019238108,language-processing##03_vector-space-models-of-semantics##02_topic-models##01_topic-modeling-a-way-to-navigate-through-text-collections_w3_l3_e1_new.txt##slide2,0.015949475,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide2,0.015872478
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide19,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide17,0.033590667,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.022887954,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide25,0.993657344,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.018471522,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.020854038,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.018471522,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide25,0.993657344,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.022887954,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide9,0.020286885,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.018471522
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide24,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide17,0.033590667,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.022887954,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide25,0.993657344,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.018471522,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide2,0.020854038,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.018471522,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide25,0.993657344,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide5,0.022887954,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide9,0.020286885,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide12,0.018471522
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide12,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide1,0.033533707,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide2,0.029967375,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide8,0.027498182,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide8,0.026426127,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide6,0.02401178,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide7,0.023099824,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide7,0.023099824,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide7,0.023099824,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide12,0.945115585,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide8,0.02266838
language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide2,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide4,0.033457072,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide9,0.032649674,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide9,0.029413563,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide7,0.025574705,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.024027107,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide5,0.022898223,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide4,0.022544781,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide4,0.022537036,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide4,0.022491663,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,0.021609067
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide11,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide5,0.033386285,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide3,0.024820882,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide1,0.020387123,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide12,0.018940876,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.018890058,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide6,0.015723978,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide23,0.015522099,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide7,0.015217914,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide6,0.015157276,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide12,0.015011762
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide16,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide15,0.033210323,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide6,0.030554098,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide1,0.025543246,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.02420976,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide2,0.023957937,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide4,0.01962223,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide7,0.019050058,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide15,0.018908323,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##01_distributional-semantics-bee-and-honey-vs-bee-an-bumblebee_w3_l1_e1.txt##slide3,0.01735619,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide7,0.017316253
cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide6,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide10,0.033208287,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide4,0.03240768,cs-410##02_week-1##02_week-1-lessons##01_lesson-1-1-natural-language-content-analysis_1.1_TR-Natural_Language_Content_Analysis.txt##slide4,0.02804226,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide1,0.027704612,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide10,0.026487688,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide3,0.026036527,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide4,0.023749004,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide3,0.02349927,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide9,0.021605968,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide0,0.021458059
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide4,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide9,0.032914014,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.030706801,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide0,0.027547917,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide2,0.025980048,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide4,0.025195739,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide2,0.024743128,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide1,0.02439124,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide2,0.024289559,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.024121158,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide1,0.02405741
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide3,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide10,0.032598868,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide11,0.029886952,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##02_conjugate-distributions_w1b2.txt##slide2,0.02094061,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.020390558,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide28,0.020076682,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide8,0.019691145,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide8,0.016933748,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide2,0.972469583,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide9,0.096280949,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide10,0.018183081
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide17,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide2,0.032551422,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.023558628,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide4,0.023550621,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.023495977,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide5,0.02123379,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide5,0.020808307,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide1,0.018984544,language-processing##03_vector-space-models-of-semantics##02_topic-models##02_how-to-train-plsa_w3_l3_e2_new.txt##slide5,0.018970917,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide5,0.018448266,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide2,0.017996536
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide9,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide1,0.032415264,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide8,0.028652332,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.028534609,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide1,0.027526053,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.027352111,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide12,0.026183517,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.02483094,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.023857615,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.02336166,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide3,0.023158428
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide4,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.031629093,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,0.031629093,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide0,0.027143464,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.026302423,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.021801226,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide5,0.019568907,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide11,0.01818993,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide11,0.01818993,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide10,0.017948363,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.016418095
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide7,0.031614143,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide7,0.031614143,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide7,0.031614143,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide5,0.02510853,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide3,0.023292045,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm_w2b3_alex.txt##slide24,0.017717753,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.017150349,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide13,0.016296212,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide12,0.015716483,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.015627912
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide9,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide16,0.031577763,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.018606111,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.018606111,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.017774253,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide7,0.015550722,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide7,0.015550722,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide7,0.015550722,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide5,0.013335374,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide13,0.012378677,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide5,0.011745532
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide11,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide4,0.031203712,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide1,0.028826933,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide4,0.022948632,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide10,0.021420461,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide4,0.020142,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide4,0.018338646,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide2,0.017699469,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.017200432,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.016908786,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide5,0.016370767
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide7,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide5,0.030606983,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.030362911,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.019005058,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide8,0.018012596,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.017144535,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide0,0.016553852,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.016520529,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,0.015463398,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.015392161,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide4,0.015388113
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide9,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide5,0.030542997,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.02176407,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide0,0.016737046,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide4,0.015979118,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.014400231,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.014201157,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.013491291,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.013235292,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide4,0.013168694,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.012973024
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide2,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide8,0.030394436,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide2,0.024410786,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##03_adding-context-to-nlu_w5_l3.txt##slide5,0.023648186,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide1,0.021115575,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.020595926,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide1,0.020302119,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.018458178,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide10,0.017943498,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide4,0.017666368,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide3,0.017245214
cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide8,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.029962713,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.018032268,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide5,0.015612868,cs-410##11_week-10##02_week-10-lessons##01_10-1-text-clustering-motivation_TM-22-clustering.txt##slide6,0.014085069,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.013193285,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide2,0.013100671,cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide7,0.012742531,cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide7,0.012742531,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide6,0.012695803,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide3,0.012584033
language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide6,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide12,0.029697315,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide9,0.026165889,cs-410##12_week-11##02_week-11-lessons##03_11-3-text-categorization-evaluation-part-1_TM-33-text-cat-eval-part1.txt##slide3,0.021042363,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide6,0.013566607,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide5,0.012027309,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide3,0.011399953,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.010381348,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide5,0.008740279,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide3,0.00846536,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide10,0.008368945
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide10,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.029454074,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide3,0.020371406,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide4,0.018531069,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide6,0.016852558,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide1,0.015784612,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide13,0.015142998,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.014424141,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide6,0.014258411,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide5,0.013985659,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide9,0.013815613
cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide4,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.028794174,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide7,0.024982443,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide3,0.019584402,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide6,0.018427298,cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide9,0.018414559,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide9,0.018414559,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.017746228,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide5,0.017601624,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide4,0.017273861,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide4,0.017259455
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide24,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide10,0.028161719,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide4,0.024366191,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide6,0.022949522,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide5,0.021014653,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide8,0.020263393,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide12,0.019464812,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide8,0.01795193,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide1,0.017923067,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide22,0.017775758,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##03_gaussian-mixture-model_w2a3_alex.txt##slide0,0.017309609
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide17,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide10,0.028005551,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide3,0.021829117,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide8,0.020834591,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide12,0.01926089,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide12,0.01482935,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide4,0.014601793,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide9,0.013485525,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.013350044,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide18,0.013308378,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide3,0.012661846
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide10,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##01_scaling-variational-inference-unbiased-estimates_w5a1_alex.txt##slide16,0.027506817,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide8,0.026374886,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide0,0.024494961,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##03_using-cnns-with-a-mixture-of-gaussians_w5a2_alex.txt##slide14,0.021245943,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide8,0.020482313,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##04_scaling-variational-em_w5a3_alex.txt##slide7,0.018357738,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide5,0.015442592,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide5,0.014490572,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide27,0.013045922,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide17,0.012950082
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide10,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide11,0.027480021,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide2,0.02285374,cs-410##03_week-2##02_week-2-lessons##04_lesson-2-4-implementation-of-tr-systems_2.4_Implementation_of_TR_Systems.txt##slide7,0.021551004,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide1,0.02104767,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide3,0.017122983,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide5,0.016865173,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##03_example-ising-model_w3a3.txt##slide9,0.016787578,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.016475088,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##04_4-3-divisive-clustering-algorithms_4.3._Divisive_Clustering_Algorithms.txt##slide2,0.014403247,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide4,0.013823371
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide5,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide10,0.027253106,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide5,0.019048577,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide2,0.01853025,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide18,0.016266234,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide12,0.015798696,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide3,0.015691117,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide3,0.015583652,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide6,0.015125417,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide16,0.014948835,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide6,0.014848745
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide8,cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide5,0.026625925,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide12,0.023914174,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.022699005,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide2,0.021665435,cs-410##12_week-11##02_week-11-lessons##06_11-6-opinion-mining-and-sentiment-analysis-sentiment-classification_TM-36-sentiment-method.txt##slide3,0.016298008,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide3,0.015863559,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide0,0.015775246,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide9,0.015176327,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide9,0.015176327,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##05_6-4-external-measures-1-matching-based-measures_6.4._External_Measures_1_Matching-Based_Measures.txt##slide1,0.014715655
cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide7,cs-410##06_week-5##02_week-5-lessons##04_lesson-5-4-web-search-introduction-web-crawler_5.4_Web_Search.txt##slide2,0.025715062,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.02199221,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide10,0.018668274,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide6,0.01754926,cs-410##13_week-12##02_week-12-lessons##06_12-6-contextual-text-mining-mining-topics-with-social-network-context_TM-43-netplsa.txt##slide2,0.017435817,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide4,0.017423683,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide4,0.017122365,cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide4,0.017122365,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide4,0.016676483,cs-410##06_week-5##02_week-5-lessons##02_lesson-5-2-feedback-in-vector-space-model-rocchio_5.2_Feedback_in_Vector_Space_Model__Rocchio.txt##slide6,0.016357627
language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide4,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide10,0.025504011,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide7,0.011046269,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide1,0.009909672,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.008997272,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide29,0.00876563,language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##01_neural-language-models_w2_l3_e1_new.txt##slide4,0.008616304,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide3,0.008109408,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.008070813,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide3,0.008061036,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##06_3-5-the-k-medians-and-k-modes-clustering-methods_3.5._The_K-Medians_and_K-Modes_Clustering_Methods.txt##slide1,0.007598952
language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide10,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##05_example-of-gmm-training_w2a5_alex.txt##slide17,0.025436153,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide6,0.022137476,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide6,0.022137476,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.018700551,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide7,0.01713436,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide7,0.01713436,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide7,0.01713436,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide5,0.013709028,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide5,0.013357914,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide5,0.013357914
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide16,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide10,0.025283504,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##03_gp-for-machine-learning_w6a3.txt##slide3,0.020896095,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide12,0.017163484,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.016177382,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide6,0.014606817,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide12,0.013098313,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.013034314,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide4,0.012682279,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide9,0.012259136,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide3,0.011486439
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide9,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide2,0.025264649,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide2,0.018246268,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide4,0.017411154,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide4,0.016041011,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide6,0.015937819,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide6,0.015937819,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide2,0.015928131,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide7,0.01579974,cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide5,0.015772346,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide3,0.015316791
bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide7,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide7,0.024935297,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide7,0.018907393,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##01_jensens-inequality-kullback-leibler-divergence_w2b1_alex.txt##slide1,0.016983221,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide14,0.016022875,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide14,0.01464842,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##07_applications-of-bayesian-optimization_w6a6.txt##slide5,0.013382444,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide2,0.012798107,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide12,0.011797546,cs-410##02_week-1##02_week-1-lessons##06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation_1.6_TR-Vector_Space_Model_Simplest_Instantiation.txt##slide1,0.011554229,cs-410##02_week-1##02_week-1-lessons##04_lesson-1-4-overview-of-text-retrieval-methods_1.4_TR-Overview_Text_Retrieval_Methods.txt##slide2,0.0111118
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,0.02457758,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide7,0.010905161,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide16,0.010791802,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide7,0.009756287,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide0,0.008798181,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide0,0.008798181,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide0,0.008798181,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide0,0.008798181,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide0,0.008798181,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide13,0.00757139
language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide23,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide4,0.024081151,cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide3,0.020734732,cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide3,0.020734732,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide30,0.020107531,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide11,0.019396694,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide3,0.018731637,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide5,0.017203456,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide2,0.016106475,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide13,0.015901021,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##02_noisy-channel-said-in-english-received-in-french_w4_l1_e2.txt##slide9,0.021715645
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide6,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide5,0.023917247,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.023522758,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide0,0.019730057,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide2,0.017901457,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide4,0.01773486,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide2,0.016419706,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide6,0.015870631,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide1,0.015253378,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide1,0.015049037,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide7,0.015032089
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide13,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##04_gibbs-sampling_w4b2.1_alex.txt##slide1,0.02390703,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##11_bayesian-neural-networks_w4c2_alex.txt##slide1,0.02158768,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide3,0.020989573,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide1,0.02085798,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide12,0.019716014,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.018118745,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide21,0.016495188,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide9,0.01646081,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide0,0.015579673,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide13,0.945470276
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide11,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,0.02358066,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide8,0.020513334,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide11,0.014232302,cs-410##13_week-12##02_week-12-lessons##01_12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1_TM-38-sentiment-lara-part1.txt##slide2,0.014058866,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide3,0.013699709,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.012832638,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide8,0.012765142,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide8,0.012760361,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide4,0.012233604,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide7,0.011507943
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide9,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide0,0.023455255,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide0,0.023455255,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide0,0.023455255,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide0,0.023455255,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide0,0.023455255,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,0.021620447,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide7,0.017578303,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.008676595,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide13,0.008356576,cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide3,0.007821641
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide4,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide1,0.022613364,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide3,0.019496165,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide12,0.019074335,cs-410##02_week-1##02_week-1-lessons##02_lesson-1-2-text-access_1.2_TR-Text_Access.txt##slide5,0.015926188,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide0,0.014930619,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide1,0.014625185,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide8,0.014320492,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide4,0.01384474,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide5,0.013753903,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide3,0.01364429
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide14,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##04_adding-lexicon-to-nlu_w5_l4.txt##slide4,0.02228776,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide9,0.021361984,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide3,0.020358299,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide5,0.016782793,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide9,0.012801483,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.011935317,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide16,0.011714419,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide1,0.011282729,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide13,0.010899835,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide5,0.010500035
cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##02_4-6-cure-clustering-using-well-scattered-representatives_4.6._CURE_Clustering_Using_Well-Scattered_Representatives.txt##slide0,cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide3,0.022249921,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide1,0.020182169,cluster-analysis##02_module-1##01_lesson-1-.txt##slide0,0.018198954,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide0,0.018198954,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.017230583,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##05_3-4-the-k-medoids-clustering-method_3.4._The_K-Medoids_Clustering_Method.txt##slide1,0.016137813,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide5,0.013812485,cluster-analysis##03_week-2##03_lesson-4-hierarchical-clustering-methods##05_4-4-extensions-to-hierarchical-clustering_4.4._Extensions_to_Hierarchical_Clustering.txt##slide1,0.013274286,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide1,0.012310696,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##07_3-6-kernel-k-means-clustering_3.6._Kernel_K-Means_Clustering.txt##slide4,0.011887528
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide4,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.022206098,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide3,0.018151381,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.01710899,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide0,0.0164463,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.015908805,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide2,0.014529805,cs-410##06_week-5##02_week-5-lessons##03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm_5.3_Feedback_in_Language_Models_for_Retrieval.txt##slide5,0.014063942,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide2,0.013849558,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide3,0.013548116,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide6,0.013061662
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide9,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.021747462,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide5,0.016788023,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.016207733,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide11,0.016001842,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.014315106,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide9,0.014306256,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide6,0.014294035,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.014109848,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide2,0.01387652,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.013385259
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide19,cluster-analysis##05_week-4##01_lesson-6-methods-for-clustering-validation##06_6-5-external-measure-2-entropy-based-measures_6.5._External_Measure_2_Entropy-Based_Measures.txt##slide1,0.021540761,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide1,0.018216306,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide6,0.016322779,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##09_markov-chain-monte-carlo-summary_w4b5_alex.txt##slide13,0.015916443,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##04_training-gmm_w2a4_alex.txt##slide12,0.014868555,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide3,0.013664017,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide9,0.013552637,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##02_explicit-and-implicit-matrix-factorization_w3_l1_e2.txt##slide7,0.013208774,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide8,0.013101116,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##02_k-means-from-probabilistic-perspective_w2c2.1_alex.txt##slide9,0.013055979
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide5,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide3,0.020957798,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##02_probabilistic-clustering_w2a2_alex.txt##slide0,0.016497018,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide6,0.011143219,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide2,0.008251223,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide2,0.00747493,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide15,0.007228071,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide15,0.007228071,cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide3,0.007033603,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##07_5-6-clique-grid-based-subspace-clustering_5.6._CLIQUE_Grid-Based_Subspace_Clustering.txt##slide2,0.006568499,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide8,0.006422159
language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##02_viterbi-algorithm-what-are-the-most-probable-tags_w2_l2_e2.txt##slide24,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide5,0.020802852,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide5,0.020802852,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide5,0.020802852,cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide5,0.020725925,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide9,0.019689327,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##03_word-alignment-models_w4_l1_e3_new.txt##slide16,0.015629841,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide6,0.01444011,cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide5,0.014282634,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide9,0.013820381,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide3,0.013060361
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide8,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.019396022,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.017989586,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide11,0.014305515,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide11,0.012707264,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide6,0.012371273,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.012115334,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide0,0.011987916,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide1,0.011724024,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide7,0.011336378,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide5,0.010247245
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide6,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##02_intent-classifier-and-slot-tagger-nlu_w5_l2.txt##slide2,0.01927661,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide3,0.018534094,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide0,0.018399134,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide9,0.017774023,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide7,0.017457831,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide1,0.01710107,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide1,0.017083816,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide2,0.016891967,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##01_encoder-decoder-architecture_w4_l2_e1.txt##slide2,0.016890962,cs-410##06_week-5##02_week-5-lessons##05_lesson-5-5-web-indexing_5.5_Web_Indexing.txt##slide3,0.016256014
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide5,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide0,0.018979679,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide8,0.017513509,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide2,0.015981487,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide3,0.015831822,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##03_smoothing-what-if-we-see-new-n-grams_w2_l1_e3_new.txt##slide6,0.013087228,cs-410##11_week-10##02_week-10-lessons##06_10-6-text-clustering-evaluation_TM-27-clustering-eval.txt##slide2,0.012775516,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.01273185,cs-410##05_week-4##02_week-4-lessons##06_lesson-4-6-smoothing-methods-part-1_4.6_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide3,0.012709798,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.012449575,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide1,0.011967695
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide8,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide8,0.018948355,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide0,0.016113746,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide2,0.013640252,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##01_monte-carlo-estimation_w4a1_alex.txt##slide4,0.011998415,cs-410##05_week-4##02_week-4-lessons##02_lesson-4-2-statistical-language-model_4.2_Statistical_Language_Models.txt##slide4,0.011884359,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide4,0.009938242,language-processing##04_sequence-to-sequence-tasks##01_statistical-machine-translation##01_introduction-to-machine-translation_w4_l1_e1_new.txt##slide1,0.009824161,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide4,0.009806133,cs-410##05_week-4##02_week-4-lessons##01_lesson-4-1-probabilistic-retrieval-model-basic-idea_4.1_Probabilistic_Retrieval_Model__Basic_Idea.txt##slide2,0.00975586,cs-410##02_week-1##02_week-1-lessons##03_lesson-1-3-text-retrieval-problem_1.3_TR-Text_Retrieval_Problem.txt##slide9,0.009663086
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide8,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide0,0.018830284,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide0,0.018830284,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide0,0.018830284,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide0,0.018830284,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide0,0.018830284,cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide7,0.014095935,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide2,0.01175325,cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide9,0.007639421,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide13,0.007192327,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##02_perplexity-is-our-model-surprised-with-a-real-text_w2_l1_e2_new.txt##slide16,0.006643631
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide7,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide12,0.018730664,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide6,0.014091833,cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide5,0.012031035,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide11,0.011953268,language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##02_get-to-the-point-summarization-with-pointer-generator-networks_w4_l3_e2.txt##slide17,0.01187239,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide2,0.011815704,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide4,0.011062406,language-processing##03_vector-space-models-of-semantics##02_topic-models##03_the-zoo-of-topic-models_w3_l3_e3_new.txt##slide2,0.010895308,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide10,0.010238716,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.009897867
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide12,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.018067566,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.015262533,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide10,0.011403934,cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide4,0.010197647,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide11,0.010082462,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide7,0.009404032,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide4,0.009139251,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide7,0.008735574,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide13,0.008720933,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide19,0.008436513
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide6,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##03_latent-dirichlet-allocation_w3b3.txt##slide4,0.017931492,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##10_mcmc-for-lda_w4c1_alex.txt##slide2,0.017587744,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##02_latent-dirichlet-allocation##01_topic-modeling_w3b1.txt##slide5,0.01732372,cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide2,0.01514798,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.015119848,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide4,0.013834899,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide3,0.0137599,cs-410##04_week-3##02_week-3-lessons##01_lesson-3-1-evaluation-of-tr-systems_3.1_Evaluation_of_TR_Systems.txt##slide1,0.013548965,cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide6,0.012925316,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide1,0.012820846
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##01_think-bayesian-statistics-review_w1a1.txt##slide1,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide9,0.01767327,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##04_word-analogies-without-magic-king-man-woman-queen_w3_l1_e4.txt##slide0,0.016335795,cs-410##03_week-2##02_week-2-lessons##05_lesson-2-5-system-implementation-inverted-index-construction_2.5_System_Implementation_-_Inverted_Index_Construction.txt##slide2,0.01241858,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##02_feature-extraction-from-text_w1_l2.txt##slide9,0.011824819,cs-410##03_week-2##02_week-2-lessons##01_lesson-2-1-vector-space-model-improved-instantiation_2.1_TR-Vector_Space_Model_Improve_Instantiation.txt##slide8,0.010259699,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide2,0.008624667,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##03_linear-models-for-sentiment-analysis_w1_l3.txt##slide4,0.008054635,cs-410##03_week-2##02_week-2-lessons##02_lesson-2-2-tf-transformation_2.2_TR-TF_Transformation.txt##slide4,0.007987945,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide30,0.00779245,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##04_3-3-initialization-of-k-means-clustering_3.3._Initialization_of_K-Means_Clustering.txt##slide2,0.006998814
language-processing##04_sequence-to-sequence-tasks##03_summarization-and-simplification-tasks##01_sequence-to-sequence-learning-one-size-fits-all_w4_l3_e1.txt##slide9,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide1,0.017361571,cs-410##13_week-12##02_week-12-lessons##08_12-8-summary-for-exam-2_TM-45-summary.txt##slide2,0.017220883,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide2,0.016758356,cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide4,0.016466501,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##07_optional-linguistic-knowledge-in-nlp_w1_l1_e3.txt##slide2,0.016462188,cs-410##13_week-12##02_week-12-lessons##03_12-3-text-based-prediction_TM-40-prediction.txt##slide5,0.016319012,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide1,0.016224879,cs-410##09_week-8##02_week-8-lessons##10_8-10-probabilistic-topic-models-mining-one-topic_TM-16-one-topic.txt##slide1,0.01599566,cs-410##13_week-12##02_week-12-lessons##04_12-4-contextual-text-mining-motivation_TM-41-contextual.txt##slide1,0.015887612,cs-410##11_week-10##02_week-10-lessons##07_10-7-text-categorization-motivation_TM-28-text-cat.txt##slide1,0.01554536
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide13,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.01715016,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide3,0.015524838,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide10,0.013069433,cs-410##13_week-12##02_week-12-lessons##07_12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervision_TM-44-causal.txt##slide2,0.012014822,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide7,0.011718047,language-processing##05_dialog-systems##01_natural-language-understanding-nlu##01_task-oriented-dialog-systems_w5_l1.txt##slide13,0.011442993,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##01_text-preprocessing_w1_l1.txt##slide6,0.011307223,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide6,0.011169037,cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide3,0.010939219,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.010933691
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide5,language-processing##05_dialog-systems##02_dialog-manager-dm##03_final-remarks_w5_l7.txt##slide7,0.016788961,cs-410##13_week-12##02_week-12-lessons##02_12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2_TM-39-sentiment-lara-part2.txt##slide6,0.016734589,language-processing##05_dialog-systems##02_dialog-manager-dm##02_policy-optimisation-in-dm_w5_l6.txt##slide4,0.016708093,language-processing##01_intro-and-text-classification##02_how-to-from-plain-texts-to-their-classification##04_hashing-trick-in-spam-filtering_w1_l4.txt##slide7,0.015189716,cs-410##07_week-6##02_week-6-lessons##04_lesson-6-4-future-of-web-search_6.4_Future_Web_Search.txt##slide2,0.01460946,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.013752304,cs-410##12_week-11##02_week-11-lessons##05_11-5-opinion-mining-and-sentiment-analysis-motivation_TM-35-sentiment.txt##slide6,0.013660208,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##04_welcome-video_w1_l1_e1-1.txt##slide4,0.013016254,language-processing##02_language-modeling-and-sequence-tagging##01_language-modeling-its-all-about-counting##01_count-n-gram-language-models_w2_l1_e1_new.txt##slide8,0.012699243,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide13,0.012571156
cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##03_5-2-dbscan-a-density-based-clustering-algorithm_5.2._DBSCAN_A_Density-Based_Clustering_Algorithm.txt##slide4,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide3,0.016296773,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide10,0.013259382,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##04_5-3-optics-ordering-points-to-identify-clustering-structure_5.3._OPTICS_Ordering_Points_To_Identify_Clus.txt##slide2,0.012638929,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##04_4-8-probabilistic-hierarchical-clustering_4.8._Probabilistic_Hierarchical_Clustering.txt##slide5,0.012405779,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##02_sampling-from-1-d-distributions_w4a2_alex.txt##slide16,0.008779738,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide4,0.00743399,cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide6,0.006889429,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##05_linear-regression_w1a5.txt##slide11,0.006650497,cluster-analysis##04_week-3##01_lesson-4-hierarchical-clustering-methods-continued##03_4-7-chameleon-graph-partitioning-on-the-knn-graph-of-the-data_4.7._CHAMELEON_Graph_Partitioning_on_the_KNN_Gra.txt##slide1,0.006540499,cluster-analysis##04_week-3##02_lesson-5-density-based-and-grid-based-clustering-methods##02_5-1-density-based-and-grid-based-clustering-methods_5.1._Density-Based_and_Grid-Based_Clustering_Methods.txt##slide1,0.006508212
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01530816,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.014112733,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##07_reparameterization-trick_w5a6_alex.txt##slide0,0.013131485,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##06_log-derivative-trick_w5a5_alex.txt##slide0,0.013131485,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.012220805,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide13,0.011958424,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##05_gradient-of-decoder_w5a4_alex.txt##slide0,0.01194906,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##03_example-normal-precision_w1b3.txt##slide18,0.011919461,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide1,0.011492248,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide14,0.011132129
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide7,0.014949493,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.014940171,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide0,0.014618823,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide0,0.012755096,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide24,0.01255253,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.01155206,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide11,0.011471031,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.011113869,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.010438208,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010400669
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##04_m-step-details_w2b5_alex.txt##slide8,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##07_summary-of-expectation-maximization_w2b7_alex.txt##slide7,0.014949493,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.014940171,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide0,0.014618823,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##02_expectation-maximization-algorithm##03_e-step-details_w2b4_alex.txt##slide0,0.012755096,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##02_conjugate-priors##04_example-bernoulli_w1b4.txt##slide24,0.01255253,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.01155206,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide11,0.011471031,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,0.011113869,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##05_example-of-gibbs-sampling_w4b2.2_alex.txt##slide19,0.010438208,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010400669
bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.014494576,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide0,0.013558236,cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide0,0.013558236,cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide0,0.013558236,cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide0,0.013558236,cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide0,0.013558236,cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide0,0.013558236,cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide0,0.013558236,cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide0,0.013558236,cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide0,0.013558236
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##02_week-1##02_week-1-lessons##05_lesson-1-5-vector-space-model-basic-idea_1.5_TR-Vector_Space_Model_Basic_Idea.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##03_week-2##02_week-2-lessons##03_lesson-2-3-doc-length-normalization_2.3_TR-Doc_Length_Normalization.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##05_week-4##02_week-4-lessons##03_lesson-4-3-query-likelihood-retrieval-function_4.3_Query_Likelihood_Retrieval_Function.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##05_week-4##02_week-4-lessons##04_lesson-4-4-statistical-language-model-part-1_4.4_Smoothing_of_Language_Model.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##05_week-4##02_week-4-lessons##07_lesson-4-7-smoothing-methods-part-2_4.7_Smoothing_Methods__Jelinek-Mercer_and_Dirichlet_Prior.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##06_week-5##02_week-5-lessons##01_lesson-5-1-feedback-in-text-retrieval_5.1_Feedback_in_Text_Retrieval.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##06_week-5##02_week-5-lessons##06_lesson-5-6-link-analysis-part-1_5.6_Link_Analysis.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##07_week-6##02_week-6-lessons##01_lesson-6-1-learning-to-rank-part-1-optional_6.1_Learning_to_Rank.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##07_week-6##02_week-6-lessons##05_lesson-6-5-recommender-systems-content-based-filtering-part-1_6.5_Recommender_Systems__Content-Based_Filtering.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##08_week-7##02_week-7-lessons##01_7-1-overview-text-mining-and-analytics-part-1_TM-3-overview.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##08_week-7##02_week-7-lessons##03_7-3-natural-language-content-analysis-part-1_TM-4-nlp.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##08_week-7##02_week-7-lessons##04_7-4-natural-language-content-analysis-part-2_TM-4-nlp_1.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##08_week-7##02_week-7-lessons##05_7-5-text-representation-part-1_TM-5-textrep.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##08_week-7##02_week-7-lessons##06_7-6-text-representation-part-2_TM-5-textrep_1.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##08_week-7##02_week-7-lessons##07_7-7-word-association-mining-and-analysis_TM-6-word-relation-corrected.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##08_week-7##02_week-7-lessons##08_7-8-paradigmatic-relation-discovery-part-1_TM-78-para-relation.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##08_week-7##02_week-7-lessons##09_7-9-paradigmatic-relation-discovery-part-2_TM-78-para-relation_1.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##09_week-8##02_week-8-lessons##01_8-1-syntagmatic-relation-discovery-entropy_TM-9-syn-relation.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##09_week-8##02_week-8-lessons##03_8-3-syntagmatic-relation-discovery-mutual-information-part-1_TM-11-mutual-info-corrected.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##09_week-8##02_week-8-lessons##04_8-4-syntagmatic-relation-discovery-mutual-information-part-2_TM-11-mutual-info-corrected_1.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##09_week-8##02_week-8-lessons##06_8-6-topic-mining-and-analysis-term-as-topic_TM-13-term-as-topic.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##09_week-8##02_week-8-lessons##07_8-7-topic-mining-and-analysis-probabilistic-topic-models_TM-14-topic-model.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##10_week-9##02_week-9-lessons##01_9-1-probabilistic-topic-models-mixture-of-unigram-language-models_TM-17-mixture-model.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##10_week-9##02_week-9-lessons##02_9-2-probabilistic-topic-models-mixture-model-estimation-part-1_TM-18-mixture-model-est.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##10_week-9##02_week-9-lessons##03_9-3-probabilistic-topic-models-mixture-model-estimation-part-2_TM-18-mixture-model-est_1.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##10_week-9##02_week-9-lessons##07_9-7-probabilistic-latent-semantic-analysis-plsa-part-1_TM-20-plsa.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##10_week-9##02_week-9-lessons##09_9-9-latent-dirichlet-allocation-lda-part-1_TM-21-lda.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##10_week-9##02_week-9-lessons##10_9-10-latent-dirichlet-allocation-lda-part-2_TM-21-lda_1.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##11_week-10##02_week-10-lessons##04_10-4-text-clustering-generative-probabilistic-models-part-3-optional_TM-25-clustering-gen-model-part3.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##11_week-10##02_week-10-lessons##05_10-5-text-clustering-similarity-based-approaches_TM-26-clustering-similarity.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##11_week-10##02_week-10-lessons##09_10-9-text-categorization-generative-probabilistic-models_TM-30-text-cat-model.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##12_week-11##02_week-11-lessons##02_11-2-text-categorization-discriminative-classifier-part-2-optional_TM-32-text-cat-discrim-part2.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##12_week-11##02_week-11-lessons##04_11-4-text-categorization-evaluation-part-2_TM-34-text-cat-eval-part2.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
cs-410##13_week-12##02_week-12-lessons##05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt##slide0,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.014028415,bayesian-methods-in-machine-learning##03_variational-inference-latent-dirichlet-allocation##01_variational-inference##04_variational-em-review_w3a4.txt##slide5,0.011426284,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010862184,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##06_mle-estimation-of-gaussian-mean_MLE_for_Gaussian.txt##slide0,0.010836899,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##02_gaussian-processes_w6a2.txt##slide17,0.010400156,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##06_bayesian-optimization_w6a5.txt##slide9,0.010362011,bayesian-methods-in-machine-learning##05_variational-autoencoder##01_variational-autoencoders##02_modeling-a-distribution-of-images_w5a1.5_alex.txt##slide0,0.01017279,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide8,0.009482427,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.009354901,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.009354901
bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##04_probabilistic-pca_w2c4_alex.txt##slide0,cs-410##10_week-9##02_week-9-lessons##05_9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2_TM-19-cond-entropy_1.txt##slide0,0.013638535,cs-410##10_week-9##02_week-9-lessons##06_9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3_TM-19-cond-entropy_2.txt##slide0,0.013638535,cs-410##10_week-9##02_week-9-lessons##04_9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1_TM-19-cond-entropy.txt##slide0,0.013638535,cs-410##09_week-8##02_week-8-lessons##08_8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1_TM-15-language-model.txt##slide0,0.013638535,cs-410##09_week-8##02_week-8-lessons##09_8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2_TM-15-language-model_1.txt##slide0,0.013638535,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide0,0.011403194,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide0,0.011067888,bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##08_example-of-metropolis-hastings_w4b4_alex.txt##slide0,0.010632902,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##04_example-thief-alarm_w1a4.txt##slide14,0.009651822,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide10,0.0095719
language-processing##02_language-modeling-and-sequence-tagging##03_deep-learning-for-the-same-tasks##02_whether-you-need-to-predict-a-next-word-or-a-label-lstm-is-here-to-help_w2_l3_e2.txt##slide0,cs-410##11_week-10##02_week-10-lessons##08_10-8-text-categorization-methods_TM-29-text-cat-methods.txt##slide5,0.013498204,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##04_how-to-implement-a-conversational-chat-bot_w4_l2_e4.txt##slide11,0.012235247,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##05_why-words-from-character-to-sentence-embeddings_w3_l1_e5.txt##slide4,0.011427347,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##06_brief-overview-of-the-next-weeks_w1_l1_e2.txt##slide4,0.00744287,bayesian-methods-in-machine-learning##06_gaussian-processes-bayesian-optimization##01_gaussian-processes-and-bayesian-optimization##05_nuances-of-gp_w6a4.txt##slide14,0.006947553,bayesian-methods-in-machine-learning##01_introduction-to-bayesian-methods-conjugate-priors##01_introduction-to-bayesian-methods##02_bayesian-approach-to-statistics_w1a2.txt##slide12,0.00659448,cs-410##12_week-11##02_week-11-lessons##01_11-1-text-categorization-discriminative-classifier-part-1_TM-31-text-cat-discrim-part1.txt##slide5,0.006161142,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##03_applications-and-examples##05_em-for-probabilistic-pca_w2c5_alex.txt##slide0,0.005765737,language-processing##04_sequence-to-sequence-tasks##02_encoder-decoder-attention-arhitecture##02_attention-mechanism_w4_l2_e2.txt##slide5,0.005547782,cluster-analysis##02_module-1##01_lesson-1-.txt##slide1,0.005446614
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide7,bayesian-methods-in-machine-learning##02_expectation-maximization-algorithm##01_latent-variable-models##01_latent-variable-models_w2a1_alex.txt##slide9,0.012940027,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##02_neural-networks-for-characters_w1_l6.txt##slide10,0.011275005,language-processing##01_intro-and-text-classification##03_simple-deep-learning-for-text-classification##01_neural-networks-for-words_w1_l5.txt##slide15,0.008727665,cs-410##09_week-8##02_week-8-lessons##02_8-2-syntagmatic-relation-discovery-conditional-entropy_TM-10-cond-entropy.txt##slide4,0.008669476,cs-410##04_week-3##02_week-3-lessons##03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1_3.3_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_1.txt##slide0,0.008378089,cs-410##08_week-7##02_week-7-lessons##02_7-2-overview-text-mining-and-analytics-part-2_TM-3-overview_1.txt##slide0,0.007797938,cs-410##04_week-3##02_week-3-lessons##05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements_3.5_Evaluation_of_TR_Systems_-_Multi-Level_Judgements.txt##slide0,0.007637595,cs-410##10_week-9##02_week-9-lessons##08_9-8-probabilistic-latent-semantic-analysis-plsa-part-2_TM-20-plsa_1.txt##slide0,0.007396305,cs-410##05_week-4##02_week-4-lessons##05_lesson-4-5-statistical-language-model-part-2_4.5_Smoothing_of_Language_Model.txt##slide0,0.007396305,cs-410##04_week-3##02_week-3-lessons##04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2_3.4_Evaluation_of_TR_Systems_-_Evaluating_Ranked_Lists_Part_2.txt##slide0,0.007396305
cs-410##01_orientation##01_orientation-information##01_course-introduction-video_410DSO-intro.txt##slide10,cluster-analysis##03_week-2##01_lesson-3-partitioning-based-clustering-methods##03_3-2-k-means-clustering-method_3.2._K-Means_Clustering_Method.txt##slide3,0.012074293,cs-410##07_week-6##02_week-6-lessons##10_lesson-6-10-summary-for-exam-1_6.10_Course_Summary.txt##slide4,0.010912246,cs-410##03_week-2##02_week-2-lessons##06_lesson-2-6-system-implementation-fast-search_2.6_System_Implementation_-_Fast_Search.txt##slide6,0.009252233,language-processing##05_dialog-systems##02_dialog-manager-dm##01_state-tracking-in-dm_w5_l5.txt##slide11,0.009180021,language-processing##02_language-modeling-and-sequence-tagging##02_sequence-tagging-with-probabilistic-models##03_memms-crfs-and-other-sequential-models-for-named-entity-recognition_w2_l2_e3_new.txt##slide6,0.008420401,cluster-analysis##01_course-orientation##01_about-the-course##01_course-introduction_0._Course_Introduction.txt##slide6,0.008349552,cs-410##04_week-3##02_week-3-lessons##06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt##slide1,0.007879437,language-processing##01_intro-and-text-classification##01_introduction-to-nlp-and-our-course##05_main-approaches-in-nlp_w1_l1_e1-2.txt##slide7,0.007413615,language-processing##03_vector-space-models-of-semantics##01_word-and-sentence-embeddings##03_word2vec-and-doc2vec-and-how-to-evaluate-them_w3_l1_e3_new.txt##slide7,0.007258802,cs-410##07_week-6##02_week-6-lessons##07_lesson-6-7-recommender-systems-collaborative-filtering-part-1_6.7_Recommender_Systems__Collaborative_Filtering.txt##slide10,0.007055648
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide1,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide2,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide3,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide4,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide5,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide6,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide7,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide8,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide9,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide10,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide11,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide15,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide16,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide17,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide18,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide19,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide20,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide21,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide22,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide23,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide24,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide25,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide26,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide29,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide30,,,,,,,,,,,,,,,,,,,,
bayesian-methods-in-machine-learning##04_markov-chain-monte-carlo##01_mcmc##03_markov-chains_w4b1_alex.txt##slide31,,,,,,,,,,,,,,,,,,,,